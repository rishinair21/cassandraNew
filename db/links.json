{
  "page": 2,
  "limit": 200,
  "pages": 3,
  "total": 552,
  "_links": {
    "self": {
      "href": "http://admin.stage.leaves.anant.us/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=2&perPage=200"
    },
    "first": {
      "href": "http://admin.stage.leaves.anant.us/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=1&perPage=200"
    },
    "last": {
      "href": "http://admin.stage.leaves.anant.us/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=3&perPage=200"
    },
    "next": {
      "href": "http://admin.stage.leaves.anant.us/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=3&perPage=200"
    },
    "previous": {
      "href": "http://admin.stage.leaves.anant.us/api/entries?sort=created&order=desc&tags=cassandra&since=0&page=1&perPage=200"
    }
  },
  "_embedded": {
    "items": [
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          }
        ],
        "is_public": false,
        "id": 12169,
        "uid": null,
        "title": "Using Cassandra Stress to model a time series workload - Instaclustr",
        "url": "https://www.instaclustr.com/using-cassandra-stress-to-model-a-time-series-workload/",
        "content": "<p><b>Motivation</b></p><p>When examining whether <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a> is a good fit for your needs, it is good practice to stress test Cassandra using a workload that looks similar to the expected workload in Production. </p><p>In the past we have examined the richness of features using YAML profiles in Cassandra’s stress tool – if you haven’t seen the previous post or are unfamiliar with YAML profiles in Cassandra stress, I’d recommend <a href=\"https://www.instaclustr.com/deep-diving-cassandra-stress-part-3-using-yaml-profiles/\">checking it out now</a>. </p><p>YAML profiles are all fine and dandy when it comes to mixed or general workloads using SizeTieredCompactionStrategy (STCS) or LeveledCompactionStrategy (LCS), but sometimes we may want to model a time series workload using TimeWindowCompactionStrategy (TWCS). How would we do that with the current options available to us in stress? Ideally, we would be able to do such a thing without having to schedule cassandra-stress instances every X minutes. </p><p><b>Native functions</b></p><p>As it turns out, Cassandra has a native function <b>now()</b> that returns the current time as a <b>timeuuid</b>, which is a unique representation of time. Cassandra also ships with the function <b>toTimestamp()</b> that accepts a <b>timeuuid</b>. Putting the two together, we are able to obtain the following result:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm.png\"><img class=\"aligncenter size-full wp-image-9517\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm.png\" alt=\"\" width=\"670\" height=\"280\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm.png 670w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm-300x125.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm-640x267.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm-115x48.png 115w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm-258x108.png 258w\" /></a></p><p>So we can use that to our advantage in a YAML profile:</p><div id=\"crayon-5b9a77320c0b8296795884\" class=\"crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p><p>36</p><p>37</p><p>38</p><p>39</p><p>40</p><p>41</p><p>42</p><p>43</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p>table_definition: |</p><p>CREATE TABLE twcstest (</p><p>id text,</p><p>time timestamp,</p><p>metric int,</p><p>value blob,</p><p>PRIMARY KEY((id), time)</p><p>) WITH CLUSTERING ORDER BY (time DESC)</p><p>AND compaction = { 'class':'TimeWindowCompactionStrategy', 'compaction_window_unit':'MINUTES', 'compaction_window_size':'20' }</p><p>AND comment='A table to see what happens with TWCS &amp; Stress'</p><p>columnspec:</p><p>- name: id</p><p>  size: fixed(64)</p><p>  population: uniform(1..1500M)</p><p>- name: time</p><p>  cluster: fixed(288)</p><p>- name: value</p><p>  size: fixed(50)</p><p>queries:</p><p>putindata:</p><p>  cql: insert into twcstest (id, time, metric, value) VALUES (?, toTimestamp(now()), ?, ?)</p></div></td>\n</tr></table></div></div><p>Based on that YAML above, we can now insert time series data as part of our stress. Additionally, please be aware that the <b>compaction_window_unit</b> property has been deliberately kept much smaller than is typical of a normal production compaction strategy!</p><p>The only snag to be aware of is that stress will insert timestamps rapidly, so you may want to tweak the values a little to generate suitably sized partitions with respect to your production workload. </p><p><b>That’s great, now how do I select data?</b></p><p>Well, intuitively we would just make use of the same helpful native functions that got us out from the tight spot before. So we may try this:</p><p>We appear to be a little stuck because selects may not be as straightforward as we had expected.</p><ol><li>We could try qualifying with just <b>&lt;=</b>, but then that would be a whole lot of data we select (You aren’t going to do this in Production, are you?), unless <b>id</b> is bucketed…but it isn’t in our situation. </li>\n<li>We could try qualifying with just <b>&gt;=</b>, but then nothing will be returned (You aren’t testing a case like this either, surely).</li>\n</ol><p>Unfortunately for us, it doesn’t look like Cassandra has anything available to help us out here natively. But it certainly has something we can leverage. </p><p><b>UDFs for the win</b></p><p>User defined functions (UDFs) have been added to Cassandra since <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-7395\">2.2</a>. If you aren’t familiar with them, there are examples of them available in a <a href=\"https://www.instaclustr.com/user-defined-functions-and-aggregates/\">previous blog post</a> and the <a href=\"http://cassandra.apache.org/doc/latest/cql/functions.html\">official cassandra documentation</a>. Since Cassandra doesn’t have any other native functions to help us, we can just write our own UDF, as it should be. </p><p>Typically we may expect to want to select a slice up to a certain number of minutes ago. So we want to write a UDF to allow us to do that.</p><p>This UDF is quite self explanatory so I won’t go into too much detail. Needless to say, it returns a <b>bigint</b> of <b>arg </b>minutes ago. </p><p>Here is a test to illustrate just to be safe:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm.png\"><img class=\"aligncenter size-full wp-image-9516\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm.png\" alt=\"\" width=\"987\" height=\"950\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm.png 987w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm-300x289.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm-768x739.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm-935x900.png 935w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm-640x616.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm-50x48.png 50w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-1.12.56-pm-112x108.png 112w\" /></a></p><p>Here is our new and improved YAML profile:</p><p>Now, when we execute cassandra-stress with <b>simple1</b>, we can expect just data within a certain time frame instead of selecting the whole partition. We can also keep varying the query to select older data if we like, for example, <b>time &gt;= minutesAgo(600) and time &lt;= minutesAgo(590) </b>for data up to 10 hours ago.</p><p><b>A variation with bucketing</b></p><p>We can also create UDFs that model bucketing behaviour. For example, suppose now we have a schema that has data bucketed, like this:</p><p>And we want to be able to insert data in 5 minute buckets. We can create UDFs like so:</p><p>The UDF <b>bucket</b> is quite self explanatory as well – it just returns the nearest 5 minute bucket smaller than <b>arg</b>. This assumes UTC time and 5 minute buckets, but the code can easily be tailored to be more general. </p><p>However, our UDF doesn’t understand <b>timeuuid</b>. Which is why we need another helper function, which is the function <b>nowInMilliSec</b>(). </p><p>The final UDF generates a random bucket based on a lower and upper bound time. The expected input bounds should be in epoch milliseconds. This will help in selecting old/random data bucketed to within 5 minutes in a range. </p><p>And now here is our new and modified YAML profile to accommodate our desires of having stress follow a bucketed workload:</p><p>1524117600000 happens to be Thursday, April 19, 2018 5:20:00 AM in GMT time while 1524129600000 happens to be Thursday, April 19, 2018 9:20:00 AM. It can be tailored to suit needs. It’s kind of ugly, but it will do the job. </p><p>And there we go: Tap into UDFs to be able to model a TWCS workload with Cassandra stress.</p><p>There’s always an option of writing your own client and using that to perform stress instead, with the obvious benefit that there’s no need to write UDFs and you have control over everything. The downside is that you would have to write code that includes rate limiting and reporting of metrics whereas cassandra stress is the stressing tool that comes with Cassandra out of the box and has very rich statistics, down to latency for each query. </p>",
        "created_at": "2018-09-13T15:11:07+0000",
        "updated_at": "2018-09-13T15:11:12+0000",
        "published_at": "2018-05-09T09:34:14+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2018/05/Screen-Shot-2018-04-23-at-12.27.59-pm.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12169"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 962,
            "label": "lambda",
            "slug": "lambda"
          }
        ],
        "is_public": false,
        "id": 12168,
        "uid": null,
        "title": "AWS Lambda with Managed Cassandra - Part 1: Let's Build a POC - Instaclustr",
        "url": "https://www.instaclustr.com/aws-lambda-managed-cassandra-part-1/",
        "content": "<p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1.png\"><img class=\"aligncenter size-full wp-image-11121\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1.png\" alt=\"\" width=\"925\" height=\"471\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1.png 925w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1-300x153.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1-768x391.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1-640x326.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1-94x48.png 94w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1-212x108.png 212w\" /></a></p><p>Serverless architecture is an attractive solution for both small businesses and large enterprises. The former gains the ability to go to market quickly while the latter are able to reduce IT costs by abstracting away complexity. One of the key players in serverless architecture is AWS with its Lambda offering. Lambda is a simple way to execute small portions of code on demand and without the need to provision any servers. Alongside the growth in Lambda’s popularity has come a greater interest in combining it with our Cassandra managed service. As such, we thought it would be a good time to investigate the pros and cons of using Lambda with Cassandra and to share some tips. </p><p>This will be a three-part series of blog posts. The first post will focus on developing a POC at next to no cost and will look like a tutorial. The focus here is mostly functional. The second post will focus on performance. The last will cover security and cost savings.</p><p>In this POC we are going to build a minimalistic REST API with Cassandra as our backend storage. Here is what we will use:</p><ul><li><a href=\"https://console.instaclustr.com/user/signup\">Instaclustr managed service free trial</a> (14 day free trial on t2.small nodes)</li> <li><a href=\"https://aws.amazon.com/lambda/pricing/\">AWS Lambda free tier</a> (1 million requests per month, 400,000 GB-seconds)</li> <li><a href=\"https://aws.amazon.com/api-gateway/pricing/\">AWS API Gateway free tier</a> (1 million API calls per month for new customers)</li> </ul><p>Running this tutorial should incur next to no cost. Obviously, you are responsible for maintaining your usage within the free tier AWS allowance, and <a href=\"https://console.instaclustr.com/user/signup\">Instaclustr free trial</a>. Running this tutorial assumes you have some general AWS knowledge, though you don’t need to have experience with AWS Lambda or AWS API Gateway.</p><p>The architecture is quite simple:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-2-1.png\"><img class=\"aligncenter size-full wp-image-11120\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-2-1.png\" alt=\"\" width=\"917\" height=\"151\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-2-1.png 917w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-2-1-300x49.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-2-1-768x126.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-2-1-640x105.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-2-1-291x48.png 291w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-2-1-600x99.png 600w\" /></a></p><p>The REST API will be minimalistic: We are going to create a service to create and retrieve orders. Here is an example using Swagger:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-3-1.png\"><img class=\"aligncenter size-full wp-image-11119\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-3-1.png\" alt=\"\" width=\"1299\" height=\"564\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1.png 1299w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-300x130.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-768x333.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-1024x445.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-1200x521.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-966x419.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-640x278.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-111x48.png 111w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-3-1-249x108.png 249w\" /></a></p><h2>Step1: Create a Cassandra Cluster on AWS.</h2><p>This is extremely easy using Instaclustr’s managed service. You can use the <a href=\"https://console.instaclustr.com/user/signup\">14-day free trial</a> to create a 3 node t2.small cluster. This type of cluster is perfect for basic functional testing. Let’s break down this process:</p><h3>Step 1.a. Create an <a href=\"https://console.instaclustr.com/user/signup\">Instaclustr Account</a>:</h3><p>Once you create your account, you will be able to run a 14 day trial cluster for free.</p><h3>Step 1.b Create a Cluster, with the Following Properties:</h3><h3><b>Name</b>: aws-lambda-small</h3><h3><b>Applications</b>: Apache Cassandra 3.11.2 (latest as of today)</h3><ul><li><b>No add-ons</b></li> <li><b>Infrastructure provider</b>: AWS</li> <li><b>Region</b>: Choose something close to you. I’ll be using US West (Oregon)</li> <li><b>Custom Name</b>: Let’s leave it to the default, i.e. AWS_VPC_US_WEST_2 for me as I am running in Oregon. If you are running in another region, take note as you will need to use the data center name later in this tutorial</li> <li><b>Data Centre Network</b>: Let’s leave it to the default, i.e. 10.224.0.0/16</li> <li><b>Node Size</b>: You will need to choose the Starter node (t2.small)</li> <li><b>EBS Encryption</b>: not required</li> <li><b>Replication Factor</b>: Let’s choose the most common replication factor when using Cassandra: 3</li> <li><b>Nodes</b>: 3</li> <li><b>Network</b>: We are going to tick the box to use private IP for node broadcast. The client will need to connect to Cassandra using the private IP. <ul><li><b>Security</b>: You can let Instaclustr add your local IP address to the firewall, thought that’s not required.</li> <li><b>Password auth</b>: For simplicity, we are going to disable password authentication and user authorization. As your client can only connect to the cluster via private IP, this is not a huge risk for a Proof of Concept. Don’t do that in production.</li> <li><b>Client – Node encryption</b> this is not supported for nodes that are this small</li> </ul></li> </ul><p>And that’s it! Click on <b>Create Cluster</b>, and you will have it running within 5 minutes.</p><h2>Step 2: Configure the AWS VPC Infrastructure.</h2><p>This configuration step is to enable communication between the AWS Lambda you will create, and the Cassandra cluster you just created. It may seem like a boring ‘plumbing’ step, but pay close attention as you will not get traffic to flow correctly if you miss a step.</p><h3>Step 2.a Create an AWS Account.</h3><p>If you don’t have one, create an AWS account. You will need to provide a credit card.</p><h3>Step 2.b Create a VPC in your AWS Account</h3><p>Use the same region you chose for your Cassandra cluster. Importantly, choose a CIDR block that does not overlap with the CIDR block you choose for your Cassandra cluster. For example, 10.225.0.0/16</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-4-1.png\"><img class=\"aligncenter size-full wp-image-11118\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-4-1.png\" alt=\"\" width=\"904\" height=\"558\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-4-1.png 904w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-4-1-300x185.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-4-1-768x474.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-4-1-640x395.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-4-1-78x48.png 78w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-4-1-175x108.png 175w\" /></a></p><h3>Step 2.c. Create a subnet per availability zone.</h3><p>You will want to do that to achieve High Availability with your AWS Lambda. In my case, I use the subnet CIDR block as follow: 10.225.0.0/24 for us-west-2a; 10.225.1.0/24 for us-west-2b; and 10.225.2.0/24 for us-west-2c.</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-5-1.png\"><img class=\"aligncenter size-full wp-image-11117\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-5-1.png\" alt=\"\" width=\"1166\" height=\"621\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1.png 1166w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-300x160.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-768x409.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-1024x545.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-966x514.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-640x341.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-90x48.png 90w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-5-1-203x108.png 203w\" /></a></p><h3>Step 2.d. Request VPC Peering between your Cassandra Cluster and your New VPC.</h3><p>The request needs to be initiated from the Instaclustr console. Look for the Settings section of your cluster at the bottom. You will need to provide your AWS account number, the id of your aws-lambda-small VPC, and it’s corresponding CIDR (in my case, 10.225.0.0/16). You should choose the option to have your VPC network added to the cluster firewall. Once you submit the VPC peering request, you need to accept it on your AWS account, and you need to update your VPC route table to route the traffic to Cassandra. In my case, the traffic to 10.224.0.0/16 needs to be routed to the new VPC peering connection.</p><p><a href=\"https://www.instaclustr.com/support/documentation/cluster-management/using-vpc-peering-aws/\">You can see the full details in this support article</a>. </p><h3>Step 2.e. Create a Security Group.</h3><p>You need to create a security group for you AWS Lambda as the Lambda containers will be executed in the VPC. At a minimum, a lot of outbound traffic to TCP port 9042 (Cassandra port) towards your Cassandra cluster, in my case 10.224.0.0/16</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-6-1.png\"><img class=\"aligncenter size-full wp-image-11116\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-6-1.png\" alt=\"\" width=\"1498\" height=\"713\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1.png 1498w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-300x143.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-768x366.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-1024x487.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-1200x571.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-966x460.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-640x305.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-101x48.png 101w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-6-1-227x108.png 227w\" /></a></p><h2><b>Step 3: Create your AWS Lambda.</b></h2><p>AWS Lambda are small portions of code that can be executed on demand on AWS, without the need for the user to provision an instance. Behind the scenes, Lambda are executed as containers. The very first time the Lambda is executed, AWS will instantiate the container and run the code. Once the Lambda application exits, AWS will freeze the container and keep it around ready to execute new Lambda requests. If it remains unused, the Lambda container is destroyed. </p><p>To achieve the best performance, code executed on Lambda can be split into initialization and execution. For working with Cassandra, we will create the cluster connection (which can take a few seconds), and create the prepared statement in the initialization phase, so that only the first execution will incur the initialization overhead, and subsequent execution will be much faster. For the purpose of this POC, I will create two Lambda that executes Python code. The first Lambda is responsible for creating the Cassandra data model schema and will be called only once. The second Lambda will receive the PUT and GET requests from the API gateway and will execute the write and read code.</p><h3>Step 3.a. Navigate to the AWS Lambda page, and Click to Create a Function</h3><p><b>Name</b>: cassandra-schema-init</p><p><b>Runtime</b>: Python 2.7</p><p><b>Role</b>: Create new role from template(s)</p><ul><li>Role name aws-lambda-small</li> <li>Policy templates: According to the documentation, you should be using “VPCAccess” that will allow the Lambda to create the NetworkInterface to run in the VPC. However, VPCAccess is not in the list of templates as of now. So just leave empty and we will fix that in a few steps.</li> </ul><h3>Step 3.b. Create and Upload the Python Code for cassandra-schema-init</h3><p>You will see that the AWS console lets you edit code directly. However, as we need to add some dependencies (<code>cassandra-driver</code>), we will need to edit the code from our computer and package the code with the dependencies. Change the “Code entry type” to “Upload a .ZIP file.” On your computer, create a directory, such as <code>aws-lambda-schema</code>, and start by downloading the dependencies. </p><p>If you are using Linux you can use the Python package manager ‘pip’ to install everything. To get the cassandra-driver, run this command: <code><code>pip install cassandra-driver -t</code> .</code> Then run this command to get the twisted library: <code><code>pip install twisted -t</code></code>. You may be wondering why we are downloading the twisted library. Twisted lets us reuse an existing Cassandra connection at each invocation of the AWS Lambda. This is the only way to achieve two digit millisecond level performance. Downloading the <code>cassandra-driver</code> and <code>twisted</code> will take a few minutes. Good time for a coffee break.</p><p>The Python code to create the schema is simple. Save the following into the file <code>cassandra_lambda.py</code>:</p><p>Your aws-lambda-schema directory should now contain cassandra_lambda.py, and all the dependencies downloaded by pip. The next step is to zip the contents of the directory and upload it to the AWS console. Importantly, <b>do not zip the directory.</b> <b>You need to zip the contents</b> of the directory in one file. AWS is sensitive about that.</p><h3>Step 3.c. Configure AWS Lambda Settings from the AWS Console</h3><p><b>Function code: </b>If you named your file cassandra_lambda.py, then make sure to update the Handler with <code>cassandra_lambda.lambda_handler</code></p><p><b>The environment variables</b>. In my case given the Cassandra IP addresses and the name of the data center:</p><ul><li>endpoint 10.224.17.181,10.224.97.78,10.224.188.162</li>\n<li>local_dc AWS_VPC_US_WEST_2</li>\n</ul><p>Those are used in the code.</p><p><b>Basic settings</b>: You will want to increase the timeout since initializing the Cassandra connection and updating the schema can take a few seconds. Ten seconds should be enough.</p><p><b>Network section</b>: you need to set your Lambda to execute in your aws-lambda-small VPC. Select the security group and three subnets you created earlier.</p><p><b>Execution Role</b>: You will need to create a new role using a template. The one that was created earlier did not automatically have permissions to create network interfaces. Now that you have chosen the VPC in the Network section, AWS will be able to automatically add the VPC Access policy to the role. Let’s call the role <code>aws-lambda-small-vpc</code>. No need to choose a template from the Policy.</p><p>click on <b>Save</b> at the top right.</p><h3>Step 3.d. Test the Lambda cassandra-schema-init</h3><p>Just click on “Test” at the top right. You will be prompted to configure a test event. This Lambda does not respond to events as it is used only for initializing the Cassandra schema. You can use the default <strong>Hello World</strong> event.</p><p>If everything works well, you should see some success logs on the AWS Lambda console and your schema will be created. Now let’s create the Lambda that will handle the POST and GET calls for your API. In this example, we will create a single Lambda to handle both POST and GET. In practice, you will need to consider the pros and cons of having multiple Lambdas for multiple methods.</p><h3>Step 3.e. Create a New Lambda: cassandra-api</h3><p><b>Name</b>: cassandra-api</p><p><b>Runtime</b>: Python 2.7</p><p><b>Role</b>: Choose an existing role</p><p><b>Existing role</b>: service-role/aws-lambda-small-vpc which is the one you created earlier</p><h3>Step 3.f. Configure the Lambda cassandra-api</h3><p><b>Function code</b>:</p><ul><li>Code entry type: Upload a ZIP file</li>\n<li><strong>Handler:</strong> <code>cassandra_lambda.lambda_handler</code> – That’s assuming you will name your python file cassandra_lambda.py</li>\n</ul><p><b>Environment variables:</b></p><ul><li>endpoint 10.224.18.118,10.224.75.58,10.224.133.133</li>\n<li>local_dc AWS_VPC_US_WEST_2</li>\n</ul><p><b>Basic settings</b>: set the timeout to 10 sec.</p><p><b>Network</b>: use VPC, and choose the existing subnets/securitygroups.</p><p>Click on <b>Save </b>at the top right of the screen.</p><h3>Step 3.g. Create and Upload the Code for cassandra-api</h3><p>Similar to what you did for <code>cassandra-schema-init</code>, you will need to create a directory, download the dependencies with pip, create and edit the file <code>cassandra_lambda.py</code>, zip the contents (<b>remember, don’t zip the parent directory</b>) and finally upload that to AWS. </p><p>Use the following code:</p><div id=\"crayon-5b9a772d2619a141924888\" class=\"crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-toolbar\" data-settings=\"mouseover overlay hide delay\"><div class=\"crayon-tools\">Python</div></div><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p><p>36</p><p>37</p><p>38</p><p>39</p><p>40</p><p>41</p><p>42</p><p>43</p><p>44</p><p>45</p><p>46</p><p>47</p><p>48</p><p>49</p><p>50</p><p>51</p><p>52</p><p>53</p><p>54</p><p>55</p><p>56</p><p>57</p><p>58</p><p>59</p><p>60</p><p>61</p><p>62</p><p>63</p><p>64</p><p>65</p><p>66</p><p>67</p><p>68</p><p>69</p><p>70</p><p>71</p><p>72</p><p>73</p><p>74</p><p>75</p><p>76</p><p>77</p><p>78</p><p>79</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p>import time</p><p>import uuid</p><p>import json</p><p>import os</p><p>import boto3</p><p>from base64 import b64decode</p><p>from cassandra.cluster import Cluster</p><p>from cassandra.auth import PlainTextAuthProvider</p><p>from cassandra.policies import DCAwareRoundRobinPolicy</p><p>from cassandra.io.twistedreactor import TwistedConnection</p><p>from cassandra import ConsistencyLevel</p><p># Keep track of container id, for perf testing.</p><p>container_id=uuid.uuid4()</p><p># Get Cassandra endpoint from aws Lambda env variables</p><p>ENDPOINT = os.environ['endpoint']</p><p>LOCAL_DC = os.environ['local_dc']</p><p># the cassandra session, and prepared statement will</p><p>cassandra_session = None</p><p>cassandra_insert = None</p><p>cassandra_lookup = None</p><p># the code to handle POST calls.</p><p># In practice, you will want to use your favorite API framework, i.e. Flask</p><p>def post(event):</p><p>   myjson = json.loads(event['body'])</p><p>   order_id=uuid.uuid4()</p><p>   name = myjson['name']</p><p>   address = myjson['address']</p><p>   phone = myjson['phone']</p><p>   item = myjson['item']</p><p>   cassandra_session.execute(cassandra_insert, [order_id, name, address, phone, item])</p><p>   return {</p><p>       'isBase64Encoded': False,</p><p>       'statusCode': 200,</p><p>       'body': json.dumps({\"order_id\": str(order_id)}),</p><p>       'headers': {}</p><p>   }</p><p># the code to handle GET calls</p><p># In practice, you will want to use your favorite API framework, i.e. Flask</p><p>def get(event):</p><p>   order_id = event['pathParameters']['id']</p><p>   rows = cassandra_session.execute(cassandra_lookup, [uuid.UUID(order_id)])</p><p>   if not rows:</p><p>       return {</p><p>           'isBase64Encoded': False,</p><p>           'statusCode': 404,</p><p>           'body': {},</p><p>           'headers': {}</p><p>       }</p><p>   return {</p><p>       'isBase64Encoded': False,</p><p>       'statusCode': 200,</p><p>       'body': json.dumps({\"order_id\": order_id,</p><p>                           \"name\": rows[0].name,</p><p>                           \"address\": rows[0].address,</p><p>                           \"phone\": rows[0].phone,</p><p>                           \"item\": rows[0].item}),</p><p>       'headers': {}</p><p>   }</p><p>method_map = {'GET': get, 'POST': post}</p><p>def Lambda_handler(event, context):</p><p>   global container_id, cassandra_session, cassandra_insert, cassandra_lookup</p><p>   print('Running container', container_id)</p><p>   if not cassandra_session:</p><p>       cluster = Cluster(ENDPOINT.split(\",\"),connection_class=TwistedConnection, load_balancing_policy=DCAwareRoundRobinPolicy(local_dc=LOCAL_DC))</p><p>       cassandra_session = cluster.connect()</p><p>       cassandra_session.default_consistency_level = ConsistencyLevel.QUORUM</p><p>       cassandra_insert = cassandra_session.prepare(\"INSERT INTO ks.tb (order_id, name, address, phone, item) VALUES (?, ?, ?, ?, ?)\")</p><p>       cassandra_lookup = cassandra_session.prepare(\"SELECT name, address, phone, item FROM ks.tb WHERE order_id = ?\")</p><p>   method = event['httpMethod']</p><p>   return method_map[method](event)</p></div></td>\n</tr></table></div></div><p>You might want to use the AWS command line interface (aws-cli) at this stage. Assuming you have configured your AWS profile(s), this could look like:</p><p>By the way, as you may have noticed, you will have to zip the python files and the dependencies quite often. To accelerate this, have a look at the -u option of the zip command (Linux / OSX): it will only update the archive with the changed file (which should only be your python file), making the process much faster.</p><h3>Step 3.h Test your cassandra-api</h3><p>If you look at the code of cassandra-api, you will see that the GET and PUT methods expect some json extracted from the body of an HTTP request. We are going to set up two tests: one for GET, and one for POST.</p><p>Click on <strong>Test</strong> (top right)</p><p>Configure your test event:</p><ul><li><strong>Event name:</strong> post</li>\n<li><strong>Content of the event:</strong> use the following.</li>\n</ul><div id=\"crayon-5b9a772d261a4749147695\" class=\"crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-toolbar\" data-settings=\"mouseover overlay hide delay\"><div class=\"crayon-tools\">JavaScript</div></div><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p><p>36</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p>{</p><p> \"resource\": \"/order\",</p><p> \"path\": \"/order\",</p><p> \"httpMethod\": \"POST\",</p><p> \"headers\": null,</p><p> \"queryStringParameters\": null,</p><p> \"pathParameters\": null,</p><p> \"stageVariables\": null,</p><p> \"requestContext\": {</p><p>   \"path\": \"/order\",</p><p>   \"accountId\": \"597284863061\",</p><p>   \"resourceId\": \"rmbyew\",</p><p>   \"stage\": \"test-invoke-stage\",</p><p>   \"requestId\": \"dca8be26-7816-11e8-a831-6dbe534d0ae4\",</p><p>   \"identity\": {</p><p>     \"cognitoIdentityPoolId\": null,</p><p>     \"cognitoIdentityId\": null,</p><p>     \"apiKey\": \"test-invoke-api-key\",</p><p>     \"cognitoAuthenticationType\": null,</p><p>     \"userArn\": \"arn:aws:iam::597284863061:user/christophe-workshop\",</p><p>     \"apiKeyId\": \"test-invoke-api-key-id\",</p><p>     \"userAgent\": \"aws-internal/3\",</p><p>     \"accountId\": \"597284863061\",</p><p>     \"caller\": \"AIDAJBAW4OXNUFHSWCLF4\",</p><p>     \"sourceIp\": \"test-invoke-source-ip\",</p><p>     \"accessKey\": \"ASIAIS5LPR47MN7RRAVA\",</p><p>     \"cognitoAuthenticationProvider\": null,</p><p>     \"user\": \"AIDAJBAW4OXNUFHSWCLF4\"</p><p>   },</p><p>   \"resourcePath\": \"/order\",</p><p>   \"httpMethod\": \"POST\",</p><p>   \"extendedRequestId\": \"JA-z9HWIjoEFqdA=\",</p><p>   \"apiId\": \"f6ugtbwyjg\"</p><p> },</p><p> \"body\": \"{\\n    \\\"name\\\" :\\\"joe\\\",\\n    \\\"address\\\" : \\\"Sydney\\\",\\n    \\\"phone\\\": \\\"0123456789\\\",\\n \\\"item\\\": \\\"pizza\\\"}\"</p><p>}</p></div></td>\n</tr></table></div></div><p>Now test your POST a few times. Keep track of one value for order_id uuid as you will need it in a moment. Did you notice the first invocation took longer (maybe a few seconds), while other invocations took a few milliseconds? That’s because at the first invocation, there is some overhead:</p><ul><li>AWS creates one container to run the Lambda</li>\n<li>AWS create a network interface in the VPC, and attaches it to the Lambda</li>\n<li>The Lambda initializes the Cassandra connection.</li>\n</ul><p>AWS will keep around your container for some indeterminate amount of time. Most user reports that the container will stay around for a few hours after the last invocation. That’s something to keep in mind. Importantly, in our test, we are doing serial invocations of our Lambda. If running in parallel, multiple containers will be created by AWS (Lambda auto scale), and the initialization cost will be paid a few times. We will see in our next blog how to optimize this.</p><p>Let’s try our GET method. For that, create a new Test (Configure test events). Let’s call it “get”, and let’s use the following event:</p><div id=\"crayon-5b9a772d261a8623796114\" class=\"crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-toolbar\" data-settings=\"mouseover overlay hide delay\"><div class=\"crayon-tools\">JavaScript</div></div><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p><p>36</p><p>37</p><p>38</p><p>39</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p>{</p><p> \"resource\": \"/order/{id+}\",</p><p> \"path\": \"/order/INSERT_YOUR_UUID\",</p><p> \"httpMethod\": \"GET\",</p><p> \"headers\": null,</p><p> \"queryStringParameters\": null,</p><p> \"pathParameters\": {</p><p>   \"id\": \"INSERT_YOUR_UUID\"</p><p> },</p><p> \"stageVariables\": null,</p><p> \"requestContext\": {</p><p>   \"path\": \"/order/{id+}\",</p><p>   \"accountId\": \"597284863061\",</p><p>   \"resourceId\": \"wm947q\",</p><p>   \"stage\": \"test-invoke-stage\",</p><p>   \"requestId\": \"f88fdaa2-781a-11e8-8075-79171804d71a\",</p><p>   \"identity\": {</p><p>     \"cognitoIdentityPoolId\": null,</p><p>     \"cognitoIdentityId\": null,</p><p>     \"apiKey\": \"test-invoke-api-key\",</p><p>     \"cognitoAuthenticationType\": null,</p><p>     \"userArn\": \"arn:aws:iam::597284863061:user/christophe-workshop\",</p><p>     \"apiKeyId\": \"test-invoke-api-key-id\",</p><p>     \"userAgent\": \"aws-internal/3\",</p><p>     \"accountId\": \"597284863061\",</p><p>     \"caller\": \"AIDAJBAW4OXNUFHSWCLF4\",</p><p>     \"sourceIp\": \"test-invoke-source-ip\",</p><p>     \"accessKey\": \"ASIAIS5LPR47MN7RRAVA\",</p><p>     \"cognitoAuthenticationProvider\": null,</p><p>     \"user\": \"AIDAJBAW4OXNUFHSWCLF4\"</p><p>   },</p><p>   \"resourcePath\": \"/order/{id+}\",</p><p>   \"httpMethod\": \"GET\",</p><p>   \"extendedRequestId\": \"JBDHtGUFjoEFfEg=\",</p><p>   \"apiId\": \"f6ugtbwyjg\"</p><p> },</p><p> \"body\": null,</p><p> \"isBase64Encoded\": false</p><p>}</p></div></td>\n</tr></table></div></div><p>You will need to update (in two places) the code above to replace INSERT_YOUR_UUID with a uuid of an order you posted earlier. Using this you can test your GET command.</p><p>You now have a one time use Lambda for creating the schema and you have Lambdas that handle POST and GET requests.  For now, you can only trigger a test manually. The next step is to hook this up with the API gateway so that requests can be triggered programmatically from the internet.</p><h2>Step 4: Create the AWS API gateway.</h2><h3>Step 4.a Create an empty API.</h3><p>Navigate to the AWS API Gateway console, and create a New API.</p><p><strong>Name:</strong> cassandra-api</p><p><strong>Endpoint Type:</strong> Regional</p><h3>Step 4.b. Create a new order resource.</h3><p>Click on <strong>Actions</strong> drop-down menu and create a new order Resource:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-7-1.png\"><img class=\"aligncenter wp-image-11115 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-7-1.png\" alt=\"\" width=\"1600\" height=\"462\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1.png 1600w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-300x87.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-768x222.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-1024x296.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-1200x347.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-966x279.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-640x185.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-166x48.png 166w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-7-1-374x108.png 374w\" /></a></p><h3>Step 4.c. Create a POST method</h3><p>Make sure the <strong>/order</strong> resource is selected, then click on the drop down menu <strong>Actions</strong> and create a new POST method, configured as follow:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-8-1.png\"><img class=\"aligncenter size-full wp-image-11114\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-8-1.png\" alt=\"\" width=\"1600\" height=\"618\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1.png 1600w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-300x116.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-768x297.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-1024x396.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-1200x464.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-966x373.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-640x247.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-124x48.png 124w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-8-1-280x108.png 280w\" /></a></p><p>Let’s test your POST method. Make sure to select the POST method, then click on TEST. All you need to do is to add a Request Body string, such as:</p><p>If everything works well, you should get an order_id back. Copy the order_id as you will use it soon.</p><h3>Step 4.d. Create a greedy resource</h3><p>Make sure <strong>/order</strong> resource is select, click on <strong>Actions</strong> and create a new resource configured as follow:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-9-1.png\"><img class=\"aligncenter size-full wp-image-11113\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-9-1.png\" alt=\"\" width=\"1590\" height=\"535\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1.png 1590w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-300x101.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-768x258.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-1024x345.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-1200x404.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-966x325.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-640x215.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-143x48.png 143w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-9-1-321x108.png 321w\" /></a></p><p>You will see that it automatically created the ANY method. You can delete it, and create a GET method:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-10-1.png\"><img class=\"aligncenter size-full wp-image-11112\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-10-1.png\" alt=\"\" width=\"1600\" height=\"552\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1.png 1600w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-300x104.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-768x265.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-1024x353.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-1200x414.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-966x333.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-640x221.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-139x48.png 139w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-10-1-313x108.png 313w\" /></a></p><p>Let’s test our GET method. Make sure to select the GET method, then click on TEST. This time you will provide the order_id uuid (from Step 4.c.) in the PATH {id} box. If everything works well, you should get back the order.</p><h3>Step 4.e. Deploy your API.</h3><p>Click on the <strong>Actions</strong> drop down menu, and deploy API. Give it a Deployment stage name, let’s use “test”</p><p>The first thing you will want to do is to throttle your API to a low number, i.e. 1 per seconds, just to make sure your API won’t be called a large number of times, as the cost will apply beyond the free tier.</p><h3>Step 4.f Create an API Key (optional)</h3><p>If you want to control the access to your API, you might consider creating an API Key as follow.</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-11-1.png\"><img class=\"aligncenter size-full wp-image-11111\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-11-1.png\" alt=\"\" width=\"1600\" height=\"670\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1.png 1600w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-300x126.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-768x322.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-1024x429.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-1200x503.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-966x405.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-640x268.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-115x48.png 115w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-11-1-258x108.png 258w\" /></a></p><p>Click on <strong>Show</strong> to revel the key, and keep it for later usage.</p><h3>Step 4.g Create a usage plan for your API key (optional)</h3><p>Usage plan lets you control how an API key can be used. Click on Usage Plans. You might need to click on <strong>Enable Usage Plans</strong> first. The AWS page is a little bit buggy here, and you might need to reload the page a few time with your browser.</p><p>Once you have access to the create button, create your Usage Plan, and associate it with the cassandra-api API, with the test Stage. Then associate the Usage Plan with the API key you created.</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-12-1.png\"><img class=\"aligncenter size-full wp-image-11110\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-12-1.png\" alt=\"\" width=\"1600\" height=\"866\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1.png 1600w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-300x162.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-768x416.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-1024x554.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-1200x650.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-966x523.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-640x346.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-89x48.png 89w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-12-1-200x108.png 200w\" /></a></p><h3>Step 4.h Add Authentication to your GET and POST Method (optional)</h3><p>You will need to navigate to the POST resource, and click on the <b>Method Request</b></p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-13-1.png\"><img class=\"aligncenter size-full wp-image-11109\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/08/awslambda-13-1.png\" alt=\"\" width=\"1208\" height=\"367\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1.png 1208w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-300x91.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-768x233.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-1024x311.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-1200x365.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-966x293.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-640x194.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-158x48.png 158w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/08/awslambda-13-1-355x108.png 355w\" /></a></p><p>Then, set the API Key Required to True. Repeat for the GET method. Don’t forget to redeploy your API (use the same stages: test).</p><h3>Step 4.i Test your API from your Computer.</h3><p>First, retrieve your API endpoint by navigating to the Stages section of your API and clicking on the POST method. This will provide you with the Invoke URL.</p><p>Second, make sure you have your API Keys.</p><p>You can now do a post from a curl / postman etc… Below is an example with curl, in which I am providing an AWS API KEY. If you didn’t configure the API Key, you don’t need to provide it.</p><p>Assuming the id of the new resource is: <code>fb72a94f-c0d9-4bd5-a355-fc8014d125fd</code>, you can retrieve your resource with the following curl command:</p><p>We built a simple scalable REST API using AWS API Gateway to receive API calls, AWS Lambda to execute code, and Cassandra as our backend storage. This POC can be built at next to no cost as <a href=\"https://console.instaclustr.com/user/signup\">Instaclustr provides 14 </a>day free trial on the small developer cluster, and using AWS gateway / AWS Lambda for this POC should remain within the free tier usage (you might still have some small cost, i.e. a few dollars, for data transfer, or for using cloudwatch). This POC demonstrates the simplicity of using a serverless approach where the code is executed by AWS Lambda without managing any ec2 instances. Furthermore the data is stored in a fully managed, scalable, highly available and low latency database –  Cassandra. </p><p>The next step would be to consider performance, which will be the focus of our next blog post.</p>",
        "created_at": "2018-09-13T15:10:51+0000",
        "updated_at": "2018-09-13T15:10:59+0000",
        "published_at": "2018-08-27T15:59:15+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 18,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2018/08/picture-lambda-blog-2-1.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12168"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          }
        ],
        "is_public": false,
        "id": 12167,
        "uid": null,
        "title": "Apache Cassandra LDAP Authentication - Instaclustr",
        "url": "https://www.instaclustr.com/apache-cassandra-ldap-authentication/",
        "content": "<p>We’ve seen an increasing need for LDAP integration into Apache Cassandra, and continually hearing of cases where people have written their own LDAP authenticators for Cassandra.<br /></p><p>However, if you search around you’ll have a hard time finding one of these implementations, and you’ll likely have to write one yourself, which is no easy feat.</p><p>So, to solve this issue we’ve created an <a href=\"https://github.com/instaclustr/cassandra-ldap\">open source LDAP authenticator</a> plug-in for Apache Cassandra that goes hand in hand with the existing CassandraAuthorizer implementation. At the moment it supports a basic usage of LDAP which should suffice for most cases, however improvements are welcome if you need to modify it to suit your needs and encouraged to submit pull requests for any enhancements.</p><p>This plug-in authenticator is freely available for anyone for use and is also be included in support scope for customers with Apache Cassandra Enterprise Support from Instaclustr.</p><p>The LDAPAuthenticator is implemented using JNDI, and authentication requests will be made by Cassandra to the LDAP server using the username and password provided by the client. At this time only plain text authentication is supported. </p><p>If you configure a service LDAP user in the ldap.properties file, on startup Cassandra will authenticate the service user and create a corresponding role in the system_auth.roles table. This service user will then be used for future authentication requests received from clients. Alternatively (not recommended), if you have anonymous access enabled for your LDAP server, the authenticator allows authentication without a service user configured. The service user will be configured as a superuser role in Cassandra, and you will need to log in as the service user to define permissions for other users once they have authenticated.</p><p>On successful authentication of a client, a corresponding role will be created in the <code>system_auth.roles</code> table. The password for the role <i>is not</i> stored in the roles table, and credentials will always be passed through directly to the configured LDAP server. The only credentials stored in Cassandra is the Distinguished Name of the user. However, if caching is enabled the password/hashed password will be stored in the cache, in memory only, on the nodes. Permissions-wise, this role will have no access to any keyspaces/tables, so GRANT’s will need to be issued before the user can perform any useful queries.</p><p><br />Once created, the role will never be deleted, and all authentication of the role will be handled through LDAP while the LDAPAuthenticator is in place. Removing or disabling the user in LDAP will disallow future connections as that user, but not clean up the user from <code>system_auth.roles</code>. This can be done manually if so desired and should be done if you wish to switch to a different authentication mechanism.<br />Regarding security, as the authenticator only supports plain text from clients you should ensure you have enabled and are using client encryption in Cassandra. On the LDAP side, you <i>must</i> use LDAPS otherwise credentials will be sent in the clear between Cassandra and the LDAP server. As all SSL configuration is performed through JNDI, simply specifying LDAPS as your protocol for the LDAP server (assuming it’s enabled on your server) will enable LDAPS.  </p><p>On 3.11 and later versions, a cache has been implemented to avoid thrashing your LDAP server. This cache will be populated with the username and either the provided password or a hash of the password based on the cache_hashed_password property in ldap.properties. Note that hashing the password will incur a performance hit as the hash needs to be calculated on each auth. The password/hash is only stored in memory on the Cassandra nodes, so if you don’t enable hashing ensure appropriate security controls are in place for your Cassandra nodes.</p><p>LDAP JNDI properties can be set via the <code>ldap.properties</code> file. Simply specify the desired property and it will be set as part of the servers context. For example, you can set the LDAP read timeout like so:</p><p><code>    com.sun.jndi.ldap.read.timeout: 2000<br /></code><br />These properties and their documentation can be found <a href=\"https://docs.oracle.com/javase/8/docs/technotes/guides/jndi/jndi-ldap.html\">here</a>.</p><p>To transition to the <code>LDAPAuthenticator</code> you can do so in a rolling fashion with no downtime as long as you are using <code>AllowAllAuthenticator</code> to start with, or you handle auth failures from both LDAP and your old password authenticator in your client and try the alternate auth on the next request. Pre-creating the LDAP users in <code>system.roles</code> is also possible however not recommended as you will need to store the LDAP user passwords in Cassandra for it to work. </p><p>Alternatively you can do the switch with downtime with no issues however this requires turning off all the nodes simultaneously. To ensure no errors on startup due to creation of service roles you should start one node first and wait until it’s running before starting the rest of the nodes.<br /></p><p>You can find the LDAP authenticator source code on GitHub <a href=\"https://github.com/instaclustr/cassandra-ldap\">here</a>, with instructions on setup and usage in the README. Currently the authenticator is supported for 2.2, 3.0, and 3.11 versions. Use the corresponding branch in the repo for your desired version.</p>",
        "created_at": "2018-09-13T15:10:18+0000",
        "updated_at": "2018-09-13T15:10:22+0000",
        "published_at": "2018-06-29T15:30:45+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12167"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1052,
            "label": "tools",
            "slug": "tools"
          }
        ],
        "is_public": false,
        "id": 12166,
        "uid": null,
        "title": "Instaclustr Open-sources Cassandra sstable Analysis Tools",
        "url": "https://www.instaclustr.com/instaclustr-open-sources-cassandra-sstable-analysis-tools/",
        "content": "<p>At Instaclustr we spend a lot of time managing Cassandra clusters – we have team of engineers that 24×7 do nothing but manage Cassandra clusters. Big clusters, tiny clusters, clusters with awesome data models and clusters with less awesome data models – we manage them all.</p><p>Over time, we’ve developed a lot of tricks and tools to help us in this job. We’re happy to announce that, as part of our commitment to the <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a> open source community, we’re making our most generally useful tools available for open use.</p><p>The tools (that we’ve imaginatively called ic-tools) supplement the information available from the nodetool utility that is part of core <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a>. Whereas nodetool tends to report based on summary statistics maintained as Cassandra services operate, ic-tools directly reads Cassandra’s data files when executed. This allows reporting of more detailed and accurate statistics.</p><p>We’ve found the information available from these tools to be invaluable in answering questions to help diagnose Cassandra issues or just better understand what Cassandra is doing with your data. The information available from the tools is pretty broad. Some highlights that will resonate with many Cassandra users include:</p><ul><li>Partition keys of the largest partitions by data size, number of columns and sstables spanned</li> <li>Information about data age (timespan) of data in sstables</li> <li>Tombstone information including partition keys of partitions with the most tombstones and calculation of potentially reclaimable space if/when tombstones are purged</li> </ul><p>This is just a highlights list of key data. See the <a href=\"https://instaclustr.zendesk.com/hc/en-us/articles/235656328\">help page</a> and examples below for a more complete list.</p><p>The tools are available on a supported basis for our enterprise support customers and on an unsupported basis for the general community (although we’ll probably answer questions on the C* user email list). For users of <a href=\"https://www.instaclustr.com/platform/\">Instaclustr’s Managed Service</a>, our Technical Operations team will run these as needed when working with you to help diagnose issues.</p><p>The source code is published on our <a href=\"https://github.com/instaclustr/cassandra-sstable-tools\">github</a>. We’re more than happy to take pull requests and other suggestions for improvements. We’ll also be talking to the C* project to see if any of this code makes sense in the core project.</p><p>We hope these tools will be as useful for the rest of the Cassandra community as we’ve found them in our work. Let us know in the comments if you have any feedack.</p><div class=\"foogallery foogallery-container foogallery-image-viewer foogallery-link-image foogallery-lightbox-foobox-free fg-center fg-image-viewer fg-light fg-border-thin fg-shadow-outline fg-loading-default fg-loaded-fade-in\" id=\"foogallery-gallery-4274\" data-foogallery=\"{&quot;item&quot;:{&quot;showCaptionTitle&quot;:false,&quot;showCaptionDescription&quot;:true},&quot;lazy&quot;:true,&quot;src&quot;:&quot;data-src-fg&quot;,&quot;srcset&quot;:&quot;data-srcset-fg&quot;}\" data-fg-common-fields=\"1\"><div class=\"fiv-inner\"><div class=\"fiv-ctrls\"><p>Prev</p><label class=\"fiv-count\">1of5</label><p>Next</p></div></div></div><p>Rotate through the gallery above for more information.</p>",
        "created_at": "2018-09-13T15:10:09+0000",
        "updated_at": "2018-09-13T15:10:13+0000",
        "published_at": "2017-02-13T02:23:38+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12166"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1052,
            "label": "tools",
            "slug": "tools"
          }
        ],
        "is_public": false,
        "id": 12165,
        "uid": null,
        "title": "instaclustr/cassandra-sstable-tools",
        "url": "https://github.com/instaclustr/cassandra-sstable-tools",
        "content": "<pre>$ git clone git@github.com:instaclustr/cassandra-sstable-tools.git\n$ cd cassandra-sstable-tools\n# Select the correct branch for major version (default is cassandra-3.11)\n$ git checkout cassandra-3.11\n$ ant\n</pre><p>You can compile against an older minor version with <code>-Dcassandra.version=&lt;version&gt;</code>. For example:</p><pre>$ ant -Dcassandra.version=3.11.2\n</pre><p>However only the version specified in build.xml is officially supported,\nas compatibility between minor versions can break.</p><p>Copy ic-sstable-tools.jar to Cassandra JAR folder, eg. <code>/usr/share/cassandra</code></p><p>Copy the bin/ic-* files into your $PATH</p><table><thead><tr><th>Command</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>ic-summary</td>\n<td>Summary information about all column families including how much of the data is repaired</td>\n</tr><tr><td>ic-sstables</td>\n<td>Print out metadata for sstables the belong to a column family</td>\n</tr><tr><td>ic-pstats</td>\n<td>Partition size statistics for a column family</td>\n</tr><tr><td>ic-cfstats</td>\n<td>Detailed statistics about cells in a column family</td>\n</tr><tr><td>ic-purge</td>\n<td>Statistics about reclaimable data for a column family</td>\n</tr></tbody></table><h2>ic-summary</h2><p>Provides summary information about all column families. Useful for finding\nthe largest column families and how much data has been repaired by incremental repairs.</p><h3>Usage</h3><pre>ic-summary\n</pre><h3>Output</h3><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Keyspace</td>\n<td>Keyspace the column family belongs to</td>\n</tr><tr><td>Column Family</td>\n<td>Name of column family</td>\n</tr><tr><td>SSTables</td>\n<td>Number of sstables on this node for the column family</td>\n</tr><tr><td>Disk Size</td>\n<td>Compressed size on disk for this node</td>\n</tr><tr><td>Data Size</td>\n<td>Uncompressed size of the data for this node</td>\n</tr><tr><td>Last Repaired</td>\n<td>Maximum repair timestamp on sstables</td>\n</tr><tr><td>Repair %</td>\n<td>Percentage of data marked as repaired</td>\n</tr></tbody></table><h2>ic-sstables</h2><p>Print out sstable metadata for a column family. Useful in helping to tune compaction settings.</p><h3>Usage</h3><pre>ic-sstables &lt;keyspace&gt; &lt;column-family&gt;\n</pre><h3>Output</h3><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>SSTable</td>\n<td>Data.db filename of sstable</td>\n</tr><tr><td>Disk Size</td>\n<td>Size of sstable on disk</td>\n</tr><tr><td>Total Size</td>\n<td>Uncompressed size of data contained in the sstable</td>\n</tr><tr><td>Min Timestamp</td>\n<td>Minimum cell timestamp contained in the sstable</td>\n</tr><tr><td>Max Timestamp</td>\n<td>Maximum cell timestamp contained in the sstable</td>\n</tr><tr><td>Duration</td>\n<td>The time span between minimum and maximum cell timestamps</td>\n</tr><tr><td>Min Deletion Time</td>\n<td>The minimum deletion time</td>\n</tr><tr><td>Max Deletion Time</td>\n<td>The maximum deletion time</td>\n</tr><tr><td>Level</td>\n<td>Leveled Tiered Compaction sstable level</td>\n</tr><tr><td>Keys</td>\n<td>Number of partition keys</td>\n</tr><tr><td>Avg Partition Size</td>\n<td>Average partition size</td>\n</tr><tr><td>Max Partition Size</td>\n<td>Maximum partition size</td>\n</tr><tr><td>Avg Column Count</td>\n<td>Average number of columns in a partition</td>\n</tr><tr><td>Max Column Count</td>\n<td>Maximum number of columns in a partition</td>\n</tr><tr><td>Droppable</td>\n<td>Estimated droppable tombstones</td>\n</tr><tr><td>Repaired At</td>\n<td>Time when marked as repaired by incremental repair</td>\n</tr></tbody></table><h2>ic-pstats</h2><p>Tool for finding largest partitions. Reads the Index.db files so is relatively quick.</p><h3>Usage</h3><pre>ic-pstats [-n &lt;num&gt;] [-t &lt;snapshot&gt;] [-f &lt;filter&gt;] &lt;keyspace&gt; &lt;column-family&gt;\n</pre><table><thead><tr><th>-h</th>\n<th>Display help</th>\n</tr></thead><tbody><tr><td>-b</td>\n<td>Batch mode. Uses progress indicator that is friendly for running in batch jobs.</td>\n</tr><tr><td>-n </td>\n<td>Number of partitions to display</td>\n</tr><tr><td>-t </td>\n<td>Snapshot to analyse. Snapshot is created if none is specified.</td>\n</tr><tr><td>-f </td>\n<td>Comma separated list of Data.db sstables to filter on</td>\n</tr></tbody></table><h3>Output</h3><p>Summary: Summary statistics about partitions</p><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Count (Size)</td>\n<td>Number of partition keys on this node</td>\n</tr><tr><td>Total (Size)</td>\n<td>Total uncompressed size of all partitions on this node</td>\n</tr><tr><td>Total (SSTable)</td>\n<td>Number of sstables on this node</td>\n</tr><tr><td>Minimum (Size)</td>\n<td>Minimum uncompressed partition size</td>\n</tr><tr><td>Minimum (SSTable)</td>\n<td>Minimum number of sstables a partition belongs to</td>\n</tr><tr><td>Average (Size)</td>\n<td>Average (mean) uncompressed partition size</td>\n</tr><tr><td>Average (SSTable)</td>\n<td>Average (mean) number of sstables a partition belongs to</td>\n</tr><tr><td>std dev. (Size)</td>\n<td>Standard deviation of partition sizes</td>\n</tr><tr><td>std dev. (SSTable)</td>\n<td>Standard deviation of number of sstables for a partition</td>\n</tr><tr><td>50% (Size)</td>\n<td>Estimated 50th percentile of partition sizes</td>\n</tr><tr><td>50% (SSTable)</td>\n<td>Estimated 50th percentile of sstables for a partition</td>\n</tr><tr><td>75% (Size)</td>\n<td>Estimated 75th percentile of partition sizes</td>\n</tr><tr><td>75% (SSTable)</td>\n<td>Estimated 75th percentile of sstables for a partition</td>\n</tr><tr><td>90% (Size)</td>\n<td>Estimated 90th percentile of partition sizes</td>\n</tr><tr><td>90% (SSTable)</td>\n<td>Estimated 90th percentile of sstables for a partition</td>\n</tr><tr><td>95% (Size)</td>\n<td>Estimated 95th percentile of partition sizes</td>\n</tr><tr><td>95% (SSTable)</td>\n<td>Estimated 95th percentile of sstables for a partition</td>\n</tr><tr><td>99% (Size)</td>\n<td>Estimated 99th percentile of partition sizes</td>\n</tr><tr><td>99% (SSTable)</td>\n<td>Estimated 99th percentile of sstables for a partition</td>\n</tr><tr><td>99.9% (Size)</td>\n<td>Estimated 99.9th percentile of partition sizes</td>\n</tr><tr><td>99.9% (SSTable)</td>\n<td>Estimated 99.9th percentile of sstables for a partition</td>\n</tr><tr><td>Maximum (Size)</td>\n<td>Maximum uncompressed partition size</td>\n</tr><tr><td>Maximum (SSTable)</td>\n<td>Maximum number of sstables a partition belongs to</td>\n</tr></tbody></table><p>Largest partitions: The top N largest partitions</p><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr></tbody></table><p>SSTable Leaders: The top N partitions that belong to the most sstables</p><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr></tbody></table><p>SSTables: Metadata about sstables as it relates to partitions.</p><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>SSTable</td>\n<td>Data.db filename of SSTable</td>\n</tr><tr><td>Size</td>\n<td>Uncompressed size</td>\n</tr><tr><td>Min Timestamp</td>\n<td>Minimum cell timestamp in the sstable</td>\n</tr><tr><td>Max Timestamp</td>\n<td>Maximum cell timestamp in the sstable</td>\n</tr><tr><td>Level</td>\n<td>Leveled Tiered Compaction level of sstable</td>\n</tr><tr><td>Partitions</td>\n<td>Number of partition keys in the sstable</td>\n</tr><tr><td>Avg Partition Size</td>\n<td>Average uncompressed partition size in sstable</td>\n</tr><tr><td>Max Partition Size</td>\n<td>Maximum uncompressed partition size in sstable</td>\n</tr></tbody></table><h2>ic-cfstats</h2><p>Tool for getting detailed cell statistics that can help identify issues with data model.</p><h3>Usage</h3><pre>ic-cfstats [-r &lt;limit&gt;] [-n &lt;num&gt;] [-t &lt;snapshot&gt;] [-f &lt;filter&gt;] &lt;keyspace&gt; &lt;column-family&gt;\n</pre><table><thead><tr><th>-h</th>\n<th>Display help</th>\n</tr></thead><tbody><tr><td>-b</td>\n<td>Batch mode. Uses progress indicator that is friendly for running in batch jobs.</td>\n</tr><tr><td>-r </td>\n<td>Limit read throughput to ratelimit MB/s</td>\n</tr><tr><td>-n </td>\n<td>Number of partitions to display</td>\n</tr><tr><td>-t </td>\n<td>Snapshot to analyse. Snapshot is created if none is specified.</td>\n</tr><tr><td>-f </td>\n<td>Comma separated list of Data.db sstables to filter on</td>\n</tr></tbody></table><h3>Output</h3><p>Summary: Summary statistics about partitions</p><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Count (Size)</td>\n<td>Number of partition keys on this node</td>\n</tr><tr><td>Rows (Size)</td>\n<td>Number of clustering rows</td>\n</tr><tr><td>(deleted)</td>\n<td>Number of clustering row deletions</td>\n</tr><tr><td>Total (Size)</td>\n<td>Total uncompressed size of all partitions on this node</td>\n</tr><tr><td>Total (SSTable)</td>\n<td>Number of sstables on this node</td>\n</tr><tr><td>Minimum (Size)</td>\n<td>Minimum uncompressed partition size</td>\n</tr><tr><td>Minimum (SSTable)</td>\n<td>Minimum number of sstables a partition belongs to</td>\n</tr><tr><td>Average (Size)</td>\n<td>Average (mean) uncompressed partition size</td>\n</tr><tr><td>Average (SSTable)</td>\n<td>Average (mean) number of sstables a partition belongs to</td>\n</tr><tr><td>std dev. (Size)</td>\n<td>Standard deviation of partition sizes</td>\n</tr><tr><td>std dev. (SSTable)</td>\n<td>Standard deviation of number of sstables for a partition</td>\n</tr><tr><td>50% (Size)</td>\n<td>Estimated 50th percentile of partition sizes</td>\n</tr><tr><td>50% (SSTable)</td>\n<td>Estimated 50th percentile of sstables for a partition</td>\n</tr><tr><td>75% (Size)</td>\n<td>Estimated 75th percentile of partition sizes</td>\n</tr><tr><td>75% (SSTable)</td>\n<td>Estimated 75th percentile of sstables for a partition</td>\n</tr><tr><td>90% (Size)</td>\n<td>Estimated 90th percentile of partition sizes</td>\n</tr><tr><td>90% (SSTable)</td>\n<td>Estimated 90th percentile of sstables for a partition</td>\n</tr><tr><td>95% (Size)</td>\n<td>Estimated 95th percentile of partition sizes</td>\n</tr><tr><td>95% (SSTable)</td>\n<td>Estimated 95th percentile of sstables for a partition</td>\n</tr><tr><td>99% (Size)</td>\n<td>Estimated 99th percentile of partition sizes</td>\n</tr><tr><td>99% (SSTable)</td>\n<td>Estimated 99th percentile of sstables for a partition</td>\n</tr><tr><td>99.9% (Size)</td>\n<td>Estimated 99.9th percentile of partition sizes</td>\n</tr><tr><td>99.9% (SSTable)</td>\n<td>Estimated 99.9th percentile of sstables for a partition</td>\n</tr><tr><td>Maximum (Size)</td>\n<td>Maximum uncompressed partition size</td>\n</tr><tr><td>Maximum (SSTable)</td>\n<td>Maximum number of sstables a partition belongs to</td>\n</tr></tbody></table><p>Row Histogram: Histogram of number of rows per partition</p><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Percentile</td>\n<td>Minimum, average, standard deviation (std dev.), percentile, maximum</td>\n</tr><tr><td>Count</td>\n<td>Estimated number of rows per partition for the given percentile</td>\n</tr></tbody></table><p>Largest partitions: Partitions with largest uncompressed size</p><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>Rows</td>\n<td>Total number of clustering rows in the partition</td>\n</tr><tr><td>(deleted)</td>\n<td>Number of row deletions in the partition</td>\n</tr><tr><td>Tombstones</td>\n<td>Number of cell or range tombstones</td>\n</tr><tr><td>(droppable)</td>\n<td>Number of tombstones that can be dropped as per gc_grace_seconds</td>\n</tr><tr><td>Cells</td>\n<td>Number of cells in the partition</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr></tbody></table><p>Widest partitions: Partitions with the most cells</p><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>Rows</td>\n<td>Total number of clustering rows in the partition</td>\n</tr><tr><td>(deleted)</td>\n<td>Number of row deletions in the partition</td>\n</tr><tr><td>Cells</td>\n<td>Number of cells in the partition</td>\n</tr><tr><td>Tombstones</td>\n<td>Number of cell or range tombstones</td>\n</tr><tr><td>(droppable)</td>\n<td>Number of tombstones that can be dropped as per gc_grace_seconds</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr></tbody></table><p>Most Deleted Rows: Partitions with the most row deletions</p><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>Rows</td>\n<td>Total number of clustering rows in the partition</td>\n</tr><tr><td>(deleted)</td>\n<td>Number of row deletions in the partition</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr></tbody></table><p>Tombstone Leaders: Partitions with the most tombstones</p><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>Tombstones</td>\n<td>Number of cell or range tombstones</td>\n</tr><tr><td>(droppable)</td>\n<td>Number of tombstones that can be dropped as per gc_grace_seconds</td>\n</tr><tr><td>Rows</td>\n<td>Total number of clustering rows in the partition</td>\n</tr><tr><td>Cells</td>\n<td>Number of cells in the partition</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr></tbody></table><p>SSTable Leaders: Partitions that are in the most sstables</p><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>SSTable Count</td>\n<td>Number of sstables that contain the partition</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>Rows</td>\n<td>Total number of clustering rows in the partition</td>\n</tr><tr><td>Cells</td>\n<td>Number of cells in the partition</td>\n</tr><tr><td>Tombstones</td>\n<td>Number of cell or range tombstones</td>\n</tr><tr><td>(droppable)</td>\n<td>Number of tombstones that can be dropped as per gc_grace_seconds</td>\n</tr></tbody></table><p>SSTables: Metadata about sstables as it relates to partitions.</p><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>SSTable</td>\n<td>Data.db filename of SSTable</td>\n</tr><tr><td>Size</td>\n<td>Uncompressed size</td>\n</tr><tr><td>Min Timestamp</td>\n<td>Minimum cell timestamp in the sstable</td>\n</tr><tr><td>Max Timestamp</td>\n<td>Maximum cell timestamp in the sstable</td>\n</tr><tr><td>Partitions</td>\n<td>Number of partitions</td>\n</tr><tr><td>(deleted)</td>\n<td>Number of row level partition deletions</td>\n</tr><tr><td>(avg size)</td>\n<td>Average uncompressed partition size in sstable</td>\n</tr><tr><td>(max size)</td>\n<td>Maximum uncompressed partition size in sstable</td>\n</tr><tr><td>Rows</td>\n<td>Total number of clustering rows in sstable</td>\n</tr><tr><td>(deleted)</td>\n<td>Number of row deletions in sstable</td>\n</tr><tr><td>Cells</td>\n<td>Number of cells in the SSTable</td>\n</tr><tr><td>Tombstones</td>\n<td>Number of cell or range tombstones in the SSTable</td>\n</tr><tr><td>(droppable)</td>\n<td>Number of tombstones that are droppable according to gc_grace_seconds</td>\n</tr><tr><td>(range)</td>\n<td>Number of range tombstones</td>\n</tr><tr><td>Cell Liveness</td>\n<td>Percentage of live cells. Does not consider tombstones or cell updates shadowing cells. That is it is percentage of non-tombstoned cells to total number of cells.</td>\n</tr></tbody></table><h2>ic-purge</h2><p>Finds the largest reclaimable partitions (GCable). Intensive process, effectively does \"fake\" compactions to calculate metrics.</p><h3>Usage</h3><pre>ic-purge [-r &lt;limit&gt;] [-n &lt;num&gt;] [-t &lt;snapshot&gt;] [-f &lt;filter&gt;] &lt;keyspace&gt; &lt;column-family&gt;\n</pre><table><thead><tr><th>-h</th>\n<th>Display help</th>\n</tr></thead><tbody><tr><td>-b</td>\n<td>Batch mode. Uses progress indicator that is friendly for running in batch jobs.</td>\n</tr><tr><td>-r </td>\n<td>Limit read throughput to ratelimit MB/s</td>\n</tr><tr><td>-n </td>\n<td>Number of partitions to display</td>\n</tr><tr><td>-t </td>\n<td>Snapshot to analyse. Snapshot is created if none is specified.</td>\n</tr></tbody></table><h3>Output</h3><p>Largest reclaimable partitions: Partitions with the largest amount of reclaimable data</p><table><thead><tr><th>Column</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td>Key</td>\n<td>The partition key</td>\n</tr><tr><td>Size</td>\n<td>Total uncompressed size of the partition</td>\n</tr><tr><td>Reclaim</td>\n<td>Reclaimable uncompressed size</td>\n</tr><tr><td>Generations</td>\n<td>SSTable generations the partition belongs to</td>\n</tr></tbody></table>",
        "created_at": "2018-09-13T15:10:01+0000",
        "updated_at": "2019-03-22T03:26:08+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 8,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/11550580?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12165"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1052,
            "label": "tools",
            "slug": "tools"
          }
        ],
        "is_public": false,
        "id": 12164,
        "uid": null,
        "title": "Update released for Instaclustr sstable analysis tools for Apache Cassandra - Instaclustr",
        "url": "https://www.instaclustr.com/update-released-instaclustr-sstable-analysis-tools-apache-cassandra/",
        "content": "<header id=\"page-masthead\"><div id=\"page-content\"><div class=\"container\"><div class=\"row\"><div class=\"col-md-6 col-md-offset-0 col-sm-8 col-sm-offset-2\"><div class=\"primary first-p-bold\"><p>Instaclustr is pleased to announce the latest update for it’s open-sourced sstable analysis tools for Apache Cassandra.</p><p>These tools, first released in February 2017, help operators to gain an accurate picture of the on-disk data stored by Cassandra which can be invaluable in diagnose and resolving operational issues.</p><p>A full run-through of the existing functionality of the tools can be found in <a href=\"https://www.instaclustr.com/instaclustr-open-sources-cassandra-sstable-analysis-tools/\">this blog post</a>.</p><p>This latest release, available in source from <a href=\"https://github.com/instaclustr/cassandra-sstable-tools\">Instaclustr Github</a> or in compiled download from our <a href=\"https://www.instaclustr.com/support/documentation/tools/ic-tools-for-cassandra-sstables/\">support page</a>, </p><ul><li>Improved support of TWCS – sorting by maximum timestamp in the sstable listing</li> <li>For Apache Cassandra 3.x, ic-cfstats reports about rows including: <ul><li>Total number of rows</li> <li>Total number of row deletions</li> <li>A row histogram giving number of rows per partition</li> <li>Largest partitions/widest partitions report includes number of rows and how many row deletions</li> <li>Added a Most Deleted Rows section reporting partitions with most row deletions</li> <li>Tombstone Leaders and SSTable Leaders reports number of rows</li> </ul></li> <li>ic-cfstats and ic-pstats includes histogram of partition sizes and sstables/partition in the summary</li> </ul><p>Instaclustr uses these tools to help support customers on our <a href=\"https://www.instaclustr.com/solutions/managed-apache-cassandra/\">Apache Cassandra Managed Service</a> and provides support for the use of the tools for our <a href=\"https://www.instaclustr.com/services/cassandra-support/\">Apache Cassandra Enterprise Support</a> customers.</p></div></div></div></div></div></header>",
        "created_at": "2018-09-13T15:09:49+0000",
        "updated_at": "2018-09-13T15:09:54+0000",
        "published_at": "2018-08-21T16:57:58+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12164"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          }
        ],
        "is_public": false,
        "id": 12163,
        "uid": null,
        "title": "Why We Built an Open Source Cassandra-Operator to Run Apache Cassandra on Kubernetes - Instaclustr",
        "url": "https://www.instaclustr.com/why-we-built-apache-cassandra-operator-to-run-on-kubernetes/",
        "content": "<p>As Kubernetes becomes the de facto for container orchestration, more and more developers (and enterprises) want to run Apache Cassandra on Kubernetes. It’s easy to get started with this – especially considering the capabilities that Kubernetes’ <a href=\"https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/\">StatefulSets</a> bring to the table. Kubernetes, though, certainly has room to improve when it comes to storing data in-state and understanding how different databases work.</p><p>For example, Kubernetes doesn’t know if you’re writing to a leader or a follower database, to a multi-sharded leader infrastructure, or to a single database instance. StatefulSets – workload API objects used to manage stateful applications – offer the building blocks required for stable unique network identifiers, stable persistent storage, ordered and smooth deployment and scaling, deletion and termination, and automated rolling updates. However, while getting <i>started</i> with Cassandra on Kubernetes might be easy, it can still be a challenge to run and manage (and running Docker is another challenge in itself). </p><p>To overcome some of these hurdles, we decided to build an open source Cassandra-operator that runs and operates Cassandra within Kubernetes. Think of it as Cassandra-as-a-Service on top of Kubernetes. We’ve made this Cassandra-operator open source and freely available on <a href=\"https://github.com/instaclustr/cassandra-operator\">GitHub</a>. It remains a work in progress between myself, others on my team, and a number of partner contributors – but it is functional and ready for use. The Cassandra-operator supports Docker images, which are open source and available as well (via <a href=\"https://github.com/instaclustr/cassandra-operator\">the same link</a>).</p><p>This Cassandra-operator is designed to provide “operations-free” Cassandra: it takes care of deployment and allows users to manage and run Cassandra, in a safe way, within Kubernetes environments. It also makes it simple to utilize consistent and reproducible environments. </p><p>While it’s possible for developers to build scripts for managing and running Cassandra on Kubernetes, the Cassandra-operator offers the advantage of providing the same consistent reproducible environment, as well as the same consistent reproducible set of operations through different production clusters. And this is true across development, staging, and QA environments. Furthermore, because best practices are already built into the operator, development teams are spared from operational concerns and are able to focus on their core capabilities.</p><h2><b>What is a Kubernetes operator?</b></h2><p>A Kubernetes operator consists of two components: a controller and a custom resource definition (CRD). The CRD allows devs to create Cassandra objects in Kubernetes. It’s an extension of Kubernetes that allows us to define custom objects or resources using Kubernetes that our controller can then listen to for any changes to the resource definition. Devs can define an object in Kubernetes that contains configuration options for Cassandra, such as cluster name, node count, jvm tuning options, etc. – all the information you want to give Kubernetes about how to deploy Cassandra. </p><p>You can isolate the Cassandra-operator to a specific Kubernetes namespace, define what kinds of persistent volumes it should use, and more. The Cassandra-operator controller listens to state changes on the Cassandra CRD and will create its own StatefulSets to match those requirements. It will also manage those operations and can ensure repairs, backups, and safe scaling as specified via the CRD. In this way, it leverages the Kubernetes concept of building controllers upon other controllers in order to achieve intelligent and helpful behaviours. </p><h2><b>How does it work?</b></h2><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2018/09/Page-2.png\"><img class=\"aligncenter size-full wp-image-11231\" src=\"https://www.instaclustr.com/wp-content/uploads/2018/09/Page-2.png\" alt=\"\" width=\"3508\" height=\"2482\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2.png 3508w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-300x212.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-768x543.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-1024x725.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-1200x849.png 1200w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-871x616.png 871w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-640x453.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-68x48.png 68w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2018/09/Page-2-153x108.png 153w\" /></a></p><p>Architecturally, the Cassandra controller itself connects to the Kubernetes Master. It listens to state changes and manipulates Pod definitions and CRDs. It then deploys those, waits for changes to occur, and repeats until the entirety of necessary changes is fully completed.</p><p>The Cassandra controller can, of course, perform operations within the Cassandra cluster itself. For example, want to scale down your Cassandra cluster? Instead of manipulating the StatefulSet to handle this task, the controller will first see the CRD change. The node count will change to a lower number (say from six to five). The controller will get that state change, and it will first run a decommission operation on the Cassandra node that’s going to be removed. This ensures that the Cassandra node stops gracefully and that it will redistribute and rebalance the data it held across the remaining nodes. Once the Cassandra controller sees that this has happened successfully, it will modify that StatefulSet definition to allow Kubernetes to finally decommission that particular Pod. Thus, the Cassandra controller brings needed intelligence to the Kubernetes environment to run Cassandra properly and ensure smoother operations.</p><p>As we continue this project and iterate on the Cassandra-operator, our goal is to add new components that will continue to expand the tool’s features and value. A good example is the Cassandra SideCar (included in the diagram above), which will begin to take responsibility for tasks like backups and repairs. Current and future features of the project can be <a href=\"https://github.com/instaclustr/cassandra-operator/issues\">viewed on GitHub</a>. Our goal for the Cassandra-operator is to give devs a powerful open source option for running Cassandra on Kubernetes with a simplicity and grace that has not yet been all that easy to achieve.</p><p><i>Ben Bromhead is CTO at </i><a href=\"https://www.instaclustr.com\"><i>Instaclustr</i></a><i>, which provides a managed service platform of open source technologies such as Apache Cassandra, Apache Spark, Elasticsearch and Apache Kafka.</i></p>",
        "created_at": "2018-09-13T15:08:59+0000",
        "updated_at": "2018-09-13T15:09:06+0000",
        "published_at": "2018-09-13T14:15:14+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2018/09/Page-2.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12163"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12162,
        "uid": null,
        "title": "Cassandra Collections: Hidden Tombstones and How to Avoid Them - Instaclustr",
        "url": "https://www.instaclustr.com/cassandra-collections-hidden-tombstones-and-how-to-avoid-them/",
        "content": "<h2>Overview</h2><p>Multi-value data types (sets, lists and maps) are a powerful feature of <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a>, aiding you in denormalisation while allowing you to still retrieve and set data at a very fine-grained level. However, some of Cassandra’s behaviour when handling these data types is not always as expected and can cause issues.</p><p>In particular, there can be hidden surprises when you update the value of a collection type column. For simple-type columns, Cassandra performs an update by simply writing a new value for the cell and the most recently written value wins when the data is read. However, when you overwrite a collection Cassandra can’t simply write the new elements because all the existing elements in the map have their own individual cells and would still be returned alongside the new elements whenever a read is performed on the map.</p><h2>The options</h2><p>This leaves Cassandra with two options:</p><ol><li>Perform a read and discover all the existing map elements and either delete them or update them if they were specified in the overwrite.</li> <li>Forget about all existing elements in the map by deleting them.</li> </ol><p>Option 1 doesn’t sound very optimised does it? A read for every write you perform? Ouch.<br />Cassandra chooses option 2, because it just can’t resist those performance gains. It knows you’re performing an overwrite, and that you obviously don’t care about the contents of those columns, so it will delete them for you, and we can all pretend they never existed in the first place.</p><p>Or so we thought… until one day your queries start failing because you’ve hit 100k tombstones. Didn’t expect that, especially when you never delete any data.<br />In most cases, compactions will just handle this problem for you and the tombstones will be gone before you even get close to the query failure limit. However, compaction strategies aren’t perfect and depending on how much you overwrite, plus how well compactions remove those tombstones, there are many cases where this behaviour can become a huge issue. If you are performing many writes, and all of them are overwrites where a collection type is involved, you will be generating a tombstone for every single write.</p><h2>Examples for avoiding the issue</h2><p>I’ve created a very basic schema with a map and a few fields, as below:</p><p>I then inserted a single row and performed a flush:</p><p>And I now have an SSTable in my tombs.staff data directory.</p><p>Using sstable2json to analyse the data, as expected we have one key, a, however it has two locations entries, despite the fact we only did one write.</p><p>This is to do with the map, and the whole overwrite thing I was talking about earlier. Already we can see that C* has written a range tombstone for the locations cell immediately before writing the value that I inserted.</p><p>Now this is kind of a spoiler, as we haven’t actually done any “overwrites” yet, but we’ve identified the feature we’re talking about. This is because in Cassandra, overwrites, updates, and inserts, are really all just the same thing. The insert against the map will do the same thing whether the key already exists or not.</p><p>Anyway, we can see how this delete first strategy begins to work if we simply insert another record with the same key:</p><p>We now have 2 sstables: tombs-staff-ka-1-Data.db and tombs-staff-ka-2-Data.db. And if we run sstable2json on the new SSTable, we see a very similar entry:</p><p>Nothing surprising, and furthermore, if we trigger a major compaction against our 2 SSTables:</p><p>And run sstable2json against our new SSTable…</p><p>We have the latest range tombstone plus the latest insert, and compactions have, as expected,  gotten rid of the previous insert as it knows everything older than the latest range tombstone is moot.</p><p>Now you can start to see where issues can arise when overwriting a key with a collection type. If it weren’t for the compaction, I’d have 2 tombstones for that single row across 2 SSTables. Obviously it’s very likely those SSTables will compact and the tombstones will get cleared out, however things are not always as clear cut, especially when you are frequently overwriting keys and the tombstones get spread across many SSTables of differing sizes, causing tombstone bloat that may not be removed when left up to minor compactions.</p><p>So how can we avoid this potential catastrophe? A simple solution would be to instead store JSON and leave the updates to your application, however there is an alternative. You can use the provided append and subtraction operators. These operators will modify the collection without having to perform a read, and also won’t create any range tombstones. This works for specific use cases where you simply need to insert/append/prepend, however if you frequently find yourself having to rewrite a whole collection you will need to take a different approach. You can also specify a collection as frozen which would give the the desired overwrite behaviour, but you will no longer be able to add and remove elements using the +, -, and [] operators.</p><p>Here is an example of performing collection operations on a list.</p><div id=\"crayon-5b9a5da245d96328149275\" class=\"crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p>ALTER TABLE staff ADD leave_dates list&amp;lt;text&amp;gt;; # Creates a tombstone and an entry in the list    </p><p>insert into staff (id, leave_dates) values ('c', ['20160620']);   </p><p>$ nodetool flush    </p><p>$ sstable2json tombs-staff-ka-6-Data.db    </p><p>[    </p><p>{\"key\": \"c\",     \"cells\": [[\"\",\"\",1466427765961455],</p><p>[\"leave_dates:_\",\"leave_dates:!\",1466427765961454,\"t\",1466427765],   </p><p>[\"leave_dates:484b79b036e711e681757906eb0f5a6e\",\"3230313630363230\",1466427765961455]]}    </p><p>]    </p><p># Prepends an element to the list without creating any additional tombstones    </p><p>UPDATE staff SET leave_dates = [ '20160621' ] + leave_dates where id='c';   </p><p> $ nodetool flush    # The new SSTable has only a single entry in the list, no extra tombstone.   </p><p> # This works the same for appending to the list as well.    </p><p>$ sstable2json tombs-staff-ka-7-Data.db    </p><p>[   </p><p>{\"key\": \"c\",     \"cells\":    [[\"leave_dates:af13b22fb5e911d781757906eb0f5a6e\",\"3230313630363231\",1466427869996855]]}    </p><p>]</p></div></td>\n</tr></table></div></div><p>Be careful when using addition and subtraction on list types, as removing elements from a list can be an expensive operation. Cassandra will have to read in the entire list in order to remove a single entry. Note that this is not true for sets, removing a single entry from a set requires no reads, as Cassandra will simply write a tombstone for the matching cell.</p><p>See the below trace for a deletion from a list, where we can clearly see C* performing a read query before making the modifications.</p><p>The following statements for the SET type result in similar functionality. Note that appending and prepending is non-existent with sets, it is simply add and remove.</p><div id=\"crayon-5b9a5da245d9e680868585\" class=\"crayon-syntax crayon-theme-classic crayon-font-monaco crayon-os-mac print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p>    ALTER TABLE staff ADD leave_dates list&amp;lt;text&amp;gt;;</p><p>    # Creates a tombstone and an entry in the list</p><p>    insert into staff (id, leave_dates) values ('c', ['20160620']);</p><p>    $ nodetool flush</p><p>    $ sstable2json tombs-staff-ka-6-Data.db</p><p>    [</p><p>    {\"key\": \"c\",</p><p>     \"cells\": [[\"\",\"\",1466427765961455],</p><p>    [\"leave_dates:_\",\"leave_dates:!\",1466427765961454,\"t\",1466427765],</p><p>    [\"leave_dates:484b79b036e711e681757906eb0f5a6e\",\"3230313630363230\",1466427765961455]]}</p><p>    ]</p><p>    # Prepends an element to the list without creating any additional tombstones</p><p>    UPDATE staff SET leave_dates = [ '20160621' ] + leave_dates where id='c';</p><p>    $ nodetool flush</p><p>    # The new SSTable has only a single entry in the list, no extra tombstone.</p><p>    # This works the same for appending to the list as well.</p><p>    $ sstable2json tombs-staff-ka-7-Data.db</p><p>    [</p><p>    {\"key\": \"c\",</p><p>     \"cells\": [[\"leave_dates:af13b22fb5e911d781757906eb0f5a6e\",\"3230313630363231\",1466427869996855]]}</p><p>    ]</p></div></td>\n</tr></table></div></div>",
        "created_at": "2018-09-13T15:07:16+0000",
        "updated_at": "2018-09-13T15:07:20+0000",
        "published_at": "2016-11-24T04:29:18+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12162"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 391,
            "label": "big.data",
            "slug": "big-data"
          }
        ],
        "is_public": false,
        "id": 12161,
        "uid": null,
        "title": "Apache Cassandra: The Big Data Foundation - Instaclustr",
        "url": "https://www.instaclustr.com/apache-cassandra-big-data-foundation/",
        "content": "<h2>Introduction</h2><p>To state the obvious, we here at Instaclustr are massive fans of <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a>. We have built our company and our managed service around this database technology and the awesomeness of its capability.</p><p>There are a lot of well documented use cases and amazing companies providing examples of how they are using open source Apache Cassandra, but as a managed service we get to see first hand the power of this amazing technology and what it can do for our diverse customer base.</p><p>Suffice to say that our experience over the last two years has us even more convinced that Apache Cassandra is the foundation technology for the next wave of global-scale applications and solutions.</p><h2>Apache Cassandra use cases</h2><p>It is fair to say that we have probably seen it all with the diverse range of deployments – the good the bad and sometimes the ugly. Here is our take on the most common deployments:</p><ul><li><strong>Security.</strong>  The fraud and threat detection use case is very active in our environment.  We see the application in most cases is related to identifying anomalies through data mining and deep analytics to identify security-related events of interest.</li> <li><strong><a href=\"https://www.instaclustr.com/customers/messaging/\">Messaging</a>.</strong>  Several of our customers have social media and data sharing applications that are being used with messaging services at it’s core.</li> <li><strong><a href=\"https://www.instaclustr.com/customers/iot/\">IoT</a>.</strong>  This is probably the most common of our customers use cases.  We have many customers representing a wide range of industries, using Cassandra as an IoT solution.  We also work with a number of customers who are providing IoT platforms to their own customer base.</li> <li><strong>Recommendation &amp; Personalisation.</strong>  Many of our customers are using the power of personalisation. This is a very common within the AdTech industry, but also some of our customers are building unique learning platforms that are personalized to an individual student.</li> <li><strong>Catalogues &amp; Playlists.</strong> This particular use case we haven’t seen as much of the others but the data models and usage patterns typical seen within catalogues and playlists are usually a small part of a much larger application.</li> </ul><p>The most popular industries? We have a large customer base within the <a href=\"https://www.instaclustr.com/customers/ad-tech/\">AdTech</a> space, where the key metrics of performance and scalability are important.  We also have core customers in the FinTech industry where personalization, high availability and security are all important.   We also have several customers in the EdTech space developing specialized and personalized learning platforms.</p><p>Another interesting insight is that we have an amazingly diverse client base that ranges from personal projects to early stage start-ups all the way through to 140-year-old, billion dollar companies looking to transform and enhance their business.  We can see first hand that you don’t have to be a large company to be working with large datasets.</p><p>With several of our original customers we have been with them on the journey from an initial 3 node cluster, through to large production clusters with separate staging and testing environments.</p><h2>Diverse use cases help us improve everyday</h2><p>The beauty of having grown our customer base so rapidly and widely is that we have benefited from gaining insight and understanding into the wide application of this technology and the details of specific use cases.  This provides us with a unique perspective of the deployment of Apache Cassandra. We see its adaptability, but we also see its complexity and its temperament when it is not handled well.</p><p>We see the specific nuances associated with operating an efficient production grade environment and cluster for all of the different use cases. Having such a wide range of different deployments under our care is giving us an ever increasing richness in our own data that we are now analysing through our <a href=\"https://www.instaclustr.com/monitoring-cassandra-and-it-infrastructure-with-riemann/\">Instametrics monitoring environment</a>.  This is helping us to continually improve our capability and to continue to automate and refine our service offering.</p><p>We have also been in the unique position of growing with our customers and helping them scale, in some cases rapidly.  This also provides us with insight into how to build out a cluster or environment efficiently when an application goes viral, or the application has to ingest vast amounts of data.</p><h2>With great power comes great responsibility</h2><p>There is no doubt that Apache Cassandra provides great power, but the trade-off is that this also comes with a certain level of complexity.   You can’t expect that a database technology like Apache Cassandra can simply scale rapidly, provide high throughput performance and be continuously on without there being some work to do.</p><p>Continual monitoring, maintenance and performance tuning are important activities that must go with any database and associated technology environment to keep it operating efficiently and effectively.  But probably just as important is good design and planning up front.</p><h2>When the data layer is an afterthought</h2><p>In many cases we see that the data layer follows the lead from the application. That is, the time effort and focus at first for many start-ups is the application and what the customer is building on the front-end. This is often necessary to demonstrate a concept to an investor or to simply get things up and running quickly while finding market fit.  This approach means that often the data is an afterthought.</p><p>When the data is an afterthought we often see that the application and database will work okay at the beginning, but it is when they try to scale that things get ugly with Cassandra. If you don’t treat the data layer with a certain amount of mechanical sympathy, and you don’t plan effectively from the start, then there can be consequences down the track.</p><h2>Plan to be big from the start</h2><p>When Apache Cassandra has been selected as the database by an engineer at the beginning of a project, we know that they mean business and that they are planning on their application or solution being global.   What we often find is that while they are thinking big from the start, that sometimes the design and planning isn’t equally as big.</p><p>We see that effective planning and design of the data architecture and infrastructure from the start means that our customers tend to prosper and scaling and performance are rarely an issue. When the team is only thinking big, and not designing and planning big, then problems can arise down the track.</p><p>And yes we can speak from experience.  Bringing your infrastructure and database back from the brink can be a difficult and painful experience.</p><p>You are much better off doing the work up front.  Even if your environment works well initially, it is when you get to the point of having to scale is when you will start to see issues.</p><h2>Conclusion</h2><p>If you are thinking big, then plan to be big. If you design the architecture and infrastructure yourself, get an independent expert with some experience to validate your work.  Check and check again.</p><p>Doing it right the first time will set you up for efficient scaling, high performance and a continuously on environment and save you weeks of pain when you have terabytes of data structured the wrong way.  Your application might work okay at the start, but it is when it comes to the point of scaling that we see most of the issues arise for our customers.</p>",
        "created_at": "2018-09-13T15:07:05+0000",
        "updated_at": "2018-09-13T15:07:07+0000",
        "published_at": "2017-01-31T06:52:36+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12161"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 51,
            "label": "blog",
            "slug": "blog"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12160,
        "uid": null,
        "title": "Cassandra Archives - Instaclustr",
        "url": "https://www.instaclustr.com/category/technical/cassandra/",
        "content": "<main id=\"main\"><section id=\"content\"><header id=\"page-title\"><div id=\"page-content\"><div class=\"container\"><div class=\"row\"><div class=\"col-sm-9 col-md-offset-1\"><div class=\"loop loop-news\"><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/why-we-built-apache-cassandra-operator-to-run-on-kubernetes/\" class=\"section-fade\"> <div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Why We Built an Open Source Cassandra-Operator to Run Apache Cassandra on Kubernetes</h2><p class=\"subtitle\"><strong>Thursday 13th September 2018</strong> by Ben Bromhead</p>As Kubernetes becomes the de facto for container orchestration, more and more developers (and enterprises) want to run Apache Cassandra on Kubernetes. It’s easy to get started with this – especially considering the...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/aws-lambda-managed-cassandra-part-1/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">AWS Lambda with Managed Cassandra - Part 1: Let's Build a POC</h2><p class=\"subtitle\"><strong>Monday 27th August 2018</strong> by Christophe Schmitz</p>Serverless architecture is an attractive solution for both small businesses and large enterprises. The former gains the ability to go to market quickly while the latter are able to reduce IT costs by...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/update-released-instaclustr-sstable-analysis-tools-apache-cassandra/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Update released for Instaclustr sstable analysis tools for Apache Cassandra</h2><p class=\"subtitle\"><strong>Tuesday 21st August 2018</strong> by Instaclustr </p>Instaclustr is pleased to announce the latest update for it’s open-sourced sstable analysis tools for Apache Cassandra. These tools, first released in February 2017, help operators to gain an accurate picture of the...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/apache-cassandra-ldap-authentication/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Apache Cassandra LDAP Authentication</h2><p class=\"subtitle\"><strong>Friday 29th June 2018</strong> by Kurt Greaves</p>We’ve seen an increasing need for LDAP integration into Apache Cassandra, and continually hearing of cases where people have written their own LDAP authenticators for Cassandra. However, if you search around you’ll have...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/using-cassandra-stress-to-model-a-time-series-workload/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Using Cassandra Stress to model a time series workload</h2><p class=\"subtitle\"><strong>Wednesday 9th May 2018</strong> by Instaclustr </p>Motivation When examining whether Cassandra is a good fit for your needs, it is good practice to stress test Cassandra using a workload that looks similar to the expected workload in Production. In...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/apache-kafka-connect-architecture-overview/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Apache Kafka Connect Architecture Overview</h2><p class=\"subtitle\"><strong>Wednesday 9th May 2018</strong> by Paul Brebner</p>Kafka Connect is an API and ecosystem of 3rd party connectors that enables Apache Kafka to be scalable, reliable, and easily integrated with other heterogeneous systems (such as Cassandra, Spark, and Elassandra) without...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/february-2018-apache-cassandra-releases/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> News   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">February 2018 Apache Cassandra Releases</h2><p class=\"subtitle\"><strong>Thursday 22nd March 2018</strong> by Kurt Greaves</p>In February, the Apache Cassandra project issued releases for all currently supported branches of Apache Cassandra. As far as releases go the change list was modest, which shows that we’re seeing fewer bugs...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/benchmark-results-linux-kernel-meltdown-patch-effects-on-apache-cassandra/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Benchmark Results: Linux Kernel \"Meltdown\" patch effects on Apache Cassandra 3.11.1</h2><p class=\"subtitle\"><strong>Friday 2nd February 2018</strong> by Instaclustr </p>In our Security Advisory published 8 January, we advised of up to 20% increase in CPU utilization and small increase in latency across managed clusters in AWS and GCP following the rollout of...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/surveying-cassandra-compatible-database-landscape/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Surveying the Cassandra-compatible database landscape</h2><p class=\"subtitle\"><strong>Wednesday 17th January 2018</strong> by Ben Slater</p>Overview The popularity of Apache Cassandra and the applicability of it’s development model has seen it clearly emerge as the leading NoSQL technology for scale, performance and availability. One only needs to survey...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://diginomica.com/2017/11/28/how-adstage-moved-beyond-startup-scale-with-apache-cassandra-as-a-data-service/\" class=\"section-fade\" target=\"_blank\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> News   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">How AdStage moved beyond startup scale with Apache Cassandra as a data service <i class=\"ion-android-open\"></i></h2><p class=\"subtitle\"><strong>Wednesday 13th December 2017</strong> by Instaclustr </p>Startup growth is never a bad thing – but the strain it puts on an IT team is real. Jason Wu of AdStage told me how AdStage addressed its data scale issues, and...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"https://www.instaclustr.com/picknmix-cassandra-spark-zeppelin-elassandra-kibana-kafka/\" class=\"section-fade\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> Technical   <i class=\"ion-record\"> Technical — Cassandra   <i class=\"ion-record\"> Technical — Kafka   <i class=\"ion-record\"> Technical — Spark   <i class=\"ion-record\"> Technical — Elasticsearch  </i></i></i></i></i></p><h2 class=\"h3\">Pick‘n’Mix: Cassandra, Spark, Zeppelin, Elassandra, Kibana, &amp; Kafka</h2><p class=\"subtitle\"><strong>Tuesday 5th December 2017</strong> by Paul Brebner</p>Kafkaesque:  \\ käf-kə-ˈesk \\ Marked by a senseless, disorienting, menacing, nightmarishly complexity. One morning when I woke from troubled dreams, I decided to blog about something potentially Kafkaesque: Which Instaclustr managed open-source-as-a-service(s) can be used...</div></div></a></article><article class=\"loop-item loop-item-news\"><a href=\"http://www.computerweekly.com/blog/Open-Source-Insider/Instaclustr-7-easy-steps-to-Cassandra-cluster-migration\" class=\"section-fade\" target=\"_blank\">\n<div class=\"row\"><div class=\"col-lg-5 col-md-6\"><p class=\"cat-label\">  <i class=\"ion-record\"> News   <i class=\"ion-record\"> Technical — Cassandra  </i></i></p><h2 class=\"h3\">Instaclustr: 7 Easy Steps to Cassandra Cluster Migration <i class=\"ion-android-open\"></i></h2><p class=\"subtitle\"><strong>Friday 1st December 2017</strong> by Ben Slater</p> Instaclustr offers managed and supported solutions for a wide range of open source technologies including Apache Cassandra, ScyllaDB, Elasticsearch, Apache Spark, Apache Zeppelin, Kibana and Apache Lucene.  CPO Ben Slater provides their 7-step guide...</div></div></a></article></div><p>1<a href=\"https://www.instaclustr.com/category/technical/cassandra/page/2\" class=\"page-numbers\">2</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/3\" class=\"page-numbers\">3</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/4\" class=\"page-numbers\">4</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/5\" class=\"page-numbers\">5</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/6\" class=\"page-numbers\">6</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/7\" class=\"page-numbers\">7</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/8\" class=\"page-numbers\">8</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/9\" class=\"page-numbers\">9</a><a href=\"https://www.instaclustr.com/category/technical/cassandra/page/2\" class=\"next\">Next</a></p></div></div></div></div></header></section></main><aside id=\"contact\"><div class=\"container\"><div class=\"row\"><div class=\"col-lg-3 col-sm-5\"><p></p><h2 class=\"h1\">Get in touch <br />with Instaclustr</h2></div></div></div></aside><p class=\"credit\">Site by <a title=\"Swell Design Group\" href=\"http://swelldesigngroup.com/?utm_campaign=instaclustr\" target=\"_blank\">Swell Design Group</a></p>\n<noscript></noscript>",
        "created_at": "2018-09-13T15:03:44+0000",
        "updated_at": "2018-09-13T15:03:47+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12160"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          }
        ],
        "is_public": false,
        "id": 12159,
        "uid": null,
        "title": "A Benevolent Hacker Is Warning Owners of Unsecured Cassandra Databases",
        "url": "https://www.bleepingcomputer.com/news/security/a-benevolent-hacker-is-warning-owners-of-unsecured-cassandra-databases/",
        "content": "<p><img alt=\"Cassandra logo\" height=\"465\" src=\"https://www.bleepstatic.com/content/hl-images/2017/01/24/Cassandra_logo.jpg\" width=\"1250\" class=\"b-lazy\" /></p><p>An unknown hacker is accessing public and unsecured Apache Cassandra databases and adding an extra table through which it warns server owners that their DB was left exposed to online attacks.</p><p>The first cases of Cassandra databases with this extra table were spotted by a Twitter user that goes by the nickname of  <a href=\"https://twitter.com/dk_effect\" target=\"_blank\" rel=\"nofollow\">DunningKrugerEffect</a>.</p><p>The name of this table is \"<strong>your_db_is_not_secure</strong>,\" and the table doesn't hold any type of information inside.</p><p>The purpose of this table is to warn Cassandra owners that their database can be very easily held for ransom in the upcoming few days if left online unprotected. According to Shodan, there are currently over 2,600 Cassandra database instances left accessible online.</p><p>Since the start of the year, multiple criminal groups have been hijacking database servers left unprotected online, wiping data and requesting a ransom payment.</p><p>First attacks hit <a href=\"https://www.bleepingcomputer.com/news/security/mongodb-apocalypse-professional-ransomware-group-gets-involved-infections-reach-28k-servers/\" target=\"_blank\">MongoDB servers</a> and were quickly followed by attacks against <a href=\"https://www.bleepingcomputer.com/news/security/mongodb-hijackers-move-on-to-elasticsearch-servers/\" target=\"_blank\">ElasticSearch clusters</a>, <a href=\"https://www.bleepingcomputer.com/news/security/database-ransom-attacks-hit-couchdb-and-hadoop-servers/\" target=\"_blank\">Hadoop servers and CouchDB databases</a>.</p><p>All previous attacks have been tracked by Victor Gevers and other members of the GDI.foundation, who created spreadsheets that keep track of ongoing attacks.</p><p>One such spreadsheet is available for Cassandra attacks. These are the latest statistics regarding database ransom attacks:</p><ul><li><a href=\"https://docs.google.com/spreadsheets/d/1QonE9oeMOQHVh8heFIyeqrjfKEViL0poLnY8mAakKhM/edit#gid=1781677175\" target=\"_blank\" rel=\"nofollow\">MongoDB</a> - 40,291 servers</li>\n<li><a href=\"https://docs.google.com/spreadsheets/d/1-txUnn6HFuETgN1mQwI4g_Rk5SY0LEkjgOX8sG0KaFY/edit#gid=0\" target=\"_blank\" rel=\"nofollow\">ElasticSearch</a> - 5,044 servers</li>\n<li><a href=\"https://docs.google.com/spreadsheets/d/18-zmpzp87TX9oIbLwChJ3Fn0ldCGysSm-aoje_VvSSc/edit#gid=0\" target=\"_blank\" rel=\"nofollow\">Apache Hadoop</a> - 186 servers</li>\n<li><a href=\"https://docs.google.com/spreadsheets/d/1iO8nINe1Ia2s40byeOj8BRiXZMpiBkKGJR5AuV7EExY/edit#gid=0\" target=\"_blank\" rel=\"nofollow\">Apache CouchDB</a> - 452 servers</li>\n<li><a href=\"https://docs.google.com/spreadsheets/d/1D8dSqMJuWSzLiwHaf4caSNYkcNpjhmwCW7_Z4QNJHVM/edit#gid=0\" target=\"_blank\" rel=\"nofollow\">Apache Cassandra</a> - 49 servers</li>\n</ul><p>Currently, multiple members of the GDI.foundation \"have been investigating these cases deploying honeypots and getting intel on the attacks,\" Gevers tells Bleeping Computer.</p><p>The GDI.foundation has also been working with local CERT teams and attempting to notify database owners before attackers hijack their data. Despite this, very few server owners heeded their warnings, with many servers still remaining unsecured.</p><p>If you're wondering what other database servers attackers could hit, there are Neo4J, Riak, or Redis systems that have not yet been targeted by these types of ransom attempts.</p>",
        "created_at": "2018-09-13T14:58:58+0000",
        "updated_at": "2018-09-13T14:59:02+0000",
        "published_at": "2017-01-24T20:00:09+0000",
        "published_by": [
          "Catalin Cimpanu"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "www.bleepingcomputer.com",
        "preview_picture": "https://www.bleepstatic.com/content/hl-images/2017/01/24/Cassandra_logo.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12159"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          }
        ],
        "is_public": false,
        "id": 12158,
        "uid": null,
        "title": "Apache Cassandra Security - Instaclustr",
        "url": "https://www.instaclustr.com/apache-cassandra-security/",
        "content": "<header id=\"page-masthead\"><div id=\"page-content\"><div class=\"container\"><div class=\"row\"><div class=\"col-md-6 col-md-offset-0 col-sm-8 col-sm-offset-2\"><div class=\"primary first-p-bold\"><p><a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a> security has hit the new lately with a benevolent hacker attempting to warn owners of unsecured Cassandra databases about their exposure (see <a href=\"https://www.bleepingcomputer.com/news/security/a-benevolent-hacker-is-warning-owners-of-unsecured-cassandra-databases/\">https://www.bleepingcomputer.com/news/security/a-benevolent-hacker-is-warning-owners-of-unsecured-cassandra-databases/</a>).</p><p>At Instaclustr, we take security very seriously. We were confident that our default configurations would not allow access to this type of scan. We have used our central management system to check all clusters that we currently have under management for the presence of the tell-tale keyspace and confirm that none of our managed clusters had been detected by the scan.</p><p>Of course, not being picked up by some random, external scan is no guarantee of security so it’s worth re-capping some of the things we do at Instaclustr to make it easy to maximise the security of your cluster:</p><ul><li>The use of TLS (SSL) and password authentication to connect to Cassandra can be configured with the click of a check box at cluster creation. We even generate sample code for connecting to the cluster to make it as easy as possible.</li> <li>Firewall rules block all access to the cluster by default with exception added at the control of the cluster owner through our console.</li> <li>We support VPC peering and the use of private IPs to minimise public access points through the firewall.</li> <li>We disable access by the default Cassandra user, preventing any attacks using this well-known user.</li> <li>We regularly commission external penetration tests of our clusters and other components of our system.</li> </ul><p>In addition to these current measures, we have a continuing focus on enhanced security technology and processes which benefit all of our customers as they become available. For example, current engineering initiatives include enhanced intrusion detection across all components of our system and additional security certifications.</p><p>At Instaclustr, we’re proud of our capability, focus and record when it comes to security. While it’s not an area that we often talk about publicly we’re more than happy to go into details of our approach with any customers or potential customers – just contact us to set up a chat.</p></div></div></div></div></div></header>",
        "created_at": "2018-09-13T14:58:27+0000",
        "updated_at": "2018-09-13T14:58:31+0000",
        "published_at": "2017-01-31T07:00:57+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12158"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12157,
        "uid": null,
        "title": "Apache Cassandra Documentation : Cassandra Config",
        "url": "http://cassandra.apache.org/doc/latest/configuration/cassandra_config_file.html",
        "content": "<div class=\"section\" id=\"cluster-name\"><h2><code class=\"docutils literal\">cluster_name</code></h2><p>The name of the cluster. This is mainly used to prevent machines in\none logical cluster from joining another.</p><p><em>Default Value:</em> ‘Test Cluster’</p></div><div class=\"section\" id=\"num-tokens\"><h2><code class=\"docutils literal\">num_tokens</code></h2><p>This defines the number of tokens randomly assigned to this node on the ring\nThe more tokens, relative to other nodes, the larger the proportion of data\nthat this node will store. You probably want all nodes to have the same number\nof tokens assuming they have equal hardware capability.</p><p>If you leave this unspecified, Cassandra will use the default of 1 token for legacy compatibility,\nand will use the initial_token as described below.</p><p>Specifying initial_token will override this setting on the node’s initial start,\non subsequent starts, this setting will apply even if initial token is set.</p><p>If you already have a cluster with 1 token per node, and wish to migrate to\nmultiple tokens per node, see <a class=\"reference external\" href=\"http://wiki.apache.org/cassandra/Operations\">http://wiki.apache.org/cassandra/Operations</a></p><p><em>Default Value:</em> 256</p></div><div class=\"section\" id=\"allocate-tokens-for-keyspace\"><h2><code class=\"docutils literal\">allocate_tokens_for_keyspace</code></h2><p><em>This option is commented out by default.</em></p><p>Triggers automatic allocation of num_tokens tokens for this node. The allocation\nalgorithm attempts to choose tokens in a way that optimizes replicated load over\nthe nodes in the datacenter for the replication strategy used by the specified\nkeyspace.</p><p>The load assigned to each node will be close to proportional to its number of\nvnodes.</p><p>Only supported with the Murmur3Partitioner.</p><p><em>Default Value:</em> KEYSPACE</p></div><div class=\"section\" id=\"initial-token\"><h2><code class=\"docutils literal\">initial_token</code></h2><p><em>This option is commented out by default.</em></p><p>initial_token allows you to specify tokens manually.  While you can use it with\nvnodes (num_tokens &gt; 1, above) – in which case you should provide a\ncomma-separated list – it’s primarily used when adding nodes to legacy clusters\nthat do not have vnodes enabled.</p></div><div class=\"section\" id=\"hinted-handoff-enabled\"><h2><code class=\"docutils literal\">hinted_handoff_enabled</code></h2><p>See <a class=\"reference external\" href=\"http://wiki.apache.org/cassandra/HintedHandoff\">http://wiki.apache.org/cassandra/HintedHandoff</a>\nMay either be “true” or “false” to enable globally</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"hinted-handoff-disabled-datacenters\"><h2><code class=\"docutils literal\">hinted_handoff_disabled_datacenters</code></h2><p><em>This option is commented out by default.</em></p><p>When hinted_handoff_enabled is true, a black list of data centers that will not\nperform hinted handoff</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>#    - DC1\n#    - DC2\n</pre></div></div></div><div class=\"section\" id=\"max-hint-window-in-ms\"><h2><code class=\"docutils literal\">max_hint_window_in_ms</code></h2><p>this defines the maximum amount of time a dead host will have hints\ngenerated.  After it has been dead this long, new hints for it will not be\ncreated until it has been seen alive and gone down again.</p><p><em>Default Value:</em> 10800000 # 3 hours</p></div><div class=\"section\" id=\"hinted-handoff-throttle-in-kb\"><h2><code class=\"docutils literal\">hinted_handoff_throttle_in_kb</code></h2><p>Maximum throttle in KBs per second, per delivery thread.  This will be\nreduced proportionally to the number of nodes in the cluster.  (If there\nare two nodes in the cluster, each delivery thread will use the maximum\nrate; if there are three, each will throttle to half of the maximum,\nsince we expect two nodes to be delivering hints simultaneously.)</p><p><em>Default Value:</em> 1024</p></div><div class=\"section\" id=\"max-hints-delivery-threads\"><h2><code class=\"docutils literal\">max_hints_delivery_threads</code></h2><p>Number of threads with which to deliver hints;\nConsider increasing this number when you have multi-dc deployments, since\ncross-dc handoff tends to be slower</p><p><em>Default Value:</em> 2</p></div><div class=\"section\" id=\"hints-directory\"><h2><code class=\"docutils literal\">hints_directory</code></h2><p><em>This option is commented out by default.</em></p><p>Directory where Cassandra should store hints.\nIf not set, the default directory is $CASSANDRA_HOME/data/hints.</p><p><em>Default Value:</em>  /var/lib/cassandra/hints</p></div><div class=\"section\" id=\"hints-flush-period-in-ms\"><h2><code class=\"docutils literal\">hints_flush_period_in_ms</code></h2><p>How often hints should be flushed from the internal buffers to disk.\nWill <em>not</em> trigger fsync.</p><p><em>Default Value:</em> 10000</p></div><div class=\"section\" id=\"max-hints-file-size-in-mb\"><h2><code class=\"docutils literal\">max_hints_file_size_in_mb</code></h2><p>Maximum size for a single hints file, in megabytes.</p><p><em>Default Value:</em> 128</p></div><div class=\"section\" id=\"hints-compression\"><h2><code class=\"docutils literal\">hints_compression</code></h2><p><em>This option is commented out by default.</em></p><p>Compression to apply to the hint files. If omitted, hints files\nwill be written uncompressed. LZ4, Snappy, and Deflate compressors\nare supported.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n</pre></div></div></div><div class=\"section\" id=\"batchlog-replay-throttle-in-kb\"><h2><code class=\"docutils literal\">batchlog_replay_throttle_in_kb</code></h2><p>Maximum throttle in KBs per second, total. This will be\nreduced proportionally to the number of nodes in the cluster.</p><p><em>Default Value:</em> 1024</p></div><div class=\"section\" id=\"authenticator\"><h2><code class=\"docutils literal\">authenticator</code></h2><p>Authentication backend, implementing IAuthenticator; used to identify users\nOut of the box, Cassandra provides org.apache.cassandra.auth.{AllowAllAuthenticator,\nPasswordAuthenticator}.</p><ul class=\"simple\"><li>AllowAllAuthenticator performs no checks - set it to disable authentication.</li>\n<li>PasswordAuthenticator relies on username/password pairs to authenticate\nusers. It keeps usernames and hashed passwords in system_auth.roles table.\nPlease increase system_auth keyspace replication factor if you use this authenticator.\nIf using PasswordAuthenticator, CassandraRoleManager must also be used (see below)</li>\n</ul><p><em>Default Value:</em> AllowAllAuthenticator</p></div><div class=\"section\" id=\"role-manager\"><h2><code class=\"docutils literal\">role_manager</code></h2><p>Part of the Authentication &amp; Authorization backend, implementing IRoleManager; used\nto maintain grants and memberships between roles.\nOut of the box, Cassandra provides org.apache.cassandra.auth.CassandraRoleManager,\nwhich stores role information in the system_auth keyspace. Most functions of the\nIRoleManager require an authenticated login, so unless the configured IAuthenticator\nactually implements authentication, most of this functionality will be unavailable.</p><ul class=\"simple\"><li>CassandraRoleManager stores role data in the system_auth keyspace. Please\nincrease system_auth keyspace replication factor if you use this role manager.</li>\n</ul><p><em>Default Value:</em> CassandraRoleManager</p></div><div class=\"section\" id=\"roles-validity-in-ms\"><h2><code class=\"docutils literal\">roles_validity_in_ms</code></h2><p>Validity period for roles cache (fetching granted roles can be an expensive\noperation depending on the role manager, CassandraRoleManager is one example)\nGranted roles are cached for authenticated sessions in AuthenticatedUser and\nafter the period specified here, become eligible for (async) reload.\nDefaults to 2000, set to 0 to disable caching entirely.\nWill be disabled automatically for AllowAllAuthenticator.</p><p><em>Default Value:</em> 2000</p></div><div class=\"section\" id=\"permissions-validity-in-ms\"><h2><code class=\"docutils literal\">permissions_validity_in_ms</code></h2><p>Validity period for permissions cache (fetching permissions can be an\nexpensive operation depending on the authorizer, CassandraAuthorizer is\none example). Defaults to 2000, set to 0 to disable.\nWill be disabled automatically for AllowAllAuthorizer.</p><p><em>Default Value:</em> 2000</p></div><div class=\"section\" id=\"credentials-validity-in-ms\"><h2><code class=\"docutils literal\">credentials_validity_in_ms</code></h2><p>Validity period for credentials cache. This cache is tightly coupled to\nthe provided PasswordAuthenticator implementation of IAuthenticator. If\nanother IAuthenticator implementation is configured, this cache will not\nbe automatically used and so the following settings will have no effect.\nPlease note, credentials are cached in their encrypted form, so while\nactivating this cache may reduce the number of queries made to the\nunderlying table, it may not  bring a significant reduction in the\nlatency of individual authentication attempts.\nDefaults to 2000, set to 0 to disable credentials caching.</p><p><em>Default Value:</em> 2000</p></div><div class=\"section\" id=\"partitioner\"><h2><code class=\"docutils literal\">partitioner</code></h2><p>The partitioner is responsible for distributing groups of rows (by\npartition key) across nodes in the cluster.  You should leave this\nalone for new clusters.  The partitioner can NOT be changed without\nreloading all data, so when upgrading you should set this to the\nsame partitioner you were already using.</p><p>Besides Murmur3Partitioner, partitioners included for backwards\ncompatibility include RandomPartitioner, ByteOrderedPartitioner, and\nOrderPreservingPartitioner.</p><p><em>Default Value:</em> org.apache.cassandra.dht.Murmur3Partitioner</p></div><div class=\"section\" id=\"data-file-directories\"><h2><code class=\"docutils literal\">data_file_directories</code></h2><p><em>This option is commented out by default.</em></p><p>Directories where Cassandra should store data on disk.  Cassandra\nwill spread data evenly across them, subject to the granularity of\nthe configured compaction strategy.\nIf not set, the default directory is $CASSANDRA_HOME/data/data.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>#     - /var/lib/cassandra/data\n</pre></div></div></div><div class=\"section\" id=\"commitlog-directory\"><h2><code class=\"docutils literal\">commitlog_directory</code></h2><p><em>This option is commented out by default.</em>\ncommit log.  when running on magnetic HDD, this should be a\nseparate spindle than the data directories.\nIf not set, the default directory is $CASSANDRA_HOME/data/commitlog.</p><p><em>Default Value:</em>  /var/lib/cassandra/commitlog</p></div><div class=\"section\" id=\"cdc-enabled\"><h2><code class=\"docutils literal\">cdc_enabled</code></h2><p>Enable / disable CDC functionality on a per-node basis. This modifies the logic used\nfor write path allocation rejection (standard: never reject. cdc: reject Mutation\ncontaining a CDC-enabled table if at space limit in cdc_raw_directory).</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"cdc-raw-directory\"><h2><code class=\"docutils literal\">cdc_raw_directory</code></h2><p><em>This option is commented out by default.</em></p><p>CommitLogSegments are moved to this directory on flush if cdc_enabled: true and the\nsegment contains mutations for a CDC-enabled table. This should be placed on a\nseparate spindle than the data directories. If not set, the default directory is\n$CASSANDRA_HOME/data/cdc_raw.</p><p><em>Default Value:</em>  /var/lib/cassandra/cdc_raw</p></div><div class=\"section\" id=\"disk-failure-policy\"><h2><code class=\"docutils literal\">disk_failure_policy</code></h2><p>Policy for data disk failures:</p><dl class=\"docutils\"><dt>die</dt>\n<dd>shut down gossip and client transports and kill the JVM for any fs errors or\nsingle-sstable errors, so the node can be replaced.</dd>\n<dt>stop_paranoid</dt>\n<dd>shut down gossip and client transports even for single-sstable errors,\nkill the JVM for errors during startup.</dd>\n<dt>stop</dt>\n<dd>shut down gossip and client transports, leaving the node effectively dead, but\ncan still be inspected via JMX, kill the JVM for errors during startup.</dd>\n<dt>best_effort</dt>\n<dd>stop using the failed disk and respond to requests based on\nremaining available sstables.  This means you WILL see obsolete\ndata at CL.ONE!</dd>\n<dt>ignore</dt>\n<dd>ignore fatal errors and let requests fail, as in pre-1.2 Cassandra</dd>\n</dl><p><em>Default Value:</em> stop</p></div><div class=\"section\" id=\"commit-failure-policy\"><h2><code class=\"docutils literal\">commit_failure_policy</code></h2><p>Policy for commit disk failures:</p><dl class=\"docutils\"><dt>die</dt>\n<dd>shut down the node and kill the JVM, so the node can be replaced.</dd>\n<dt>stop</dt>\n<dd>shut down the node, leaving the node effectively dead, but\ncan still be inspected via JMX.</dd>\n<dt>stop_commit</dt>\n<dd>shutdown the commit log, letting writes collect but\ncontinuing to service reads, as in pre-2.0.5 Cassandra</dd>\n<dt>ignore</dt>\n<dd>ignore fatal errors and let the batches fail</dd>\n</dl><p><em>Default Value:</em> stop</p></div><div class=\"section\" id=\"key-cache-size-in-mb\"><h2><code class=\"docutils literal\">key_cache_size_in_mb</code></h2><p>Maximum size of the key cache in memory.</p><p>Each key cache hit saves 1 seek and each row cache hit saves 2 seeks at the\nminimum, sometimes more. The key cache is fairly tiny for the amount of\ntime it saves, so it’s worthwhile to use it at large numbers.\nThe row cache saves even more time, but must contain the entire row,\nso it is extremely space-intensive. It’s best to only use the\nrow cache if you have hot rows or static rows.</p><p>NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.</p><p>Default value is empty to make it “auto” (min(5% of Heap (in MB), 100MB)). Set to 0 to disable key cache.</p></div><div class=\"section\" id=\"key-cache-save-period\"><h2><code class=\"docutils literal\">key_cache_save_period</code></h2><p>Duration in seconds after which Cassandra should\nsave the key cache. Caches are saved to saved_caches_directory as\nspecified in this configuration file.</p><p>Saved caches greatly improve cold-start speeds, and is relatively cheap in\nterms of I/O for the key cache. Row cache saving is much more expensive and\nhas limited use.</p><p>Default is 14400 or 4 hours.</p><p><em>Default Value:</em> 14400</p></div><div class=\"section\" id=\"key-cache-keys-to-save\"><h2><code class=\"docutils literal\">key_cache_keys_to_save</code></h2><p><em>This option is commented out by default.</em></p><p>Number of keys from the key cache to save\nDisabled by default, meaning all keys are going to be saved</p><p><em>Default Value:</em> 100</p></div><div class=\"section\" id=\"row-cache-class-name\"><h2><code class=\"docutils literal\">row_cache_class_name</code></h2><p><em>This option is commented out by default.</em></p><p>Row cache implementation class name. Available implementations:</p><dl class=\"docutils\"><dt>org.apache.cassandra.cache.OHCProvider</dt>\n<dd>Fully off-heap row cache implementation (default).</dd>\n<dt>org.apache.cassandra.cache.SerializingCacheProvider</dt>\n<dd>This is the row cache implementation availabile\nin previous releases of Cassandra.</dd>\n</dl><p><em>Default Value:</em> org.apache.cassandra.cache.OHCProvider</p></div><div class=\"section\" id=\"row-cache-size-in-mb\"><h2><code class=\"docutils literal\">row_cache_size_in_mb</code></h2><p>Maximum size of the row cache in memory.\nPlease note that OHC cache implementation requires some additional off-heap memory to manage\nthe map structures and some in-flight memory during operations before/after cache entries can be\naccounted against the cache capacity. This overhead is usually small compared to the whole capacity.\nDo not specify more memory that the system can afford in the worst usual situation and leave some\nheadroom for OS block level cache. Do never allow your system to swap.</p><p>Default value is 0, to disable row caching.</p><p><em>Default Value:</em> 0</p></div><div class=\"section\" id=\"row-cache-save-period\"><h2><code class=\"docutils literal\">row_cache_save_period</code></h2><p>Duration in seconds after which Cassandra should save the row cache.\nCaches are saved to saved_caches_directory as specified in this configuration file.</p><p>Saved caches greatly improve cold-start speeds, and is relatively cheap in\nterms of I/O for the key cache. Row cache saving is much more expensive and\nhas limited use.</p><p>Default is 0 to disable saving the row cache.</p><p><em>Default Value:</em> 0</p></div><div class=\"section\" id=\"row-cache-keys-to-save\"><h2><code class=\"docutils literal\">row_cache_keys_to_save</code></h2><p><em>This option is commented out by default.</em></p><p>Number of keys from the row cache to save.\nSpecify 0 (which is the default), meaning all keys are going to be saved</p><p><em>Default Value:</em> 100</p></div><div class=\"section\" id=\"counter-cache-size-in-mb\"><h2><code class=\"docutils literal\">counter_cache_size_in_mb</code></h2><p>Maximum size of the counter cache in memory.</p><p>Counter cache helps to reduce counter locks’ contention for hot counter cells.\nIn case of RF = 1 a counter cache hit will cause Cassandra to skip the read before\nwrite entirely. With RF &gt; 1 a counter cache hit will still help to reduce the duration\nof the lock hold, helping with hot counter cell updates, but will not allow skipping\nthe read entirely. Only the local (clock, count) tuple of a counter cell is kept\nin memory, not the whole counter, so it’s relatively cheap.</p><p>NOTE: if you reduce the size, you may not get you hottest keys loaded on startup.</p><p>Default value is empty to make it “auto” (min(2.5% of Heap (in MB), 50MB)). Set to 0 to disable counter cache.\nNOTE: if you perform counter deletes and rely on low gcgs, you should disable the counter cache.</p></div><div class=\"section\" id=\"counter-cache-save-period\"><h2><code class=\"docutils literal\">counter_cache_save_period</code></h2><p>Duration in seconds after which Cassandra should\nsave the counter cache (keys only). Caches are saved to saved_caches_directory as\nspecified in this configuration file.</p><p>Default is 7200 or 2 hours.</p><p><em>Default Value:</em> 7200</p></div><div class=\"section\" id=\"counter-cache-keys-to-save\"><h2><code class=\"docutils literal\">counter_cache_keys_to_save</code></h2><p><em>This option is commented out by default.</em></p><p>Number of keys from the counter cache to save\nDisabled by default, meaning all keys are going to be saved</p><p><em>Default Value:</em> 100</p></div><div class=\"section\" id=\"saved-caches-directory\"><h2><code class=\"docutils literal\">saved_caches_directory</code></h2><p><em>This option is commented out by default.</em></p><p>saved caches\nIf not set, the default directory is $CASSANDRA_HOME/data/saved_caches.</p><p><em>Default Value:</em>  /var/lib/cassandra/saved_caches</p></div><div class=\"section\" id=\"commitlog-sync\"><h2><code class=\"docutils literal\">commitlog_sync</code></h2><p><em>This option is commented out by default.</em></p><p>commitlog_sync may be either “periodic” or “batch.”</p><p>When in batch mode, Cassandra won’t ack writes until the commit log\nhas been fsynced to disk.  It will wait\ncommitlog_sync_batch_window_in_ms milliseconds between fsyncs.\nThis window should be kept short because the writer threads will\nbe unable to do extra work while waiting.  (You may need to increase\nconcurrent_writes for the same reason.)</p><p><em>Default Value:</em> batch</p></div><div class=\"section\" id=\"commitlog-sync-batch-window-in-ms\"><h2><code class=\"docutils literal\">commitlog_sync_batch_window_in_ms</code></h2><p><em>This option is commented out by default.</em></p><p><em>Default Value:</em> 2</p></div><div class=\"section\" id=\"id1\"><h2><code class=\"docutils literal\">commitlog_sync</code></h2><p>the other option is “periodic” where writes may be acked immediately\nand the CommitLog is simply synced every commitlog_sync_period_in_ms\nmilliseconds.</p><p><em>Default Value:</em> periodic</p></div><div class=\"section\" id=\"commitlog-sync-period-in-ms\"><h2><code class=\"docutils literal\">commitlog_sync_period_in_ms</code></h2><p><em>Default Value:</em> 10000</p></div><div class=\"section\" id=\"commitlog-segment-size-in-mb\"><h2><code class=\"docutils literal\">commitlog_segment_size_in_mb</code></h2><p>The size of the individual commitlog file segments.  A commitlog\nsegment may be archived, deleted, or recycled once all the data\nin it (potentially from each columnfamily in the system) has been\nflushed to sstables.</p><p>The default size is 32, which is almost always fine, but if you are\narchiving commitlog segments (see commitlog_archiving.properties),\nthen you probably want a finer granularity of archiving; 8 or 16 MB\nis reasonable.\nMax mutation size is also configurable via max_mutation_size_in_kb setting in\ncassandra.yaml. The default is half the size commitlog_segment_size_in_mb * 1024.</p><p>NOTE: If max_mutation_size_in_kb is set explicitly then commitlog_segment_size_in_mb must\nbe set to at least twice the size of max_mutation_size_in_kb / 1024</p><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"commitlog-compression\"><h2><code class=\"docutils literal\">commitlog_compression</code></h2><p><em>This option is commented out by default.</em></p><p>Compression to apply to the commit log. If omitted, the commit log\nwill be written uncompressed.  LZ4, Snappy, and Deflate compressors\nare supported.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>#   - class_name: LZ4Compressor\n#     parameters:\n#         -\n</pre></div></div></div><div class=\"section\" id=\"seed-provider\"><h2><code class=\"docutils literal\">seed_provider</code></h2><p>any class that implements the SeedProvider interface and has a\nconstructor that takes a Map&lt;String, String&gt; of parameters will do.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre># Addresses of hosts that are deemed contact points.\n# Cassandra nodes use this list of hosts to find each other and learn\n# the topology of the ring.  You must change this if you are running\n# multiple nodes!\n- class_name: org.apache.cassandra.locator.SimpleSeedProvider\n  parameters:\n      # seeds is actually a comma-delimited list of addresses.\n      # Ex: \"&lt;ip1&gt;,&lt;ip2&gt;,&lt;ip3&gt;\"\n      - seeds: \"127.0.0.1\"\n</pre></div></div></div><div class=\"section\" id=\"concurrent-reads\"><h2><code class=\"docutils literal\">concurrent_reads</code></h2><p>For workloads with more data than can fit in memory, Cassandra’s\nbottleneck will be reads that need to fetch data from\ndisk. “concurrent_reads” should be set to (16 * number_of_drives) in\norder to allow the operations to enqueue low enough in the stack\nthat the OS and drives can reorder them. Same applies to\n“concurrent_counter_writes”, since counter writes read the current\nvalues before incrementing and writing them back.</p><p>On the other hand, since writes are almost never IO bound, the ideal\nnumber of “concurrent_writes” is dependent on the number of cores in\nyour system; (8 * number_of_cores) is a good rule of thumb.</p><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"concurrent-writes\"><h2><code class=\"docutils literal\">concurrent_writes</code></h2><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"concurrent-counter-writes\"><h2><code class=\"docutils literal\">concurrent_counter_writes</code></h2><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"concurrent-materialized-view-writes\"><h2><code class=\"docutils literal\">concurrent_materialized_view_writes</code></h2><p>For materialized view writes, as there is a read involved, so this should\nbe limited by the less of concurrent reads or concurrent writes.</p><p><em>Default Value:</em> 32</p></div><div class=\"section\" id=\"file-cache-size-in-mb\"><h2><code class=\"docutils literal\">file_cache_size_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Maximum memory to use for sstable chunk cache and buffer pooling.\n32MB of this are reserved for pooling buffers, the rest is used as an\ncache that holds uncompressed sstable chunks.\nDefaults to the smaller of 1/4 of heap or 512MB. This pool is allocated off-heap,\nso is in addition to the memory allocated for heap. The cache also has on-heap\noverhead which is roughly 128 bytes per chunk (i.e. 0.2% of the reserved size\nif the default 64k chunk size is used).\nMemory is only allocated when needed.</p><p><em>Default Value:</em> 512</p></div><div class=\"section\" id=\"buffer-pool-use-heap-if-exhausted\"><h2><code class=\"docutils literal\">buffer_pool_use_heap_if_exhausted</code></h2><p><em>This option is commented out by default.</em></p><p>Flag indicating whether to allocate on or off heap when the sstable buffer\npool is exhausted, that is when it has exceeded the maximum memory\nfile_cache_size_in_mb, beyond which it will not cache buffers but allocate on request.</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"disk-optimization-strategy\"><h2><code class=\"docutils literal\">disk_optimization_strategy</code></h2><p><em>This option is commented out by default.</em></p><p>The strategy for optimizing disk read\nPossible values are:\nssd (for solid state disks, the default)\nspinning (for spinning disks)</p><p><em>Default Value:</em> ssd</p></div><div class=\"section\" id=\"memtable-heap-space-in-mb\"><h2><code class=\"docutils literal\">memtable_heap_space_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Total permitted memory to use for memtables. Cassandra will stop\naccepting writes when the limit is exceeded until a flush completes,\nand will trigger a flush based on memtable_cleanup_threshold\nIf omitted, Cassandra will set both to 1/4 the size of the heap.</p><p><em>Default Value:</em> 2048</p></div><div class=\"section\" id=\"memtable-offheap-space-in-mb\"><h2><code class=\"docutils literal\">memtable_offheap_space_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p><em>Default Value:</em> 2048</p></div><div class=\"section\" id=\"memtable-cleanup-threshold\"><h2><code class=\"docutils literal\">memtable_cleanup_threshold</code></h2><p><em>This option is commented out by default.</em></p><p>memtable_cleanup_threshold is deprecated. The default calculation\nis the only reasonable choice. See the comments on  memtable_flush_writers\nfor more information.</p><p>Ratio of occupied non-flushing memtable size to total permitted size\nthat will trigger a flush of the largest memtable. Larger mct will\nmean larger flushes and hence less compaction, but also less concurrent\nflush activity which can make it difficult to keep your disks fed\nunder heavy write load.</p><p>memtable_cleanup_threshold defaults to 1 / (memtable_flush_writers + 1)</p><p><em>Default Value:</em> 0.11</p></div><div class=\"section\" id=\"memtable-allocation-type\"><h2><code class=\"docutils literal\">memtable_allocation_type</code></h2><p>Specify the way Cassandra allocates and manages memtable memory.\nOptions are:</p><dl class=\"docutils\"><dt>heap_buffers</dt>\n<dd>on heap nio buffers</dd>\n<dt>offheap_buffers</dt>\n<dd>off heap (direct) nio buffers</dd>\n<dt>offheap_objects</dt>\n<dd>off heap objects</dd>\n</dl><p><em>Default Value:</em> heap_buffers</p></div><div class=\"section\" id=\"commitlog-total-space-in-mb\"><h2><code class=\"docutils literal\">commitlog_total_space_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Total space to use for commit logs on disk.</p><p>If space gets above this value, Cassandra will flush every dirty CF\nin the oldest segment and remove it.  So a small total commitlog space\nwill tend to cause more flush activity on less-active columnfamilies.</p><p>The default value is the smaller of 8192, and 1/4 of the total space\nof the commitlog volume.</p><p><em>Default Value:</em> 8192</p></div><div class=\"section\" id=\"memtable-flush-writers\"><h2><code class=\"docutils literal\">memtable_flush_writers</code></h2><p><em>This option is commented out by default.</em></p><p>This sets the number of memtable flush writer threads per disk\nas well as the total number of memtables that can be flushed concurrently.\nThese are generally a combination of compute and IO bound.</p><p>Memtable flushing is more CPU efficient than memtable ingest and a single thread\ncan keep up with the ingest rate of a whole server on a single fast disk\nuntil it temporarily becomes IO bound under contention typically with compaction.\nAt that point you need multiple flush threads. At some point in the future\nit may become CPU bound all the time.</p><p>You can tell if flushing is falling behind using the MemtablePool.BlockedOnAllocation\nmetric which should be 0, but will be non-zero if threads are blocked waiting on flushing\nto free memory.</p><p>memtable_flush_writers defaults to two for a single data directory.\nThis means that two  memtables can be flushed concurrently to the single data directory.\nIf you have multiple data directories the default is one memtable flushing at a time\nbut the flush will use a thread per data directory so you will get two or more writers.</p><p>Two is generally enough to flush on a fast disk [array] mounted as a single data directory.\nAdding more flush writers will result in smaller more frequent flushes that introduce more\ncompaction overhead.</p><p>There is a direct tradeoff between number of memtables that can be flushed concurrently\nand flush size and frequency. More is not better you just need enough flush writers\nto never stall waiting for flushing to free memory.</p><p><em>Default Value:</em> 2</p></div><div class=\"section\" id=\"cdc-total-space-in-mb\"><h2><code class=\"docutils literal\">cdc_total_space_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Total space to use for change-data-capture logs on disk.</p><p>If space gets above this value, Cassandra will throw WriteTimeoutException\non Mutations including tables with CDC enabled. A CDCCompactor is responsible\nfor parsing the raw CDC logs and deleting them when parsing is completed.</p><p>The default value is the min of 4096 mb and 1/8th of the total space\nof the drive where cdc_raw_directory resides.</p><p><em>Default Value:</em> 4096</p></div><div class=\"section\" id=\"cdc-free-space-check-interval-ms\"><h2><code class=\"docutils literal\">cdc_free_space_check_interval_ms</code></h2><p><em>This option is commented out by default.</em></p><p>When we hit our cdc_raw limit and the CDCCompactor is either running behind\nor experiencing backpressure, we check at the following interval to see if any\nnew space for cdc-tracked tables has been made available. Default to 250ms</p><p><em>Default Value:</em> 250</p></div><div class=\"section\" id=\"index-summary-capacity-in-mb\"><h2><code class=\"docutils literal\">index_summary_capacity_in_mb</code></h2><p>A fixed memory pool size in MB for for SSTable index summaries. If left\nempty, this will default to 5% of the heap size. If the memory usage of\nall index summaries exceeds this limit, SSTables with low read rates will\nshrink their index summaries in order to meet this limit.  However, this\nis a best-effort process. In extreme conditions Cassandra may need to use\nmore than this amount of memory.</p></div><div class=\"section\" id=\"index-summary-resize-interval-in-minutes\"><h2><code class=\"docutils literal\">index_summary_resize_interval_in_minutes</code></h2><p>How frequently index summaries should be resampled.  This is done\nperiodically to redistribute memory from the fixed-size pool to sstables\nproportional their recent read rates.  Setting to -1 will disable this\nprocess, leaving existing index summaries at their current sampling level.</p><p><em>Default Value:</em> 60</p></div><div class=\"section\" id=\"trickle-fsync\"><h2><code class=\"docutils literal\">trickle_fsync</code></h2><p>Whether to, when doing sequential writing, fsync() at intervals in\norder to force the operating system to flush the dirty\nbuffers. Enable this to avoid sudden dirty buffer flushing from\nimpacting read latencies. Almost always a good idea on SSDs; not\nnecessarily on platters.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"trickle-fsync-interval-in-kb\"><h2><code class=\"docutils literal\">trickle_fsync_interval_in_kb</code></h2><p><em>Default Value:</em> 10240</p></div><div class=\"section\" id=\"storage-port\"><h2><code class=\"docutils literal\">storage_port</code></h2><p>TCP port, for commands and data\nFor security reasons, you should not expose this port to the internet.  Firewall it if needed.</p><p><em>Default Value:</em> 7000</p></div><div class=\"section\" id=\"ssl-storage-port\"><h2><code class=\"docutils literal\">ssl_storage_port</code></h2><p>SSL port, for encrypted communication.  Unused unless enabled in\nencryption_options\nFor security reasons, you should not expose this port to the internet.  Firewall it if needed.</p><p><em>Default Value:</em> 7001</p></div><div class=\"section\" id=\"listen-address\"><h2><code class=\"docutils literal\">listen_address</code></h2><p>Address or interface to bind to and tell other Cassandra nodes to connect to.\nYou _must_ change this if you want multiple nodes to be able to communicate!</p><p>Set listen_address OR listen_interface, not both.</p><p>Leaving it blank leaves it up to InetAddress.getLocalHost(). This\nwill always do the Right Thing _if_ the node is properly configured\n(hostname, name resolution, etc), and the Right Thing is to use the\naddress associated with the hostname (it might not be).</p><p>Setting listen_address to 0.0.0.0 is always wrong.</p><p><em>Default Value:</em> localhost</p></div><div class=\"section\" id=\"listen-interface\"><h2><code class=\"docutils literal\">listen_interface</code></h2><p><em>This option is commented out by default.</em></p><p>Set listen_address OR listen_interface, not both. Interfaces must correspond\nto a single address, IP aliasing is not supported.</p><p><em>Default Value:</em> eth0</p></div><div class=\"section\" id=\"listen-interface-prefer-ipv6\"><h2><code class=\"docutils literal\">listen_interface_prefer_ipv6</code></h2><p><em>This option is commented out by default.</em></p><p>If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\nyou can specify which should be chosen using listen_interface_prefer_ipv6. If false the first ipv4\naddress will be used. If true the first ipv6 address will be used. Defaults to false preferring\nipv4. If there is only one address it will be selected regardless of ipv4/ipv6.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"broadcast-address\"><h2><code class=\"docutils literal\">broadcast_address</code></h2><p><em>This option is commented out by default.</em></p><p>Address to broadcast to other Cassandra nodes\nLeaving this blank will set it to the same value as listen_address</p><p><em>Default Value:</em> 1.2.3.4</p></div><div class=\"section\" id=\"listen-on-broadcast-address\"><h2><code class=\"docutils literal\">listen_on_broadcast_address</code></h2><p><em>This option is commented out by default.</em></p><p>When using multiple physical network interfaces, set this\nto true to listen on broadcast_address in addition to\nthe listen_address, allowing nodes to communicate in both\ninterfaces.\nIgnore this property if the network configuration automatically\nroutes  between the public and private networks such as EC2.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"internode-authenticator\"><h2><code class=\"docutils literal\">internode_authenticator</code></h2><p><em>This option is commented out by default.</em></p><p>Internode authentication backend, implementing IInternodeAuthenticator;\nused to allow/disallow connections from peer nodes.</p><p><em>Default Value:</em> org.apache.cassandra.auth.AllowAllInternodeAuthenticator</p></div><div class=\"section\" id=\"start-native-transport\"><h2><code class=\"docutils literal\">start_native_transport</code></h2><p>Whether to start the native transport server.\nThe address on which the native transport is bound is defined by rpc_address.</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"native-transport-port\"><h2><code class=\"docutils literal\">native_transport_port</code></h2><p>port for the CQL native transport to listen for clients on\nFor security reasons, you should not expose this port to the internet.  Firewall it if needed.</p><p><em>Default Value:</em> 9042</p></div><div class=\"section\" id=\"native-transport-port-ssl\"><h2><code class=\"docutils literal\">native_transport_port_ssl</code></h2><p><em>This option is commented out by default.</em>\nEnabling native transport encryption in client_encryption_options allows you to either use\nencryption for the standard port or to use a dedicated, additional port along with the unencrypted\nstandard native_transport_port.\nEnabling client encryption and keeping native_transport_port_ssl disabled will use encryption\nfor native_transport_port. Setting native_transport_port_ssl to a different value\nfrom native_transport_port will use encryption for native_transport_port_ssl while\nkeeping native_transport_port unencrypted.</p><p><em>Default Value:</em> 9142</p></div><div class=\"section\" id=\"native-transport-max-threads\"><h2><code class=\"docutils literal\">native_transport_max_threads</code></h2><p><em>This option is commented out by default.</em>\nThe maximum threads for handling requests (note that idle threads are stopped\nafter 30 seconds so there is not corresponding minimum setting).</p><p><em>Default Value:</em> 128</p></div><div class=\"section\" id=\"native-transport-max-frame-size-in-mb\"><h2><code class=\"docutils literal\">native_transport_max_frame_size_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>The maximum size of allowed frame. Frame (requests) larger than this will\nbe rejected as invalid. The default is 256MB. If you’re changing this parameter,\nyou may want to adjust max_value_size_in_mb accordingly.</p><p><em>Default Value:</em> 256</p></div><div class=\"section\" id=\"native-transport-max-concurrent-connections\"><h2><code class=\"docutils literal\">native_transport_max_concurrent_connections</code></h2><p><em>This option is commented out by default.</em></p><p>The maximum number of concurrent client connections.\nThe default is -1, which means unlimited.</p><p><em>Default Value:</em> -1</p></div><div class=\"section\" id=\"native-transport-max-concurrent-connections-per-ip\"><h2><code class=\"docutils literal\">native_transport_max_concurrent_connections_per_ip</code></h2><p><em>This option is commented out by default.</em></p><p>The maximum number of concurrent client connections per source ip.\nThe default is -1, which means unlimited.</p><p><em>Default Value:</em> -1</p></div><div class=\"section\" id=\"rpc-address\"><h2><code class=\"docutils literal\">rpc_address</code></h2><p>The address or interface to bind the native transport server to.</p><p>Set rpc_address OR rpc_interface, not both.</p><p>Leaving rpc_address blank has the same effect as on listen_address\n(i.e. it will be based on the configured hostname of the node).</p><p>Note that unlike listen_address, you can specify 0.0.0.0, but you must also\nset broadcast_rpc_address to a value other than 0.0.0.0.</p><p>For security reasons, you should not expose this port to the internet.  Firewall it if needed.</p><p><em>Default Value:</em> localhost</p></div><div class=\"section\" id=\"rpc-interface\"><h2><code class=\"docutils literal\">rpc_interface</code></h2><p><em>This option is commented out by default.</em></p><p>Set rpc_address OR rpc_interface, not both. Interfaces must correspond\nto a single address, IP aliasing is not supported.</p><p><em>Default Value:</em> eth1</p></div><div class=\"section\" id=\"rpc-interface-prefer-ipv6\"><h2><code class=\"docutils literal\">rpc_interface_prefer_ipv6</code></h2><p><em>This option is commented out by default.</em></p><p>If you choose to specify the interface by name and the interface has an ipv4 and an ipv6 address\nyou can specify which should be chosen using rpc_interface_prefer_ipv6. If false the first ipv4\naddress will be used. If true the first ipv6 address will be used. Defaults to false preferring\nipv4. If there is only one address it will be selected regardless of ipv4/ipv6.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"broadcast-rpc-address\"><h2><code class=\"docutils literal\">broadcast_rpc_address</code></h2><p><em>This option is commented out by default.</em></p><p>RPC address to broadcast to drivers and other Cassandra nodes. This cannot\nbe set to 0.0.0.0. If left blank, this will be set to the value of\nrpc_address. If rpc_address is set to 0.0.0.0, broadcast_rpc_address must\nbe set.</p><p><em>Default Value:</em> 1.2.3.4</p></div><div class=\"section\" id=\"rpc-keepalive\"><h2><code class=\"docutils literal\">rpc_keepalive</code></h2><p>enable or disable keepalive on rpc/native connections</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"internode-send-buff-size-in-bytes\"><h2><code class=\"docutils literal\">internode_send_buff_size_in_bytes</code></h2><p><em>This option is commented out by default.</em></p><p>Uncomment to set socket buffer size for internode communication\nNote that when setting this, the buffer size is limited by net.core.wmem_max\nand when not setting it it is defined by net.ipv4.tcp_wmem\nSee also:\n/proc/sys/net/core/wmem_max\n/proc/sys/net/core/rmem_max\n/proc/sys/net/ipv4/tcp_wmem\n/proc/sys/net/ipv4/tcp_wmem\nand ‘man tcp’</p></div><div class=\"section\" id=\"internode-recv-buff-size-in-bytes\"><h2><code class=\"docutils literal\">internode_recv_buff_size_in_bytes</code></h2><p><em>This option is commented out by default.</em></p><p>Uncomment to set socket buffer size for internode communication\nNote that when setting this, the buffer size is limited by net.core.wmem_max\nand when not setting it it is defined by net.ipv4.tcp_wmem</p></div><div class=\"section\" id=\"incremental-backups\"><h2><code class=\"docutils literal\">incremental_backups</code></h2><p>Set to true to have Cassandra create a hard link to each sstable\nflushed or streamed locally in a backups/ subdirectory of the\nkeyspace data.  Removing these links is the operator’s\nresponsibility.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"snapshot-before-compaction\"><h2><code class=\"docutils literal\">snapshot_before_compaction</code></h2><p>Whether or not to take a snapshot before each compaction.  Be\ncareful using this option, since Cassandra won’t clean up the\nsnapshots for you.  Mostly useful if you’re paranoid when there\nis a data format change.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"auto-snapshot\"><h2><code class=\"docutils literal\">auto_snapshot</code></h2><p>Whether or not a snapshot is taken of the data before keyspace truncation\nor dropping of column families. The STRONGLY advised default of true\nshould be used to provide data safety. If you set this flag to false, you will\nlose data on truncation or drop.</p><p><em>Default Value:</em> true</p></div><div class=\"section\" id=\"column-index-size-in-kb\"><h2><code class=\"docutils literal\">column_index_size_in_kb</code></h2><p>Granularity of the collation index of rows within a partition.\nIncrease if your rows are large, or if you have a very large\nnumber of rows per partition.  The competing goals are these:</p><ul class=\"simple\"><li>a smaller granularity means more index entries are generated\nand looking up rows withing the partition by collation column\nis faster</li>\n<li>but, Cassandra will keep the collation index in memory for hot\nrows (as part of the key cache), so a larger granularity means\nyou can cache more hot rows</li>\n</ul><p><em>Default Value:</em> 64</p></div><div class=\"section\" id=\"column-index-cache-size-in-kb\"><h2><code class=\"docutils literal\">column_index_cache_size_in_kb</code></h2><p>Per sstable indexed key cache entries (the collation index in memory\nmentioned above) exceeding this size will not be held on heap.\nThis means that only partition information is held on heap and the\nindex entries are read from disk.</p><p>Note that this size refers to the size of the\nserialized index information and not the size of the partition.</p><p><em>Default Value:</em> 2</p></div><div class=\"section\" id=\"concurrent-compactors\"><h2><code class=\"docutils literal\">concurrent_compactors</code></h2><p><em>This option is commented out by default.</em></p><p>Number of simultaneous compactions to allow, NOT including\nvalidation “compactions” for anti-entropy repair.  Simultaneous\ncompactions can help preserve read performance in a mixed read/write\nworkload, by mitigating the tendency of small sstables to accumulate\nduring a single long running compactions. The default is usually\nfine and if you experience problems with compaction running too\nslowly or too fast, you should look at\ncompaction_throughput_mb_per_sec first.</p><p>concurrent_compactors defaults to the smaller of (number of disks,\nnumber of cores), with a minimum of 2 and a maximum of 8.</p><p>If your data directories are backed by SSD, you should increase this\nto the number of cores.</p><p><em>Default Value:</em> 1</p></div><div class=\"section\" id=\"compaction-throughput-mb-per-sec\"><h2><code class=\"docutils literal\">compaction_throughput_mb_per_sec</code></h2><p>Throttles compaction to the given total throughput across the entire\nsystem. The faster you insert data, the faster you need to compact in\norder to keep the sstable count down, but in general, setting this to\n16 to 32 times the rate you are inserting data is more than sufficient.\nSetting this to 0 disables throttling. Note that this account for all types\nof compaction, including validation compaction.</p><p><em>Default Value:</em> 16</p></div><div class=\"section\" id=\"sstable-preemptive-open-interval-in-mb\"><h2><code class=\"docutils literal\">sstable_preemptive_open_interval_in_mb</code></h2><p>When compacting, the replacement sstable(s) can be opened before they\nare completely written, and used in place of the prior sstables for\nany range that has been written. This helps to smoothly transfer reads\nbetween the sstables, reducing page cache churn and keeping hot rows hot</p><p><em>Default Value:</em> 50</p></div><div class=\"section\" id=\"cas-contention-timeout-in-ms\"><h2><code class=\"docutils literal\">cas_contention_timeout_in_ms</code></h2><p>How long a coordinator should continue to retry a CAS operation\nthat contends with other proposals for the same row</p><p><em>Default Value:</em> 1000</p></div><div class=\"section\" id=\"streaming-keep-alive-period-in-secs\"><h2><code class=\"docutils literal\">streaming_keep_alive_period_in_secs</code></h2><p><em>This option is commented out by default.</em></p><p>Set keep-alive period for streaming\nThis node will send a keep-alive message periodically with this period.\nIf the node does not receive a keep-alive message from the peer for\n2 keep-alive cycles the stream session times out and fail\nDefault value is 300s (5 minutes), which means stalled stream\ntimes out in 10 minutes by default</p><p><em>Default Value:</em> 300</p></div><div class=\"section\" id=\"streaming-connections-per-host\"><h2><code class=\"docutils literal\">streaming_connections_per_host</code></h2><p><em>This option is commented out by default.</em></p><p>Limit number of connections per host for streaming\nIncrease this when you notice that joins are CPU-bound rather that network\nbound (for example a few nodes with big files).</p><p><em>Default Value:</em> 1</p></div><div class=\"section\" id=\"phi-convict-threshold\"><h2><code class=\"docutils literal\">phi_convict_threshold</code></h2><p><em>This option is commented out by default.</em></p><p>phi value that must be reached for a host to be marked down.\nmost users should never need to adjust this.</p><p><em>Default Value:</em> 8</p></div><div class=\"section\" id=\"endpoint-snitch\"><h2><code class=\"docutils literal\">endpoint_snitch</code></h2><p>endpoint_snitch – Set this to a class that implements\nIEndpointSnitch.  The snitch has two functions:</p><ul class=\"simple\"><li>it teaches Cassandra enough about your network topology to route\nrequests efficiently</li>\n<li>it allows Cassandra to spread replicas around your cluster to avoid\ncorrelated failures. It does this by grouping machines into\n“datacenters” and “racks.”  Cassandra will do its best not to have\nmore than one replica on the same “rack” (which may not actually\nbe a physical location)</li>\n</ul><p>CASSANDRA WILL NOT ALLOW YOU TO SWITCH TO AN INCOMPATIBLE SNITCH\nONCE DATA IS INSERTED INTO THE CLUSTER.  This would cause data loss.\nThis means that if you start with the default SimpleSnitch, which\nlocates every node on “rack1” in “datacenter1”, your only options\nif you need to add another datacenter are GossipingPropertyFileSnitch\n(and the older PFS).  From there, if you want to migrate to an\nincompatible snitch like Ec2Snitch you can do it by adding new nodes\nunder Ec2Snitch (which will locate them in a new “datacenter”) and\ndecommissioning the old ones.</p><p>Out of the box, Cassandra provides:</p><dl class=\"docutils\"><dt>SimpleSnitch:</dt>\n<dd>Treats Strategy order as proximity. This can improve cache\nlocality when disabling read repair.  Only appropriate for\nsingle-datacenter deployments.</dd>\n<dt>GossipingPropertyFileSnitch</dt>\n<dd>This should be your go-to snitch for production use.  The rack\nand datacenter for the local node are defined in\ncassandra-rackdc.properties and propagated to other nodes via\ngossip.  If cassandra-topology.properties exists, it is used as a\nfallback, allowing migration from the PropertyFileSnitch.</dd>\n<dt>PropertyFileSnitch:</dt>\n<dd>Proximity is determined by rack and data center, which are\nexplicitly configured in cassandra-topology.properties.</dd>\n<dt>Ec2Snitch:</dt>\n<dd>Appropriate for EC2 deployments in a single Region. Loads Region\nand Availability Zone information from the EC2 API. The Region is\ntreated as the datacenter, and the Availability Zone as the rack.\nOnly private IPs are used, so this will not work across multiple\nRegions.</dd>\n<dt>Ec2MultiRegionSnitch:</dt>\n<dd>Uses public IPs as broadcast_address to allow cross-region\nconnectivity.  (Thus, you should set seed addresses to the public\nIP as well.) You will need to open the storage_port or\nssl_storage_port on the public IP firewall.  (For intra-Region\ntraffic, Cassandra will switch to the private IP after\nestablishing a connection.)</dd>\n<dt>RackInferringSnitch:</dt>\n<dd>Proximity is determined by rack and data center, which are\nassumed to correspond to the 3rd and 2nd octet of each node’s IP\naddress, respectively.  Unless this happens to match your\ndeployment conventions, this is best used as an example of\nwriting a custom Snitch class and is provided in that spirit.</dd>\n</dl><p>You can use a custom Snitch by setting this to the full class name\nof the snitch, which will be assumed to be on your classpath.</p><p><em>Default Value:</em> SimpleSnitch</p></div><div class=\"section\" id=\"dynamic-snitch-reset-interval-in-ms\"><h2><code class=\"docutils literal\">dynamic_snitch_reset_interval_in_ms</code></h2><p>controls how often to reset all host scores, allowing a bad host to\npossibly recover</p><p><em>Default Value:</em> 600000</p></div><div class=\"section\" id=\"dynamic-snitch-badness-threshold\"><h2><code class=\"docutils literal\">dynamic_snitch_badness_threshold</code></h2><p>if set greater than zero and read_repair_chance is &lt; 1.0, this will allow\n‘pinning’ of replicas to hosts in order to increase cache capacity.\nThe badness threshold will control how much worse the pinned host has to be\nbefore the dynamic snitch will prefer other replicas over it.  This is\nexpressed as a double which represents a percentage.  Thus, a value of\n0.2 means Cassandra would continue to prefer the static snitch values\nuntil the pinned host was 20% worse than the fastest.</p><p><em>Default Value:</em> 0.1</p></div><div class=\"section\" id=\"server-encryption-options\"><h2><code class=\"docutils literal\">server_encryption_options</code></h2><p>Enable or disable inter-node encryption\nJVM defaults for supported SSL socket protocols and cipher suites can\nbe replaced using custom encryption options. This is not recommended\nunless you have policies in place that dictate certain settings, or\nneed to disable vulnerable ciphers or protocols in case the JVM cannot\nbe updated.\nFIPS compliant settings can be configured at JVM level and should not\ninvolve changing encryption settings here:\n<a class=\"reference external\" href=\"https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html\">https://docs.oracle.com/javase/8/docs/technotes/guides/security/jsse/FIPS.html</a>\n<em>NOTE</em> No custom encryption options are enabled at the moment\nThe available internode options are : all, none, dc, rack</p><p>If set to dc cassandra will encrypt the traffic between the DCs\nIf set to rack cassandra will encrypt the traffic between the racks</p><p>The passwords used in these options must match the passwords used when generating\nthe keystore and truststore.  For instructions on generating these files, see:\n<a class=\"reference external\" href=\"http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore\">http://download.oracle.com/javase/6/docs/technotes/guides/security/jsse/JSSERefGuide.html#CreateKeystore</a></p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>internode_encryption: none\nkeystore: conf/.keystore\nkeystore_password: cassandra\ntruststore: conf/.truststore\ntruststore_password: cassandra\n# More advanced defaults below:\n# protocol: TLS\n# algorithm: SunX509\n# store_type: JKS\n# cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n# require_client_auth: false\n# require_endpoint_verification: false\n</pre></div></div></div><div class=\"section\" id=\"client-encryption-options\"><h2><code class=\"docutils literal\">client_encryption_options</code></h2><p>enable or disable client/server encryption.</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>enabled: false\n# If enabled and optional is set to true encrypted and unencrypted connections are handled.\noptional: false\nkeystore: conf/.keystore\nkeystore_password: cassandra\n# require_client_auth: false\n# Set trustore and truststore_password if require_client_auth is true\n# truststore: conf/.truststore\n# truststore_password: cassandra\n# More advanced defaults below:\n# protocol: TLS\n# algorithm: SunX509\n# store_type: JKS\n# cipher_suites: [TLS_RSA_WITH_AES_128_CBC_SHA,TLS_RSA_WITH_AES_256_CBC_SHA,TLS_DHE_RSA_WITH_AES_128_CBC_SHA,TLS_DHE_RSA_WITH_AES_256_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA,TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA]\n</pre></div></div></div><div class=\"section\" id=\"internode-compression\"><h2><code class=\"docutils literal\">internode_compression</code></h2><p>internode_compression controls whether traffic between nodes is\ncompressed.\nCan be:</p><dl class=\"docutils\"><dt>all</dt>\n<dd>all traffic is compressed</dd>\n<dt>dc</dt>\n<dd>traffic between different datacenters is compressed</dd>\n<dt>none</dt>\n<dd>nothing is compressed.</dd>\n</dl><p><em>Default Value:</em> dc</p></div><div class=\"section\" id=\"inter-dc-tcp-nodelay\"><h2><code class=\"docutils literal\">inter_dc_tcp_nodelay</code></h2><p>Enable or disable tcp_nodelay for inter-dc communication.\nDisabling it will result in larger (but fewer) network packets being sent,\nreducing overhead from the TCP protocol itself, at the cost of increasing\nlatency if you block for cross-datacenter responses.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"tracetype-query-ttl\"><h2><code class=\"docutils literal\">tracetype_query_ttl</code></h2><p>TTL for different trace types used during logging of the repair process.</p><p><em>Default Value:</em> 86400</p></div><div class=\"section\" id=\"tracetype-repair-ttl\"><h2><code class=\"docutils literal\">tracetype_repair_ttl</code></h2><p><em>Default Value:</em> 604800</p></div><div class=\"section\" id=\"transparent-data-encryption-options\"><h2><code class=\"docutils literal\">transparent_data_encryption_options</code></h2><p>Enables encrypting data at-rest (on disk). Different key providers can be plugged in, but the default reads from\na JCE-style keystore. A single keystore can hold multiple keys, but the one referenced by\nthe “key_alias” is the only key that will be used for encrypt opertaions; previously used keys\ncan still (and should!) be in the keystore and will be used on decrypt operations\n(to handle the case of key rotation).</p><p>It is strongly recommended to download and install Java Cryptography Extension (JCE)\nUnlimited Strength Jurisdiction Policy Files for your version of the JDK.\n(current link: <a class=\"reference external\" href=\"http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html\">http://www.oracle.com/technetwork/java/javase/downloads/jce8-download-2133166.html</a>)</p><p>Currently, only the following file types are supported for transparent data encryption, although\nmore are coming in future cassandra releases: commitlog, hints</p><p><em>Default Value (complex option)</em>:</p><div class=\"highlight-default\"><div class=\"highlight\"><pre>enabled: false\nchunk_length_kb: 64\ncipher: AES/CBC/PKCS5Padding\nkey_alias: testing:1\n# CBC IV length for AES needs to be 16 bytes (which is also the default size)\n# iv_length: 16\nkey_provider:\n  - class_name: org.apache.cassandra.security.JKSKeyProvider\n    parameters:\n      - keystore: conf/.keystore\n        keystore_password: cassandra\n        store_type: JCEKS\n        key_password: cassandra\n</pre></div></div></div><div class=\"section\" id=\"tombstone-warn-threshold\"><h2><code class=\"docutils literal\">tombstone_warn_threshold</code></h2><div class=\"section\" id=\"safety-thresholds\"><h3>SAFETY THRESHOLDS #</h3><p>When executing a scan, within or across a partition, we need to keep the\ntombstones seen in memory so we can return them to the coordinator, which\nwill use them to make sure other replicas also know about the deleted rows.\nWith workloads that generate a lot of tombstones, this can cause performance\nproblems and even exaust the server heap.\n(<a class=\"reference external\" href=\"http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets\">http://www.datastax.com/dev/blog/cassandra-anti-patterns-queues-and-queue-like-datasets</a>)\nAdjust the thresholds here if you understand the dangers and want to\nscan more tombstones anyway.  These thresholds may also be adjusted at runtime\nusing the StorageService mbean.</p><p><em>Default Value:</em> 1000</p></div></div><div class=\"section\" id=\"tombstone-failure-threshold\"><h2><code class=\"docutils literal\">tombstone_failure_threshold</code></h2><p><em>Default Value:</em> 100000</p></div><div class=\"section\" id=\"batch-size-warn-threshold-in-kb\"><h2><code class=\"docutils literal\">batch_size_warn_threshold_in_kb</code></h2><p>Log WARN on any multiple-partition batch size exceeding this value. 5kb per batch by default.\nCaution should be taken on increasing the size of this threshold as it can lead to node instability.</p><p><em>Default Value:</em> 5</p></div><div class=\"section\" id=\"batch-size-fail-threshold-in-kb\"><h2><code class=\"docutils literal\">batch_size_fail_threshold_in_kb</code></h2><p>Fail any multiple-partition batch exceeding this value. 50kb (10x warn threshold) by default.</p><p><em>Default Value:</em> 50</p></div><div class=\"section\" id=\"unlogged-batch-across-partitions-warn-threshold\"><h2><code class=\"docutils literal\">unlogged_batch_across_partitions_warn_threshold</code></h2><p>Log WARN on any batches not of type LOGGED than span across more partitions than this limit</p><p><em>Default Value:</em> 10</p></div><div class=\"section\" id=\"compaction-large-partition-warning-threshold-mb\"><h2><code class=\"docutils literal\">compaction_large_partition_warning_threshold_mb</code></h2><p>Log a warning when compacting partitions larger than this value</p><p><em>Default Value:</em> 100</p></div><div class=\"section\" id=\"gc-log-threshold-in-ms\"><h2><code class=\"docutils literal\">gc_log_threshold_in_ms</code></h2><p><em>This option is commented out by default.</em></p><p>GC Pauses greater than 200 ms will be logged at INFO level\nThis threshold can be adjusted to minimize logging if necessary</p><p><em>Default Value:</em> 200</p></div><div class=\"section\" id=\"gc-warn-threshold-in-ms\"><h2><code class=\"docutils literal\">gc_warn_threshold_in_ms</code></h2><p><em>This option is commented out by default.</em></p><p>GC Pauses greater than gc_warn_threshold_in_ms will be logged at WARN level\nAdjust the threshold based on your application throughput requirement. Setting to 0\nwill deactivate the feature.</p><p><em>Default Value:</em> 1000</p></div><div class=\"section\" id=\"max-value-size-in-mb\"><h2><code class=\"docutils literal\">max_value_size_in_mb</code></h2><p><em>This option is commented out by default.</em></p><p>Maximum size of any value in SSTables. Safety measure to detect SSTable corruption\nearly. Any value size larger than this threshold will result into marking an SSTable\nas corrupted.</p><p><em>Default Value:</em> 256</p></div><div class=\"section\" id=\"back-pressure-enabled\"><h2><code class=\"docutils literal\">back_pressure_enabled</code></h2><p>Back-pressure settings #\nIf enabled, the coordinator will apply the back-pressure strategy specified below to each mutation\nsent to replicas, with the aim of reducing pressure on overloaded replicas.</p><p><em>Default Value:</em> false</p></div><div class=\"section\" id=\"back-pressure-strategy\"><h2><code class=\"docutils literal\">back_pressure_strategy</code></h2><p>The back-pressure strategy applied.\nThe default implementation, RateBasedBackPressure, takes three arguments:\nhigh ratio, factor, and flow type, and uses the ratio between incoming mutation responses and outgoing mutation requests.\nIf below high ratio, outgoing mutations are rate limited according to the incoming rate decreased by the given factor;\nif above high ratio, the rate limiting is increased by the given factor;\nsuch factor is usually best configured between 1 and 10, use larger values for a faster recovery\nat the expense of potentially more dropped mutations;\nthe rate limiting is applied according to the flow type: if FAST, it’s rate limited at the speed of the fastest replica,\nif SLOW at the speed of the slowest one.\nNew strategies can be added. Implementors need to implement org.apache.cassandra.net.BackpressureStrategy and\nprovide a public constructor accepting a Map&lt;String, Object&gt;.</p></div><div class=\"section\" id=\"otc-coalescing-strategy\"><h2><code class=\"docutils literal\">otc_coalescing_strategy</code></h2><p><em>This option is commented out by default.</em></p><p>Coalescing Strategies #\nCoalescing multiples messages turns out to significantly boost message processing throughput (think doubling or more).\nOn bare metal, the floor for packet processing throughput is high enough that many applications won’t notice, but in\nvirtualized environments, the point at which an application can be bound by network packet processing can be\nsurprisingly low compared to the throughput of task processing that is possible inside a VM. It’s not that bare metal\ndoesn’t benefit from coalescing messages, it’s that the number of packets a bare metal network interface can process\nis sufficient for many applications such that no load starvation is experienced even without coalescing.\nThere are other benefits to coalescing network messages that are harder to isolate with a simple metric like messages\nper second. By coalescing multiple tasks together, a network thread can process multiple messages for the cost of one\ntrip to read from a socket, and all the task submission work can be done at the same time reducing context switching\nand increasing cache friendliness of network message processing.\nSee CASSANDRA-8692 for details.</p><p>Strategy to use for coalescing messages in OutboundTcpConnection.\nCan be fixed, movingaverage, timehorizon, disabled (default).\nYou can also specify a subclass of CoalescingStrategies.CoalescingStrategy by name.</p><p><em>Default Value:</em> DISABLED</p></div><div class=\"section\" id=\"otc-coalescing-window-us\"><h2><code class=\"docutils literal\">otc_coalescing_window_us</code></h2><p><em>This option is commented out by default.</em></p><p>How many microseconds to wait for coalescing. For fixed strategy this is the amount of time after the first\nmessage is received before it will be sent with any accompanying messages. For moving average this is the\nmaximum amount of time that will be waited as well as the interval at which messages must arrive on average\nfor coalescing to be enabled.</p><p><em>Default Value:</em> 200</p></div><div class=\"section\" id=\"otc-coalescing-enough-coalesced-messages\"><h2><code class=\"docutils literal\">otc_coalescing_enough_coalesced_messages</code></h2><p><em>This option is commented out by default.</em></p><p>Do not try to coalesce messages if we already got that many messages. This should be more than 2 and less than 128.</p><p><em>Default Value:</em> 8</p></div><div class=\"section\" id=\"otc-backlog-expiration-interval-ms\"><h2><code class=\"docutils literal\">otc_backlog_expiration_interval_ms</code></h2><p><em>This option is commented out by default.</em></p><p>How many milliseconds to wait between two expiration runs on the backlog (queue) of the OutboundTcpConnection.\nExpiration is done if messages are piling up in the backlog. Droppable messages are expired to free the memory\ntaken by expired messages. The interval should be between 0 and 1000, and in most installations the default value\nwill be appropriate. A smaller value could potentially expire messages slightly sooner at the expense of more CPU\ntime and queue contention while iterating the backlog of messages.\nAn interval of 0 disables any wait time, which is the behavior of former Cassandra versions.</p><p><em>Default Value:</em> 200</p></div><div class=\"section\" id=\"ideal-consistency-level\"><h2><code class=\"docutils literal\">ideal_consistency_level</code></h2><p><em>This option is commented out by default.</em></p><p>Track a metric per keyspace indicating whether replication achieved the ideal consistency\nlevel for writes without timing out. This is different from the consistency level requested by\neach write which may be lower in order to facilitate availability.</p><p><em>Default Value:</em> EACH_QUORUM</p></div>",
        "created_at": "2018-09-13T14:58:09+0000",
        "updated_at": "2018-09-13T14:58:23+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 38,
        "domain_name": "cassandra.apache.org",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12157"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 254,
            "label": "nosql",
            "slug": "nosql"
          }
        ],
        "is_public": false,
        "id": 12156,
        "uid": null,
        "title": "Cassandra’s Place in the NoSQL World - Instaclustr",
        "url": "https://www.instaclustr.com/cassandras-place-nosql-world/",
        "content": "<header id=\"page-masthead\"><div id=\"page-content\"><div class=\"container\"><div class=\"row\"><div class=\"col-md-6 col-md-offset-0 col-sm-8 col-sm-offset-2\"><div class=\"primary first-p-bold\"><p>A question that we commonly get asked is “how does <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a> compare to NoSQL technology X?”. While the easy answer is to say “It’s just better”, the truth of course isn’t that simple. NoSQL encompasses a much more diverse range of technologies than the relational database world with specific NoSQL products suited to particular use cases.</p><p>The definition I often use (OK, made up) for NoSQL is any database technology designed for a more specialised use case to in order to overcome the limitations of RDBMS technology in terms of:</p><ul><li>data size;</li> <li>transaction throughout;</li> <li>reliability and manageability;</li> <li>flexibility of data schema; and/or</li> <li>cost of hardware.</li> </ul><p>One implication of this definition is that it doesn’t make sense to say that NoSQL generally is “better” than relational databases. If you want to compare  NoSQL technology X with technology Y then you need to understand what you want to use it for.</p><p>Perhaps the better question to ask is which use cases are <a href=\"https://www.instaclustr.com/solutions/managed-apache-cassandra/\">Cassandra </a>and other popular NoSQL technologies such as <a href=\"https://www.mongodb.com/\">MongoDB</a> and <a href=\"https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html\">Hadoop/HDFS</a> best suited to? The table below provides a summary of the characteristics</p><p>In summary, Cassandra is a great choice where:</p><ul><li>you need an operational database that can extend to supporting analytics;</li> <li>you don’t want to be limited in ability to scale; and</li> <li>you want the highest possible levels of availability.</li> </ul><table><tbody><tr><td> </td><td><b>Cassandra</b></td> <td><b>Hadoop/HDFS</b></td> <td><b>Mongo</b></td> </tr><tr><td><strong>Primary use cases</strong></td> <td>Large-scale operational database; <p>Structured data store for analytics engines</p></td> <td>Big data analytics database</td> <td>Flexible JSON database for rapid development</td> </tr><tr><td><strong>Development Model</strong></td> <td>Apache Foundation community maintained</td> <td>Apache Foundation <p>community maintained</p></td> <td>Proprietary development released under AGPL open source</td> </tr><tr><td><strong>Reliability</strong></td> <td>Extreme reliability, masterless and replicated. No failover required. <p>Full bi-directional multi-datacenter support</p></td> <td>High availability with automated master fail-over.</td> <td>High availability with multiple replicas and automated failover.</td> </tr><tr><td><strong>Read/write latency</strong></td> <td>Typically 5-15 milliseconds for standard operations. Consistent as dataset grows.</td> <td>Engineered for batch throughput rather than latency.</td> <td>Similar to Cassandra for simple operations. More complex querying capability can lead to greater variability.</td> </tr><tr><td><strong>Scalability</strong></td> <td>No practical limits. Operational clusters in the multi-PB range<br />(eg Apple).</td> <td>No practical  limits. Multi-PB scale not uncommon.</td> <td>Can scale to TB and beyond but requires sharding and is therefore less manageable at very large scale.</td> </tr><tr><td><strong>Query Language</strong></td> <td>CQL, an SQL-like language</td> <td>Map-reduce API plus many add-ons available (inc SQL)</td> <td>API and JSON based queries. </td> </tr><tr><td><strong>Data Model</strong></td> <td>Structured tables but allows for sparse value and multi-value fields</td> <td>Schema-less</td> <td>Schema-less JSON</td> </tr></tbody></table></div></div></div></div></div></header>",
        "created_at": "2018-09-13T14:57:24+0000",
        "updated_at": "2018-09-13T14:57:27+0000",
        "published_at": "2017-05-03T23:40:42+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12156"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          },
          {
            "id": 1574,
            "label": "cassandra.stress",
            "slug": "cassandra-stress"
          }
        ],
        "is_public": false,
        "id": 12155,
        "uid": null,
        "title": "Deep Diving cassandra-stress - Part 3 (Using YAML Profiles) - Instaclustr",
        "url": "https://www.instaclustr.com/deep-diving-cassandra-stress-part-3-using-yaml-profiles/",
        "content": "<p>In the previous two posts of this series (<a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/\">Part 1</a> and <a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-2/\">Part 2</a>) I covered some of the basic commands of <a href=\"https://www.instaclustr.com/apache-cassandra/\">cassandra</a>-stress. In this post I will start looking at the use of the stress YAML file for more advanced stress scenarios, particularly where you want to run stress against a schema that matches one you are planning to use for your application.</p><p>It’s worth noting in the intro that cassandra-stress with a YAML file use a significantly (80%?) different set of code to the standard read/write/mixed commands. So, some assumptions and learnings from the standard commands won’t hold for YAML-driven stress. To cite one example, when running based on YAML, cassandra-stress does not validate that data returned from a select has the expected values as it does with read or mixed.</p><h5>For this article, I’ll reference the following YAML specification file:</h5><p>Before explaining the contents here, let’s see what happens when we run it with the following simple scenario:<br /><code>cassandra-stress user profile=file:///eg-files/stressprofilemixed.yaml no-warmup ops(insert=1) n=100 -rate threads=1 -node x.x.x.x</code></p><p>After running this on an empty cluster, I ran <code>select count(*) from eventsrawtest;</code>. The result? 345 rows – probably not you would have guessed. Here’s how cassandra-stress gets to that:</p><ul><li>n=100 counts number of insert batches, not number of individual insert operations</li>\n<li>Each batch will contain 1 partition’s data (due to partitions=fixed(1) setting) and all 15 of the rows in the partition. There are 15 rows in every partition as the single cluster key (time) has a cluster setting of fixed(15). All the rows in the partition will be included in the batch due to the select: fixed(10)/10 setting (ie changing this to say fixed(5)/10 would result in half the rows from the partition being include in any given batch).</li>\n<li>100 batches of 15 rows each gets you to 1500 rows so how did we end up with 345? This is due (primarily, in this case) to the relatively small range of potential values for the bucket_time. This results in a high overlap in the partition key values that end up getting generated by the uniform distributions. To demonstrate, changing the population of bucket_time to uniform(1..1288) results in 540 rows. In most cases, you want to initially insert data with no overlap to build up a base data set for testing. To facilitate this, I’ve recently submitted a cassandra-stress enhancement that provides sequential generation of seed values the same as used with the write command (<a href=\"https://issues.apache.org/jira/browse/CASSANDRA-12490\">https://issues.apache.org/jira/browse/CASSANDRA-12490</a>). Changing the uniform() distribution to seq() results in the expected 1500 rows being inserted by this command.</li>\n</ul><h5>Let’s look at some of the other column settings:</h5><ul><li><strong>population</strong> – determines the distribution of seed values used in the random data generation. By controlling the distribution of the seed values you control the distribution of the actual inserted values. So, for example uniform(1..100) will allow for up 100 different values each with the same chance of being selected. guassian(1..100) will also allow for up to 100 different values but as they will follow a guassian (otherwise known as normal or bell-curve) distribution, the values around the middle will have a much higher chance of being selected than the values at the extremes (so there will be a set of values more likely to get repeated and some which will occur very infrequently).</li>\n<li><strong>size</strong> – determines the size (length in bytes) of the of the values created for the field.</li>\n<li><strong>cluster</strong> – only applies to clustering columns, specifies the number of values for the column appearing in a single partition. The maximum number of rows in a partition is therefore the product of the maximum number of row of each clustering column (eg max(row1) * max(row 2) * max(row3)).</li>\n</ul><h5>We covered most of the insert settings in the introductory points but here’s a recap:</h5><ul><li><strong>partitions</strong>: the number of different partitions to include in each generated insert batch. Once a partition is chosen for inclusion in a batch, all rows in the partition will become eligible for inclusion and then be filtered according to the select setting. Using, uniform(1..5) would result in each batches containing between 1 and 5 partitions worth of data (with an equal chance of each number in the range).</li>\n<li><strong>batchtype</strong>: logged or unlogged – determines the cassandra batch type to use</li>\n<li><strong>select</strong>: select determines the portion of rows from a partition to select (at random) for inclusion in particular batch. So, for example, fixed(5)/10 would include 50% of rows from the selected partition in each batch. uniform(1..10)/10 would result in between 10% and 100% of rows in the partition being included in the batch with a different select percentage being randomly picked for each partition in each batch.</li>\n</ul><h5>The final section the yaml file that bears some explanation is the queries section. For each query, you specify:</h5><ul><li>A name for the query (pull-for-rollup, get-a-value) which are used to refers to the queries when specifying the mix of operations through the cassandra-stress command line.</li>\n<li><strong>cql</strong> – The actual query with ? characters where values from the population will be substituted in.</li>\n<li><strong>fields</strong> – either samerow or multirow. For samerow, the key values to use for the select will be picked at random (following the same general population rules as for insert) for the list of row keys that has been generated for inserting. For multirow, each of the column values making up the key will be independently randomly selected so there is a chance of generating keys for the selection parameters that don’t exist in the set of data that will/could be inserted according the the population settings.</li>\n</ul><h5>The ops command specified as part of the command line controls the mix of different operations to run. Take for example the following command:</h5><p><code>cassandra-stress user profile=file:///eg-files/stressprofilemixed.yaml ops(insert=1, pull_for_rollup=1, get-value=10) n=120 -node x.x.x.x</code></p><p>This will execute insert batches, pull_for_rollup queries and get-value queries in the ratio 1:1:10. So for this specific example, we’d get 10 inserts, 10 pull_for_rollup queries and 100 get-value queries.</p><p>Hopefully that’s explained the key information you need to use a YAML profile for running cassandra stress. In future instalments I’ll take a look at some of the remaining command line options and walk through a full end-to-end example of designing and executing a test.</p><p>Click here for <a href=\"https://www.instaclustr.com/blog/2016/08/19/deep-diving-into-cassandra-stress-part-1/\">Part One: </a><a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/\">Deep Diving into Cassandra Stress</a><br />Click here for <a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-2/\">Part Two: Mixed Command</a></p>",
        "created_at": "2018-09-13T14:57:14+0000",
        "updated_at": "2019-07-17T23:09:15+0000",
        "published_at": "2016-08-24T05:03:19+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12155"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          },
          {
            "id": 1574,
            "label": "cassandra.stress",
            "slug": "cassandra-stress"
          }
        ],
        "is_public": false,
        "id": 12154,
        "uid": null,
        "title": "Deep Diving into cassandra-stress - Part 2 (Mixed Command) - Instaclustr",
        "url": "https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-2/",
        "content": "<p>This is the second of a series of articles explain the operations of cassandra-stress. See here for <a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/\" target=\"_blank\" rel=\"noopener\">Part 1</a>.</p><p>This series of blogs has been created as part of my prep for my Cassandra summit talk ‘Load Testing Cassandra Applications’.</p><p>In this post we’ll examine the operation of the mixed command in <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a> stress and along with that some option for changing the schema used by stress (without using the yaml configuration for a complete custom run).</p><p>Firstly, let’s look at the most simple invocation of the mixed command:</p><p><code>cassandra-stress mixed n=100 -node x.x.x.x</code></p><p>First thing to note about this command is that for it to work you need to have run a write command to create the schema and populate data before the run. The write command should be run with at least the same number of operations as the mixed command to ensure that read commands will all retrieve data. For example, to prepare for the above command you should run:</p><p><code>cassandra-stress write n=100 -node x.x.x.x</code></p><p>If you haven’t written the right data before you run the mixed test then you will receive errors like the following:</p><p><code>java.io.IOException: Operation x0 on key(s) [38374d394b504d353430]: Data returned was not validated</code></p><p>This is because cassandra-stress validates the data returned from the select operations against the generated data set expected for the row (and in this case no data was returned).</p><p>So, once you’re set up correctly, what will the command actually do? The key default settings that come into play are:</p><ul><li>Consistency level: LOCAL_ONE – particularly significant for reads as it means only a single node will need to perform each read (vs multiple nodes for the more common QUORUM consistency levels)</li> <li>Command Ratios: {READ=1.0, WRITE=1.0} – the operations will be split evenly between reads and writes</li> <li>Command Clustering Distribution: clustering=GAUSSIAN(1..10) – determines the number of consecutive read or write operations to conduct consecutively. So, in this case will perform between 1 to 10 reads before performing between 1 to 10 writes and then switching back to reads. Due to this clustering, the actual ratio of reads to writes may differ from the target specified in command ratios for small runs such as this. However, it should even out over the course of a longer run.</li> <li>Population Distribution: Gaussian: min=1,max=100,mean=50.500000,stdev=16.500000 – unlike the write command, which uses a sequence from 1 to number of ops, mixed uses randomly generated numbers from 1 to the number of ops as part of the seed for the actual values. As the range is the same as write operations, the values are guaranteed to overlap between write and mixed but the the mixed run will potentially not visit all possible values and will visit them in a random order (of seed values).</li> <li>Rate: Auto, min threads = 4, max threads = 1000 – cassandra-stress will perform multiple runs, start at 4 threads and increasing threads with each run until the total operation rate fails to increase for three consecutive runs. The number of threads used will double with each run up to 16 threads and then increase by 1.5 times with each run after that.</li> </ul><p>The rate setting illustrates an interesting point in using cassandra-stress: generally, increasing the number of client threads will increase the load (throughput) you are placing on the cluster. This is because all operations are executed synchronously (in order to time them) and so more threads allow more operations to be simultaneously thrown at the cluster. Of course, like any multi-threaded system, there is an overhead incurred managing threads on the stress client and a limit to how much work the client can do before it becomes a bottleneck to the test. So, increase threads to increase the load on your cluster but check when you hit the limit that it’s actually your cluster reaching its limit not your stress client.</p><p>That’s it for the mixed command and associated info. Future installments will look at some of the other key options and the yaml configuration file.</p><p>Click here for <a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/\">Part One: D</a><a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/\" target=\"_blank\" rel=\"noopener\">eep Diving into Cassandra Stress</a><br />Click here for <a href=\"https://www.instaclustr.com/deep-diving-cassandra-stress-part-3-using-yaml-profiles/\" target=\"_blank\" rel=\"noopener\">Part Three: Using YAML Profiles</a></p>",
        "created_at": "2018-09-13T14:57:04+0000",
        "updated_at": "2019-07-17T23:09:08+0000",
        "published_at": "2016-08-23T04:16:17+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12154"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          },
          {
            "id": 1574,
            "label": "cassandra.stress",
            "slug": "cassandra-stress"
          }
        ],
        "is_public": false,
        "id": 12153,
        "uid": null,
        "title": "Deep Diving into cassandra-stress (Part 1) - Instaclustr",
        "url": "https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/",
        "content": "<h2>Overview</h2><p>This is the first in a series of blog post I’m planning to create as part of my prep for my Cassandra summit talk ‘Load Testing Cassandra Applications’.</p><p>cassandra-stress is a great utility for stress testing <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a>. However, available documentation is a little sparse and it not always entirely clear what load cassandra-stress will generate in a given situation. In this series of blog posts, I plan to walk through a number of cassandra stress scenarios examining exactly how cassandra-stress behaves.</p><p>For this series, I will be using the latest 3.x version of cassandra-stress. If I notice any differences related to a particular point version of Cassandra I will call them out.</p><p>In this first post, I will look at what is about the most basic cassandra-stress command you can run:<br /><code>cassandra-stress write n=10 -node x.x.x.x</code></p><p>I chose this command for two reasons: firstly, using a simple command will allow us to look at some of the basic functions that apply across any cassandra-stress command and secondly, in almost all scenarios, you will want to execute a write to populate a cluster with data before running a read or mixed scenario.</p><h2>cassandra-stress</h2><p>Let’s start by looking at that components of the command itself:</p><ul><li><code>cassandra-stress</code>: invokes a shell script which in turn invokes the main function of the Java class org.apache.cassandra.stress.Stress</li> <li><code>write</code>: execute write operations (other options being read, mixed, user, counter_write and counter_read)</li> <li><code>n=10</code>: execute 10 operations</li> <li><code>-node x.x.x.x</code>: the address of a node in the cluster to establish the initial connection</li> </ul><p>Filling in with defaults, here’s all the settings cassandra-stress will actually use for the run (from my in-progress implementation of <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-11914\" target=\"_blank\" rel=\"noopener noreferrer\">CASSANDRA-11914</a>):</p><h2>Step by Step</h2><p>As you can see, there is a lot going on behind the scenes. So. let’s walk-through step by step what cassandra-stress actually does when you execute this command:</p><ol><li> <ol><li>The options provided through the command line are parsed and filled in with a whole range of default options as necessary (will touch on important defaults below and more in future articles). Interestingly, at this stage a valid cassandra.yaml will need to be loaded. However, as far as I could tell this is just a side effect of cassandra-stress using some core Cassandra classes and the contents of the cassandra.yaml have no effect on the actual cassandra-stress operations.</li>\n<li>The Cassandra Java driver is used to connect to the node specified in the command line. From this node, the driver retrieves the node list and token map for the cluster and then initiates a connection to each node in the cluster.</li>\n<li>A create keyspace (if one doesn’t already exist) command is executed with the following definition:<br />While this definition is a reasonable choice for a simple default, it’s important to note that this is unlikely to be representative of a keyspace you would want to run in production. By far the most common production scenario would be to use NetworkTopologyStrategy and a replication factor of 3. To have cassandra-stress create a keyspace with this strategy you would need to drop any existing keyspace and add the following parameters to the cassandra-stress command line:<br /><code>-schema replication(strategy=NetworkTopologyStrategy,DC_NAME=3)</code><br />Replace DC_NAME with the actual name of your Cassandra data center. On some systems you may also need to escape the brackets ie. <code>replication\\(...\\)</code></li>\n<li>Once the keyspace is created, cassandra-stress creates two tables in the keyspace: standard1 and counter1. We’ll ignore counter1 for now as it’s not used in this test. The definition of the standard1 table created is as follows:<br />While, again, this is a reasonable choice for a simple default, there are a few characteristics to keep in mind if you are trying to draw conclusions from performance using this table definition:\n<ul><li>There is no clustering key, so 1 row per partition – potentially very different performance to a scenario with many rows per partition.</li>\n<li>Compression is disabled – the overhead of compression is typically not huge but could be significant.</li>\n<li>Compact Storage is enabled – this is not enabled by default and will result in smaller representation of the data on disk (although minimal difference with Cassandra 3.x).</li>\n</ul><p>I’ll cover options for using different schemas in a later installment of this series.</p></li>\n<li>cassandra-stress will attempt to make a jmx connection to the nodes in the cluster to collect garbage collection stats. If the attempt fails, the run will proceed without collection garbage collection stats.</li>\n<li>Next, cassandra-stress will run a warmup. This is a standard practice in load testing to reduce variation from start-up variation such as code being loaded to memory and JVM hotspot compilers. The number of warm-up iterations is the lesser 25% of the target of operations or 50k – 2 operations in this trivial example. The warm-up operations are basically the same as the test operations except not timed so I won’t go into them in detail.</li>\n<li>We’ve entered the actual load test phase. cassandra-stress creates 200 client threads and begins executing the target number of operations. In a real test, using the <strong>-rate</strong> option to control the number of client threads is a good way to control load on the cluster.<br />The first attempted operation will create a CQL prepared statement as follows:<br /><code>UPDATE \"standard1\" SET \"C0\" = ?,\"C1\" = ?,\"C2\" = ?,\"C3\" = ?,\"C4\" = ? WHERE KEY=?</code><br />Although we were probably expecting an INSERT statement, updates and inserts are identical in terms of Cassandra implementation so we can expect performance to be the same.This prepared statement will then be will be executed 10 times with different, random data generated for each execution. The statement will be executed with consistent level LOCAL_ONE.cassandra-stress seeds the random generation with a static string plus the column name and a seed number which for the write command defaults to sequentially used numbers from 1 to the number of operations. That means that each column will get different values but the set of values generated will be the same over multiples runs. Generating a static set of values is necessary for read tests but does have the side effect that if you were to run our sample operation (write n=10) 1000 times the end result would still be just 10 rows of data in the table.</li>\n<li>Finally, cassandra-stress prints its results. Here’s an example from a run of this command:\n<p>Many of these results are self explanatory but some bear further explanation:<br /><strong>Op rate</strong> is the rate of execution commands. <strong>Partition rate</strong> is that rate that partitions were visited (updated or read) by those commands and <strong>Row rate</strong> is that rate that rows were visited. For simple, single-row commands all three rates will be equal. The rates will vary in more complex scenarios where a single operation might visit multiple partitions and rows.<br />Similarly, <strong>Total partitions</strong> is the total number of partitions visited during the test. It’s worth noting that this is not unique partitions so even in some write-only scenarios it may not reflect the total number of partitions created by the test.<br />The <strong>GC</strong> statistics report on garbage collection and are zero in this case as JMX ports were blocked to the test cluster.</p></li>\n</ol></li>\n</ol><h2>Conclusion</h2><p>Well, that’s a lot to write about a simple test that inserts 10 rows into a table. Putting it together has helped improved my understanding of cassandra-stress, I hope it’s useful for you too. In future installments, I’ll look some more into the different data generation operations, mixed test and customers schemas using the YAML configuration file. Let me know in the comments if there are any particular areas of interest for the future articles.</p><p>Click here for <a href=\"https://www.instaclustr.com/blog/2016/08/23/deep-diving-into-cassandra-stress-part-2/\" target=\"_blank\" rel=\"noopener\">Part Two: Mixed Command</a><br />Click here for <a href=\"https://www.instaclustr.com/deep-diving-cassandra-stress-part-3-using-yaml-profiles/\" target=\"_blank\" rel=\"noopener\">Part Three: Using YAML Profiles</a></p>",
        "created_at": "2018-09-13T14:56:56+0000",
        "updated_at": "2019-07-17T23:08:52+0000",
        "published_at": "2016-08-19T01:20:07+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12153"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          }
        ],
        "is_public": false,
        "id": 12152,
        "uid": null,
        "title": "Testing Multiple Tables with Cassandra Stress - Instaclustr",
        "url": "https://www.instaclustr.com/testing-multiple-tables-cassandra-stress/",
        "content": "<header id=\"page-masthead\"><div id=\"page-content\"><div class=\"container\"><div class=\"row\"><div class=\"col-md-6 col-md-offset-0 col-sm-8 col-sm-offset-2\"><div class=\"primary first-p-bold\"><p>The cassandra-stress tool is a powerful tool for benchmarking <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a> performance. It allows quite sophisticated specification of data and loads profiles to run against almost any table definition you can create in Cassandra. We’ve previously published detailed blog posts on the use of cassandra-stress: <a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-1/\">Part 1</a>, <a href=\"https://www.instaclustr.com/deep-diving-into-cassandra-stress-part-2/\">Part 2</a> and <a href=\"https://www.instaclustr.com/deep-diving-cassandra-stress-part-3-using-yaml-profiles/\">Part 3</a>.</p><p>One significant limitation of cassandra-stress has been that it is only able to execute operations against once table at a time. You could work around that by running multiple instances of cassandra-stress but that was not ideal.</p><p>I recently submitted a patch for <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a> that now enables multiple tables to be stressed simultaneously with cassandra-stress (<a href=\"https://issues.apache.org/jira/browse/CASSANDRA-8780\">https://issues.apache.org/jira/browse/CASSANDRA-8780</a>). This blog post provides some more explanation of how to use this new feature. (While the feature won’t hit release until Cassandra 4.0, it’s pretty easy to download the code and build cassandra-stress yourself if you want to use it in the meantime.)</p><p>The three core changes you need to know to stress multiple tables in one run are as follows:</p><ol><li>The profile= command line argument now accepts a comma delimited list of profile yaml files.</li> <li> Profile yaml files can now optionally contain a specname attribute which provide a way to identify the profile. If it’s not specified, the specname is inferred as &lt;keyspace&gt;.&lt;table&gt;.</li> <li>When specifying operation counts using the ops= command line argument you can prefix them with a specname to refer to an operation from a particular profile (eg spec1.insert). If you don’t specify a   specname, the specname from the first listed yaml file will be inferred.</li> </ol><p>The inferred specnames means that existing single yaml file cassandra-stress configurations will continue to run without requiring any change.</p><p>The following provides an example of how this can be used in practice:</p><p>(add all the other standard arguments you would pass to a cassandra-stress run).</p><p>Within table1.yaml, might look something like:</p><div id=\"crayon-5b9a6445f3531231561943\" class=\"crayon-syntax crayon-theme-sublime-text crayon-font-monaco crayon-os-pc print-yes notranslate\" data-settings=\"minimize scroll-mouseover\"><div class=\"crayon-main\"><table class=\"crayon-table\"><tr class=\"crayon-row\"><td class=\"crayon-nums\" data-settings=\"show\"> <div class=\"crayon-nums-content\"><p>1</p><p>2</p><p>3</p><p>4</p><p>5</p><p>6</p><p>7</p><p>8</p><p>9</p><p>10</p><p>11</p><p>12</p><p>13</p><p>14</p><p>15</p><p>16</p><p>17</p><p>18</p><p>19</p><p>20</p><p>21</p><p>22</p><p>23</p><p>24</p><p>25</p><p>26</p><p>27</p><p>28</p><p>29</p><p>30</p><p>31</p><p>32</p><p>33</p><p>34</p><p>35</p></div></td>\n<td class=\"crayon-code\"><div class=\"crayon-pre\"><p># Keyspace name and create CQL</p><p>#</p><p>specname: t1</p><p>keyspace: stressexample</p><p>keyspace_definition: |</p><p>CREATE KEYSPACE stressexample WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};</p><p>#</p><p># Table name and create CQL</p><p>#</p><p>table: test1</p><p>table_definition: |</p><p>CREATE TABLE test5 (</p><p>pk int,</p><p>val text,</p><p>PRIMARY KEY (pk)</p><p>)</p><p>columnspec:</p><p>- name: pk</p><p>size: fixed(64)</p><p>population: seq(1..100000)</p><p>#</p><p># Specs for insert queries</p><p>#</p><p>insert:</p><p>partitions: fixed(1) # 1 partition per batch</p><p>batchtype: UNLOGGED # use unlogged batches</p><p>select: fixed(10)/10 # no chance of skipping a row when generating inserts</p><p>#</p><p># Read queries to run against the schema</p><p>#</p><p>queries:</p><p>single_read:</p><p>cql: select * from test1 where pk = ?</p><p>fields: samerow</p></div></td>\n</tr></table></div></div><p>table2.yaml would be similar but with specname=t2.</p><p>When you run this command cassandra-stress will first ensure that keyspaces and tables specified in each of the yaml files are created, creating them itself if necessary. It will then execute operations in the ratios specified in the ops argument – in this case, 10% inserts to table test1 as specified in table1.yaml, 10% reads using the single_read query definition and 80% inserts in the table specified in table2.yaml.</p><p>One interesting feature is that the multiple yaml files can all reference the same table in your cassandra cluster. I can see this being useful, for instance, where you want to simulate one read/write pattern against the bulk of your partitions while simultaneously simulating a different pattern against a small number of hot partitions. One thing to be aware of with this approach is that the data overlap between two different specs addressing the same table is hard to predict and so may bear close inspection if it’s important to your scenario.</p><p>I really hope this feature is useful to the Cassandra community. Let me know in the comments section (<a href=\"http://www.mail-archive.com/user@cassandra.apache.org/\">or the Cassandra user mailing list</a>) if you have any queries or suggestions.</p></div></div></div></div></div></header>",
        "created_at": "2018-09-13T14:56:48+0000",
        "updated_at": "2018-09-13T14:56:53+0000",
        "published_at": "2017-05-16T05:19:23+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12152"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12151,
        "uid": null,
        "title": "Elasticsearch on Cassandra Features Overview - Instaclustr",
        "url": "https://www.instaclustr.com/elassandra-features-overview/",
        "content": "<p>Instaclustr recently announced coming availability of our Managed Service for <a href=\"http://www.elassandra.io/\">Elassandra</a>, which delivers Elasticsearch integrated with Apache Cassandra. This blog post drills down to explain the features and advantages of Elassandra – the best of Cassandra and ElasticSearch.</p><p>Elassandra adds the power of ElasticSearch’s fast, powerful indexing and querying to Cassandra.  By embedding Elasticsearch into Cassandra’s resilient, distributed architecture, Elassandra delivers a solution which combines the functionality of both Cassandra and Elasticsearch in a single integrated solution with improved availability features and simpler management.</p><p>Here are some of the key features:</p><ul><li><strong>Elassandra provides all the indexing and query capabilities of ElasticSearch</strong>.  This includes incredibly fast indexing, querying and analysis of structured and semistructured data.  It allows complex querying of large data sets.  Real-time indexing allows data to become searchable as soon as it is inserted into the database.</li> </ul><ul><li><strong>Powerful, flexible querying of Cassandra tables</strong>.  By design, Cassandra supports a relatively simple range of query operations, generally limited to simple primary key or range lookups.  Elassandra allows the full power of the Elasticsearch Query Language (Query DSL)  to be used to search data in Cassandra tables, including the ability to produce compound statements comprising filter and query blocks and relevance score calculations.</li> </ul><ul><li><strong>Elassandra is Cassandra</strong>.  At its core, Elassandra is the same engine as Cassandra and therefore is automatically part of the Cassandra ecosystem.  Tools and applications which work with Cassandra can be used with little or no change with Elassandra.  This means that the same familiar tools (e.g. nodetool, cqlsh) are used to manage Elassandra, and operations such as repair and compaction are done in the usual way.  Tools like Spark, Storm, Kafka and Zeppelin all work with Elassandra’s Cassandra API.</li> </ul><ul><li><strong>Elassandra is Elasticsearch</strong>.  Elassandra supports Elasticsearch’s RESTful API, and fits right into the Elassandra ecosystem.  Tools like Kibana, Logstash, Beats, JDBC driver, Spark and Kafka all work with Elassandra’s Elasticsearch API.</li> </ul><ul><li><strong>Flexible APIs</strong>.  Data can be loaded in Elassandra through Cassandra’s CQL, or via ElasticSearch’s RESTful API.  Regardless of which approach is used, Elassandra ensures that data is stored in a Cassandra table, and indexed in Elasticsearch.  A wide variety of language bindings can be used to access these APIs.  For CQL these include: Java, Python, Ruby, C#/.Net, Nodejs, PHP, C++, Closure, Scala, Clojure, Erlang, Go, Haskell &amp; Rust, and for Elasticsearch: Java, JavaScript, Groovy, .NET, PHP, Perl, Python, Ruby and others many other via JSON/RESTful interface.</li> </ul><ul><li><strong>Simplified data pipeline</strong>.  A single Elassandra cluster can do the work of several components: a Cassandra cluster, an ElasticSearch cluster, and ETL processes to replicate and synchronise between clusters.  Once transactional data is loaded into an Elassandra cluster, it can be searched directly without extra pipeline steps to load from Cassandra to Elassandra.</li> </ul><ul><li><strong>Masterless Distributed Architecture</strong>.   While standalone Elasticsearch has a master node, which introduces a single point of failure which must be carefully managed.  By running Elasticsearch on Cassandra’s fully distributed, masterless distributed architecture, the single point of failure is eliminated.</li> </ul><ul><li><strong>In Elassandra, Elasticsearch indexes are managed Cassandra as secondary indexes</strong>.  Elassandra fully supports other Cassandra features including: Cluster Replication between data centres; integration with Cassandra backups; and Integration with Cassandra node add/remove functions.</li> </ul><ul><li><strong>Provisioned in minutes, fully integrated, fully managed</strong>.  With Instaclustr, you can have an Elassandra cluster provisioned in minutes, and managed and supported by Instaclustr.  Optionally, your cluster can also be automatically fully configured and integrated with Zeppelin, Spark and Kibana, allowing you to quickly build and deploy your application.</li> </ul>",
        "created_at": "2018-09-13T14:52:02+0000",
        "updated_at": "2018-09-13T14:56:31+0000",
        "published_at": "2017-05-25T06:06:46+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12151"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1215,
            "label": "networking",
            "slug": "networking"
          }
        ],
        "is_public": false,
        "id": 12150,
        "uid": null,
        "title": "Demystifying Cassandra’s broadcast_address - Instaclustr",
        "url": "https://www.instaclustr.com/demystifying-cassandras-broadcast_address/",
        "content": "<p>When configuring <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a> to work in a new environment or with a new application or service we sometimes find ourselves asking <em>“What’s the difference between broadcast_address and broadcast_rpc_address again?”</em>.</p><p>The difference is <strong>broadcast_address</strong> relates to <strong>gossip</strong> and <strong>node to node</strong> <strong>communications</strong>, whereas <strong>broadcast_rpc_address</strong> is associated with <strong>client connections</strong>. Read on for more details.</p><p>The Cassandra configuration file has a few interdependent properties related to communication, which can take a bit of concentration to make sense of. Here is a (hopefully) easy to understand explanation.</p><h2>Node to node communication (i.e. gossip)</h2><p>Cassandra will bind to the <strong>listen_address</strong> or <strong>listen_interface</strong> and listen on the storage_port or ssl_storage_port for gossip. In most cases, these properties may be omitted, resulting in Cassandra binding to the hostname’s IP address (Cassandra uses <a href=\"https://docs.oracle.com/javase/8/docs/api/java/net/InetAddress.html#getLocalHost--\" target=\"_blank\" rel=\"noopener\">InetAddress.getLocalHost()</a>). <strong>Note:</strong> setting listen_address to “localhost” results in Cassandra binding to the loopback interface (not recommended as it only works as a 1-node cluster).</p><p><strong>broadcast_address</strong> is reported to nodes for peer discovery. Topologies that span separate networks need this set to a public address. If this property is omitted, the listen_address will be broadcast to nodes.  <strong>Note:</strong> Nodes can be configured to gossip via the local network and use public addresses for nodes outside its local network by setting <code>prefer_local=true</code> in <em>cassandra-rackdc.properties</em> and using certain endpoint_snitches (such as GossipingPropertyFileSnitch or Ec2MultiRegionSnitch).</p><h2>Client to node communication</h2><h4>rpc_address, rpc_interface and broadcast_rpc_address</h4><p>By “client” I mean Cassandra drivers and clqsh. The drivers may use the Thrift transport or the Native transport (CQL binary protocol). Cqlsh uses the Native transport.</p><p>Cassandra will bind to the <strong>rpc_address</strong> or <strong>rpc_interface</strong> and listen on rpc_port and native_transport_port for client connections. If these properties are omitted, Cassandra will bind to the hostname’s IP address (and would need to be specified to a locally running cqlsh because <code><strong>cqlsh</strong></code> = <code><strong>cqlsh</strong> <strong>&lt;loopback address&gt;</strong></code>).</p><p><strong>broadcast_rpc_address</strong> is a property available in Cassandra 2.1 and above. It is reported to clients during cluster discovery and as cluster metadata. It is useful for clients outside the cluster’s local network. This property is typically either:</p><ul><li>the public address if most clients are outside the cluster’s local network</li> <li>the local network address if most clients are in the cluster’s local network</li> </ul><p>If this property is omitted, rpc_address will be reported to clients.</p><p><strong>Note 1:</strong> If there are a mix of clients inside and outside the local network, use an AddressTranslator policy to compensate for unreachable addresses (only available for Java and Python drivers at the time of writing. <a href=\"https://www.instaclustr.com/apache-cassandra-deployed-on-private-and-public-networks/\" target=\"_blank\" rel=\"noopener\">Here is a Java example</a>.)</p><p><strong>Note 2: </strong><em>rpc_address</em> may be set to 0.0.0.0. In this case, Cassandra binds to all available interfaces, including loopback, which is used by cqlsh when no host is specified. But 0.0.0.0 is not routable, so Cassandra will use a different property to determine the address to broadcast to clients:</p><ul><li>for Cassandra 2.1 and later: broadcast_rpc_address must be set and will be reported to clients.</li> <li>for Cassandra prior to 2.1: broadcast_address (or listen_address if omitted) will be reported to clients.</li> </ul><p>Summary:</p><table><thead><tr><th>Cassandra Version</th> <th>Purpose</th> <th>Properties</th> <th>Typical Setting</th> </tr></thead><tbody><tr><td>All</td> <td>gossip</td> <td>listen_address or listen_interface (with storage_port or ssl_storage_port)</td> <td>Omit to bind to InetAddress.getLocalHost()</td> </tr><tr><td>All</td> <td>peer discovery (within the cluster)</td> <td>broadcast_address else listen_address</td> <td>public address</td> </tr><tr><td>All</td> <td>client requests (CQL and Thrift)</td> <td>rpc_address or rpc_interface (with rpc_port and native_transport_port)</td> <td>Omit to bind to InetAddress.getLocalHost()</td> </tr><tr><td>2.1 and later</td> <td>cluster discovery | metadata (to the client)</td> <td>broadcast_rpc_address else rpc_address</td> <td>Omit to broadcast InetAddress.getLocalHost()</td> </tr><tr><td>2.0 and prior</td> <td>cluster discovery | metadata (to the client)</td> <td>rpc_address or broadcast_address if 0.0.0.0</td> <td>Omit to broadcast InetAddress.getLocalHost()</td> </tr></tbody></table>",
        "created_at": "2018-09-13T14:51:37+0000",
        "updated_at": "2018-09-13T14:51:40+0000",
        "published_at": "2016-10-11T03:20:04+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12150"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 1196,
            "label": "metric collection",
            "slug": "metric-collection"
          }
        ],
        "is_public": false,
        "id": 12149,
        "uid": null,
        "title": "Third Contact with a Monolith: Part C - In the Pod - Instaclustr",
        "url": "https://www.instaclustr.com/third-contact-monolith-part-c-pod/",
        "content": "<p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr.jpg\"> </a></p><aside class=\"content-cta\"><div class=\"primary\"><h4>Related Articles:</h4></div></aside><br /><img class=\"aligncenter wp-image-6883 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr.jpg\" alt=\"Third Contact with a Monolith PArt C In the Pod Instaclustr\" width=\"640\" height=\"601\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr.jpg 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr-300x282.jpg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr-51x48.jpg 51w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr-115x108.jpg 115w\" /><h2>A simple classification problem: Will the Monolith react? Is it safe?!</h2><p>Maybe a cautious approach to a bigger version of the Monolith (2km long) in a POD that is only 2m in diameter is advisable.   What do we know about how Monoliths react to stimuli? A simple classification problem consists of the category (label) “no reaction” (0) or “reaction” (1), and the stimuli tried (features which will be used to predict the label).  In the following table, the first column in the label to be predicted (positive, 1, or negative, 0), the remaining columns are the features, and each row is an example:</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr.png\"><img class=\"aligncenter wp-image-6886 size-medium\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-300x292.png\" alt=\"classification problem table Instaclustr\" width=\"300\" height=\"292\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-300x292.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-768x747.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-1024x996.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-633x616.png 633w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-640x622.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-49x48.png 49w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr-111x108.png 111w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Classification-problem-table-instaclustr.png 1088w\" /></a></p><p>This problem is trivial as a positive reaction only occurred for Touch OR Sunlight (the camera flash was a co-incidence), the features are all binary, and only a single feature at a time is true. As a result of extensive tests on the Monolith imagine that there are lots more examples and data available, each feature is a floating point number, all features don’t have values for all examples, there are potentially 1,000s of features, and a sufficiently accurate classification rule may require an arbitrary number of features.</p><p>Given that HAL has been unplugged we have to analyse the data ourselves. What do we have available in the POD?</p><p>One approach is to use an appropriately 1960’s-vintage machine learning algorithm suitable for simple binary classification problems. Decision tree algorithms were invented in the 1960’s (Concept Learning System, by a Psychologist, Earl Hunt), were improved in the 1980’s (e.g. ID3, C4.5, and <a href=\"https://link.springer.com/content/pdf/10.1023%2FA%3A1022699322624.pdf\">FOIL</a>, which references my 1988 machine learning algorithm, <i>Gargantubrain</i>), and are still useful. Spark’s Machine Learning Library (MLLib) has a number of regression and classification algorithms, including decision trees, that improve on the originals by running them “transparently” in parallel on multiple servers for scalability. </p><p><a href=\"https://spark.apache.org/mllib/\">Spark’s scalable ML library is available here</a>. It’s easy to download Spark and run MLLib examples locally, but the scalability benefits are best realised after deployment to an <a href=\"https://www.instaclustr.com/solutions/managed-apache-spark/\">Instaclustr managed Spark cluster.</a> Here’s the documentation on the <a href=\"https://spark.apache.org/docs/latest/mllib-decision-tree.html\">MLLib decision tree</a> algorithm.</p><h2>Training</h2><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/download-1.jpeg\"><img class=\"aligncenter wp-image-6888 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/download-1.jpeg\" alt=\"Training Instaclustr\" width=\"198\" height=\"254\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/download-1.jpeg 198w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/download-1-37x48.jpeg 37w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/download-1-84x108.jpeg 84w\" /></a></p><p>We’ll have a look at the example Java code, starting from the call to the decision tree algorithm to build the model from the examples, and working out from there. <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/tree/DecisionTree.html\">DecisionTree</a> and <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/tree/model/DecisionTreeModel.html\">DecisionTreeModel</a> documents are relevant.  Here’s the code.  Note the API for trainClassifier:</p><p>trainClassifier(JavaRDD&lt;LabeledPoint&gt; input, int numClasses,<br />java.util.Map&lt;Integer,Integer&gt; categoricalFeaturesInfo, String<br />impurity, int maxDepth, int maxBins)</p><p>The first argument is a JavaRDD of LabeledPoint. The other arguments are for control of the algorithm:</p><p>// Set parameters for DecisionTree learning.</p><p>// Empty categoricalFeaturesInfo indicates all features are continuous.</p><p>Integer numClasses = 2;</p><p>Map&lt;Integer, Integer&gt; categoricalFeaturesInfo = new HashMap&lt;&gt;();</p><p>String impurity = “gini”; // or “entropy”</p><p>Integer maxDepth = 5;</p><p>Integer maxBins = 32;</p><p>// Train DecisionTree model</p><p>DecisionTreeModel model = <b>DecisionTree.trainClassifier</b>(<b>trainingData</b>, numClasses, categoricalFeaturesInfo, impurity, maxDepth, maxBins);</p><p>System.out.println(“Learned classification tree model:\\n” + model.toDebugString());</p><p>What is a JavaRDD and LabeledPoint?</p><h2>LabeledPoint</h2><h2><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785.png\"><img class=\"size-medium wp-image-6889 aligncenter\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785-154x300.png\" alt=\"Pinch point Instaclustr\" width=\"154\" height=\"300\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785-154x300.png 154w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785-316x616.png 316w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785-328x640.png 328w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785-25x48.png 25w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785-55x108.png 55w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/pinch-point-arrow-shape-label-lb-2785.png 410w\" /><br /></a>             .</h2><p><a href=\"https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point</a></p><p><a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/regression/LabeledPoint.html\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/regression/LabeledPoint.html</a></p><p>Decision trees are a type of supervised learner, so we need a way of telling the algorithm what class (positive or negative for binary classifiers) each training example is.  LabeledPoint is a single labelled example (a “point” in n-dimensional space). It’s a tuple consisting of a Double label (either 0 or 1 for negative or positive examples), and a Vector of features, either dense or sparse.  Features are numbered from 0 to n. These are the examples from the documentation. Note the Vectors import. This is important as the default Spark Vector is NOT CORRECT.</p><p>import org.apache.spark.mllib.linalg.Vectors;<br />import org.apache.spark.mllib.regression.LabeledPoint;</p><p>// Create a labeled point with a positive label and a dense feature vector.</p><p>// There are three features, with values 1, 0, 3.<br />LabeledPoint pos = new LabeledPoint(1.0, Vectors.dense(1.0, 0.0, 3.0));</p><p>// Create a labeled point with a negative label and a sparse feature vector.</p><p>// There are two features, 0 and 2, with values 1 and 3 respectively.</p><p>LabeledPoint neg = new LabeledPoint(0.0, Vectors.sparse(3, new int[] {0, 2}, new double[] {1.0, 3.0}));</p><h2>Resilient Distributed Datasets (RDDs)</h2><p><a href=\"https://spark.apache.org/docs/latest/rdd-programming-guide.html\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/rdd-programming-guide.html</a></p><p><a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/api/java/JavaRDD.html\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/api/java/JavaRDD.html</a></p><h6>From the documentation: </h6><p><em>“Spark revolves around the concept of a resilient distributed dataset (RDD), which is a fault-tolerant collection of elements that can be operated on in parallel. There are two ways to create RDDs: parallelizing</em><em> an existing collection in your driver </em>program<em> or referencing a dataset in an external storage system… “</em> (such as Cassandra)</p><p><a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd.html\" target=\"_blank\" rel=\"noopener\">This blog has a good explanation of the Spark architecture</a>, and explains that the features of RDDs are:</p><ul><li>Resilient, i.e. fault-tolerant with the help of <a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd.html#lineage\">RDD lineage graph</a> and so able to recompute missing or damaged partitions due to node failures.</li>\n<li>Distributed with data residing on multiple nodes in a <a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-cluster.html\">cluster</a>.</li>\n<li>Dataset is a collection of <a href=\"https://jaceklaskowski.gitbooks.io/mastering-apache-spark/spark-rdd-partitions.html\">partitioned data</a> with primitive values or values of values, e.g. tuples or other objects (that represent records of the data you work with).</li>\n</ul><p>RDD’s are also immutable and can be cached. Fault-tolerance depends on the execution model which computes a Directed Acyclic Graph (DAG) of stages for each job, runs stages in optimal locations based on data location, shuffles data as required, and re-runs failed stages. </p><h2>LIBSVM – sparse data format</h2><p><a href=\"https://spark.apache.org/docs/2.0.2/mllib-data-types.html\">https://spark.apache.org/docs/2.0.2/mllib-data-types.html</a></p><p><a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/util/MLUtils.html\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/util/MLUtils.html</a></p><p>Where does the RDD training data come from? In the example code I read it from a local file using MLUtils.loadLibSVMFile() (I’ll come back to the parameters later):</p><p>JavaRDD&lt;LabeledPoint&gt; data = MLUtils.loadLibSVMFile(sc, path).toJavaRDD();</p><h6>From the documentation:</h6><p><em>“It is very common in practice to have sparse training data. MLlib supports reading training examples stored in LIBSVM format, which is the default format used by LIBSVM and LIBLINEAR. It is a text format in which each line represents a labeled sparse feature vector using the following format:</em></p><p><em>label index1:value1 index2:value2 …</em></p><p><em>where the indices are one-based and in ascending order. After loading, the feature indices are converted to zero-based.”</em></p><p>For the above Monolith example the LIBSVM input file looks like this (assuming that feature values of “0” indicate non-existent data):</p><p><strong>1.0 1:1.0<br />0.0 2:1.0</strong><br /><strong>0.0 3:1.0</strong><br /><strong>0.0 4:1.0</strong><br /><strong>0.0 5:1.0</strong><br /><strong>0.0 6:1.0</strong><br /><strong>0.0 7:1.0</strong><br /><strong>1.0 8:1.0</strong><br /><strong>0.0 9:1.0</strong><br /><strong>0.0 10:1.0</strong></p><p>This doesn’t seem very “user friendly” as we have lost the feature names. I wonder if there is a better data format?</p><h2>Splitting (the data, not the atom)</h2><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Splitting-the-data-not-the-atom-instaclustr.jpg\"><img class=\"aligncenter wp-image-6890 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Splitting-the-data-not-the-atom-instaclustr.jpg\" alt=\"splitting the data, not the atom instaclustr\" width=\"356\" height=\"199\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Splitting-the-data-not-the-atom-instaclustr.jpg 356w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Splitting-the-data-not-the-atom-instaclustr-300x168.jpg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Splitting-the-data-not-the-atom-instaclustr-86x48.jpg 86w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Splitting-the-data-not-the-atom-instaclustr-193x108.jpg 193w\" /></a></p><p>Once we have the training data in the correct format for the algorithm (JavaRDD&lt;LabeledPoint&gt;), but before we train the model, we need to split it into two random subsets for training and testing. JavaRDD.randomSplit() does this and takes parameters:</p><p>double[] weights – weights for splits, will be normalized if they don’t sum to 1</p><p>long seed – random seed</p><p>// Split sample RDD into two sets, 60% training data, 40% testing data. 11 is a seed.</p><p>   JavaRDD&lt;LabeledPoint&gt;[] splits = data.randomSplit(new double[]{0.6, 0.4}, 11L);</p><p>   JavaRDD&lt;LabeledPoint&gt; trainingData = splits[0].cache();  // cache the data</p><p>   JavaRDD&lt;LabeledPoint&gt; testData = splits[1];</p><p>Notice the cache() call for trainingData.  What does it do? Spark is Lazy, it doesn’t evaluate RDD’s until an action forces it to. Hence RDD’s can be evaluated multiple times which is expensive. cache() creates an in memory cache “checkpoint” of an RDD which can be reused. The most obvious case is when an RDD is used multiple times (i.e. iteration), or for branching transformations (i.e. multiple different RDD’s are computed from an original RDD), in which case the original should be cached.<br /><a href=\"https://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd\" target=\"_blank\" rel=\"noopener\">https://stackoverflow.com/questions/28981359/why-do-we-need-to-call-cache-or-persist-on-a-rdd</a><br /></p><p>Note that for this example the <i>initial data </i>should be cached as we use it later to count the total, positive and negative examples.</p><h2>Evaluating the Model on the Test Data – Thumbs down or up?</h2><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr.png\"><img class=\"aligncenter wp-image-6892 size-medium_large\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-768x521.png\" alt=\"Evaluating the model on the test data Instaclustr\" width=\"768\" height=\"521\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-768x521.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-300x204.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-1024x695.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-907x616.png 907w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-640x434.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-71x48.png 71w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-Data-2-Instaclustr-159x108.png 159w\" /></a></p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr.png\"><img class=\"aligncenter wp-image-6893 size-medium_large\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-768x394.png\" alt=\"Evaluating the model on the test data Instaclustr\" width=\"768\" height=\"394\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-768x394.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-300x154.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-1024x525.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-966x496.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-640x328.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-94x48.png 94w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Evaluating-the-model-on-the-test-data-1-Instaclustr-210x108.png 210w\" /></a></p><p>We trained a decision tree model above. What can we do with it? We trained it on the trainingData subset of examples leaving us with the testData to evaluate it on. MLLib computes some useful evaluation metrics which are documented here:</p><p><a href=\"https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/mllib-evaluation-metrics.html</a></p><p><a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/evaluation/BinaryClassificationMetrics.html\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/evaluation/BinaryClassificationMetrics.html</a></p><p>Here’s the code which computes the evaluation metrics:</p><p>// Compute evaluation metrics.<br />BinaryClassificationMetrics metrics = new BinaryClassificationMetrics(predictionAndLabels.rdd());</p><p>The highlighted parameter is an RDD of (prediction, label) pairs for a subset of examples. I.e. (0, 0) (0, 1) (1, 0) (1, 0). I.e. for each example, run the model and return a tuple of predicted label and actual example label. Here’s the complete code that does this on the testData and then computes the evaluation metrics:</p><p>// For every example in testData, p, replace it by a Tuple of (predicted category, labelled category)</p><p>// E.g. (1.0,0.0) (0.0,0.0) (0.0,0.0) (0.0,1.0)</p><p>JavaPairRDD&lt;Object, Object&gt; predictionAndLabels = testData.mapToPair(p -&gt;</p><p>    new Tuple2&lt;&gt;(model.predict(p.features()), p.label()));</p><p>// Compute evaluation metrics.</p><p>BinaryClassificationMetrics metrics = new BinaryClassificationMetrics(predictionAndLabels.rdd());</p><p>How does this work? There may be a few unfamiliar things in this code which we’ll explore: Tuple2, JavaPairRDD, mapToPair, features() and label(). </p><h4>Tuple(ware)</h4><p>Java doesn’t have a built-in Tuple type, so you have to use the scala.Tuple2 class. </p><h4>MAP</h4><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Here-be-dragons-Instaclustr.jpg\"><img class=\"aligncenter wp-image-6894 size-full\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Here-be-dragons-Instaclustr.jpg\" alt=\"MAP RDD transformation Instaclustr\" width=\"363\" height=\"234\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Here-be-dragons-Instaclustr.jpg 363w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Here-be-dragons-Instaclustr-300x193.jpg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Here-be-dragons-Instaclustr-74x48.jpg 74w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Here-be-dragons-Instaclustr-168x108.jpg 168w\" /></a></p><p>Map is an RDD transformation. Transformations pass each dataset element through a function and return a new RDD representing the result. Actions return a result after running a computation on a dataset. Transformations are lazy and are not computed until required by an action. In the above example, for every example in testData, p, it is replaced by a Tuple of (predicted category, labelled category). These values are computed by running model.predict() on all the features in the example, and using the label() of the example.</p><p>See LabeledPoint <a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/regression/LabeledPoint.html\" target=\"_blank\" rel=\"noopener\">documentation</a> for features() and label() methods.</p><p><a href=\"https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/api/java/JavaPairRDD.html\" target=\"_blank\" rel=\"noopener\">JavaPairRDD</a> is a (key, value) version of RDD. Instead of the general map function you need to use mapToPair.</p><p><i>Map</i> (and <i>mapToPair</i>) are examples of “Higher Order Functions”. These are used a lot in Spark but are really pretty ancient and were first practically used in 1960 in <a href=\"http://www-formal.stanford.edu/jmc/recursive.pdf\" target=\"_blank\" rel=\"noopener\">LISP</a>. Disturbingly the abstract sounds like an early attempt at HAL:</p><p><i>“A programming system called LISP … was designed to facilitate experiments … whereby a machine could be instructed to … exhibit “common sense” in carrying out its instructions.”</i></p><p>I used LISP for a real AI project (once), but (it ((had too) (many) brackets) (for (((me (Lots of Idiotic Spurious Parentheses))) ???!!! (are these balanced?!).</p><p>preferred(I, Prolog).</p><p>The following code prints out the main evaluation metrics, precision, recall and F.</p><p>JavaRDD&lt;Tuple2&lt;Object, Object&gt;&gt; precision = metrics.precisionByThreshold().toJavaRDD();</p><p>   System.out.println(“Precision by threshold: ” + precision.collect());</p><p>   JavaRDD&lt;Tuple2&lt;Object, Object&gt;&gt; recall = metrics.recallByThreshold().toJavaRDD();</p><p>   System.out.println(“Recall by threshold: ” + recall.collect());</p><p>   JavaRDD&lt;Tuple2&lt;Object, Object&gt;&gt; f = metrics.fMeasureByThreshold().toJavaRDD();</p><p>   System.out.println(“F by threshold: ” + f.collect());</p><p>Note that the metrics are designed for algorithms that can have multiple threshold values (i.e. the classification can have an associated probability).  However, the decision tree algorithm we are using is a simple yes/no binary classification.  A “confusion matrix” is a simple way of understanding the evaluation metrics. For each of the two states of reality (no reaction or reaction from the monolith) the model can make a true or false prediction giving four possible outcomes: Correct: TN = True Negative (the model correctly predicted no reaction), TP = True Positive (correctly predicted a reaction). And Incorrect: FP = False Positive (predicted a reaction but there was no reaction), FN = False Negative (predicted no reaction but there was a reaction).  FP is often called a Type I error, and FN a type II error. </p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr.png\"><img class=\"aligncenter wp-image-6895 size-large\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-1024x382.png\" alt=\"Model Prediction Instaclustr\" width=\"1024\" height=\"382\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-1024x382.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-300x112.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-768x287.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-966x360.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-640x239.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-129x48.png 129w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr-290x108.png 290w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Model-prediction-Instaclustr.png 1544w\" /></a></p><p><b>Precision</b> = (TP)/(TP+FP), the proportion of <i>predicted</i> <i>positives</i> that were actually positive (the right column).</p><p><b>Recall = </b>(TP)/(TP+FN), the proportion of <i>actual</i> <i>positives</i> that were correctly predicted as positive (the bottom row). </p><p><a href=\"https://en.wikipedia.org/wiki/F1_score\" target=\"_blank\" rel=\"noopener\">F is the average of precision and recall.</a></p><p>And the results (on the extended When Will the Monolith React? data) were as follows:</p><p>Precision = 71%</p><p>Recall = 45%</p><p>F = 56%</p><p>Precision is better than recall. 71% of the models predicted “Reaction” cases were in fact a “Reaction”. However, the model only correctly predicted 45% of all the actual “Reaction” cases correctly. The reason for using precision and recall metrics is to check if the model performance is purely the result of guessing. For example, if only 20% of examples are positive, then just guessing will result in a model “accuracy” approaching 80%.</p><h4>FILTER</h4><p>Another common Spark transformation is filter.   We’ll use filter to count the number of positive and negative examples in the data to check if our model is any better than guessing. filter() takes a function as the argument, applies it to each element in the dataset, and only returns the element if the function evaluates to true. </p><p>This code calculates that there are 884 examples, 155 positive and 729 negative, giving:</p><p>probability of positive example = 0.1753393665158371</p><p>probability of negative example = 0.8246606334841629</p><p>This tells us that just guessing would result in close to 82% accuracy. The actual model accuracy for the example is 85% (which requires extra code, not shown).</p><h2>Spark Context</h2><p><a href=\"http://spark.apache.org/docs/latest/rdd-programming-guide.html\" target=\"_blank\" rel=\"noopener\">http://spark.apache.org/docs/latest/rdd-programming-guide.html</a></p><p><a href=\"https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html\" target=\"_blank\" rel=\"noopener\">https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html</a></p><p>This leaves us with the final but “important bit” of the code at the start.  How is Spark actually run?  A SparkContext object tells Spark how to access a cluster, and a SparkConf has information about your application.  Given that we are just running Spark locally just pass “local” to setMaster.</p><p>SparkConf conf = new SparkConf().setAppName(“Java Decision Tree Classification Example”);<br />conf.setMaster(“local”);<br />SparkContext sc = new SparkContext(conf);<br />String path = “WillTheMonolithReact.txt”;</p><p><a href=\"https://spark.apache.org/docs/latest/cluster-overview.html\" target=\"_blank\" rel=\"noopener\">To run Spark on a cluster</a> you need (a) a cluster with Spark set-up (e.g. an Instaclustr cluster with Spark add-on), and (b) <a href=\"https://support.instaclustr.com/hc/en-us/articles/213097877-Getting-Started-with-Instaclustr-Spark-Cassandra\" target=\"_blank\" rel=\"noopener\">to know more about the Spark architecture and how to package and submit applications</a>.</p><p>How does this help with our approach to the Monolith? Maybe raising the POD’s manipulators in “greeting”? Will it be friends?</p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/2001-4d.jpeg\"><img class=\"alignnone size-medium wp-image-6896\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/2001-4d-300x140.jpeg\" alt=\"\" width=\"300\" height=\"140\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4d-300x140.jpeg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4d-103x48.jpeg 103w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4d-232x108.jpeg 232w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4d.jpeg 638w\" /></a></p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/2001-4e.jpeg\"><img class=\"alignnone size-medium wp-image-6897\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/2001-4e-300x140.jpeg\" alt=\"\" width=\"300\" height=\"140\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4e-300x140.jpeg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4e-103x48.jpeg 103w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4e-232x108.jpeg 232w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4e.jpeg 638w\" /></a></p><p><a href=\"https://www.instaclustr.com/wp-content/uploads/2017/09/2001-4f.jpeg\"><img class=\"alignnone size-medium wp-image-6898\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/09/2001-4f-300x140.jpeg\" alt=\"\" width=\"300\" height=\"140\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4f-300x140.jpeg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4f-103x48.jpeg 103w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4f-232x108.jpeg 232w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/2001-4f.jpeg 638w\" /></a></p><h4>NOTE 1</h4><p>The actual data I used for this example was from the Instametrics example introduced in previous blogs, with the goal of predicting long JVM Garbage Collections in advance. I took a small sample of JVM-related metrics, and computed the min, avg and max for each 5 minute bucket. These became the features. For the label I determined if there was a long GC in the next 5 minute bucket (1) or not (0).  The real data has 1,000s of metrics, so next blog we’re going to need some serious Spark processing even just to produce the training data, and explore the interface to Cassandra, and the suitability of Cassandra for Sparse data.</p><h4>NOTE 2</h4><p>Space travel is sooooo slow, in the years we’ve been on board Spark has changed from RDD to DataFrames. I’ll revise the code for the next blog.</p><p><i>Guide </i><a href=\"https://spark.apache.org/docs/latest/ml-guide.html\" target=\"_blank\" rel=\"noopener\"><i>https://spark.apache.org/docs/latest/ml-guide.html</i></a></p><p><i>As of Spark 2.0, the </i><a href=\"https://spark.apache.org/docs/latest/programming-guide.html#resilient-distributed-datasets-rdds\" target=\"_blank\" rel=\"noopener\"><i>RDD</i></a><i>-based APIs in the </i><i>spark.mllib</i><i> package have entered maintenance mode. The primary Machine Learning API for Spark is now the </i><a href=\"https://spark.apache.org/docs/latest/sql-programming-guide.html\" target=\"_blank\" rel=\"noopener\"><i>DataFrame</i></a><i>-based API in the </i><i>spark.ml</i><i> package.</i></p>",
        "created_at": "2018-09-13T14:50:21+0000",
        "updated_at": "2018-09-13T14:50:40+0000",
        "published_at": "2017-09-29T09:16:28+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 13,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2017/09/In-the-pod-Instaclustr.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12149"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          }
        ],
        "is_public": false,
        "id": 12148,
        "uid": null,
        "title": "Third Contact With a Monolith - Beam Me Down Scotty - Instaclustr",
        "url": "https://www.instaclustr.com/third-contact-monolith-beam-scotty/",
        "content": "<p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr.jpg\"><br /><img class=\"aligncenter wp-image-6851 size-full\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr.jpg\" alt=\"Third Contact with a monolith Part B instaclustr\" width=\"596\" height=\"1340\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr.jpg 596w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-133x300.jpg 133w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-455x1024.jpg 455w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-534x1200.jpg 534w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-274x616.jpg 274w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-285x640.jpg 285w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-21x48.jpg 21w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr-48x108.jpg 48w\" /></a></p><h2>Regression Analysis is (relatively) easy</h2><p>Hypothesis: <em>Using only a subset of GC metrics we can compute linear regression functions using only heap space used to predict when the next GC occurs. To do this we don’t need access to all the metrics per host, just a subset. And we can extend it in the future to use multiple variables and/or regression classification to predict which GCs are likely to be “long”.</em></p><p>Let’s look at some sample heap space data and GC time information. Here’s a graph showing heap space used increasing over time (x-axis, seconds) and then a GC kicking in (orange line), resulting in a reduction in the heap space used. The GC value is the duration of the last GC so changes from approximately 7s to 9s.</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration.png\"><img class=\"aligncenter wp-image-6852 size-large\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-1024x529.png\" alt=\"15 min sample heap and lastGC duration\" width=\"1024\" height=\"529\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-1024x529.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-300x155.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-768x397.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-966x499.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-640x331.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-93x48.png 93w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/15-min-sample-heap-and-lastGC-duration-209x108.png 209w\" /></a></p><p>The following graph shows a linear regression function fitted to the portion of the heap space used graph in the interval between GCs. It shows a significant linear correlation with R- squared 0.71.  </p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph.png\"><img class=\"aligncenter wp-image-6853 size-large\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-1024x566.png\" alt=\"Trend line for heap Instaclustr graph\" width=\"1024\" height=\"566\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-1024x566.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-300x166.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-768x424.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-966x534.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-640x354.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-87x48.png 87w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Trend-line-for-heap-instaclustr-graph-195x108.png 195w\" /></a></p><p>Is this useful? Assuming we know (or can compute) the amount of heap used that triggers a GC then yes. This may be node-specific (depending on how much heap is available), may change (as more memory is allocated), and may be a percentage (which we have to compute, for this example it was 70%). Obviously, the slope of the function will vary across and even within nodes depending on the frequency of GC (which may depend on many other variables which we’ll look at next time), so the function needs to be computed for each node for each interval between GCs. This requires us to do a few things: Find when each GC occurs so we can determine the time period from the end of the last GC to the start of the next GC to use as the sample period to compute the regression function over; for each interval normalise the actual timestamps to relative time (in seconds) from the start of the interval; keep track of the amount of heap used that triggers the GC (or compute the percentage used); and find sufficient samples for a node to make predictions and check the accuracy and usefulness of the results.</p><p>Some preliminary inspection of sample data showed a few problems. Using GC startTime and endTime seemed to be the obvious way of finding when GCs occurred. However, there was something odd about the times as they were in the distant past, in fact, they were about 300 times less than expected. Oh, that five minutes isn’t it. Turns out that they had been truncated to bucket_time making them useless for my purpose. Another thing to watch for with Cassandra timestamps is that they are in UTC. If you convert them to a Java Date type they will be in local time. The <a href=\"https://docs.oracle.com/javase/8/docs/api/java/time/Instant.html\">Java Instant class</a> is a better representation once you read the timestamps on the client side (Instant is UTC, and has lots of useful time comparison methods).</p><p>Is there a way of using either GC duration and/or collectionCount to compute the GC start/end times? Yes, in theory. Both approaches worked ok, but I noticed large gaps in the timestamps for most nodes which made using duration tricky. The final version used collectionCount and discarded GC events that were more than 1 count different.   But why were there gaps in the data?</p><h2>Materialization accidents – “beam me down Scotty!”</h2><p>My suspicion turned to the materialized views (MVs) I had to created to make the queries easier. Obviously, MVs take time to populate from an existing table, but how do you know if they are complete? There are two system tables to check, one shows build in progress, and one shows built views completed  (system.built_views): </p><h6>keyspace_name | view_name              | generation_number | last_token<br />———————-+————————-+—————————-+—————————–<br />instametrics       | host_service_time  |             16563                 | -9178511905308517795<br />instametrics       | host_service_value |             16563                | -9178511746231999096</h6><p>So this probably explained the data gaps – a few days after creation the MVs were still being built. In fact I suspect they have got “stuck” (anyone know how to unstick them? I tried dropping a MV and trying to create another but this resulted in other nasty things). Given its sci-fi origins, I should have been wary of <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a> Materialization! The Star Trek Transporter was notoriously accident prone often resulting in duplicates, combined life forms, being turned inside out, or just plain non-existence, whoops. <a href=\"https://www.thoughtco.com/worst-transporter-accidents-on-star-trek-4046426\">Worst Star Trek Transporter accidents!</a></p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Mad-Star-Blecch.jpg\"><img class=\"aligncenter wp-image-6855 size-full\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Mad-Star-Blecch.jpg\" alt=\"\" width=\"610\" height=\"554\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Mad-Star-Blecch.jpg 610w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Mad-Star-Blecch-300x272.jpg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Mad-Star-Blecch-53x48.jpg 53w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Mad-Star-Blecch-119x108.jpg 119w\" /></a></p><p>I finally got the code working and ran it on a subset of 100 nodes. However, it was only finding a relatively small number of GCs per node due to the data gaps. I realized that the default read consistency level is only one, which means that only data from the first node it is found on is returned. I tried increasing the consistency level (to QUORUM) which resulted in more GCs found. See code on GIST:</p><p>Each regression analysis uses only the first half of the available data, and if the R-squared value is over 0.5 then the computed function is used to predict the next GC time, and the percentage error is computed and averaged over all results founds and reported as follows:</p><p><strong>Total time = 428,469ms</strong></p><p><strong>avgPerErr = 21.59 gcAvgInterval = 13,447.88s used 42.0 out of 218.0</strong></p><p>The analysis took 428 seconds, for the regressions that had a sufficiently high correlation (about 20%), the average percentage prediction error for the time of the next GC was 22%. The average time between GCs was 13,447 seconds, so the average error is about 3,000 seconds. So this approach would work well, but for only 20% of cases. There’s obviously something more complicated going on for the majority of cases requiring us to get even closer to the monolith, we need to install the analysis code closer to the data. </p><aside class=\"content-cta\"><div class=\"primary\"><h4>Related Articles:</h4></div></aside>",
        "created_at": "2018-09-13T14:49:49+0000",
        "updated_at": "2018-09-13T14:49:55+0000",
        "published_at": "2017-09-20T16:30:18+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2017/09/Space-Odyssey-image-Instaclustr.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12148"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12147,
        "uid": null,
        "title": "Third contact with a Monolith - Long Range Sensor Scan - Instaclustr",
        "url": "https://www.instaclustr.com/third-contact-monolith-long-range-sensor-scan/",
        "content": "<p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick.jpg\"><img class=\"aligncenter wp-image-6433 size-full\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick.jpg\" alt=\"2001 a space odyssey - Third Contatc with the monolith Instaclustr\" width=\"736\" height=\"1127\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick.jpg 736w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick-196x300.jpg 196w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick-669x1024.jpg 669w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick-402x616.jpg 402w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick-418x640.jpg 418w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick-31x48.jpg 31w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick-71x108.jpg 71w\" /></a></p><p><img class=\"aligncenter wp-image-6819 size-full\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Distance-of-the-Planets-from-the-sun-in-Astronomical-Units-Instaclustr.png\" alt=\"Distance of the Planets from the sun in Astronomical Units Instaclustr\" width=\"599\" height=\"196\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Distance-of-the-Planets-from-the-sun-in-Astronomical-Units-Instaclustr.png 599w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Distance-of-the-Planets-from-the-sun-in-Astronomical-Units-Instaclustr-300x98.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Distance-of-the-Planets-from-the-sun-in-Astronomical-Units-Instaclustr-147x48.png 147w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Distance-of-the-Planets-from-the-sun-in-Astronomical-Units-Instaclustr-330x108.png 330w\" /></p><h6><strong>Earth to Mars distance = 0.52 AU (1.52-1AU, 78M km)</strong></h6><h6><strong>Earth to Jupiter distance = 4.2 AU (5.2-1AU, 628M km)</strong></h6><p>It’s a long way to Jupiter, would you like to:</p><h6><strong>(a) sleep the whole way in suspended animation?  (bad choice, you don’t wake up)</strong></h6><h6><strong>(b) be embodied as HAL the AI? (go crazy and get unplugged)?</strong></h6><h6><strong>(c) be one of the two crew to stay awake the entire journey (only one of you survives)?</strong></h6><p>What can we do to find out more about our eventual destination the meantime and relieve the boredom?</p><p>In the traditions of the best sci-fi, there are always tricks to speed things up or cut down on the special effects budget. In <i>Star Trek</i> these were Sensors to Scan distant phenomenon:</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display.jpg\"><img class=\"aligncenter wp-image-6820 size-large\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-1024x765.jpg\" alt=\"Spock and the Enterprise bridge sensor display\" width=\"1024\" height=\"765\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display.jpg 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-300x224.jpg 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-768x574.jpg 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-825x616.jpg 825w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-640x478.jpg 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-64x48.jpg 64w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Spock-and-the-Enterprise-bridge-sensor-display-145x108.jpg 145w\" /></a></p><p>And the Transporter which used Materialization and Dematerialization to beam people to and from the ship and the surface of planets. </p><p>Energize!</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr.png\"><img class=\"wp-image-6821 size-large aligncenter\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-1024x764.png\" alt=\"Start Trek - Energize Instaclustr\" width=\"1024\" height=\"764\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-1024x764.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-300x224.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-768x573.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-825x616.png 825w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-640x478.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-64x48.png 64w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr-145x108.png 145w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/09/Star-Trek-Energize-Instaclustr.png 1522w\" /></a></p><p>At the risk of mixing sci-fi movie metaphors:</p><p><em>“Captain’s Log, Stardate 1672.1. Warp drive offline. We’re running on auxiliary impulse engines to Jupiter to explore an anomaly. Our mission is to explore a snapshot of the Instametrics monitoring data on a Cassandra cluster that has appeared around the orbit of Jupiter. Our goal is to predict long JVM garbage collection durations at least five minutes in advance. Initiating long-range sensor scans after which I will attempt a risky long-range transportation down to the surface of the anomaly.”</em></p><p>Let’s get <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a> to do all the hard work to reduce the amount of data we need to transfer from the cluster to my laptop. There are two approaches that may result in some useful insights. Calculating some summary statistics, and creating a materialized view.</p><p>The first challenge is connecting to the Cassandra cluster. Unlike the trial cluster example I used previously, this cluster had been created with private IP broadcast_rpc_adresses rather than public IP addresses. Does this mean I can’t connect to the cluster from outside the private address range at all? Not exactly. You can still connect via a single public IP address, which will then become the only controller node your client communicates with. For testing, this is probably ok, but there is a <a href=\"https://www.instaclustr.com/apache-cassandra-deployed-on-private-and-public-networks/\">better solution</a> using VPC peering and the private IP addresses.</p><p>The next step is to see what JVM GC related metrics were collected. </p><p>Connecting via the cqlshell, we can run some CQL commands. </p><p>Describe instametrics;</p><p>Reveals that there are some meta-data tables which contain all the host names, and all the metric (“service”) names for each host:</p><p>How many nodes are there in this sample data set? </p><p>cqlsh&gt; select count(host) from instametrics.host;</p><p><b>system.count(host)<br /></b>————————–<br /><b>                               591</b></p><p>The next question I had was how many metrics are there per node? Using a sample of the host names found in the host table I ran this query a few times:</p><p>select count(service) from instametrics.service_per_host where host=’random host name’;</p><p>The somewhat surprising answer was between 1 and lots (16122!). About 1000 seemed an average so this means about 591*1000 metrics in total = 591,000. </p><p>From this distance, we therefore need to focus on just a few metrics. Selecting all the metrics available for one host we find that the following are the only metrics directly related to JVM GC.</p><p><b>collectionCount</b> is the count of the GC, and <b>collectionTime</b> is the total GC collection time, since last JVM restart.</p><h6><strong>/cassandra/jvm/gc/ConcurrentMarkSweep/collectionCount                                                                               </strong></h6><h6><strong>/cassandra/jvm/gc/ConcurrentMarkSweep/collectionTime</strong></h6><p><b>duration</b> is the time of the last occurring GC, and <b>startTime</b> and <b>endTime</b> are the time of the start and end of the last occurring GC.</p><h6><strong>/cassandra/jvm/gc/ConcurrentMarkSweep/lastGc/duration</strong></h6><h6><strong>/cassandra/jvm/gc/ConcurrentMarkSweep/lastGc/endTime</strong></h6><h6><strong>/</strong>cassandra<strong>/</strong>jvm<strong>/</strong>gc<strong>/ConcurrentMarkSweep/</strong>lastGc<strong>/</strong>startTime</h6><p>I normally start a data analysis by looking at statistics for some of the metrics of interest, such as distribution (e.g. a CDF graph) or a histogram.  But to compute these statistics you need access to all the data…</p><p>Kirk: “Mister Spock. Full sensor scan of the anomaly, please.”</p><p>Spock:  “Captain,  the results are not logical – I’m picking up what looks like Buckets – Fascinating!”</p><p>Buckets can be problematic:</p><ul><li>A drop in the bucket</li>\n<li>To kick the bucket</li>\n<li>There’s a hole in my bucket</li>\n</ul><p>The table with the actual values for the hosts and services is defined as follows:</p><p>The partition key requires values for <b>host, bucket_time </b>and<b> service</b> to select a subset of rows.   What’s bucket_time? Based on the name of the table and inspecting a sample of the rows, it’s the timestamp truncated to 5 minute bucket periods. How do we find the range of bucket_time values for the table? Here’s a few ways… You just “know it” (but I don’t).  The range has been recorded in another table (not that I can see). Or, start from now and search backwards in time by 5 minute intervals. This assumes that the data is really from the past and not the future. Start from the Epoch and search forward? Do a binary search between the Epoch and now? Look at samples of the data? This revealed that at least some of the data was from the early part of 2016. This would give us a starting point to search forwards and backwards in time until gaps are detected when we can assume they are the temporal boundaries of the data (assuming there aren’t large gaps in the data).</p><p>An alternative approach is to make a copy of the table with bucket_time removed from the partition key. Is this possible? Well, yes, in theory. <a href=\"http://cassandra.apache.org/doc/latest/cql/mvs.html\">Materialized VIews </a>are possible in Cassandra, and can even be created from existing tables. I created two materialized views, one with the values ordered (descending), and one with the time ordered (ascending).</p><p>Note that the rules for Materialized Views are very specific. You have to include all the columns from the original PRIMARY KEY in the new table (but they can be in the partition or cluster keys and in different orders), you can include at most one extra column in the new primary key, and the select has to be present and include at least one column – even though the partition key columns are selected by default). The idea was to use the first materialized view to get some summary statistics of the GC durations (e.g. min, average, max), and the second for queries over specific time periods and metrics (to find GCs, and heap memory used between GCs).</p><p>This worked as expected, taking 172 seconds, with these results:</p><p>Total time = 172343, nodes = 591</p><p>Overall stats: Min = 47.0</p><p>Overall stats: Avg = 6555.39</p><p>Overall stats: Max = 56095.0</p><p>The maximum GC duration was 56s, with an “average” of 6.5s.</p><p>Here’s the partial code I used:</p><p>On closer inspection, the average values were incorrect, as the GC duration values are repeated for multiple timestamps.  However, I also obtained the max GC duration for each node, revealing that about 200 nodes (out of 591) had maximum GC durations greater than 10s. This is a good start for “remote” data analysis.</p><aside class=\"content-cta\"><div class=\"primary\"><h4>Related Articles:</h4></div></aside>",
        "created_at": "2018-09-13T14:49:16+0000",
        "updated_at": "2018-09-13T14:49:23+0000",
        "published_at": "2017-09-14T16:31:02+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2017/09/6de5fa5f6be3a4d882e36395e3def498-a-space-odyssey-stanley-kubrick.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12147"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 12146,
        "uid": null,
        "title": "Cassandra NoSQL Data Model Design - Instaclustr",
        "url": "https://www.instaclustr.com/cassandra-nosql-data-model-design-2/",
        "content": "<h2>Abstract</h2><p>This paper describes the process that we follow at<br />Instaclustr to design a <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a> data model for our<br />customers.</p><p>While not a prescriptive, formal process it does define<br />phases and steps that our team follows when we are<br />design a new data model for our customers:</p><p><strong>Phase 1: Understand the data</strong></p><p><strong>Phase 2: Define the entities</strong></p><p><strong>Phase 3: Review &amp; tune</strong></p><p>As well as defining the process we also provide a<br />worked example based on building a database to<br />store and retrieve log messages from multiple servers.</p><h2>Overview</h2><p>We recently published a blog post on the most common data modelling mistakes that we see with<br /><a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a>. This post was very popular and led me to think about what advice we could provide on<br />how to approach designing your Cassandra data model so as to come up with a quality design that<br />avoids the traps.</p><p>There are a number of good articles around that with rules and patterns to fit your data model into:</p><p><a href=\"https://www.instaclustr.com/resource/6-step-guide-to-apache-cassandra-data-modelling-white-paper/\" target=\"_blank\" rel=\"noopener\">6 Step Guide to Apache Cassandra Data Modelling </a></p><p>and</p><p><a href=\"https://support.instaclustr.com/hc/en-us/articles/207071957-Data-Modelling-Recommended-Practices\" target=\"_blank\" rel=\"noopener\">Data Modelling Recommended Practices</a></p><p>However, we haven’t found a step by step guide to analysing your data to determine how to fit in<br />these rules and patterns. This white paper is a quick attempt at filling that gap.</p><h2>Phase 1: Understand the data</h2><p>This phase has two distinct steps that are both designed to gain a good understanding of the data that<br />you are modelling and the access patterns required.</p><h3>Define the data domain</h3><p>The first step is to get a good understanding of your data domain. As someone very familiar with<br />relation data modelling, I tend to sketch (or at least think) ER diagrams to understand the entities,<br />their keys and relationships. However, if you’re familiar with another notation then it would likely<br />work just as well. The key things you need to understand at a logical level are:</p><p>• What are the entities (or objects) in your data model?<br />• What are the primary key attributes of the entities?<br />• What are the relationships between the entities (i.e. references from one to the other)?<br />• What is the relative cardinality of the relationships (i.e. if you have a one to many is it one to<br />10 or one to 10,000 on average)?</p><p>Basically, these are the same things you’d expect in from logical ER model (although we probably<br />don’t need a complete picture of all the attributes) along with a complete understanding of the<br />cardinality of relationships that you’d normally need for a relational model.<br />An understanding of the demographics of key attributes (cardinality, distribution) will also be useful<br />in finalising your Cassandra model. Also, understand which key attributes are fixed and which change<br />over the life of a record.</p><h3>Define the required access patterns</h3><p>The next step, or quite likely a step carried out in conjunction with step 1, is to understand how you<br />will need to access your data:</p><ul><li>List out the paths you will follow to access the data, such as: <ul><li>Start with a customer id, search for transactions in a date range and then look up all the<br />details about a particular transaction from the search resultsStart with a particular server and metric, retrieve x metrics values in ascending age</li> <li>Start with a particular server and metric, retrieve x metrics values in ascending age starting at a particular point in time.</li> <li>For a given sensor, retrieve all readings of multiple metrics for a given day.</li> <li>For a given sensor, retrieve the current value.</li> </ul></li> </ul><ul><li>Remember that any updates of a record are an access path that needs to be considered</li> <li>Determine which accesses are the most crucial from a performance point of view – are there some which need to be as quick as possible while performance requirements for others allow time for multiple reads or range scans?</li> <li>Remember that you need a pretty complete understanding of how you will access your data at this stage – part of the trade-off for Cassandra’s performance, reliability and scalability is a fairly restricted set of methods for accessing data in a particular table.</li> </ul><h2>Phase 2: Understand the entities</h2><p>This phase has two specific steps designed to gain an understanding of both the primary and<br />secondary entities associated with the data.</p><h3>Identify primary access entities</h3><p>Now we’re moving from analysing your data domain and application requirements to starting to<br />design your data model. You really want to be pretty solid on steps 1 and 2 before moving on to this<br />stage.</p><p>The idea here is to denormalize your data into the smallest number of tables possible based on your<br />access patterns. For each lookup by key that your access patterns require, you will need a table to<br />satisfy that lookup. I’ve coined the term primary access entity to describe the entity your using for<br />the lookup (for example, a lookup by client id is using client as the primary access entity, a lookup by<br />server and metric name is using a server-metric entity as the primary access entity).</p><p>The primary access entity defines the partition level (or grain if you’re familiar with dimensional<br />modelling) of the resulting denormalized table (i.e. there will be one partition in the table for each<br />instance of the primary access entity).</p><p>You may choose to satisfy some access patterns using secondary indexes rather than complete replicas<br />of the data with a different primary access entity. Keep in mind that columns in include in a secondary<br />index should have a significantly lower cardinality than the table being indexed and be aware of the<br />frequency of updates of the indexed value.</p><p>For the example access patterns above, we would define the following primary access entities:</p><ul><li>customer and transaction (get a list of transactions from the customer entity and then use that<br />to look up transaction details from the transaction entity)</li> <li>server-metric</li> <li>sensor</li> <li>sensor</li> </ul><h3>Allocate secondary entities</h3><p>The next step is to find a place to store the data that belongs to entities that have not been chosen as<br />primary access entities (I’ll call these entities secondary entities). You can choose to:</p><ul><li><strong> Push down</strong> by taking data from a parent secondary entity (one side) of a one to many<br />relationship and storing multiple copies of it at the primary access entity level (for example,<br />storing customer phone number in each customer order record); or</li> <li><strong>Push up</strong> by taking data from the child secondary entity (many side) of a one to many<br />relationship and storing it at the primary access entity level either by use of cluster keys or by<br />use of multi-value types (list and maps) (for example adding a list of line items to a transaction<br />level table).</li> </ul><p>For some secondary entities, there will only be one related primary access entity and so there is no<br />need to choose where and which direction to push. For other entities, you will need to choose will<br />need to choose which primary access entities to push the data into.</p><p>For optimal read performance, you should push a copy of the data to every primary access entity that<br />is used as an access path for the data in the secondary entity.</p><p>However, this comes at an insert/update performance and application complexity cost of maintaining<br />multiple copies the data. This trade-off between read performance and data maintenance cost needs<br />to be judged in the context of the specific performance requirements of your application.</p><p>The other decision to be made at this stage is between using a cluster key or a multi-value type for<br />pushing up. In general:</p><ul><li>Use a clustering key where there is only one child secondary entity to push up and particularly<br />where the child secondary entity itself has children to roll-up.</li> <li>Use multi-value types where there are multiple child entities to push up into the primary entity</li> </ul><p>Note that these rules are probably oversimplified but serve as a starting point for more detailed<br />consideration.</p><h2>Phase 3: Review &amp; Tune</h2><p>The last phase provides an opportunity to review the data model, test and to tune as necessary.</p><h3>Review partition &amp; cluster keys</h3><p>Entering this stage, you have all the data you need to store allocated to a table or tables and your<br />tables support accessing that data according to your required access patterns. The next step is to<br />check that the resulting data model makes efficient use of Cassandra and, if not, to adjust. The items<br />to check and adjust at this stage are:</p><ul><li><strong>Do your partition keys have sufficient cardinality?</strong> If not, it may be necessary to move<br />columns from the clustering key to the partition key (e.g. changing primary key (client_id,<br />timestamp) to primary key ((client_id, timestamp))) or introduce new columns which group<br />multiple cluster keys into partitions (e.g. changing primary key (client_id, timestamp) to<br />primary key ((client_id, day), timestamp).</li> <li><strong>Will the values in your partition keys be updated frequently?</strong> Updates of a primary key<br />value will result in deletion and re-insertion of the record which can result in issues with<br />tombstones. For example, trying to maintain a table with all clients of a particular status, you<br />might have primary key (status, client ID). However, this will result in a delete and re-insert<br />every time a client’s status changes. This would be a good candidate to use a set or list data<br />type rather than including client ID as the cluster key.</li> <li><strong>Is the number of records in each partition bounded?</strong> Extremely large partitions and/or very<br />unevenly sized partitions can cause issues. For example, if you have a client_updates table<br />with primary key (client_id, update_timestamp) there is potentially no limit to how many times<br />a particular client record can be update and you may have significant unevenness if you have a<br />small number of clients that have been around for 10 years and most clients only having<br />a day or two’s history. This is another example where it’s useful to introduce new columns<br />which group multiple cluster keys into partitions partitions (e.g. changing primary key (client_<br />id, update_timestamp) to primary key ((client_id, month), update_timestamp).</li> </ul><h3>Test and tune</h3><p>The final step is perhaps the most important – test your data model and tune it as required. Keep in<br />mind that issues like partitions or rows growing too large or tombstones building up in a table may<br />only become visible after days (or longer) of use under real-world load. It’s therefore important to test<br />as closely as possible to real-world load and to monitor closely for any warning signs (the nodetool<br />cfstats and cfhistograms commands are very useful for this).</p><p>At this stage you may also consider tuning some of the settings that effect the physical storage of your<br />data. For example:</p><ul><li>changing compaction strategy;</li> <li>reducing gc_grace_seconds if you are only deleting data using TTL; <strong>or</strong></li> <li>setting caching options.</li> </ul><h2>A Worked Example</h2><p>To illustrate this, I’ll walk through a basic example based on building a database to store and retrieve<br />log messages from multiple servers. Note this is quite simplified compared to most real-world<br />requirements.</p><h3>Step 1: Define the data domain</h3><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design.png\"><img class=\"aligncenter wp-image-6928 size-large\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-1024x616.png\" alt=\"Defining the data model domain Instaclustr Data Model design\" width=\"1024\" height=\"616\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-1024x616.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-300x180.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-768x462.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-966x581.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-640x385.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-80x48.png 80w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-180x108.png 180w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design.png 1576w\" /></a></p><p>The previous ER diagram illustrated the data domain. We have:</p><ul><li>Lots (millions) of log messages which have a timestamp and a body. Although message ID is<br />shown as the primary key in the ER diagram, message time plus message type is an alternate<br />primary key.</li> <li>Each log message has a message type and types are further grouped into a message category<br />(for example, a message type might be “out of memory error” and category might be “error”).<br />There a couple of hundred message types and around 20 categories.</li> <li>Each log message comes from a message source. The message source is the server that<br />generated the message. There are 1000s of servers in our system. Each message source has a<br />source type to categorise the source (e.g. red hat server, ubuntu server, windows server,<br />router, etc.). There are around 20 source types. There are ~10,000 messages per source per<br />day.</li> <li>The message body can be parsed and stored as multiple message parts (basically key, value<br />pairs). There is typically less than 20 parts per message.</li> </ul><h3>Step 2: Define the required access patterns</h3><p>We need to be able to:</p><ul><li>Retrieve all available information about the most recent 10 messages for a given source (and<br />be able to work back in time from there).</li> <li>Retrieve all available information about the most recent 10 message for a given source type.</li> </ul><h3>Step 3: Identify primary access entities</h3><p>There are two primary access entities here – source and source type. The cardinality (~20) of source<br />type makes it a good candidate for a secondary index so we will use source as the primary access<br />entity and add a secondary index for source type.</p><h3>Step 4: Allocate secondary entities</h3><p>In this example, this step is relatively simple as all data needs to roll into the log source primary access<br />entity. So we:</p><ul><li>Push down source type name</li> <li>Push down message category and message type to log message</li> <li>Push up log message as the clustering key for the new entity</li> <li>Push up message part as a map type with.</li> </ul><p>The end result is that would be a single table with a partition key of source ID and a clustering key of<br />(message time, message type).</p><h3>Step 5: Review partition and cluster keys</h3><p>Checking these partition and cluster keys against the checklist:</p><ul><li>Do your partition keys have sufficient cardinality? Yes, there are 1000s of sources.</li> <li>Will the values in your partition keys being updated frequently? No, all the data is write-once.</li> <li>Is the number of records in each partition bounded? No – messages could build up indefinitely over time.</li> </ul><p>So, we need to address the unbound partition size. A typical pattern to address that in time series<br />data such as this is to introduce a grouping of time periods into the cluster key. In this case 10,000<br />messages per day is a reasonable number to include in one partition so we’ll use day as part of our<br />partition key.</p><p>The resulting Cassandra table will look some like:</p><h2>Conclusion</h2><p>Hopefully, this process and basic example will help you start to get familiar with Cassandra data<br />modelling. We’ve only covered a basic implementation that fits well with Cassandra, however there<br />are many other examples on the web which can help you work through more complex requirements.<br />Instaclustr also provides our customers with data modelling review and assistance, so get in touch<br />with us if you need some hands-on assistance.</p>",
        "created_at": "2018-09-13T14:47:12+0000",
        "updated_at": "2018-09-13T14:48:58+0000",
        "published_at": "2017-10-05T12:22:11+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 11,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-1024x616.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12146"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12145,
        "uid": null,
        "title": "Apache Cassandra Scalability: Allow Filtering and Partition Keys - Instaclustr",
        "url": "https://www.instaclustr.com/apache-cassandra-scalability-allow-filtering-partition-keys/",
        "content": "<p>The ‘ALLOW FILTERING’ clause in <a href=\"https://www.instaclustr.com/apache-cassandra/\">Cassandra</a>  CQL provides greatly increased flexibility of querying. However, this flexibility comes at a substantial performance cost that should be aware of before using ‘ALLOW FILTERING’.  This post explains the costs and benefits of ALLOW FILTERING.</p><p>The data storage and query operations in Cassandra work on top of partitioned data which is similar to most of other Big Data management systems. As most people would be familiar with from relational databases, each table definition has a primary key defined which can be a single attribute or combination of 2 or more. One or more attributes from the primary key are used to specify the ‘partition key’ for the table.</p><p>Example:</p><p>Here, the primary key is composed of (machine, cpu,mtime) out of which (machine, cpu) are used as partition key and (mtime) is used as clustering key.</p><p>The purpose of a partition key is to split the data into partitions where an entire partition is stored on a single node in the cluster (with each node storing many partitions). When data is read or written from the cluster, a function called <i>Partitioner</i> is used to compute the hash value of the partition key. This hash value is used to determine the node/partition which contains that row. The clustering key is used further to search for a row within a given partition. </p><p>Select queries in Apache Cassandra look a lot like select queries from a relational database. However, they are significantly more restricted. The attributes allowed in ‘where’ clause of Cassandra query must include the full partition key and additional clauses may only reference the clustering key columns or a secondary index of the table being queried. </p><p>Requiring the partition key attributes in the ‘where’ helps Cassandra to maintain constant result-set retrieval time as the cluster is scaled-out by allowing Cassandra to determine the partition, and thus the node (and even data files on disk), that the query must be directed to. </p><p>If a query does not specify the values for all the columns from the primary key  in the ‘where’ clause, Cassandra will not execute it and give the following warning :</p><p>‘<i>InvalidRequest: Error from server: code=2200 [Invalid query] message=”Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING</i>” ‘</p><p>You can use execute queries that use a secondary index without ALLOW FILTERING – more on that later.</p><p>The reason behind this warning is that when the complete partition key is not included in the WHERE clause, there is no way for Cassandra to identify the node which contains the required results, and thus it will need to scan the complete dataset on each node to ensure it has found the required data. Hence, the query execution time is not proportional to the size of the result returned but rather to the total amount of data stored in the referenced table. One implication of this, other than initial poor performance, is that query performance is likely to get worse as your cluster scales (unlike partition key queries which exhibit constant response times while scaling)  </p><p>As I mentioned early, you can also execute queries that reference a secondary index without specifying a partition key value or using ALLOW FILTERING.</p><p>Let’s take an example to understand this:</p><p>Consider the following table holding user profiles with their year of birth (with a secondary index on it) and country of residence:</p><p><b>CREATE</b> <b>INDEX</b> <b>ON</b> <b>users</b>(birth_year);</p><p>Then the following queries are valid:</p><p><b>Query 1: SELECT</b> <b>*</b> <b>FROM</b> <b>users</b>;</p><p><b>Query 2: SELECT</b> <b>*</b> <b>FROM</b> <b>users</b> <b>WHERE</b> birth_year <b>=</b> 1981;</p><p>However, the following query will be rejected:</p><p><b>Query 3: SELECT</b> <b>*</b> <b>FROM</b> <b>users</b> <b>WHERE</b> birth_year <b>=</b> 1981 <b>AND</b> country <b>=</b> ‘AU’;</p><p>This is because Cassandra cannot guarantee that it won’t have to scan a large amount of data even if the result of the query is small. Typically, it will scan all the index entries for users born in 1981 even if only a handful are actually from Australia. However, if you “know what you are doing”, you can force the execution of this query by using ALLOW FILTERING and so the following query is valid:</p><p><b>Query 3 (with ALLOW FILTERING):SELECT * FROM users WHERE birth_year = 1981 AND country = ‘AU’ ALLOW FILTERING;</b></p><p>When using secondary indexes, it is important to understand that all three of these queries will need to hit every node in the cluster (or at least a complete set of replicas) in order to return results. In queries like this in Cassandra, one node in the cluster will act as the coordinator node, send out the query to the other nodes in the cluster that need to participate and collating the results to send back to the client. For Query 2, the coordinator will need to send a subquery  to each node in the cluster which in turn will look up the index for the data it stores and return matching rows to the coordinator which in turn returns them to the client. Query 3 is executed in much the same manner but the individual nodes not only look up the index and return data but also filter it before returning. </p><p>Thus for Query 3 the size of the result set is not related to the amount of data scanned and ALLOW FILTERING is required. However, both Query 2 and Query 3 require all nodes in the cluster to participate in the query and thus the amount of work required to complete the query increases as the cluster size grows and you will not see the linear horizontal scaling that Cassandra is famous for.</p><p>So, while ALLOW FILTERING should definitely be avoided, any query that does not specify a partition key should also be avoided to enable your Cassandra cluster to scale.</p>",
        "created_at": "2018-09-13T14:46:49+0000",
        "updated_at": "2018-09-13T14:47:01+0000",
        "published_at": "2017-10-31T13:54:18+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12145"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 931,
            "label": "data.engineering",
            "slug": "data-engineering"
          },
          {
            "id": 1351,
            "label": "migration",
            "slug": "migration"
          }
        ],
        "is_public": false,
        "id": 12144,
        "uid": null,
        "title": "Instaclustr: 7 easy steps to Cassandra cluster migration",
        "url": "https://www.computerweekly.com/blog/Open-Source-Insider/Instaclustr-7-easy-steps-to-Cassandra-cluster-migration",
        "content": "<p><em>This is a guest post for the Computer Weekly Open Source Insider blog written by <a href=\"https://www.linkedin.com/in/ben-slater-2720562/\">Ben Slater</a> in his capacity as chief product officer at Instaclustr.</em></p><p><em><a href=\"https://www.instaclustr.com/\">Instaclustr</a> positions itself as firm offering managed and supported solutions for Apache Cassandra, ScyllaDB, Elasticsearch, Apache Spark, Apache Zeppelin, Kibana and Apache Lucene. </em></p><p>Indeed, Instaclustr is known for its willingness to describe itself as a managed open source as a service company, if that expression actually exists.</p><p>The original title in full for this piece was: Migrating Your Cassandra Cluster – with Zero Downtime – in 7 Easy Steps.</p><p>Slater’s moves for writing this piece are (obviously) directed at companies who are looking to move a live Apache Cassandra deployment to a new location.</p><p>With this task in mind, it is (obviously) natural that these same companies will have some concerns, such as how you can keep Cassandra clusters 100% available throughout the process.</p><p>Arguing that if your application is able to remain online throughout connection setting changes, Slater says it can also remain fully available during this transition.</p><p><strong>NOTE:</strong> For extra protection and peace of mind, the following technique also includes a rapid rollback strategy to return to your original configuration, up until the moment the migration is completed.</p><p><strong>Slater writes as follows:</strong></p><p>Here’s a recommended 7-step Cassandra cluster migration order-of-operations that will avoid any downtime:</p><section class=\"section main-article-chapter\" data-menu-title=\"1) Get your existing environment ready\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">1) Get your existing environment ready</i></h3><p>First of all, make sure that your application is using a datacentre-aware load balancing policy, as well as LOCAL_*. Also, check that <u>all</u> of the keyspaces that will be copied over to the new cluster are set to use NetworkTopologyStrategy as their replication strategy. It’s also recommended that all keyspaces use this replication strategy when created, as altering this later can become complicated.</p></section><section class=\"section main-article-chapter\" data-menu-title=\"2) Create the new cluster\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">2) Create the new cluster</i></h3><p>Now it’s time to create the new cluster that you’ll be migrating to. A few things to be careful about here: be sure that the new cluster and the original cluster use the same Cassandra version and cluster name. Also, the new datacenter name that you use must be different from the name of the existing datacenter.</p></section><section class=\"section main-article-chapter\" data-menu-title=\"3) Join the clusters together\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">3) Join the clusters together</i></h3><p>To do this, first make any necessary firewall rule changes in order to allow the clusters to be joined, remembering that some changes to the source cluster may also be necessary. Then, change the new cluster’s seed nodes – and start them. Once this is done, the new cluster will be a second datacenter in the original cluster.</p></section><section class=\"section main-article-chapter\" data-menu-title=\"4) Change the replication settings\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">4) Change the replication settings<strong> </strong></i></h3><p>Next, in the existing cluster, update the replication settings for the keyspaces that will be copied, so that data will now be replicated with the new datacenter as the destination.</p></section><section class=\"section main-article-chapter\" data-menu-title=\"5) Copy the data to the new cluster\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">5) Copy the data to the new cluster</i></h3><p>When the clusters are joined together, Cassandra will begin to replicate writes to the new cluster. It’s still necessary, however, to copy any existing data over with the nodetool rebuild function. It’s a best practice to perform this function on the new cluster one or two nodes at a time, so as not to place an overwhelming streaming load on the existing cluster.</p></section><section class=\"section main-article-chapter\" data-menu-title=\"6) Change over the application’s connection points\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">6) Change over the application’s connection points</i></h3><p>After all uses of the rebuild function are completed, each of the clusters will contain a complete copy of the data being migrated, which Cassandra will keep in sync automatically. It’s now time to change the initial connection points of your application over to the nodes in the new cluster. Once this is completed, all reads and writes will be served by the new cluster, and will subsequently be replicated in the original cluster. Finally, it’s smart to run a repair function across the cluster, in order to ensure that all data has been replicated successfully from the original.<strong> </strong></p></section><section class=\"section main-article-chapter\" data-menu-title=\"7) Shut down the original cluster\"><h3 class=\"section-title\"><i class=\"icon\" data-icon=\"1\">7) Shut down the original cluster</i></h3><p>Complete the process with a little post-migration clean up, removing the original cluster. First, change the firewall rules to disconnect the original cluster from the new one. Then, update the replication settings in the new cluster to cease replication of data to the original cluster. Lastly, shut the original cluster down.</p><p>There you have it: your Apache Cassandra deployment has been fully migrated, with zero downtime, low risk and in a manner completely seamless and transparent from the perspective of your end users.</p><p><em>You can follow </em><em><a href=\"https://twitter.com/instaclustr?lang=en\">Instaclustr on Twitter</a>.</em></p><p><img class=\"wp-align alignnone size-full wp-image-1934\" src=\"https://itknowledgeexchange.techtarget.com/open-source-insider/files/2017/11/111P0AlzIUIAA0fc-.jpg\" alt=\"\" width=\"580\" height=\"441\" srcset=\"https://itknowledgeexchange.techtarget.com/open-source-insider/files/2017/11/111P0AlzIUIAA0fc-.jpg 580w, https://itknowledgeexchange.techtarget.com/open-source-insider/files/2017/11/111P0AlzIUIAA0fc--300x228.jpg 300w\" /></p></section>",
        "created_at": "2018-09-13T14:46:22+0000",
        "updated_at": "2018-09-13T14:46:33+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "www.computerweekly.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12144"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 1122,
            "label": "zeppelin",
            "slug": "zeppelin"
          }
        ],
        "is_public": false,
        "id": 12143,
        "uid": null,
        "title": "Pick‘n’Mix: Cassandra, Spark, Zeppelin, Elassandra, Kibana, & Kafka - Instaclustr",
        "url": "https://www.instaclustr.com/picknmix-cassandra-spark-zeppelin-elassandra-kibana-kafka/",
        "content": "<h2><b>Kafkaesque</b>:</h2><h3> \\ käf-kə-ˈesk \\</h3><p><em>Marked by a senseless, disorienting, menacing, nightmarishly complexity.</em></p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic.png\"><img class=\"alignnone size-full wp-image-8009\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic.png\" alt=\"Kafkaesque Cassandra, Spark, Zeppelin, Elassandra, Kibana, &amp; Kafka\" width=\"734\" height=\"524\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic.png 734w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic-300x214.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic-640x457.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic-67x48.png 67w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafkaesque-comic-151x108.png 151w\" /></a></p><p><a href=\"https://www.gutenberg.org/files/5200/5200-h/5200-h.htm\" target=\"_blank\" rel=\"noopener\">One morning when I woke from troubled dreams</a>, I decided to blog about something potentially Kafkaesque: Which Instaclustr managed open-source-as-a-service(s) can be used together (current and future)? Which combinations are actually possible? Which ones are realistically sensible? And which are nightmarishly Kafkaesque!?</p><p>In previous blogs, I’ve explored Instaclustr managed <a href=\"https://www.instaclustr.com/apache-cassandra/\">Apache Cassandra</a>, <a href=\"https://www.instaclustr.com/apache-spark/\">Spark</a> (batch), Spark Streaming, Spark MLLib, and Zeppelin.</p><p>Instaclustr also supports managed <a href=\"https://www.instaclustr.com/solutions/managed-elasticsearch/\" target=\"_blank\" rel=\"noopener\">Elassandra</a> and <a href=\"https://www.instaclustr.com/fantasy-basketball-data-exploration/\" target=\"_blank\" rel=\"noopener\">Kibana</a>. Elassandra is an integrated <a href=\"https://github.com/elastic/elasticsearch\" target=\"_blank\" rel=\"noopener\">Elasticsearch</a> and Cassandra service which computes secondary indexes for data and supports fast queries over the indexes. Kibana is an open source data visualization plugin for Elasticsearch.  Together with <a href=\"https://github.com/elastic/logstash\" target=\"_blank\" rel=\"noopener\">Logstash</a> they form the “Elastic stack” (previously the ELK stack).</p><p><a href=\"https://www.instaclustr.com/apache-kafka/\">Apache Kafka</a>, a distributed streaming platform (massively horizontally scalable, high-throughput low-latency, high-reliability, high-availability real-time streaming data processing), is another popular service in the same Open Source ecosystem as Cassandra, Spark and Elasticsearch.  Kafka is on the Instaclustr product roadmap for 2018, and we have <a href=\"https://www.instaclustr.com/support/documentation/apache-spark/spark-streaming-kafka-and-cassandra-tutorial/\" target=\"_blank\" rel=\"noopener\">a tutorial on spark streaming with kafka and cassandra to wet your appetite. </a></p><p>Rather than jumping straight into a deep dive of Elassandra and/or Kafka I’m going to take a more architectural perspective. I started by putting all the services of interest on a diagram, and then connecting them together based on documented support for each integration combination and direction (source and/or sink):</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1.png\"><img class=\"alignnone size-full wp-image-8010\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1.png\" alt=\"Kafka Architecture Diagram\" width=\"4349\" height=\"2240\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1.png 4349w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-300x155.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-768x396.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-1024x527.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-966x498.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-640x330.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-93x48.png 93w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-1-210x108.png 210w\" /></a></p><p>Note that Cassandra, Elassandra, Spark (batch) and Spark Streaming, Spark MLLib, Zeppelin and Kibana are tightly integrated, and support most logically possible interactions. Instaclustr also co-locates all of these services on the same nodes by default. </p><p>I’ve spent some time examining the Kafka documentation to check what related ecosystem services it can connect to, and in what direction. Kafka supports Source and Sink Connectors which enable integration with numerous other services. Lots of different event sources are supported, enabling data to be ingested from both external and internal devices and systems. AWS S3 is supported as a Kafka sink only, and JDBC as both source and sink. Elassandra is supported as a sink only, and Spark Streaming and Cassandra as source and sink. </p><p>Also note that implicitly most services can “talk to” themselves (i.e. read data from, process data, and write data back. This is what the card replacement rule achieves). What’s more interesting is that they can also interact with themselves on the same or different <i>clusters</i>, and for the same or different <i>locations</i> (e.g. in another AWS AZ, or in another region). The diagram shows a Service interacting with itself (same cluster), another instance of the service in the same location (different cluster), and another instance in a different cluster and location (different location):</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2.png\"><img class=\"alignnone size-full wp-image-8011\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2.png\" alt=\"Kafka Architecture Diagram\" width=\"3631\" height=\"1973\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2.png 3631w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-300x163.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-768x417.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-1024x556.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-966x525.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-640x348.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-88x48.png 88w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-2-199x108.png 199w\" /></a></p><p>This opens up powerful internal service architectural richness and use cases. For example: Differentiation of clusters running the same service (e.g. write-intensive Cassandra cluster feeding data into a read-intensive cassandra cluster); A Kafka cluster dedicated to ingestion only, connecting to others for processing; mirroring or replicating data from one Cassandra cluster (location) to another (e.g. using Spark to read from a Cassandra cluster in one location and write to a Cassandra cluster in another location); Peer-to-Peer Kafka clusters, where each cluster subscribes to events that are local to all other Kafka clusters and aggregates the events locally), etc.</p><h2>Kafka – some key features</h2><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Some-key-features-of-Apache-Kafka.png\"><img class=\"alignnone size-full wp-image-8012\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Some-key-features-of-Apache-Kafka.png\" alt=\"Key features of Apache Kafka\" width=\"604\" height=\"513\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Some-key-features-of-Apache-Kafka.png 604w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Some-key-features-of-Apache-Kafka-300x255.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Some-key-features-of-Apache-Kafka-57x48.png 57w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Some-key-features-of-Apache-Kafka-127x108.png 127w\" /></a></p><p>The main Kafka APIs are Connectors, Producers, Consumers and Streams. Kafka is stream event-based, and producers publish events onto one or more topics. Topics are multi-subscriber and can have zero or more consumers that process events. Kafka maintains a (partitioned) immutable commit log of events for each topic, and therefore keeps all published events for a specified retention period. This approach for message processing has a number of benefits. The more obvious benefits are speed, fault-tolerance, high concurrency and scalability. The surprising benefits are that consumers and producers can be very loosely coupled, and events can be shared! More than one consumer can consume the same event, and consumers also control which events to consume – they can consume new events and also re-consume past events.</p><p>Kafka’s performance is claimed to be constant with respect to data size, so storing events for an arbitrary length of time (as long as you have disk space!) is encouraged, by design. Because events can be processed more than once, by the same or different consumers, what do we end up with? A database for streaming events!</p><p>Let’s explore some permutations of the ecosystem of services (not all permutations will be covered in this blog), starting with Kafka.  In answer to the question “What is Kafka good for?”, the Kafka documentation suggests two broad classes of application. The focus of this blog is on the first use case – getting (streaming) data between systems.</p><ol><li>Building real-time streaming data pipelines that reliably get data between systems or applications (this blog)</li> <li>Building real-time streaming applications that transform or react to the streams of data (next blog).</li> </ol><h2>Use Case: Kafka as a Database (Teenagers bedroom. Stuff goes in, stuff rarely comes out).</h2><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr.png\"><img class=\"alignnone size-full wp-image-8013\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr.png\" alt=\"Kafka as a database Instaclustr\" width=\"965\" height=\"447\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr.png 965w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr-300x139.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr-768x356.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr-640x296.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr-104x48.png 104w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-database-Instaclustr-233x108.png 233w\" /></a></p><p>Kafka only, one or more source connectors, producer(s) publishing to topic(s). No consumers:</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3.png\"><img class=\"alignnone size-full wp-image-8015\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3.png\" alt=\"Kafka Architecture diagram\" width=\"2948\" height=\"1266\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3.png 2948w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-300x129.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-768x330.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-1024x440.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-966x415.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-640x275.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-112x48.png 112w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-3-251x108.png 251w\" /></a></p><p>This is a trivial, and somewhat counterintuitive use case for Kafka but illustrates one of the surprising benefits of the architecture, that it is designed from the “bed” up as an event streaming <i>database</i> – not just for event movement. All the events arriving will be published to topic(s), and persisted to disk. Events can subsequently be consumed multiple times by multiple consumers, who do not have to be subscribed yet. Is this interesting? Yes! It suggests lots of powerful use cases around event persistence, and reprocessing/replaying of events, and adding derived events (e.g. failure handling, support for multiple consumers and purposes for DevOps to maintain derived stateful data back in Kafka for future use, as well as for processing events from past, present and future, including predictions, in a unified manner).</p><h2>Use Case: Kafka as a temporary Buffer (Doctors waiting room)</h2><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer.png\"><img class=\"alignnone size-full wp-image-8016\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer.png\" alt=\"Kafka as a temporary buffer\" width=\"798\" height=\"524\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer.png 798w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer-300x197.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer-768x504.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer-640x420.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer-73x48.png 73w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Kafka-as-a-temporary-Buffer-164x108.png 164w\" /></a></p><p>This pattern has one Kafka cluster feeding into another one:</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4.png\"><img class=\"alignnone size-full wp-image-8017\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4.png\" alt=\"Kafka Architecture Diagram\" width=\"3326\" height=\"1266\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4.png 3326w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-300x114.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-768x292.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-1024x390.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-966x368.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-640x244.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-126x48.png 126w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-4-284x108.png 284w\" /></a></p><p>This “Buffer” (waiting room) pattern has a Kafka cluster dedicated solely to event ingestion, and another cluster for the event consumers.  This leverages the ability of Kafka to store events indefinitely, and isolate event producers from consumers.  The event production and consumption rates can be significantly different with no loss of events or overloading of consumers. This pattern is ideal for use cases where an incoming event storm can temporarily exceed the processing capacity of the consumers cluster, or if there is some other temporary failure or slowdown preventing the consumers processing events in real-time. The Ingestion cluster buffers all the events until the consumers are ready to process them again. In the wild, this buffer pattern is used by <a href=\"https://medium.com/netflix-techblog/kafka-inside-keystone-pipeline-dd5aeabaf6bb\" target=\"_blank\" rel=\"noopener\">Netflix</a>.</p><p>Kafka can act as a event buffer, concentrator, and router in-front of other services in our ecosystem as well. For example, Cassandra, Spark streaming or Elassandra can all be sinks for Kafka events.</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5.png\"><img class=\"alignnone size-full wp-image-8032\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5.png\" alt=\"Kafka Architecture Diagram 5\" width=\"3326\" height=\"1635\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5.png 3326w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-300x147.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-768x378.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-1024x503.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-966x475.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-640x315.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-98x48.png 98w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/11/Architecture-Diagram-5-220x108.png 220w\" /></a></p><h2>Use Case: Kafka active-passive replication</h2><p>In the Use Cases so far we’ve only used Kafka as a pass-through buffer or longer term persistence mechanism. Kafka producers and consumers can publish/subscribe to/from multiple topics, enabling more complex topologies to be created. In particular, some less obvious patterns can be used to support <a href=\"https://www.slideshare.net/GuozhangWang/building-stream-infrastructure-across-multiple-data-centers-with-apache-kafka\">data replication across multiple Kafka clusters and locations</a>.  </p><p>There are a couple of use cases for data replication across clusters/locations. One is for reliability/redundancy and is often called active-passive replication. Data from the source (active) cluster is copied to the passive (target) cluster.  The “passive” cluster can be used in case of failure of the active cluster, or it can be used to reduce latency for consumers that are geo-located near it.</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6.png\"><img class=\"alignnone size-full wp-image-8072\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6.png\" alt=\"Kafka Architecture Diagram\" width=\"3905\" height=\"1519\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6.png 3905w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-300x117.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-768x299.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-1024x398.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-966x376.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-640x249.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-123x48.png 123w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-6-278x108.png 278w\" /></a></p><h2>Use Case: Kafka active-active replication</h2><p>A more interesting use case is when unique events are collected at different locations, and must be shared among all the locations. This can be between just two locations, or many (P2P). This is an active-active pattern and can be viewed as a generalisation of the active-passive pattern as each cluster acts as both a source and a target for every other cluster, and the events copied from other clusters need to be merged with the events from the local cluster in a new topic (Topic 2 in the diagram below), from which consumers can get all the events. Note that it has to be different topic otherwise you get an event loop!</p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7.png\"><img class=\"alignnone size-full wp-image-8073\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7.png\" alt=\"Kafka Architecture Diagram\" width=\"5012\" height=\"1632\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7.png 5012w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-300x98.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-768x250.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-1024x333.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-966x315.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-640x208.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-147x48.png 147w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Architecture-Diagram-7-332x108.png 332w\" /></a></p><h2>(Magic) Mirror Maker</h2><p><a href=\"http://www.kyotojournal.org/renewal/the-magic-mirror-maker/\"><i>In Japan, bronze mirrors are known as magic mirrors, or makkyo</i></a><i> (魔鏡). One side is brightly polished, while an embossed design decorates the reverse side. Remarkably, when light is directed onto the face of the mirror, and reflected to a flat surface, an image “magically” </i><i>appears (usually the one featured on its back):</i></p><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am.png\"><img class=\"alignnone size-full wp-image-8074\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am.png\" alt=\"Magic Mirror Maker Makkyo\" width=\"2004\" height=\"1250\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am.png 2004w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-300x187.png 300w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-768x479.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-1024x639.png 1024w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-966x603.png 966w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-640x399.png 640w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-77x48.png 77w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.40.38-am-173x108.png 173w\" /></a></p><p>For the use cases involving events being moved between Kafka clusters, how can this be achieved? One obvious mechanism is just to pretend that the clusters are “local”, and read data from the source cluster topic with a consumer and publish it to another topic on the target cluster. This approach can work with low-latency WANs (e.g. clusters on the same AWS AZ). However, there are also a number of more sophisticated solutions. <a href=\"https://kafka.apache.org/documentation.html#basic_ops_mirror_maker\">Mirror maker</a> can be used (which also just reads data from the source cluster using a consumer and publishes it to the target cluster using a producer!). Will mirror maker actually work for the active-active use case given that mirror maker can only read/write to/from topics of the same name? <a href=\"https://www.linkedin.com/pulse/2-way-replication-apache-kafka-marcio-andrada\">Maybe, here’s a trick.</a> More sophisticated solutions exist, including uReplicator from Uber.</p><ul><li><a href=\"https://eng.uber.com/ureplicator/\">https://eng.uber.com/ureplicator/</a></li> <li><a href=\"https://github.com/uber/uReplicator\">https://github.com/uber/uReplicator</a></li> <li><a href=\"https://github.com/uber/uReplicator/wiki/uReplicator-Design\">https://github.com/uber/uReplicator/wiki/uReplicator-Design</a></li> </ul><h2>Next blog:</h2><h4>What’s the difference between Spark and Kafka Streaming? Event reprocessing/replaying, unifying stream and batch processing, producing and using state, fun with time, large messages, topic discovery, and more!</h4><p><a href=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am.png\"><img class=\"alignnone size-full wp-image-8078\" src=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am.png\" alt=\"Kafka Comic\" width=\"1060\" height=\"1332\" srcset=\"https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am.png 1060w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-239x300.png 239w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-768x965.png 768w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-815x1024.png 815w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-955x1200.png 955w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-490x616.png 490w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-509x640.png 509w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-38x48.png 38w, https://24b4dt1v60e526bo2p349l4c-wpengine.netdna-ssl.com/wp-content/uploads/2017/12/Screen-Shot-2017-12-05-at-11.56.25-am-86x108.png 86w\" /></a></p><aside class=\"content-cta\"><div class=\"primary\"><h4>Related Articles:</h4></div></aside>",
        "created_at": "2018-09-13T14:45:33+0000",
        "updated_at": "2018-09-13T14:45:51+0000",
        "published_at": "2017-12-05T12:26:07+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 8,
        "domain_name": "www.instaclustr.com",
        "preview_picture": "https://www.instaclustr.com/wp-content/uploads/2017/11/Kafkaesque-comic.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12143"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1333,
            "label": "shell",
            "slug": "shell"
          }
        ],
        "is_public": false,
        "id": 12094,
        "uid": null,
        "title": "eevans/cdsh",
        "url": "https://github.com/eevans/cdsh",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>A simple wrapper script around <a href=\"https://www.netfort.gr.jp/%7Edancer/software/dsh.html.en\" rel=\"nofollow\">DSH</a>\nfor <a href=\"http://cassandra.apache.org\" rel=\"nofollow\">Apache Cassandra</a>.</p>\n<h2><a id=\"user-content-description\" class=\"anchor\" aria-hidden=\"true\" href=\"#description\"></a>Description</h2>\n<p><code>dsh</code> restricts you to flat groups lists, <code>cdsh</code> wraps <code>dsh</code> to allow you to define your\nclusters in a single YAML file (<code>~/.cdsh</code> by default), and then specify the hosts for remote\ncommands using arguments for cluster name, data-center, and rack.</p>\n<h2><a id=\"user-content-usage\" class=\"anchor\" aria-hidden=\"true\" href=\"#usage\"></a>Usage</h2>\n<pre>usage: cdsh [-h] [-c CLUSTER] [-d DATA_CENTER] [-r RACK] [--config CONFIG]\n            [-P]\n            [args [args ...]]\nA dsh wrapper for Cassandra\npositional arguments:\n  args                  arguments to dsh\noptional arguments:\n  -h, --help            show this help message and exit\n  -c CLUSTER, --cluster CLUSTER\n                        cluster name\n  -d DATA_CENTER, --data-center DATA_CENTER\n                        data-center name\n  -r RACK, --rack RACK  rack name(s)\n  --config CONFIG       yaml configuration file\n  -P, --print-hosts     output matching hosts (no command run)\n  -e EXCLUDES, --exclude EXCLUDES\n                        hosts to exclude (glob)\n</pre>\n<h2><a id=\"user-content-examples\" class=\"anchor\" aria-hidden=\"true\" href=\"#examples\"></a>Examples</h2>\n<p>Executing a command on all nodes of cluster <code>default</code>:</p>\n<pre>$ cdsh -c default -- uname -r\n</pre>\n<p>Executing a command on <code>rack1</code> of cluster <code>test</code> in data-center <code>datacenter1</code>:</p>\n<pre>$ cdsh -c test -d datacenter1 -r rack1 -- nodetool setstreamthroughput 200\n</pre>\n<p>Using the host list for other commands:</p>\n<pre>$ for i in `cdsh -c default -P`; do rsync cassandra.yaml $i:/etc/cassandra; done\n</pre>\n</article>",
        "created_at": "2018-09-07T20:25:17+0000",
        "updated_at": "2018-09-07T20:25:22+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/64348?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12094"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          }
        ],
        "is_public": false,
        "id": 12092,
        "uid": null,
        "title": "Advanced Time Series Data Modelling",
        "url": "https://www.datastax.com/dev/blog/advanced-time-series-data-modelling",
        "content": "<h2><a href=\"https://www.datastax.com/wp-content/uploads/2015/12/nest.jpg\"><img class=\"alignnone size-full wp-image-43180\" src=\"https://www.datastax.com/wp-content/uploads/2015/12/nest.jpg\" alt=\"nest\" srcset=\"https://www.datastax.com/wp-content/uploads/2015/12/nest.jpg 640w, https://www.datastax.com/wp-content/uploads/2015/12/nest-250x125.jpg 250w, https://www.datastax.com/wp-content/uploads/2015/12/nest-120x60.jpg 120w\" /></a></h2><h2>Collecting Time Series Vs Storing Time Series</h2><p>Cassandra is well known as the database of choice when collecting time series events. These may be messages, events or similar transactions that have a time element to them. If you are not familiar on how Cassandra holds time series, there is a useful data modelling tutorial on the DataStax academy website.</p><p><a href=\"https://academy.datastax.com/demos/getting-started-time-series-data-modeling\" target=\"_blank\">https://academy.datastax.com/demos/getting-started-time-series-data-modeling</a></p><p>In this document I will try to explain some of the pros and cons of using time series in Cassandra and show some techniques and tips which make make your application better not just for now but also 5 years down the line.</p><p><strong>Choosing you long term storage</strong></p><p>Choosing your long term storage is not really a trivial thing. In most applications there are business requirements about how long data will need to be held for and sometimes these requirements change. More and more, business want and are required to hold data for longer. For example, a lot of financial companies must keep audit data for up to seven years.</p><p><strong>Using some sample applications.</strong></p><p>We will look at some examples and see how time series is used for each.</p><p>1. A credit card account which shows transaction for a particular account number. Data is streamed in real time.</p><p>2. Collecting energy data for a smart meter. Data comes from files sent from devices after one day of activity.</p><p>3. Tick data for a financial instrument. Data is streamed in real time.</p><p>All of the above use cases are time series examples and would benefit from using Cassandra. But when we look at the queries and retention policies for this data we may look at different ways of storing them.</p><p><strong>Clustering columns for time series.</strong></p><p>The credit card application will need to query a users transactions and show them to the user. They will need to be in descending order with the latest transaction first. The data may be paged over multiple pages. This data needs to be kept for 7 years.</p><p>Using a simply clustering column in the table definition, will allow all the transactions for a particular account to be on one row for extremely fast retrieval.</p><p>Our table model would be similar to this</p><pre>create table if not exists latest_transactions(&#13;\n credit_card_no text,&#13;\n transaction_time timestamp,&#13;\n transaction_id text,&#13;\n user_id text,&#13;\n location text,&#13;\n items map&lt;text, double&gt;,&#13;\n merchant text,&#13;\n amount double,&#13;\n status text,&#13;\n notes text,&#13;\n PRIMARY KEY (credit_card_no, transaction_time)&#13;\n) WITH CLUSTERING ORDER BY ( transaction_time desc);</pre><p>The smart meter application is a little different. The data will come in for each meter no with data every 30 mins of increments to the meter value. Eg. 00:00 - 13, 00:30 11, 01:00 3......23:30 10. So the daily amount is an aggregation of all the data points together.</p><p>The business requirement state that the data must be held for 5 years and a days data will always be looked up together. Cassandra's has column type of Map which can be used to hold our daily readings in a format of time offset and value.</p><p>Our table model would look something like this</p><pre>create table if not exists smart_meter_reading (&#13;\n meter_id int,&#13;\n date timestamp,&#13;\n source_id text,&#13;\n readings map&lt;text, double&gt;,&#13;\n PRIMARY KEY(meter_id, date)&#13;\n) WITH CLUSTERING ORDER BY(date desc);</pre><p>This seems sensible until we look at how much data we will be holding and for how long. This application has 10 Million meters and hopes to double that over the next 3 years. If we start at 10M customers holding 365*5 years of data which 48 columns of offset data per day (every half hour), this can quickly add up to over 50o billion points (a map of 48 entries is held as 48 columns) and we haven't talked about the increase over those years. Since we don't have to query the reading individually it might suit better to look at other storage capabilities. A map can be simply transformed to and from a JSON string which would allow us to hold the same data but not have the over head of all the columns.</p><pre>create table if not exists smart_meter_reading (&#13;\n meter_id int,&#13;\n date timestamp,&#13;\n source_id text,&#13;\n readings text,&#13;\n PRIMARY KEY(meter_id, date)&#13;\n) WITH CLUSTERING ORDER BY(date desc);</pre><p>So instead of 500 billion points we now have 5 billion.</p><p>Now we finally look at application no 3. In this case we have data streaming to our application for thousands of different instruments. We can expect on 100,000 ticks a day on some of the instruments. If the requirement is to hold this data long term and be able to create different views of the data for charting capabilities, the storage requirement will be extremely large.</p><p><strong>Collecting data vs Storing data.</strong></p><p>We can collect the data in the traditional way using a clustering column with a table like so</p><pre>CREATE TABLE tick_data ( &#13;\n symbol text,&#13;\n date timestamp,&#13;\n time_of_day timestamp,&#13;\n value double,&#13;\n PRIMARY KEY ((symbol,time_of_day), date)&#13;\n) WITH CLUSTERING ORDER BY (date DESC);</pre><p>When we think of keeping this data long term we have to understand the implications of having billions of columns in our tables. Our normal queries will be charting the last 5 days of instrument data in 15 min intervals or show the open, high, low and close of an instrument for the last 10 days in 30 mins intervals. So 99% of the queries will be looking at the whole days data. In this example we can then change the long term storage for this table to create a second table which handles any requests for data that is not today. At the end of each day we can compress and the store the data more efficiently for the rest of its life in the database.</p><p>For example we can use the following</p><pre>CREATE TABLE tick_data_binary ( &#13;\n symbol text,&#13;\n date timestamp;&#13;\n dates blob,&#13;\n ticks blob,&#13;\n PRIMARY KEY ((symbol,date))&#13;\n);</pre><p>Inserting into the tick_data_binary table can sustain inserts and reads of around 5 million ticks per server,  compared to 25000 for the tick_data table. The tick_data_binary table is also three times less storage that the tick_data table. This is not surprising as instead of holding 100,000 TTLs for all the columns, in the binary example we only hold 1. But there are bigger advantages when it comes to Cassandra's management services like compaction and repair. Compaction needs to be able to search for tombstones(deleted columns) which means that the more columns we have, the longer compaction can take. A similar problem arises in repair as this is in fact a compaction job. Comparing the repair time of a table with clustering columns and a table with binary data shows an increase of 10 times for the clustering table over the binary table.</p><h2>Trade offs</h2><p>There are always trade offs to each of the models above. For example the binary data in particular can't be filtered using CQL, the filtering needs to happen in some code. This post isn't supposed to be a catch all for time series applications but it it is supposed to help with the modelling of your data, both current and future, and the thought process that goes into that. In particular, don't be afraid to change the data model structure once its usefulness has decreased.</p><p>Check out <a href=\"https://academy.datastax.com/tutorials\">https://academy.datastax.com/tutorials</a> for more information of Cassandra and data modelling. Also have a look at the certification options that you can achieve <a href=\"https://academy.datastax.com/certifications\">https://academy.datastax.com/certifications</a>.</p><p>For examples of this data models and see the github projects below.</p><p><a href=\"https://github.com/DataStaxCodeSamples/datastax-creditcard-demo\" target=\"_blank\">https://github.com/DataStaxCodeSamples/datastax-creditcard-demo</a></p><p><a href=\"https://github.com/DataStaxCodeSamples/datastax-iot-demo\" target=\"_blank\">https://github.com/DataStaxCodeSamples/datastax-iot-demo</a></p><p><a href=\"https://github.com/DataStaxCodeSamples/datastax-tickdata-comparison\" target=\"_blank\">https://github.com/DataStaxCodeSamples/datastax-tickdata-comparison</a></p><p><a href=\"https://github.com/DataStaxCodeSamples/datastax-tickdb-full\" target=\"_blank\">https://github.com/DataStaxCodeSamples/datastax-tickdb-full</a></p><hr /><p><a href=\"https://www.datastax.com/\">DataStax</a> has many ways for you to advance in your career and knowledge. \n</p><p>You can take <a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" title=\"academy.datastax.com\">free classes</a>, <a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" title=\"academy.datastax.com/certifications\">get certified</a>, or read <a href=\"https://www.datastax.com/dbas-guide-to-nosql\" target=\"_self\" title=\"dbas-guide-to-nosql\">one of our many white papers</a>.\n</p><p><a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com\">register for classes</a>\n</p><p><a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com/certifications\">get certified</a>\n</p><p><a href=\"http://www.datastax.com/dbas-guide-to-nosql?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_dbasguidetonosql\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"dbas-guide-to-nosql\">DBA's Guide to NoSQL</a>\n</p><br class=\"clear\" /><div id=\"mto_newsletter_121316_Css\"><p>Subscribe for newsletter:</p><br /></div>",
        "created_at": "2018-09-07T15:31:24+0000",
        "updated_at": "2018-09-07T15:31:44+0000",
        "published_at": "2015-12-15T17:09:39+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 6,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/uploads/2015/12/nest1.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12092"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          }
        ],
        "is_public": false,
        "id": 12091,
        "uid": null,
        "title": "cassandra-time-series-database",
        "url": "https://blog.pythian.com/cassandra-time-series-database/",
        "content": null,
        "created_at": "2018-09-07T15:31:12+0000",
        "updated_at": "2018-09-07T15:31:18+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "blog.pythian.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12091"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12090,
        "uid": null,
        "title": "Apache Cassandra, Part 7: Secondary Index, Replication, and Tips",
        "url": "https://blog.emumba.com/apache-cassandra-part-7-secondary-index-replication-and-tips-da273ea59993?gi=204022f39002",
        "content": "<section class=\"section section--body section--first\"><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"3cbe\" class=\"graf graf--p graf-after--h3\">This <a href=\"https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc\" data-href=\"https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">series of posts</a> present an introduction to Apache Cassandra. It discusses key Cassandra features, its core concepts, how it works under the hood, how it is different from other data stores, data modelling best practices with examples, and some tips &amp; tricks.</p><figure id=\"9d30\" class=\"graf graf--figure graf-after--p graf--trailing\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*_hy_RYrCzE0r4ryFWbk_gA.png\" data-width=\"400\" data-height=\"268\" src=\"https://cdn-images-1.medium.com/max/1600/1*_hy_RYrCzE0r4ryFWbk_gA.png\" alt=\"image\" /></div></figure></div></div></section><section class=\"section section--body section--last\"><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><h3 id=\"6f7f\" class=\"graf graf--h3 graf--leading\">Secondary Index in Cassandra</h3><p id=\"7c52\" class=\"graf graf--p graf-after--h3\">The purpose of secondary indexes in Cassandra is not to provide fast access to data using attributes other than partition key, rather it just provides a convenience in writing queries and fetching data. The two disk pass approach of secondary indexes, one for reading the secondary index file and second for accessing the actual data makes it inefficient in terms of performance. You can read more on this <a href=\"https://www.datastax.com/dev/blog/cassandra-native-secondary-index-deep-dive\" data-href=\"https://www.datastax.com/dev/blog/cassandra-native-secondary-index-deep-dive\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">here</a>.</p><h3 id=\"cb39\" class=\"graf graf--h3 graf-after--p\">Replication in Cassandra</h3><p id=\"3c49\" class=\"graf graf--p graf-after--h3\">Cassandra supports async replication based on a specified replication factor. Consider a scenario where you have 99 partitions with a replication factor of 3. Cassandra will replicate data of each partition on two other partitions. As a result, if a query requests to read all data, Cassandra can find the required data by reading only 33 partitions, hence reducing the number of partitions to read. Replication not only plays an important role in read optimization, but more importantly, it enables fault tolerance by ensuring access to data in case some node goes down in a Cassandra cluster.</p><h3 id=\"a025\" class=\"graf graf--h3 graf-after--p\">Cassandra Tips</h3><p id=\"0312\" class=\"graf graf--p graf-after--h3\">Here are some tips that may come handy during your journey through Cassandra.</p><ul class=\"postList\"><li id=\"893c\" class=\"graf graf--li graf-after--p\">Test your data model as early as possible. Build prototype, insert data, write queries, make sure your workflow works end to end.</li><li id=\"2997\" class=\"graf graf--li graf-after--li\">Use <a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCStress_t.html\" data-href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCStress_t.html\" class=\"markup--anchor markup--li-anchor\" rel=\"noopener\" target=\"_blank\">Cassandra stress</a> to generate reads, writes, and to measure performance.</li><li id=\"a677\" class=\"graf graf--li graf-after--li\">Do not try to minimize writes, extra writes to improve reads is worth it.</li><li id=\"0a08\" class=\"graf graf--li graf-after--li\">Data duplication is fact of life in Cassandra, don’t be afraid of it. Disk space is the cheapest available resource.</li><li id=\"0df3\" class=\"graf graf--li graf-after--li\">Always use async writes to keep your code non blocking.</li><li id=\"809f\" class=\"graf graf--li graf-after--li\">Batch inserts are anti pattern unless batch data belongs to same partition.</li><li id=\"2e63\" class=\"graf graf--li graf-after--li\">Regularly view Cassandra logs to look for warnings and suggestions.</li><li id=\"60e0\" class=\"graf graf--li graf-after--li\">Benchmark to measure performance against your needs e.g. data ingestion rate (events/sec) and query execution time.</li></ul><h3 id=\"2eb8\" class=\"graf graf--h3 graf-after--li\">References and Further Reading</h3><p id=\"8b15\" class=\"graf graf--p graf-after--h3\">Here are some resources that helped me in writing this series.</p><p><a href=\"https://academy.datastax.com/developer-blog\" data-href=\"https://academy.datastax.com/developer-blog\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://academy.datastax.com/developer-blog\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Developer Blog | DataStax Academy: Free Cassandra Tutorials and Training</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">DataStax Enterprise is powered by the best distribution of Apache Cassandra™. ©2017 DataStax, All rights reserved…</em>academy.datastax.com</a></p><p><a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\" data-href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Cassandra Data Modeling Best Practices, Part 1</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">This is the first in a series of posts on Cassandra data modeling, implementation, operations, and related practices…</em>www.ebayinc.com</a></p><p><a href=\"https://intellidzine.blogspot.com/2014/01/cassandra-data-modelling-primary-keys.html\" data-href=\"https://intellidzine.blogspot.com/2014/01/cassandra-data-modelling-primary-keys.html\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://intellidzine.blogspot.com/2014/01/cassandra-data-modelling-primary-keys.html\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Cassandra Data Modelling — Primary Keys</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">In the previous blog we discussed how data is stored in Cassandra by creating a keyspace and a table. We also inserted…</em>intellidzine.blogspot.com</a></p></div></div></section>",
        "created_at": "2018-09-07T15:30:40+0000",
        "updated_at": "2018-09-07T15:30:42+0000",
        "published_at": "2018-02-10T10:32:18+0000",
        "published_by": [
          "Haris Hasan"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 2,
        "domain_name": "blog.emumba.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*_hy_RYrCzE0r4ryFWbk_gA.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12090"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          }
        ],
        "is_public": false,
        "id": 12089,
        "uid": null,
        "title": "Apache Cassandra, Part 6: Time Series Modelling",
        "url": "https://blog.emumba.com/apache-cassandra-part-6-time-series-modelling-b9b0ad2e96d9?gi=a84a27f36228",
        "content": "<section class=\"section section--body section--first\"><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"8c30\" class=\"graf graf--p graf-after--h3 graf--trailing\">This <a href=\"https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc\" data-href=\"https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">series of posts</a> present an introduction to Apache Cassandra. It discusses key Cassandra features, its core concepts, how it works under the hood, how it is different from other data stores, data modelling best practices with examples, and some tips &amp; tricks.</p></div></div></section><section class=\"section section--body section--last\"><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><figure id=\"aa5d\" class=\"graf graf--figure graf--leading\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*cS211TKA5VDdyTai6iIhzQ.png\" data-width=\"400\" data-height=\"260\" src=\"https://cdn-images-1.medium.com/max/1600/1*cS211TKA5VDdyTai6iIhzQ.png\" alt=\"image\" /></div></figure><p id=\"00b6\" class=\"graf graf--p graf-after--figure\">Consider a scenario where we have weather data coming in from various weather stations and we have to store key value pairs of time and temperature against each station.</p><p id=\"e0f9\" class=\"graf graf--p graf-after--p\">Cassandra supports up to 2 Billion key value pairs in a row which means we can design our data model like this.</p><figure id=\"f3e5\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*HblEiC167FNClfQ9iEzgTA.png\" data-width=\"800\" data-height=\"95\" data-action=\"zoom\" data-action-value=\"1*HblEiC167FNClfQ9iEzgTA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*HblEiC167FNClfQ9iEzgTA.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" data-href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener noopener\" target=\"_blank\">https://academy.datastax.com/resources/getting-started-time-series-data-modeling</a></figcaption></figure><p id=\"0449\" class=\"graf graf--p graf-after--figure\">In this design station Id is partition key, therefore, we will be able to easily find partition containing data from a particular station. Each station would be transmitting data points at same rate, so each partition will have same amount of data. Hence the proposed data model satisfies both of the Cassandra’s data modelling goals.</p><p id=\"010a\" class=\"graf graf--p graf-after--p\">But there is a problem, if a weather station transmits a new entry every second, we are will end up with huge partitions pretty soon. An improvement could be to create a composite partition key using station Id and date like this.</p><figure id=\"d516\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*3P0oxHxASLCry3qAP_Cjdg.png\" data-width=\"800\" data-height=\"144\" data-action=\"zoom\" data-action-value=\"1*3P0oxHxASLCry3qAP_Cjdg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*3P0oxHxASLCry3qAP_Cjdg.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" data-href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener noopener\" target=\"_blank\">https://academy.datastax.com/resources/getting-started-time-series-data-modeling</a></figcaption></figure><p id=\"bc62\" class=\"graf graf--p graf-after--figure\">This way we achieve manageable sized partitions and efficient date wise access to weather data.</p><p id=\"5d89\" class=\"graf graf--p graf-after--p\">In weather domain most of the queries will involve latest data. Our current model appends each new entry at the end, therefore, to find the latest N records Cassandra will go until the end of partition and read last N records. A better approach could be to store data in reverse timestamp order like this.</p><figure id=\"c620\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*BwA91ED4qAGd4UyCPG7ydA.png\" data-width=\"800\" data-height=\"122\" data-action=\"zoom\" data-action-value=\"1*BwA91ED4qAGd4UyCPG7ydA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*BwA91ED4qAGd4UyCPG7ydA.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" data-href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener noopener\" target=\"_blank\">https://academy.datastax.com/resources/getting-started-time-series-data-modeling</a></figcaption></figure><p id=\"d0ff\" class=\"graf graf--p graf-after--figure\">This approach will reduce our read cost by keeping fresh data at start of partition. Older data can be deleted from the end to keep the partition size manageable.</p><p id=\"a354\" class=\"graf graf--p graf-after--p\">Cassandra also provides a data type <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">time-uuid</em></strong> that comes handy when multiple events might come in with same timestamp. To avoid timestamp collision time-uuid appends timestamp with a random Id which guarantees record uniqueness.</p><p id=\"3724\" class=\"graf graf--p graf-after--p graf--trailing\"><a href=\"https://blog.emumba.com/apache-cassandra-part-7-secondary-index-replication-and-tips-da273ea59993\" data-href=\"https://blog.emumba.com/apache-cassandra-part-7-secondary-index-replication-and-tips-da273ea59993\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Next: Apache Cassandra, Part 7: Secondary Index, Replication, Tips</a></p></div></div></section>",
        "created_at": "2018-09-07T15:30:36+0000",
        "updated_at": "2018-09-07T15:30:37+0000",
        "published_at": "2018-02-10T10:31:54+0000",
        "published_by": [
          "Haris Hasan"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "blog.emumba.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*cS211TKA5VDdyTai6iIhzQ.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12089"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 12088,
        "uid": null,
        "title": "Apache Cassandra, Part 5: Data Modelling with Examples",
        "url": "https://blog.emumba.com/apache-cassandra-part-5-data-modelling-in-cassandra-9e81a58f4ada?gi=a32a921885fe",
        "content": "<p id=\"0352\" class=\"graf graf--p graf--leading\">Data modelling in Cassandra revolves around two goals:</p><ol class=\"postList\"><li id=\"9b10\" class=\"graf graf--li graf-after--p\">Spread data evenly among partitions.</li><li id=\"83ff\" class=\"graf graf--li graf-after--li\">A read query should (ideally) hit only a single partition.</li></ol><p id=\"cf55\" class=\"graf graf--p graf-after--li\">Often you will find yourself in scenarios, and as you will see later in examples, these rules start conflicting, so you have to balance them according to business requirements.</p><h3 id=\"5503\" class=\"graf graf--h3 graf-after--p\">Data Modelling Tips</h3><p id=\"30ef\" class=\"graf graf--p graf-after--h3\">Before going through the data modelling examples, let’s review some of the points to keep in mind while modelling the data in Cassandra.</p><ul class=\"postList\"><li id=\"c786\" class=\"graf graf--li graf-after--p\">Model your data around queries and not around relationships. In other words, your data model should be heavily driven by your read requirements and use cases.</li><li id=\"01a0\" class=\"graf graf--li graf-after--li\">Create tables where you can satisfy your queries by reading (ideally) one or minimum number of partitions.</li><li id=\"e082\" class=\"graf graf--li graf-after--li\">Each table should pre-build the answer to a high-level query that you need to support.</li><li id=\"5ed8\" class=\"graf graf--li graf-after--li\">Key point to remember is <strong class=\"markup--strong markup--li-strong\">Optimize for Reads</strong>.</li><li id=\"d890\" class=\"graf graf--li graf-after--li\">Remember <strong class=\"markup--strong markup--li-strong\">Writes are Cheap</strong>, disks are available at very low costs, and data duplication is fact of life in Cassandra. Don’t be afraid of data duplication while trying to optimize for reads.</li></ul><h3 id=\"67d9\" class=\"graf graf--h3 graf-after--li\">Data Modelling Examples</h3><p id=\"17c8\" class=\"graf graf--p graf-after--h3\">Lets look at few examples and try to apply the knowledge we have gained so far on Cassandra.</p><h4 id=\"7dd1\" class=\"graf graf--h4 graf-after--p\">Example 1</h4><p id=\"d1e1\" class=\"graf graf--p graf-after--h4\">Consider a scenario where we have a large number of <strong class=\"markup--strong markup--p-strong\">users</strong> and we want to look up a user by <strong class=\"markup--strong markup--p-strong\">username</strong> or by <strong class=\"markup--strong markup--p-strong\">email</strong>. With either method, we should get the full details of matching user.</p><figure id=\"aa99\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*VM1p0xpblqcJg0PXi5u4rg.png\" data-width=\"500\" data-height=\"272\" src=\"https://cdn-images-1.medium.com/max/1600/1*VM1p0xpblqcJg0PXi5u4rg.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" data-href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling</a></figcaption></figure><p id=\"dd1e\" class=\"graf graf--p graf-after--figure\">In first implementation we have created two tables. One has partition key <strong class=\"markup--strong markup--p-strong\">username</strong> and other one <strong class=\"markup--strong markup--p-strong\">email</strong>. Note that we are duplicating information (age) in both tables.</p><figure id=\"ad1f\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*WEKG9Khc-se7Nnc5dts0pw.png\" data-width=\"500\" data-height=\"393\" src=\"https://cdn-images-1.medium.com/max/1600/1*WEKG9Khc-se7Nnc5dts0pw.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" data-href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling</a></figcaption></figure><p id=\"ca68\" class=\"graf graf--p graf-after--figure\">Another way to model this data could be what’s shown above. In this case we have three tables, but we have avoided the data duplication by using last two tables as reference tables. Just like before reference tables are partitioning the data using username and email, but user details are located at a single place.</p><p id=\"8559\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">So which data model is better?</em></strong></p><p id=\"b752\" class=\"graf graf--p graf-after--p\">Recall two primary rules of data modelling in Cassandra (1) each partition should have roughly same amount of data (2) read operations should access minimum partitions, ideally only one.</p><p id=\"3466\" class=\"graf graf--p graf-after--p\">If we apply these rules to model #1, we will only need to access one partition to get full user details. Additionally, we can easily find which partition contains our data as both username and email have been used as partition key. Assuming usernames and emails have uniform distribution, we will have roughly same quantity of data in each partition.</p><p id=\"6f19\" class=\"graf graf--p graf-after--p\">Now, lets evaluate model #2. Although there is no data duplication in this model (keep in mind it’s okay to have data duplication as long as it optimizes read performance) but it comes with one big disadvantage: it violates the rule of reading from a single partition. To find user details, first we will go to a partition using username (or email), and then we will do another partition lookup through uuid.</p><p id=\"e579\" class=\"graf graf--p graf-after--p\">Therefore, in this scenario it is better to use model #1.</p><h4 id=\"ca69\" class=\"graf graf--h4 graf-after--p\">Example 2</h4><p id=\"e845\" class=\"graf graf--p graf-after--h4\">Our requirements have been updated. Now, each user belongs to a group. And we want to fetch all users of a group by group name including all details of its users.</p><figure id=\"4aee\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*8srORb1aPHlpw6qNyLU8-g.png\" data-width=\"500\" data-height=\"158\" src=\"https://cdn-images-1.medium.com/max/1600/1*8srORb1aPHlpw6qNyLU8-g.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" data-href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling</a></figcaption></figure><p id=\"250b\" class=\"graf graf--p graf-after--figure\">Here, data will be distributed among nodes based on group_name (partition key) and sorted within a node by username (clustering key). Each group stores complete information of its users.</p><p id=\"0d75\" class=\"graf graf--p graf-after--p\">Our proposed model satisfies the first data modeling principle, i.e. to read any group’s data we will have to hit only a single partition and there will find all users of that group. But does this model satisfy the 2nd principle? The answer is probably not. Why? Because generally, in the real world, there will be some very populated groups while other groups might be very small. Which means the size of partitions won’t be roughly same, and that is violation of our data modelling principle.</p><p id=\"f011\" class=\"graf graf--p graf-after--p\">One way to fix this could be this model</p><figure id=\"590f\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*uYkMW2vouxRfzvtELjIyPg.png\" data-width=\"1218\" data-height=\"344\" data-action=\"zoom\" data-action-value=\"1*uYkMW2vouxRfzvtELjIyPg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*uYkMW2vouxRfzvtELjIyPg.png\" alt=\"image\" /></div></figure><p id=\"6e9d\" class=\"graf graf--p graf-after--figure\">Now we are using a composite partition key, where hash_prefix is a just random number. This composite key will ensure that data is uniformly distributed among nodes hence our model no longer violates the 2nd principle. But it comes with a catch. As data will be distributed uniformly among partitions, in order to fetch all users in a particular group we will have to read more than one partition, hence violating the 1st principle.</p><p id=\"ae76\" class=\"graf graf--p graf-after--p\">This example teaches us an interesting lesson in Cassandra data modelling.</p><blockquote id=\"aab7\" class=\"graf graf--blockquote graf-after--p\"><div>The two data modeling principles often conflict, therefore you have to find a balance between the two based on domain understanding and business needs.</div></blockquote><h4 id=\"156e\" class=\"graf graf--h4 graf-after--blockquote\">Example 3</h4><p id=\"5563\" class=\"graf graf--p graf-after--h4\">Continuing on the previous examples, our new requirement is to add support for getting X newest users in a group. Here is one data model</p><figure id=\"19cf\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*5LesR_IYYrlRIh_qjnHzLA.png\" data-width=\"500\" data-height=\"188\" src=\"https://cdn-images-1.medium.com/max/1600/1*5LesR_IYYrlRIh_qjnHzLA.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" data-href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling</a></figcaption></figure><p id=\"38bd\" class=\"graf graf--p graf-after--figure\">All user data is located at a single place, data is partitioned by group_name and ordered by join date. [Ignoring the issue of data distribution we already discussed in last example] Model looks decent. Lookup by group name would take us to exact partition containing group data, sorting by join date means we only need to read last X records from the end. There is one further improvement we can do on this model which will make our queries more faster.</p><figure id=\"7059\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*qEjC070JbN_KlBw0dilqFA.png\" data-width=\"500\" data-height=\"167\" src=\"https://cdn-images-1.medium.com/max/1600/1*qEjC070JbN_KlBw0dilqFA.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" data-href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling</a></figcaption></figure><p id=\"3c28\" class=\"graf graf--p graf-after--figure\">Here, we are still sorting the partition data by join date, but in descending order. Now, we won’t even need to go to end of partition in order to read last X records. We can simply read top X records from the start of partition, hence reducing the IO cost.</p><p id=\"8a50\" class=\"graf graf--p graf-after--p\">Recall the data distribution problem we touched upon earlier, one way to solve that could be this.</p><figure id=\"14ec\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*F-ij4Ec1viZHqe2i1ZWazQ.png\" data-width=\"500\" data-height=\"163\" src=\"https://cdn-images-1.medium.com/max/1600/1*F-ij4Ec1viZHqe2i1ZWazQ.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" data-href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling</a></figcaption></figure><p id=\"b2d5\" class=\"graf graf--p graf-after--figure\">We have made the partition key composite by adding join date. This way a new partition will be created everyday for every group. Assuming each group is roughly inducting same number of users daily, this model can work. But now, in order to get last X joined users in a group, we will have to read more than one partition if number of users who joined that group in last day are less than X. One way to fix this could be to benchmark the number of users joining daily and set the composite key in such a way that last X users query only need to access a single partition.</p><h4 id=\"047b\" class=\"graf graf--h4 graf-after--p\">Example 4</h4><p id=\"9064\" class=\"graf graf--p graf-after--h4\">Let’s implement a simple Facebook use case where we need to fetch recent posts posted by a specific user. Here is a possible data model</p><figure id=\"6fd1\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*q7gMcWJtuAqWWdDiTmeE4Q.png\" data-width=\"500\" data-height=\"170\" src=\"https://cdn-images-1.medium.com/max/1600/1*q7gMcWJtuAqWWdDiTmeE4Q.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Couldn’t find the original source.</figcaption></figure><p id=\"e66f\" class=\"graf graf--p graf-after--figure\">Here, partition key user_id can easily take us to partition carrying posts of a particular user, sorting by post_id will help us in locating top X posts, and the descending order will further reduce our disk seeks by removing the need to go until the end of partition.</p><h4 id=\"76ed\" class=\"graf graf--h4 graf-after--p\">Example 5</h4><p id=\"1747\" class=\"graf graf--p graf-after--h4\">Let’s say we are developing an e-commerce site and we need to support following queries.</p><ol class=\"postList\"><li id=\"f585\" class=\"graf graf--li graf-after--p\">Get user by user id.</li><li id=\"c64f\" class=\"graf graf--li graf-after--li\">Get item by item id.</li><li id=\"bfc5\" class=\"graf graf--li graf-after--li\">Get all the items that a particular user likes.</li><li id=\"9625\" class=\"graf graf--li graf-after--li\">Get all the users who like a particular item.</li></ol><p id=\"46d0\" class=\"graf graf--p graf-after--li\">Here is an entity relationship diagram showing relationship between entities described in the use case.</p><figure id=\"5b7a\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*oLs1pcSZQ7DLAKM1Qmm_rQ.png\" data-width=\"600\" data-height=\"360\" data-is-featured=\"true\" src=\"https://cdn-images-1.medium.com/max/1600/1*oLs1pcSZQ7DLAKM1Qmm_rQ.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\" data-href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/</a></figcaption></figure><p id=\"ea0c\" class=\"graf graf--p graf-after--figure\">Lets start with a simple relational-like data model where we have three tables. First for keeping users with user Id as partition key, second for items with item Id as partition key, and third for storing user-item likes partitioned by unique Id assigned to each like record.</p><figure id=\"0a73\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*9MrS-S_85z1-d9q26sZmmw.png\" data-width=\"600\" data-height=\"307\" src=\"https://cdn-images-1.medium.com/max/1600/1*9MrS-S_85z1-d9q26sZmmw.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\" data-href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/</a></figcaption></figure><p id=\"cbb0\" class=\"graf graf--p graf-after--figure\">If Ids are being auto generated, each partition will have roughly same amount of data which satisfies our first data modelling goal. Coming to partition reads, this data model will be able to find result of query #1 and #2 very efficiently as Id of each user and item is being used as partition key. But, there is no easy way to access data for query #3 and #4 because there is no way to predetermine what partitions might have the required data. Rather all partitions will have to be scanned, which violates our second data modelling principle.</p><p id=\"0800\" class=\"graf graf--p graf-after--p\">A better data modelling approach can be following.</p><figure id=\"a4f3\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*nRV637CWZknwzGup_dHfHQ.png\" data-width=\"600\" data-height=\"300\" src=\"https://cdn-images-1.medium.com/max/1600/1*nRV637CWZknwzGup_dHfHQ.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\" data-href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/</a></figcaption></figure><p id=\"671a\" class=\"graf graf--p graf-after--figure\">Here, we have divided the user-like relationship table into two by introducing data redundancy. Using this redundancy we are able to partition data by both user Id and item Id separately. This gives us the ability to look at specific partitions when finding results for query #3 and #4.</p><p id=\"77d7\" class=\"graf graf--p graf-after--p\">But there is still one problem, note that we are only getting Ids i.e. user Id and item Id, from the newly introduced tables. In this design, we will still have to look up user and item partitions if we want to show meaningful information about users and items involved. Here is a quick fix, again using data duplication.</p><figure id=\"118b\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*1xDLAPSyOwB8TuoOggRSMA.png\" data-width=\"600\" data-height=\"301\" src=\"https://cdn-images-1.medium.com/max/1600/1*1xDLAPSyOwB8TuoOggRSMA.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\" data-href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/</a></figcaption></figure><p id=\"fb40\" class=\"graf graf--p graf-after--figure\">It is important to note here that we are not duplicating entire information of user or item in the relationship tables. For example, to show email or item description a lookup will still be required, but that decision is driven by business context. For example, in this case, we might be showing only user name or item title on initial interface and let user click on user/item for further details.</p><p id=\"ee5b\" class=\"graf graf--p graf-after--p graf--trailing\"><a href=\"https://blog.emumba.com/apache-cassandra-part-6-time-series-modelling-b9b0ad2e96d9\" data-href=\"https://blog.emumba.com/apache-cassandra-part-6-time-series-modelling-b9b0ad2e96d9\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Next: Apache Cassandra, Part 6: Time Series Modelling in Cassandra</a></p>",
        "created_at": "2018-09-07T15:30:28+0000",
        "updated_at": "2018-09-07T15:30:31+0000",
        "published_at": "2018-02-10T10:31:09+0000",
        "published_by": [
          "Haris Hasan"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 8,
        "domain_name": "blog.emumba.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*oLs1pcSZQ7DLAKM1Qmm_rQ.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12088"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12087,
        "uid": null,
        "title": "Apache Cassandra, Part 4: Data Flow and Core Concepts",
        "url": "https://blog.emumba.com/apache-cassandra-part-4-data-flow-and-key-concepts-4dea799a3531?gi=cd8be828f62f",
        "content": "<section class=\"section section--body section--first\"><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"6468\" class=\"graf graf--p graf-after--h3 graf--trailing\">This <a href=\"https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc\" data-href=\"https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">series of posts</a> present an introduction to Apache Cassandra. It discusses key Cassandra features, its core concepts, how it works under the hood, how it is different from other data stores, data modelling best practices with examples, and some tips &amp; tricks.</p></div></div></section><section class=\"section section--body section--last\"><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"93d6\" class=\"graf graf--p graf--leading\">Data flow in Cassandra looks like this:</p><figure id=\"0e45\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"0*dS05FUEJe4Bdz-TP.\" data-width=\"490\" data-height=\"218\" src=\"https://cdn-images-1.medium.com/max/1600/0*dS05FUEJe4Bdz-TP.\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlHowDataWritten.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlHowDataWritten.html\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener noopener noopener\" target=\"_blank\">https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlHowDataWritten.html</a></figcaption></figure><p id=\"8a36\" class=\"graf graf--p graf-after--figure\">Incoming data is written into a commit log as well as in an in-memory store. Once the in-memory store is full, data is flushed onto disk in <a href=\"https://wiki.apache.org/cassandra/MemtableSSTable\" data-href=\"https://wiki.apache.org/cassandra/MemtableSSTable\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">SSTables</a>. Purpose of committing data into commit log is to provide fault tolerance. If a node goes down while there was still data in in-memory table which was yet to be persisted, on restart, node restores it’s in-memory state using commit log and resumes operation normally.</p><h3 id=\"1312\" class=\"graf graf--h3 graf-after--p\">Core Cassandra Concepts</h3><p id=\"76ce\" class=\"graf graf--p graf-after--h3\">Two concepts are of utmost importance to understand Cassandra and its data modelling.</p><p id=\"1b64\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Partition Key</strong>: determines on which node in a Cassandra cluster data is going to be stored.</p><figure id=\"0d6b\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*GxAcqjhrUB2bKoK4EPipSA.png\" data-width=\"400\" data-height=\"362\" data-is-featured=\"true\" src=\"https://cdn-images-1.medium.com/max/1600/1*GxAcqjhrUB2bKoK4EPipSA.png\" alt=\"image\" /></div></figure><p id=\"6593\" class=\"graf graf--p graf-after--figure\">The hash function or Cassandra partitioner decides, based on the partition key, which data to send at what node.</p><p id=\"3ef5\" class=\"graf graf--p graf-after--p\">For example, say you want to store data of four cities A, B, C and D. You have a 2 node Cassandra cluster. You choose city name as partition key. To store this data Cassandra will create four partitions (against 4 unique cities), 2 on each node.</p><p id=\"9002\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Clustering Key</strong>: determines how data is sorted within a partition. Continuing on our last example, let’s say each incoming data point has a city name (A, B, C or D) and an associated timestamp. You can tell Cassandra to use timestamp as clustering key to store data in sorted order within a partition. This will enable efficient data lookup within a partition.</p><p id=\"8dcf\" class=\"graf graf--p graf-after--p\">In short, partition key helps Cassandra in locating the node with specific data, while clustering key helps in efficiently finding data within a partition.</p><p id=\"1272\" class=\"graf graf--p graf-after--p\">Both partition and clustering key can be composite.</p><h3 id=\"3e08\" class=\"graf graf--h3 graf-after--p\">Column Family Store</h3><p id=\"e5b8\" class=\"graf graf--p graf-after--h3\">As discussed earlier, Cassandra is neither a row based store nor it is a column based store, rather it is a column family store. But what does that mean?</p><p id=\"1548\" class=\"graf graf--p graf-after--p\">One way to understand how Cassandra stores its data is this data structure:</p><p id=\"f46e\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">Map&lt;RowKey, SortedMap&lt;ColumnKey, ColumnValue&gt;&gt;</em></strong></p><p id=\"80ed\" class=\"graf graf--p graf-after--p\">Each row is mapped to a node using partition key, and the value is a set of key value pairs sorted by clustering key.</p><figure id=\"a48a\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"0*P86RdHPAaHFLJaEO.\" data-width=\"1122\" data-height=\"244\" data-action=\"zoom\" data-action-value=\"0*P86RdHPAaHFLJaEO.\" src=\"https://cdn-images-1.medium.com/max/1600/0*P86RdHPAaHFLJaEO.\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\" data-href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/</a></figcaption></figure><p id=\"b9b4\" class=\"graf graf--p graf-after--figure\">In other words, Cassandra is a partitioned row store, where data is partitioned by partition key and, within a partition, sorted by clustering key.</p><p id=\"e914\" class=\"graf graf--p graf-after--p graf--trailing\"><a href=\"https://blog.emumba.com/apache-cassandra-part-5-data-modelling-in-cassandra-9e81a58f4ada\" data-href=\"https://blog.emumba.com/apache-cassandra-part-5-data-modelling-in-cassandra-9e81a58f4ada\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Next: Apache Cassandra, Part 5: Data Modelling with Examples</a></p></div></div></section>",
        "created_at": "2018-09-07T15:30:03+0000",
        "updated_at": "2018-09-07T15:30:21+0000",
        "published_at": "2018-02-10T10:34:11+0000",
        "published_by": [
          "Haris Hasan"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 2,
        "domain_name": "blog.emumba.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*GxAcqjhrUB2bKoK4EPipSA.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12087"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12086,
        "uid": null,
        "title": "Apache Cassandra, Part 3: Row vs Column Stores",
        "url": "https://blog.emumba.com/apache-cassandra-part-3-row-vs-column-stores-fcc2a8d5c632?gi=f250057dcfd6",
        "content": "<h3 id=\"02b5\" class=\"graf graf--h3 graf--leading\">Row vs Column Stores</h3><p id=\"a5d7\" class=\"graf graf--p graf-after--h3\">When talking about data stores it is helpful to keep this image in mind.</p><figure id=\"78ef\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"0*dIMjUEiFROJa2Ydr.\" data-width=\"555\" data-height=\"258\" data-is-featured=\"true\" src=\"https://cdn-images-1.medium.com/max/1600/0*dIMjUEiFROJa2Ydr.\" alt=\"image\" /></div></figure><p id=\"7dda\" class=\"graf graf--p graf-after--figure\">A disk read is an expensive operation and should be used in minimum to ensure performance of a data system. With this image in mind, let’s take data shown below as an example to look at various types of storage engines and see for what scenario each engine is optimized for.</p><figure id=\"4f1a\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"0*sVKPtbTUfy13CHtN.\" data-width=\"670\" data-height=\"306\" src=\"https://cdn-images-1.medium.com/max/1600/0*sVKPtbTUfy13CHtN.\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\" data-href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener\" target=\"_blank\">https://en.wikipedia.org/wiki/Column-oriented_DBMS</a></figcaption></figure><p id=\"7f0e\" class=\"graf graf--p graf-after--figure\">Here we have data of employees, their ids, first and last names, and salaries.</p><h4 id=\"5b17\" class=\"graf graf--h4 graf-after--p\">Row Oriented Stores</h4><p id=\"eced\" class=\"graf graf--p graf-after--h4\">A row oriented store persists this data under the hood like this:</p><figure id=\"e8c9\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"0*bccSzBhLqv7Xdag8.\" data-width=\"488\" data-height=\"194\" src=\"https://cdn-images-1.medium.com/max/1600/0*bccSzBhLqv7Xdag8.\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\" data-href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener\" target=\"_blank\">https://en.wikipedia.org/wiki/Column-oriented_DBMS</a></figcaption></figure><p id=\"c592\" class=\"graf graf--p graf-after--figure\">Interesting thing to note here is the column values that belongs to a single row are stored together. This makes it easy and cheap in terms of IO cost to read a single row. You go to desired row in a single disk seek, thanks to primary index, and find all the data in that row. Row oriented stores are efficient in scenarios where you need to access fewer rows and intend to use most of the data available in rows. This data access pattern is very common in OLTP systems, for example a university management, a banking portal, etc. But imagine if you need to find out median salary. For that you will have to scan the complete table and that will involve a lot of disk reads.</p><h4 id=\"2318\" class=\"graf graf--h4 graf-after--p\">Column Oriented Stores</h4><p id=\"ea57\" class=\"graf graf--p graf-after--h4\">A column oriented store would save the same information like this</p><figure id=\"a302\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"0*hcPyoCogQV82-68G.\" data-width=\"744\" data-height=\"180\" data-action=\"zoom\" data-action-value=\"0*hcPyoCogQV82-68G.\" src=\"https://cdn-images-1.medium.com/max/1600/0*hcPyoCogQV82-68G.\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\" data-href=\"https://en.wikipedia.org/wiki/Column-oriented_DBMS\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener\" target=\"_blank\">https://en.wikipedia.org/wiki/Column-oriented_DBMS</a></figcaption></figure><p id=\"eefe\" class=\"graf graf--p graf-after--figure\">Here, unlike a row store, all column values from a particular column are stored together. Now, if we want to calculate median salary, it will be very fast because it involves only one disk seek. But, if we want to get complete information of a particular employee, we will end up doing a lot of IO operations.</p><p id=\"7e2f\" class=\"graf graf--p graf-after--p\">In a column oriented store, number of values in each row should be same. This allows joining rows using index, which would be impossible to do if number of values are unequal. Column oriented stores are useful in OLAP scenarios where generally specific columns are of interest and queries are centered around these columns.</p><p id=\"a653\" class=\"graf graf--p graf-after--p\">Cassandra is neither a row oriented store nor it is a column oriented store, rather is is column family store. To understand how it is different from a row store, lets consider a scenario where we want to store temperature values. In a row based store we typically create a table temperatures with two columns (1) timestamp (2) value. On data insertion temperatures table will grow vertically. But in Cassandra, it can be a very valid design to store data like this:</p><figure id=\"98af\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"0*N_ZIad4Lno6A29Vs.\" data-width=\"912\" data-height=\"72\" data-action=\"zoom\" data-action-value=\"0*N_ZIad4Lno6A29Vs.\" src=\"https://cdn-images-1.medium.com/max/1600/0*N_ZIad4Lno6A29Vs.\" alt=\"image\" /></div></figure><p id=\"2973\" class=\"graf graf--p graf-after--figure\">Here every new entry is creating a new column. (Cassandra supports up to 2 Billion columns).</p><p id=\"e94e\" class=\"graf graf--p graf-after--p\">Now consider another example where we need to store various attributes of fruits. Following is a perfectly valid data model in Cassandra</p><figure id=\"ebf9\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"0*23NaTvtAEhU9hdg1.\" data-width=\"588\" data-height=\"170\" src=\"https://cdn-images-1.medium.com/max/1600/0*23NaTvtAEhU9hdg1.\" alt=\"image\" /></div></figure><p id=\"781b\" class=\"graf graf--p graf-after--figure\">But you cannot do this in a column oriented store because, recall, you have to have same number of columns for each row.</p><p id=\"dd0a\" class=\"graf graf--p graf-after--p\">Hence, Cassandra is neither a row oriented store, nor it is a column oriented store because it does things differently than both of those. But the question remains: if Cassandra is a column family store then what exactly does it mean and how is it different ? Hang on, I cover this in future posts but first you need to understand a few core concepts of Cassandra.</p><p id=\"c1b3\" class=\"graf graf--p graf-after--p graf--trailing\"><a href=\"https://blog.emumba.com/apache-cassandra-part-4-data-flow-and-key-concepts-4dea799a3531\" data-href=\"https://blog.emumba.com/apache-cassandra-part-4-data-flow-and-key-concepts-4dea799a3531\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Next: Apache Cassandra, Part 4: Data Flow and Key Concepts</a></p>",
        "created_at": "2018-09-07T15:29:17+0000",
        "updated_at": "2018-09-07T15:29:24+0000",
        "published_at": "2018-02-10T10:30:20+0000",
        "published_by": [
          "Haris Hasan"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 2,
        "domain_name": "blog.emumba.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/0*dIMjUEiFROJa2Ydr.",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12086"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 182,
            "label": "mongo",
            "slug": "mongo"
          }
        ],
        "is_public": false,
        "id": 12085,
        "uid": null,
        "title": "Apache Cassandra, Part 2: Cassandra vs MongoDB",
        "url": "https://blog.emumba.com/apache-cassandra-part-2-cassandra-vs-mongodb-ab9d10ee10cb?gi=3ff71d8e5bed",
        "content": "<section class=\"section section--body section--first\"><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"2182\" class=\"graf graf--p graf-after--h3 graf--trailing\">This <a href=\"https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc\" data-href=\"https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">series of posts</a> present an introduction to Apache Cassandra. It discusses key Cassandra features, its core concepts, how it works under the hood, how it is different from other data stores, data modelling best practices with examples, and some tips &amp; tricks.</p></div></div></section><section class=\"section section--body section--last\"><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><figure id=\"0ecf\" class=\"graf graf--figure graf--leading\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*jjO-vhoXinKrQgAX8UaS0w.png\" data-width=\"500\" data-height=\"267\" src=\"https://cdn-images-1.medium.com/max/1600/1*jjO-vhoXinKrQgAX8UaS0w.png\" alt=\"image\" /></div></figure><p id=\"8498\" class=\"graf graf--p graf-after--figure\">This series is not about MongoDB or even MongoDB vs Cassandra, but “<em class=\"markup--em markup--p-em\">How is it different from MongoDB</em>?” is a commonly asked question when talking about Cassandra. So before going deep into Cassandra, I would like to describe some commonalities as well as key differences between the two.</p><h4 id=\"9fe8\" class=\"graf graf--h4 graf-after--p\">Commonalities</h4><p id=\"7ade\" class=\"graf graf--p graf-after--h4\">Here are some properties which both Cassandra and MongoDB share:</p><ul class=\"postList\"><li id=\"c61c\" class=\"graf graf--li graf-after--p\">None of these data stores are a replacement for RDBMS.</li><li id=\"a2eb\" class=\"graf graf--li graf-after--li\">They do not provide ACID compliance.</li><li id=\"0a84\" class=\"graf graf--li graf-after--li\">Both keep recent data in memory to improve performance.</li><li id=\"1c0a\" class=\"graf graf--li graf-after--li\">Both data stores discourage joins and prefer denormalization.</li><li id=\"dfbe\" class=\"graf graf--li graf-after--li\">Both are open source, have been in industry for quite some time, and have comprehensive support.</li></ul><h4 id=\"7b9a\" class=\"graf graf--h4 graf-after--li\">Differences</h4><p id=\"08a9\" class=\"graf graf--p graf-after--h4\">At a high level, some major differences between these two stores are:</p><ul class=\"postList\"><li id=\"a37d\" class=\"graf graf--li graf-after--p\">MongoDB uses B-Trees under the hood for storage while Cassandra is based on LSM trees which makes Cassandra more scalable for writes.</li><li id=\"8f41\" class=\"graf graf--li graf-after--li\">MongoDB is more closer to a RDBMS than Cassandra as you can implement concepts like relationships and joins on top of MongoDB. Same is not true for Cassandra.</li><li id=\"8479\" class=\"graf graf--li graf-after--li\">MongoDB supports nested objects, Cassandra does not.</li><li id=\"79d7\" class=\"graf graf--li graf-after--li\">MongoDB offers both primary and secondary index and also allows indexing of nested properties. While Cassandra only supports primary index. [More on this later]</li><li id=\"a28d\" class=\"graf graf--li graf-after--li\">MongoDB lets you write queries in JSON format and allow <a href=\"https://docs.mongodb.com/manual/reference/operator/query/\" data-href=\"https://docs.mongodb.com/manual/reference/operator/query/\" class=\"markup--anchor markup--li-anchor\" rel=\"noopener\" target=\"_blank\">all kinds of</a> operators. Cassandra offers <a href=\"https://docs.datastax.com/en/cql/3.1/cql/cql_intro_c.html\" data-href=\"https://docs.datastax.com/en/cql/3.1/cql/cql_intro_c.html\" class=\"markup--anchor markup--li-anchor\" rel=\"noopener\" target=\"_blank\">CQL</a> which only supports limited operators and use of these operators also depend on how you have defined the schema.</li><li id=\"c5f5\" class=\"graf graf--li graf-after--li\">MongoDB provides built-in aggregation, which works well for small to medium sized databases, Cassandra has no such feature.</li><li id=\"18e1\" class=\"graf graf--li graf-after--li\">MongoDB does not enforce schema on write, Cassandra prefers design time schema.</li><li id=\"0f25\" class=\"graf graf--li graf-after--li\">MongoDB is document based store which is somewhat similar to rows in a table, Cassandra on the other hand is a column family store (and not a column oriented store, more on this later).</li><li id=\"9582\" class=\"graf graf--li graf-after--li\">Cassandra provides high write availability through its master-less or multi master architecture as compared to MongoDB which works on master slave architecture. (More on this later)</li><li id=\"84ad\" class=\"graf graf--li graf-after--li\">Cassandra provides linear write scalability which is not the case with MongoDB.</li></ul><p id=\"5b91\" class=\"graf graf--p graf-after--li graf--trailing\"><a href=\"https://blog.emumba.com/apache-cassandra-part-3-row-vs-column-stores-fcc2a8d5c632\" data-href=\"https://blog.emumba.com/apache-cassandra-part-3-row-vs-column-stores-fcc2a8d5c632\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Next: Apache Cassandra, Part 3: Row vs Column Stores</a></p></div></div></section>",
        "created_at": "2018-09-07T15:29:04+0000",
        "updated_at": "2018-09-07T15:29:12+0000",
        "published_at": "2018-02-10T10:29:51+0000",
        "published_by": [
          "Haris Hasan"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "blog.emumba.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*jjO-vhoXinKrQgAX8UaS0w.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12085"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12084,
        "uid": null,
        "title": "Apache Cassandra, Part 1: Introduction and Key Features",
        "url": "https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc?gi=d586e7c8d950",
        "content": "<p id=\"05e2\" class=\"graf graf--p graf-after--h3\">This <a href=\"https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc\" data-href=\"https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">series of posts</a> present an introduction to Apache Cassandra. It discusses key Cassandra features, its core concepts, how it works under the hood, how it is different from other data stores, data modelling best practices with examples, and some tips &amp; tricks.</p><p id=\"14aa\" class=\"graf graf--p graf-after--li\"><a href=\"http://cassandra.apache.org/\" data-href=\"http://cassandra.apache.org/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Cassandra</a> is a popular open source NoSQL database. It is being successfully used in a variety of scenarios like analytics, time series analysis, monitoring, retail, e-commerce, etc. One common overarching theme where you find Cassandra in use are environments with high write volumes.</p><h3 id=\"b4f2\" class=\"graf graf--h3 graf-after--p\">Key Cassandra Features</h3><p id=\"708e\" class=\"graf graf--p graf-after--h3\">Here are some of the key features of Cassandra.</p><ul class=\"postList\"><li id=\"9327\" class=\"graf graf--li graf-after--p\"><strong class=\"markup--strong markup--li-strong\">Distributed</strong>: Cassandra is built to run on a cluster of nodes to provide high availability, fault tolerance and scalability.</li><li id=\"945c\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Multi Master or Master Less</strong>: Many data stores e.g. MongoDB are based on a master slave architecture.</li></ul><figure id=\"2797\" class=\"graf graf--figure graf-after--li\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*vrwu71nrbG2tspuQA4pFFg.png\" data-width=\"400\" data-height=\"338\" src=\"https://cdn-images-1.medium.com/max/1600/1*vrwu71nrbG2tspuQA4pFFg.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"http://mobicon.tistory.com/142\" data-href=\"http://mobicon.tistory.com/142\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener\" target=\"_blank\">http://mobicon.tistory.com/142</a></figcaption></figure><ul class=\"postList\"><li id=\"38a3\" class=\"graf graf--li graf-after--figure\">All the writes goes on a master node and reads are executed on slaves. On the other hand, Cassandra works in a master-less or multi master mode.</li></ul><figure id=\"ecbd\" class=\"graf graf--figure graf-after--li\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*M5GYo7SpIusR0m9AG83-MQ.png\" data-width=\"400\" data-height=\"312\" data-is-featured=\"true\" src=\"https://cdn-images-1.medium.com/max/1600/1*M5GYo7SpIusR0m9AG83-MQ.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"http://mobicon.tistory.com/142\" data-href=\"http://mobicon.tistory.com/142\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\">http://mobicon.tistory.com/142</a></figcaption></figure><ul class=\"postList\"><li id=\"be50\" class=\"graf graf--li graf-after--figure\">Writes are distributed among nodes using a hash function (more on this later) and reads are channeled onto specific nodes.</li><li id=\"1eb1\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">High Write Availability</strong>: When a master node goes down, MongoDB stops taking new writes until rest of the nodes choose a new master. On the other hand, in Cassandra, if one node goes down, the writes are redirected towards other nodes and the system continues to operate.</li><li id=\"2ea6\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Linear Scaling</strong>: due to its multi master architecture, Cassandra is linearly scalable, doubling the number of nodes in a cluster can handle twice the writes.</li><li id=\"d551\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Design Time Schema</strong>: Cassandra requires defining schema and data types at design time. That’s not how Cassandra started, but it evolved and now you must define schema first.</li><li id=\"8f94\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Hot Writes in RAM</strong>: Cassandra stores incoming writes in RAM to provide speedy performance (more on this later).</li><li id=\"811b\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">AP system</strong>: Cassandra is considered highly available and partition tolerant system in terms of <a href=\"https://en.wikipedia.org/wiki/CAP_theorem\" data-href=\"https://en.wikipedia.org/wiki/CAP_theorem\" class=\"markup--anchor markup--li-anchor\" rel=\"noopener\" target=\"_blank\">CAP theorem</a>.</li><li id=\"e26d\" class=\"graf graf--li graf-after--li\"><strong class=\"markup--strong markup--li-strong\">Column family Store</strong>: Cassandra is neither a row based store nor column oriented store, its a column family store which is a different concept. (More on this later)</li></ul><p id=\"3a31\" class=\"graf graf--p graf-after--li graf--trailing\"><a href=\"https://blog.emumba.com/apache-cassandra-part-2-cassandra-vs-mongodb-ab9d10ee10cb\" data-href=\"https://blog.emumba.com/apache-cassandra-part-2-cassandra-vs-mongodb-ab9d10ee10cb\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Next: Apache Cassandra, Part 2: Cassandra vs MongoDB</a></p>",
        "created_at": "2018-09-07T15:28:55+0000",
        "updated_at": "2018-09-07T15:28:58+0000",
        "published_at": "2018-02-10T10:28:44+0000",
        "published_by": [
          "Haris Hasan"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "blog.emumba.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*M5GYo7SpIusR0m9AG83-MQ.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12084"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          }
        ],
        "is_public": false,
        "id": 12083,
        "uid": null,
        "title": "Apache Cassandra, Part 6: Time Series Modelling",
        "url": "https://blog.emumba.com/apache-cassandra-part-6-time-series-modelling-b9b0ad2e96d9?gi=b758d98c8286",
        "content": "<section class=\"section section--body section--first\"><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"8c30\" class=\"graf graf--p graf-after--h3 graf--trailing\">This <a href=\"https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc\" data-href=\"https://blog.emumba.com/apache-cassandra-part-1-introduction-and-key-features-18d02ba0b8cc\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">series of posts</a> present an introduction to Apache Cassandra. It discusses key Cassandra features, its core concepts, how it works under the hood, how it is different from other data stores, data modelling best practices with examples, and some tips &amp; tricks.</p></div></div></section><section class=\"section section--body section--last\"><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><figure id=\"aa5d\" class=\"graf graf--figure graf--leading\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*cS211TKA5VDdyTai6iIhzQ.png\" data-width=\"400\" data-height=\"260\" src=\"https://cdn-images-1.medium.com/max/1600/1*cS211TKA5VDdyTai6iIhzQ.png\" alt=\"image\" /></div></figure><p id=\"00b6\" class=\"graf graf--p graf-after--figure\">Consider a scenario where we have weather data coming in from various weather stations and we have to store key value pairs of time and temperature against each station.</p><p id=\"e0f9\" class=\"graf graf--p graf-after--p\">Cassandra supports up to 2 Billion key value pairs in a row which means we can design our data model like this.</p><figure id=\"f3e5\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*HblEiC167FNClfQ9iEzgTA.png\" data-width=\"800\" data-height=\"95\" data-action=\"zoom\" data-action-value=\"1*HblEiC167FNClfQ9iEzgTA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*HblEiC167FNClfQ9iEzgTA.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" data-href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener noopener\" target=\"_blank\">https://academy.datastax.com/resources/getting-started-time-series-data-modeling</a></figcaption></figure><p id=\"0449\" class=\"graf graf--p graf-after--figure\">In this design station Id is partition key, therefore, we will be able to easily find partition containing data from a particular station. Each station would be transmitting data points at same rate, so each partition will have same amount of data. Hence the proposed data model satisfies both of the Cassandra’s data modelling goals.</p><p id=\"010a\" class=\"graf graf--p graf-after--p\">But there is a problem, if a weather station transmits a new entry every second, we are will end up with huge partitions pretty soon. An improvement could be to create a composite partition key using station Id and date like this.</p><figure id=\"d516\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*3P0oxHxASLCry3qAP_Cjdg.png\" data-width=\"800\" data-height=\"144\" data-action=\"zoom\" data-action-value=\"1*3P0oxHxASLCry3qAP_Cjdg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*3P0oxHxASLCry3qAP_Cjdg.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" data-href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener noopener\" target=\"_blank\">https://academy.datastax.com/resources/getting-started-time-series-data-modeling</a></figcaption></figure><p id=\"bc62\" class=\"graf graf--p graf-after--figure\">This way we achieve manageable sized partitions and efficient date wise access to weather data.</p><p id=\"5d89\" class=\"graf graf--p graf-after--p\">In weather domain most of the queries will involve latest data. Our current model appends each new entry at the end, therefore, to find the latest N records Cassandra will go until the end of partition and read last N records. A better approach could be to store data in reverse timestamp order like this.</p><figure id=\"c620\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*BwA91ED4qAGd4UyCPG7ydA.png\" data-width=\"800\" data-height=\"122\" data-action=\"zoom\" data-action-value=\"1*BwA91ED4qAGd4UyCPG7ydA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*BwA91ED4qAGd4UyCPG7ydA.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><a href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" data-href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener noopener noopener noopener noopener\" target=\"_blank\">https://academy.datastax.com/resources/getting-started-time-series-data-modeling</a></figcaption></figure><p id=\"d0ff\" class=\"graf graf--p graf-after--figure\">This approach will reduce our read cost by keeping fresh data at start of partition. Older data can be deleted from the end to keep the partition size manageable.</p><p id=\"a354\" class=\"graf graf--p graf-after--p\">Cassandra also provides a data type <strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">time-uuid</em></strong> that comes handy when multiple events might come in with same timestamp. To avoid timestamp collision time-uuid appends timestamp with a random Id which guarantees record uniqueness.</p><p id=\"3724\" class=\"graf graf--p graf-after--p graf--trailing\"><a href=\"https://blog.emumba.com/apache-cassandra-part-7-secondary-index-replication-and-tips-da273ea59993\" data-href=\"https://blog.emumba.com/apache-cassandra-part-7-secondary-index-replication-and-tips-da273ea59993\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">Next: Apache Cassandra, Part 7: Secondary Index, Replication, Tips</a></p></div></div></section>",
        "created_at": "2018-09-07T15:28:26+0000",
        "updated_at": "2018-09-07T15:28:29+0000",
        "published_at": "2018-02-10T10:31:54+0000",
        "published_by": [
          "Haris Hasan"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "blog.emumba.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*cS211TKA5VDdyTai6iIhzQ.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12083"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          }
        ],
        "is_public": false,
        "id": 12082,
        "uid": null,
        "title": "Scaling Time Series Data Storage — Part I",
        "url": "https://medium.com/netflix-techblog/scaling-time-series-data-storage-part-i-ec2b6d44ba39",
        "content": "<div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><h1 id=\"ffa0\" class=\"graf graf--h3 graf--leading graf--title\">Scaling Time Series Data Storage — Part I</h1><h2 id=\"32f2\" class=\"graf graf--h4 graf-after--h3 graf--subtitle\">by <a href=\"https://www.linkedin.com/in/ketan-duvedi-19395019/\" data-href=\"https://www.linkedin.com/in/ketan-duvedi-19395019/\" class=\"markup--anchor markup--h4-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Ketan Duvedi</a>, <a href=\"https://www.linkedin.com/in/jinhua-li-00830744/\" data-href=\"https://www.linkedin.com/in/jinhua-li-00830744/\" class=\"markup--anchor markup--h4-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Jinhua Li</a>, <a href=\"https://www.linkedin.com/in/dhruv-garg-1362862/\" data-href=\"https://www.linkedin.com/in/dhruv-garg-1362862/\" class=\"markup--anchor markup--h4-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Dhruv Garg</a>, <a href=\"https://www.linkedin.com/in/philfish/\" data-href=\"https://www.linkedin.com/in/philfish/\" class=\"markup--anchor markup--h4-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Philip Fisher-Ogden</a></h2><h3 id=\"4d6b\" class=\"graf graf--h3 graf-after--h4\">Introduction</h3><p id=\"f574\" class=\"graf graf--p graf-after--h3\">The growth of internet connected devices has led to a vast amount of easily accessible time series data. Increasingly, companies are interested in mining this data to derive useful insights and make data-informed decisions. Recent technology advancements have improved the efficiency of collecting, storing and analyzing time series data, spurring an increased appetite to consume this data. However this explosion of time series data can overwhelm most initial time series data architectures.</p><p id=\"0dbf\" class=\"graf graf--p graf-after--p\">Netflix, being a data-informed company, is no stranger to these challenges and over the years has enhanced its solutions to manage the growth. In this 2-part blog post series, we will share how Netflix has evolved a time series data storage architecture through multiple increases in scale.</p><h3 id=\"69a3\" class=\"graf graf--h3 graf-after--p\">Time Series Data — Member Viewing History</h3><p id=\"11bc\" class=\"graf graf--p graf-after--h3\">Netflix members watch over 140 million hours of content per day. Each member provides several data points while viewing a title and they are stored as viewing records. Netflix analyzes the viewing data and provides real time accurate bookmarks and personalized recommendations as described in these posts:</p><ul class=\"postList\"><li id=\"0451\" class=\"graf graf--li graf-after--p\"><a href=\"https://medium.com/netflix-techblog/netflixs-viewing-data-how-we-know-where-you-are-in-house-of-cards-608dd61077da\" data-href=\"https://medium.com/netflix-techblog/netflixs-viewing-data-how-we-know-where-you-are-in-house-of-cards-608dd61077da\" class=\"markup--anchor markup--li-anchor\" target=\"_blank\">How we know where you are in a show?</a></li><li id=\"a5e4\" class=\"graf graf--li graf-after--li\"><a href=\"https://medium.com/netflix-techblog/to-be-continued-helping-you-find-shows-to-continue-watching-on-7c0d8ee4dab6\" data-href=\"https://medium.com/netflix-techblog/to-be-continued-helping-you-find-shows-to-continue-watching-on-7c0d8ee4dab6\" class=\"markup--anchor markup--li-anchor\" target=\"_blank\">Helping you find shows to continue watching on Netflix</a></li></ul><p id=\"fcbe\" class=\"graf graf--p graf-after--li\">Viewing history data increases along the following 3 dimensions:</p><ol class=\"postList\"><li id=\"5ebd\" class=\"graf graf--li graf-after--p\">As time progresses, more viewing data is stored for each member.</li><li id=\"ec3d\" class=\"graf graf--li graf-after--li\">As member count grows, viewing data is stored for more members.</li><li id=\"350f\" class=\"graf graf--li graf-after--li\">As member monthly viewing hours increase, more viewing data is stored for each member.</li></ol><p id=\"6aa2\" class=\"graf graf--p graf-after--li\">As Netflix streaming has grown to 100M+ global members in its first 10 years there has been a massive increase in viewing history data. In this blog post we will focus on how we approached the big challenge of scaling storage of viewing history data.</p><h3 id=\"ed8f\" class=\"graf graf--h3 graf-after--p\">Start Simple</h3><p id=\"c308\" class=\"graf graf--p graf-after--h3\">The first cloud-native version of the viewing history storage architecture used Cassandra for the following reasons:</p><ul class=\"postList\"><li id=\"6268\" class=\"graf graf--li graf-after--p\">Cassandra has good support for <a href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" data-href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">modelling time series data</a> wherein each row can have dynamic number of columns.</li><li id=\"4d7f\" class=\"graf graf--li graf-after--li\">The viewing history data write to read ratio is about 9:1. Since Cassandra is <a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_manage_ondisk_c.html\" data-href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_manage_ondisk_c.html\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">highly efficient with writes</a>, this write heavy workload is a good fit for Cassandra.</li><li id=\"b341\" class=\"graf graf--li graf-after--li\">Considering the <a href=\"https://en.wikipedia.org/wiki/CAP_theorem\" data-href=\"https://en.wikipedia.org/wiki/CAP_theorem\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">CAP theorem</a>, the team favors eventual consistency over loss of availability. Cassandra supports this tradeoff via <a href=\"http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html\" data-href=\"http://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">tunable consistency</a>.</li></ul><p id=\"9944\" class=\"graf graf--p graf-after--li\">In the initial approach, each member’s viewing history was stored in Cassandra in a single row with row key:CustomerId. This horizontal partitioning enabled effective scaling with member growth and made the common use case of reading a member’s entire viewing history very simple and efficient. However as member count increased and, more importantly, each member streamed more and more titles, the row sizes as well as the overall data size increased. Over time, this resulted in high storage and operation cost as well as slower performance for members with large viewing history.</p><p id=\"bae4\" class=\"graf graf--p graf-after--p\">The following figure illustrates the read and write flows of the initial data model:</p><figure id=\"7224\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*QHOtYeP8F9J46FRq.\" data-width=\"720\" data-height=\"399\" data-action=\"zoom\" data-action-value=\"0*QHOtYeP8F9J46FRq.\" src=\"https://cdn-images-1.medium.com/max/1600/0*QHOtYeP8F9J46FRq.\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Figure 1: Single Table Data Model</figcaption></div></figure><h3 id=\"2cea\" class=\"graf graf--h3 graf-after--figure\">Write Flow</h3><p id=\"9835\" class=\"graf graf--p graf-after--h3\">One viewing record was inserted as a new column when a member started playing a title. That viewing record was updated after member paused or stopped the title. This single column write was fast and efficient.</p><h3 id=\"2494\" class=\"graf graf--h3 graf-after--p\">Read Flows</h3><p id=\"dd03\" class=\"graf graf--p graf-after--h3\"><em class=\"markup--em markup--p-em\">Whole row read to retrieve all viewing records for one member:</em> The read was efficient when the number of records per member was small. As a member watched more titles, the number of viewing records increased. Reading rows with a large number of columns put additional stress on Cassandra that negatively impacted read latencies.</p><p id=\"443e\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">Time range query to read a time slice of a member’s data:</em> This resulted in the same inconsistent performance as above depending on the number of viewing records within the specified time range.</p><p id=\"0800\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">Whole row read via pagination for large viewing history:</em> This was better for Cassandra as it wasn’t waiting for all the data to be ready before sending it back. This also avoided client timeouts. However it increased overall latency to read the whole row as the number of viewing records increased.</p><h3 id=\"febd\" class=\"graf graf--h3 graf-after--p\">Slowdown Reasons</h3><p id=\"65d1\" class=\"graf graf--p graf-after--h3\">Let’s look at some of the Cassandra internals to understand why our initial simple design slowed down. As the data grew, the number of <a href=\"https://wiki.apache.org/cassandra/MemtableSSTable\" data-href=\"https://wiki.apache.org/cassandra/MemtableSSTable\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">SSTables</a> increased accordingly. Since only recent data was in memory, in many cases <a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntAboutReads.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntAboutReads.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">both the memtables and SSTables had to be read</a> to retrieve viewing history. This had a negative impact on read latency. Similarly <a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntHowDataMaintain.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntHowDataMaintain.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Compaction</a> took more IOs and time as the data size increased. <a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archRepairNodesReadRepair.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archRepairNodesReadRepair.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Read repair</a> and <a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archAntiEntropyRepair.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archAntiEntropyRepair.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Full column repair</a> became slower as rows got wider.</p><h3 id=\"988b\" class=\"graf graf--h3 graf-after--p\">Caching Layer</h3><p id=\"ae36\" class=\"graf graf--p graf-after--h3\">Cassandra performed very well writing viewing history data but there was a need to improve the read latencies. To optimize read latencies, at the expense of increased work during the write path, we added an in-memory sharded caching layer (<a href=\"https://medium.com/netflix-techblog/announcing-evcache-distributed-in-memory-datastore-for-cloud-c26a698c27f7\" data-href=\"https://medium.com/netflix-techblog/announcing-evcache-distributed-in-memory-datastore-for-cloud-c26a698c27f7\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">EVCache</a>) in front of Cassandra storage. The cache was a simple key value store with the key being CustomerId and value being the compressed binary representation of viewing history data. Each write to Cassandra incurred an additional cache lookup and on cache hit the new data was merged with the existing value. Viewing history reads were serviced by the cache first. On a cache miss, the entry was read from Cassandra, compressed and then inserted in the cache.</p><p id=\"8385\" class=\"graf graf--p graf-after--p\">With the addition of the caching layer, this single Cassandra table storage approach worked very well for many years. Partitioning based on CustomerId scaled well in the Cassandra cluster. By 2012, the Viewing History Cassandra cluster was one of the biggest dedicated Cassandra clusters at Netflix. To scale further, the team needed to double the cluster size. This meant venturing into uncharted territory for Netflix’s usage of Cassandra. In the meanwhile, Netflix business was continuing to grow rapidly, including an increasing international member base and forthcoming ventures into original content.</p><h3 id=\"3495\" class=\"graf graf--h3 graf-after--p\">Redesign: Live and Compressed Storage Approach</h3><p id=\"fd31\" class=\"graf graf--p graf-after--h3\">It became clear that a different approach was needed to scale for growth anticipated over the next 5 years. The team analyzed the data characteristics and usage patterns, and redesigned viewing history storage with two main goals in mind:</p><ol class=\"postList\"><li id=\"ab0b\" class=\"graf graf--li graf-after--p\">Smaller Storage Footprint.</li><li id=\"9eea\" class=\"graf graf--li graf-after--li\">Consistent Read/Write Performance as viewing per member grows.</li></ol><p id=\"2106\" class=\"graf graf--p graf-after--li\">For each member, viewing history data is divided into two sets:</p><ul class=\"postList\"><li id=\"5139\" class=\"graf graf--li graf-after--p\"><em class=\"markup--em markup--li-em\">Live or Recent Viewing History (LiveVH):</em> Small number of recent viewing records with frequent updates. The data is stored in uncompressed form as in the simple design detailed above.</li><li id=\"2972\" class=\"graf graf--li graf-after--li\"><em class=\"markup--em markup--li-em\">Compressed or Archival Viewing History (CompressedVH):</em> Large number of older viewing records with rare updates. The data is compressed to reduce storage footprint. Compressed viewing history is stored in a single column per row key.</li></ul><p id=\"5e85\" class=\"graf graf--p graf-after--li\">LiveVH and CompressedVH are stored in different tables and are tuned differently to achieve better performance. Since LiveVH has frequent updates and small number of viewing records, compactions are run frequently and <a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntAboutDeletes.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntAboutDeletes.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">gc_grace_seconds</a> is small to reduce number of SSTables and data size. <a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archRepairNodesReadRepair.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archRepairNodesReadRepair.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Read repair</a> and<a href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archAntiEntropyRepair.html\" data-href=\"http://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbArch/archAntiEntropyRepair.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"> full column family repair</a> are run frequently to improve data consistency. Since updates to CompressedVH are rare, manual and infrequent full compactions are sufficient to reduce number of SSTables. Data is checked for consistency during the rare updates. This obviates the need for read repair as well as full column family repair.</p><h3 id=\"3c67\" class=\"graf graf--h3 graf-after--p\">Write Flow</h3><p id=\"dae1\" class=\"graf graf--p graf-after--h3\">New viewing records are written to LiveVH using the same approach as described earlier.</p><h3 id=\"06e7\" class=\"graf graf--h3 graf-after--p\">Read Flows</h3><p id=\"d975\" class=\"graf graf--p graf-after--h3\">To get the benefit of the new design, the viewing history API was updated with an option to read recent or full data:</p><ul class=\"postList\"><li id=\"d3b2\" class=\"graf graf--li graf-after--p\"><em class=\"markup--em markup--li-em\">Recent Viewing History:</em> For most cases this results in reading from LiveVH only, which limits the data size resulting in much lower latencies.</li><li id=\"49a7\" class=\"graf graf--li graf-after--li\"><em class=\"markup--em markup--li-em\">Full Viewing History:</em> Implemented as parallel reads of LiveVH and CompressedVH. Due to data compression and CompressedVH having fewer columns, less data is read thereby significantly speeding up reads.</li></ul><h3 id=\"24b1\" class=\"graf graf--h3 graf-after--li\">CompressedVH Update Flow</h3><p id=\"79c5\" class=\"graf graf--p graf-after--h3\">While reading viewing history records from LiveVH, if the number of records is over a configurable threshold then the recent viewing records are rolled up, compressed and stored in CompressedVH via a background task. Rolled up data is stored in a new row with row key:CustomerId. The new rollup is versioned and after being written is read to check for consistency. Only after verifying the consistency of the new version, the old version of rolled up data is deleted. For simplicity there is no locking during rollup and Cassandra takes care of resolving very rare duplicate writes (i.e., the last writer wins).</p><figure id=\"fdb3\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*VWfKAENoS8-OcyCL.\" data-width=\"720\" data-height=\"407\" data-is-featured=\"true\" data-action=\"zoom\" data-action-value=\"0*VWfKAENoS8-OcyCL.\" src=\"https://cdn-images-1.medium.com/max/1600/0*VWfKAENoS8-OcyCL.\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Figure 2: Live and Compressed Data Model</figcaption></div></figure><p id=\"a2fb\" class=\"graf graf--p graf-after--figure\">As shown in figure 2, the rolled up row in CompressedVH also stores metadata information like the latest version, object size and chunking information (more on that later). The version column stores a reference to the latest version of rolled up data so that reads for a CustomerId always return only the latest rolled up data. The rolled up data is stored in a single column to reduce compaction pressure. To minimize the frequency of rollups for members with frequent viewing pattern, just the last couple of days worth of viewing history records are kept in LiveVH after rollup and the rest are merged with the records in CompressedVH during rollup.</p><h3 id=\"0727\" class=\"graf graf--h3 graf-after--p\">Auto Scaling via Chunking</h3><p id=\"c814\" class=\"graf graf--p graf-after--h3\">For the majority of members, storing their entire viewing history in a single row of compressed data resulted in good performance during the read flows. For a small percentage of members with very large viewing history, reading CompressedVH from a single row started to slow down due to similar reasons as described in the first architecture. There was a need to have an upper bound on the read and write latencies for this rare case without negatively impacting the read and write latencies for the common case.</p><p id=\"b2e0\" class=\"graf graf--p graf-after--p\">To solve for this, we split the rolled up compressed data into multiple chunks if the data size is greater than a configurable threshold. These chunks are stored on different Cassandra nodes. Parallel reads and writes of these chunks results in having an upper bound on the read and write latencies even for very large viewing data.</p><figure id=\"dcc2\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*rQ0DZVO0oR_MHVNg.\" data-width=\"720\" data-height=\"647\" data-action=\"zoom\" data-action-value=\"0*rQ0DZVO0oR_MHVNg.\" src=\"https://cdn-images-1.medium.com/max/1600/0*rQ0DZVO0oR_MHVNg.\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Figure 3: Auto Scale via Chunking</figcaption></div></figure><h4 id=\"7720\" class=\"graf graf--h4 graf-after--figure\">Write Flow</h4><p id=\"be96\" class=\"graf graf--p graf-after--h4\">As figure 3 indicates, rolled up compressed data is split into multiple chunks based on a configurable chunk size. All chunks are written in parallel to different rows with row key:CustomerId$Version$ChunkNumber. Metadata is written to its own row with row key:CustomerId after successful write of the chunked data. This bounds the write latency to two writes for rollups of very large viewing data. In this case the metadata row has an empty data column to enable fast read of metadata.</p><p id=\"bd49\" class=\"graf graf--p graf-after--p\">To make the common case (compressed viewing data is smaller than the configurable threshold) fast, metadata is combined with the viewing data in the same row to eliminate metadata lookup overhead as shown in figure 2.</p><h4 id=\"80df\" class=\"graf graf--h4 graf-after--p\">Read Flow</h4><p id=\"ba04\" class=\"graf graf--p graf-after--h4\">The metadata row is first read using CustomerId as the key. For the common case, the chunk count is 1 and the metadata row also has the most recent version of rolled up compressed viewing data. For the rare case, there are multiple chunks of compressed viewing data. Using the metadata information like version and chunk count, different row keys for the chunks are generated and all chunks are read in parallel. This bounds the read latency to two reads.</p><h3 id=\"3434\" class=\"graf graf--h3 graf-after--p\">Caching Layer Changes</h3><p id=\"e1c0\" class=\"graf graf--p graf-after--h3\">The in-memory caching layer was enhanced to support chunking for large entries. For members with large viewing history, it was not possible to fit the entire compressed viewing history in a single EVCache entry. So similar to the CompressedVH model, each large viewing history cache entry is broken into multiple chunks and the metadata is stored along with the first chunk.</p><h3 id=\"1e53\" class=\"graf graf--h3 graf-after--p\">Results</h3><p id=\"c4d8\" class=\"graf graf--p graf-after--h3\">By leveraging parallelism, compression, and an improved data model, the team was able to meet all of the goals:</p><ol class=\"postList\"><li id=\"b384\" class=\"graf graf--li graf-after--p\">Smaller Storage Footprint via compression.</li><li id=\"5920\" class=\"graf graf--li graf-after--li\">Consistent Read/Write Performance via chunking and parallel reads/writes. Latency bound to one read and one write for common cases and latency bound to two reads and two writes for rare cases.</li></ol><figure id=\"c6e1\" class=\"graf graf--figure graf-after--li\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*t2YDONTlaJIDxDnf.\" data-width=\"720\" data-height=\"400\" data-action=\"zoom\" data-action-value=\"0*t2YDONTlaJIDxDnf.\" src=\"https://cdn-images-1.medium.com/max/1600/0*t2YDONTlaJIDxDnf.\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Figure 4: Results</figcaption></div></figure><p id=\"3d8e\" class=\"graf graf--p graf-after--figure\">The team achieved ~6X reduction in data size, ~13X reduction in system time spent on Cassandra maintenance, ~5X reduction in average read latency and ~1.5X reduction in average write latency. More importantly, it gave the team a scalable architecture and headroom to accommodate rapid growth of Netflix viewing data.</p><p id=\"97a7\" class=\"graf graf--p graf-after--p graf--trailing\">In the next part of this blog post series, we will explore the latest scalability challenges motivating the next iteration of viewing history storage architecture. If you are interested in solving similar problems, <a href=\"https://jobs.netflix.com/jobs/866030\" data-href=\"https://jobs.netflix.com/jobs/866030\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">join us</a>.</p></div></div>",
        "created_at": "2018-09-07T14:43:46+0000",
        "updated_at": "2018-09-07T14:44:07+0000",
        "published_at": "2018-01-23T19:29:00+0000",
        "published_by": [
          "Netflix Technology Blog"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 10,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/0*VWfKAENoS8-OcyCL.",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12082"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1198,
            "label": "benchmarking",
            "slug": "benchmarking"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          }
        ],
        "is_public": false,
        "id": 12081,
        "uid": null,
        "title": "Eye or the Tiger: Benchmarking Cassandra vs. TimescaleDB for time-series data",
        "url": "https://blog.timescale.com/time-series-data-cassandra-vs-timescaledb-postgresql-7c2cc50a89ce?gi=d30b34bb9d",
        "content": "<p id=\"a3be\" class=\"graf graf--p graf-after--h3\"><em class=\"markup--em markup--p-em\">How a 5 node TimescaleDB cluster outperforms 30 Cassandra nodes, with higher inserts, up to 5800x faster queries, 10% the cost, a more flexible data model, <br />and of course, full SQL.</em></p><figure id=\"6ea0\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*nez96S_THBQasVhTjWVacw.jpeg\" data-width=\"1024\" data-height=\"898\" data-action=\"zoom\" data-action-value=\"1*nez96S_THBQasVhTjWVacw.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*nez96S_THBQasVhTjWVacw.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\"><em class=\"markup--em markup--figure-em\">? It’s the </em></strong><a href=\"https://commons.wikimedia.org/wiki/File:Cassandra_logo.svg\" data-href=\"https://commons.wikimedia.org/wiki/File:Cassandra_logo.svg\" class=\"markup--anchor markup--figure-anchor\" rel=\"noopener\" target=\"_blank\"><strong class=\"markup--strong markup--figure-strong\"><em class=\"markup--em markup--figure-em\">eye</em></strong></a><strong class=\"markup--strong markup--figure-strong\"><em class=\"markup--em markup--figure-em\"> or the </em></strong><a href=\"http://www.timescale.com/\" data-href=\"http://www.timescale.com/\" class=\"markup--anchor markup--figure-anchor\" rel=\"noopener\" target=\"_blank\"><strong class=\"markup--strong markup--figure-strong\"><em class=\"markup--em markup--figure-em\">tiger</em></strong></a><strong class=\"markup--strong markup--figure-strong\"><em class=\"markup--em markup--figure-em\">, it’s the thrill of the fight (for time-series data) ?</em></strong></figcaption></figure><p id=\"184d\" class=\"graf graf--p graf--hasDropCapModel graf--hasDropCap graf-after--figure\">With its simple <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeDistribute.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeDistribute.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">data partitioning and distribution architecture</a>, highly tunable <a href=\"https://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html\" data-href=\"https://docs.datastax.com/en/archived/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">consistency settings</a>, and strong cluster management <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/tools/toolsNodetool.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/tools/toolsNodetool.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">tooling</a>, <strong class=\"markup--strong markup--p-strong\">Cassandra</strong> is legendary for its scalability. Developers often forego the expressiveness of SQL for the intoxicating power of being able to add write nodes to a Cassandra cluster with a single command. Moreover, Cassandra’s ability to provide sorted wide rows (more on this later) makes it a compelling use case for a scalable time-series data store.</p><p id=\"7899\" class=\"graf graf--p graf-after--p\"><a href=\"https://blog.timescale.com/scalable-postgresql-high-availability-read-scalability-streaming-replication-fb95023e2af\" data-href=\"https://blog.timescale.com/scalable-postgresql-high-availability-read-scalability-streaming-replication-fb95023e2af\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">We’ve already written</a> about how the notion of giving up the structure, maturity, and rich extensibility of PostgreSQL for scalability is a false dilemma. We’ve also pitted TimescaleDB against another popular NoSQL database in <a href=\"https://blog.timescale.com/how-to-store-time-series-data-mongodb-vs-timescaledb-postgresql-a73939734016\" data-href=\"https://blog.timescale.com/how-to-store-time-series-data-mongodb-vs-timescaledb-postgresql-a73939734016\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">our recent benchmarking post about MongoDB</a>. Nonetheless, Cassandra’s ease of use, staying power, and potential to handle time-series data well through its sequentially sorted wide rows make it a natural comparison to TimescaleDB.</p><p id=\"861f\" class=\"graf graf--p graf-after--p\">In this post, we dig deeper into using Cassandra vs. TimescaleDB for time-series workloads by comparing the scaling patterns, data model complexity, insert rates, read rates, and read throughput of each database. We start by comparing 5 node clusters for each database. Then, we benchmark a few different cluster configurations because the scaling properties of 5 node TimescaleDB and 5 node Cassandra are not perfectly analogous.</p><h4 id=\"9dfd\" class=\"graf graf--h4 graf-after--p\">Benchmarking Setup</h4><p id=\"0bf1\" class=\"graf graf--p graf-after--h4\">Let’s quickly take stock of the specs we used for these tests:</p><ul class=\"postList\"><li id=\"f9ba\" class=\"graf graf--li graf-after--p\">2 remote client machines, both on the same LAN as the databases</li><li id=\"b4f7\" class=\"graf graf--li graf-after--li\">Azure instances: Standard D8s v3 (8 vCPUs, 32 GB memory)</li><li id=\"3856\" class=\"graf graf--li graf-after--li\">5 TimescaleDB nodes, 5/10/30 Cassandra nodes (as noted)</li><li id=\"facc\" class=\"graf graf--li graf-after--li\">4 1-TB disks in a raid0 configuration (EXT4 filesystem)</li><li id=\"6880\" class=\"graf graf--li graf-after--li\">Dataset: 4,000 simulated devices generated 10 CPU metrics every 10 seconds for 3 full days (~100M reading intervals, ~1B metrics)</li><li id=\"5959\" class=\"graf graf--li graf-after--li\">For TimescaleDB, we set the chunk size to 12 hours, resulting in 6 total chunks (<a href=\"http://docs.timescale.com/v0.9/using-timescaledb/hypertables#best-practices\" data-href=\"http://docs.timescale.com/v0.9/using-timescaledb/hypertables#best-practices\" class=\"markup--anchor markup--li-anchor\" rel=\"noopener\" target=\"_blank\">more here</a>)</li></ul><h4 id=\"3ab3\" class=\"graf graf--h4 graf-after--li\">The Results</h4><p id=\"879a\" class=\"graf graf--p graf-after--h4\">Before moving forward, let’s start with a visual preview of how 5 TimescaleDB nodes fare against various sizes of Cassandra clusters:</p><figure id=\"9705\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*r0qp_ot-pVnQP0HdC4UaRQ.jpeg\" data-width=\"4056\" data-height=\"3449\" data-focus-x=\"51\" data-focus-y=\"23\" data-is-featured=\"true\" data-action=\"zoom\" data-action-value=\"1*r0qp_ot-pVnQP0HdC4UaRQ.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*r0qp_ot-pVnQP0HdC4UaRQ.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">5 node TimescaleDB cluster exhibits higher insert performance at a fraction of the cost (i.e., higher cluster efficiency) than a 30 node Cassandra cluster. Also, Cassandra scalability appears to be somewhat sub-linear.</strong></figcaption></figure><p id=\"e7db\" class=\"graf graf--p graf-after--figure\">Now, let’s look a bit further into how TimescaleDB and Cassandra achieve scalability.</p><h3 id=\"d2a6\" class=\"graf graf--h3 graf-after--p\">Scaling with TimescaleDB and Cassandra</h3><p id=\"aa2d\" class=\"graf graf--p graf-after--h3\">Cassandra provides simple scale-out functionality through a combination of its <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeDistribute.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeDistribute.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">data partitioning</a>, <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeDistribute.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archDataDistributeDistribute.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">virtual node abstraction</a>, and <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archGossipAbout.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/architecture/archGossipAbout.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">internode gossip</a>. Adding a node to the cluster redistributes the data in a manner transparent to the client and increases write throughput more-or-less linearly (actually somewhat sub-linear in our tests)¹. Cassandra also provides tunable availability with its <a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useUpdateKeyspaceRF.html\" data-href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useUpdateKeyspaceRF.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">replication factor</a> configuration, which determines the number of nodes that have a copy of a given piece of data.</p><p id=\"5447\" class=\"graf graf--p graf-after--p\">PostgreSQL and TimescaleDB support scaling out reads by way of <a href=\"https://blog.timescale.com/scalable-postgresql-high-availability-read-scalability-streaming-replication-fb95023e2af\" data-href=\"https://blog.timescale.com/scalable-postgresql-high-availability-read-scalability-streaming-replication-fb95023e2af\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">streaming replication</a>. Each replica node can be used as a read node to increase read throughput. Although PostgreSQL does not natively provide scale-out write functionality, users can often get the additional throughput they need by using RAID disk arrays or leveraging the <a href=\"https://www.postgresql.org/docs/current/static/manage-ag-tablespaces.html\" data-href=\"https://www.postgresql.org/docs/current/static/manage-ag-tablespaces.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">tablespace</a> functionality provided by PostgreSQL. In addition, unlike PostgreSQL, TimescaleDB allows users to (elastically) assign multiple tablespaces to a single hypertable if desired (e.g., multiple network-attached disks), creating the potential for massively scaling disk throughput on a single TimescaleDB instance. Moreover, as we’ll see, the write performance a single TimescaleDB instance provides for time-series data is quite often more than sufficient for a production workload — and that’s without some of the traditional NoSQL drawbacks that come with Cassandra.</p><h3 id=\"8e98\" class=\"graf graf--h3 graf-after--p\">Cassandra shortcomings: poor index support, no JOINs, restrictive query language, no referential integrity</h3><p id=\"0853\" class=\"graf graf--p graf-after--h3\">The dead simple scalability of Cassandra does not come without a cost. A number of key features that users of full-SQL databases like TimescaleDB take for granted are either not provided, very cumbersome, or not performant in Cassandra.</p><p id=\"563f\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Indexes: </strong>Unlike traditional SQL databases, Cassandra does not support global indexes, often making it prohibitive to look up or filter data by anything other than the primary or clustering keys. Clustering keys, which we’ll discuss shortly, provide ordering only for a single “row” of data. Cassandra does support <a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useSecondaryIndex.html\" data-href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useSecondaryIndex.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">secondary indexes</a>, but they are created <em class=\"markup--em markup--p-em\">locally</em> on each node to preserve the scaleable writes of Cassandra. This means that <em class=\"markup--em markup--p-em\">every</em> node must be queried each time an index lookup is performed, often leading to unacceptable performance.</p><p id=\"6192\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">JOINs</strong>: Cassandra is not a relational database and does not support natively joining data from two different sources. Users often have to create complex, error prone, and/or tedious client side logic to combine and filter data; indeed, this is what we had to do to get many of our benchmarking queries to be performant. While there are <a href=\"https://www.datastax.com/2015/03/how-to-do-joins-in-apache-cassandra-and-datastax-enterprise\" data-href=\"https://www.datastax.com/2015/03/how-to-do-joins-in-apache-cassandra-and-datastax-enterprise\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">tools available</a> for performing joins in Cassandra, they rely on implementing client-side joins either through heavyweight clients or standalone proxies in front of Cassandra itself.</p><p id=\"d99a\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Query Language: </strong>Cassandra’s query language (<a href=\"http://cassandra.apache.org/doc/latest/cql/\" data-href=\"http://cassandra.apache.org/doc/latest/cql/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">CQL</a>) lacks the expressiveness of SQL. Much of this has to do with some of the limitations brought on by Cassandra’s architecture. For example, the use of the <strong class=\"markup--strong markup--p-strong\">WHERE</strong> clause is limited to primary or clustering keys or fields that have secondary indexes defined on them², otherwise the coordinator would need to retrieve data from every node in the cluster for each query. This is also true of the <strong class=\"markup--strong markup--p-strong\">GROUPBY </strong>clause, which was only introduced in Cassandra 3.0. Another significant limitation for many application workflows is that you can only update data using its primary key. These and many other limitations make CQL a poor choice for workloads that require heavy analytical queries or data manipulation on fields beyond the primary and clustering keys.</p><p id=\"68f1\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Referential Integrity</strong>: There is no concept of referential integrity in Cassandra. Any constraints that you would typically model as a foreign key relationship in SQL are impossible in Cassandra without writing custom code to enforce them on the client side.</p><p id=\"f83f\" class=\"graf graf--p graf-after--p\">These limitations typically translate to more complicated data models on the server side and <em class=\"markup--em markup--p-em\">much</em> more complicated client-side logic than other databases we’ve benchmarked. We’ll see this on full display in the time-series data model we chose in order to make Cassandra as performant as possible.</p><p><a href=\"https://blog.dnsfilter.com/3-billion-time-series-data-points-dnsfilter-replaced-influxdb-with-timescaledb-d9f827702f8b\" data-href=\"https://blog.dnsfilter.com/3-billion-time-series-data-points-dnsfilter-replaced-influxdb-with-timescaledb-d9f827702f8b\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://blog.dnsfilter.com/3-billion-time-series-data-points-dnsfilter-replaced-influxdb-with-timescaledb-d9f827702f8b\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Towards 3B time-series data points per day: Why DNSFilter replaced InfluxDB with TimescaleDB</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Our results: 10x better resource utilization, even with 30% more requests</em>blog.dnsfilter.com</a></p><h3 id=\"387f\" class=\"graf graf--h3 graf-after--mixtapeEmbed\">Cassandra Data Model</h3><p id=\"9321\" class=\"graf graf--p graf-after--h3\"><em class=\"markup--em markup--p-em\">First, a quick note on the origins of our Cassandra data model. In the interest of using a similar foundation for comparing database performance against time series workloads, we forked InfluxDB’s </em><a href=\"https://github.com/influxdata/influxdb-comparisons\" data-href=\"https://github.com/influxdata/influxdb-comparisons\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">benchmarker</em></a><em class=\"markup--em markup--p-em\"> for our own internal benchmarks. Their benchmarker comes equipped with a Cassandra time-series data model. We adopted their model largely as is, as we found their </em><a href=\"http://get.influxdata.com/rs/972-GDU-533/images/InfluxDB%201.4%20vs.%20Cassandra%20.pdf\" data-href=\"http://get.influxdata.com/rs/972-GDU-533/images/InfluxDB%201.4%20vs.%20Cassandra%20.pdf\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">reasoning</em></a><em class=\"markup--em markup--p-em\"> sufficiently compelling and aligned with </em><a href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" data-href=\"https://academy.datastax.com/resources/getting-started-time-series-data-modeling\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><em class=\"markup--em markup--p-em\">canonical approaches</em></a><em class=\"markup--em markup--p-em\"> to time-series data in Cassandra.</em></p><p id=\"c44e\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">That being said, we definitely do not consider ourselves Cassandra gurus, so we’re all ears if the community has suggestions on a better model that would improve our benchmarks. We plan to release our benchmarker in the coming weeks, and we’re eager to hear from any experts on various databases who would like to weigh in and help make our benchmarks more robust in general.</em></p><p id=\"8c06\" class=\"graf graf--p graf-after--p\">Cassandra is a <a href=\"https://en.wikipedia.org/wiki/Column_family\" data-href=\"https://en.wikipedia.org/wiki/Column_family\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">column family store</a>. Data for a column family, which is roughly analogous to a table in relational databases, is stored as a set of unique keys. Each of these keys maps to a set of columns which each contain the values for a particular data entry. These key-&gt;column-set tuples are called “rows” (but should not be confused with rows in a relational database).</p><p id=\"a1b9\" class=\"graf graf--p graf-after--p\">In Cassandra, data is partitioned across nodes based on the column family key (called the <strong class=\"markup--strong markup--p-strong\">primary</strong> or <strong class=\"markup--strong markup--p-strong\">partition</strong> key). Additionally, Cassandra allows for <a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useCompoundPrimaryKeyConcept.html\" data-href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useCompoundPrimaryKeyConcept.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">compound primary keys</a>, where the first key in the key definition is the primary/partition key, and any additional keys are known as <strong class=\"markup--strong markup--p-strong\">clustering keys. </strong>These clustering keys specify columns on which to sort the data for each row.</p><p id=\"40ed\" class=\"graf graf--p graf-after--p\">Let’s take a look at how this plays out with the dataset we use for our benchmarks. We simulate a devops monitoring use case where 4,000 unique hosts report 10 CPU metrics every 10 seconds over the course of 3 days, resulting in a 100 million row dataset<em class=\"markup--em markup--p-em\">.</em></p><p id=\"bc74\" class=\"graf graf--p graf-after--p\">In our Cassandra model, this translates to us creating a column family like this:</p><pre id=\"c399\" class=\"graf graf--pre graf-after--p\">CREATE TABLE measurements (<br />series_id text,<br />timestamp_ns bigint,<br />value double,<br />PRIMARY KEY(series_id, timestamp_ns));</pre><p id=\"efdf\" class=\"graf graf--p graf-after--pre\">The primary key, <strong class=\"markup--strong markup--p-strong\">series_id</strong>, is a combination of the host, day, and metric type in the format <strong class=\"markup--strong markup--p-strong\">hostname#metric_type#day</strong>. This allows us to get around some of the query limitations of Cassandra discussed above, particularly the weak support for joins, indexes, and server side rollups. By encoding the host, metric type, and day into the primary key, we can quickly and easily access the subset of data we need and execute any further filtering, aggregation, and grouping more performantly on the client side.</p><p id=\"c82f\" class=\"graf graf--p graf-after--p\">We use <strong class=\"markup--strong markup--p-strong\">timestamp_ns</strong> as our clustering key, which means that data for each row is ordered by timestamp as we insert it, providing optimal time range lookups. This is what a row of 3 values of the <strong class=\"markup--strong markup--p-strong\">cpu_guest </strong>metric for a given host on a given day would look like:</p><figure id=\"d8ae\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*F3sTKziDqcErVBefj_hsEA.png\" data-width=\"2366\" data-height=\"724\" data-action=\"zoom\" data-action-value=\"1*F3sTKziDqcErVBefj_hsEA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*F3sTKziDqcErVBefj_hsEA.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">A Cassandra data model with measurements stored over time</strong></figcaption></figure><p id=\"30f0\" class=\"graf graf--p graf-after--figure\">This is what we meant when we mentioned the <strong class=\"markup--strong markup--p-strong\">wide row approach</strong> earlier. Each row contains multiple columns, which are themselves sets of key-value pairs. The number of columns for a given row grows as we insert more readings corresponding to that row’s partition key. The columns are clustered by their timestamp, guaranteeing that each row will point to a sequentially sorted set of columns.</p><p id=\"e15a\" class=\"graf graf--p graf-after--p\">This ordered data is passed down to our custom client, which maintains a fairly involved <strong class=\"markup--strong markup--p-strong\">client-side index</strong> to perform the filtering and aggregation that is not supported in a performant manner by Cassandra’s secondary indexes. We maintain a data structure that essentially duplicates Cassandra’s primary key-&gt;metrics mapping and performs filtering and aggregations as we add data from our Cassandra queries. The aggregations and rollups we do on the client side are very simple (min, max, avg, groupby, etc.), so the vast majority of the query time remains at the database level. (In other words, the client-side index works, but also takes a lot more work.)</p><h3 id=\"f115\" class=\"graf graf--h3 graf-after--p\">Insert Performance</h3><p id=\"a7eb\" class=\"graf graf--p graf-after--h3\">Unlike TimescaleDB, Cassandra does not work well with large batch inserts. In fact, batching as a performance optimization is <a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useBatch.html\" data-href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useBatch.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">explicitly discouraged</a> due to bottlenecks on the coordinator node if the transaction hits many partitions. Cassandra’s <a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/configCassandra_yaml.html#configCassandra_yaml__advProps\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/configuration/configCassandra_yaml.html#configCassandra_yaml__advProps\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">default maximum batch size setting</a> is very small at 5KB. Nonetheless, we found that a small amount of batching (batches of 100) actually did help significantly with insert throughput for our dataset, so we used a batch size of 100 for our benchmarks.</p><p id=\"3e53\" class=\"graf graf--p graf-after--p\">To give Cassandra a fair shake against TimescaleDB, which allows for far larger batch sizes (we use 10,000 for our benchmarks), we ramped up the number of concurrent workers writing to Cassandra. While we used just 8 concurrent workers to maximize our write throughput on TimescaleDB, we used 1,800 concurrent workers (spread across multiple client machines) to max out our Cassandra throughput. We tested worker counts from 1 up to 1,800 before settling on 1,800 as the optimal number of workers for maximizing write throughput. Any number of workers above that caused unpredictable server side timeouts and negligible gains (in other words, the tradeoff of latency for throughput became unacceptable).</p><p id=\"1b4b\" class=\"graf graf--p graf-after--p\">To avoid client-side bottlenecks (e.g., with data serialization, the client-side index, or network overhead), we used 2 client VMs, each using our Golang benchmarker with 900 goroutines writing concurrently. We attempted to get more throughput by spreading the client load across even more VMs, but we found no improvements beyond 2 boxes.</p><p id=\"2835\" class=\"graf graf--p graf-after--p\">Since writes are sharded across nodes in Cassandra, its replication and consistency profile is a bit different than that of TimescaleDB. TimescaleDB writes all data to a single primary node which then replicates that data to any connected replicas through <a href=\"https://blog.timescale.com/scalable-postgresql-high-availability-read-scalability-streaming-replication-fb95023e2af\" data-href=\"https://blog.timescale.com/scalable-postgresql-high-availability-read-scalability-streaming-replication-fb95023e2af\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">streaming replication</a>. Cassandra, on the other hand, shards the writes across the cluster, so no single replica stores all the cluster’s data. Instead, you define the <a href=\"http://cassandra.apache.org/doc/latest/architecture/dynamo.html#replication\" data-href=\"http://cassandra.apache.org/doc/latest/architecture/dynamo.html#replication\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">replication factor</a> for a given keyspace, which determines <em class=\"markup--em markup--p-em\">the number of nodes that will have a copy of each data item</em>. You can further<a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlConfigConsistency.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlConfigConsistency.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"> control the consistency</a> of each write transaction on the client side by specifying how many nodes the client waits for the data to be written to. PostgreSQL and TimescaleDB similarly offer <a href=\"http://docs.timescale.com/v0.9/tutorials/replication#Configure-Replication-Parameters\" data-href=\"http://docs.timescale.com/v0.9/tutorials/replication#Configure-Replication-Parameters\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">tunable consistency</a>.</p><p id=\"4fc0\" class=\"graf graf--p graf-after--p\">Given these significant differences, it’s difficult to achieve a truly apples-to-apples comparison of a 5 node TimescaleDB cluster vs. a 5 node Cassandra cluster. We decided on comparing a <strong class=\"markup--strong markup--p-strong\">TimescaleDB cluster with 1 primary and 4 read replicas, synchronous replication, and a </strong><a href=\"https://www.postgresql.org/docs/10/static/runtime-config-replication.html#synchronous_standby_names\" data-href=\"https://www.postgresql.org/docs/10/static/runtime-config-replication.html#synchronous_standby_names\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\">consistency level of ANY 1</strong></a> against a <strong class=\"markup--strong markup--p-strong\">5 node Cassandra cluster with Replication Factor set to 2 and a consistency level of ONE</strong>. In both cases, clients will wait on data to be copied to 1 replica. Eventually data will be copied to 2 nodes in the case of Cassandra, while data will be copied to all nodes in the case of TimescaleDB.</p><p id=\"c3b0\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">In theory,</strong> then, Cassandra should have an advantage in insert performance since writes will be sharded across multiple nodes. On the read side, TimescaleDB should have a small advantage for very hot sets of keys (given they may be more widely replicated), but the total read throughput of the two should be theoretically comparable.</p><p id=\"ded7\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">In practice, however, we find that TimescaleDB has an advantage over Cassandra in both reads and writes, and <em class=\"markup--em markup--p-em\">it’s</em></strong><em class=\"markup--em markup--p-em\"> </em><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">large</em></strong><em class=\"markup--em markup--p-em\">.</em></p><p id=\"1c30\" class=\"graf graf--p graf-after--p\">Let’s take a look at the insert rates for each cluster:</p><figure id=\"0c24\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*D4jKlOuwYAOiPzAbbKl0PA.jpeg\" data-width=\"4586\" data-height=\"2701\" data-action=\"zoom\" data-action-value=\"1*D4jKlOuwYAOiPzAbbKl0PA.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*D4jKlOuwYAOiPzAbbKl0PA.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">5 TimescaleDB nodes outperforming 5 Cassandra nodes by 5.4x on inserts</strong></figcaption></figure><p id=\"5beb\" class=\"graf graf--p graf-after--figure\">Despite Cassandra having the theoretical advantage of sharded writes, TimescaleDB exhibits <strong class=\"markup--strong markup--p-strong\">5.4x</strong> higher write performance than Cassandra. That actually understates the performance difference. Since TimescaleDB gets no write performance gains from adding extra nodes, we really only need a 3 node TimescaleDB cluster to achieve the same availability and write performance as our Cassandra cluster, making the real TimescaleDB performance multiplier closer to <strong class=\"markup--strong markup--p-strong\">7.6x</strong>.</p><p id=\"72d4\" class=\"graf graf--p graf-after--p\">This assumes that Cassandra scales perfectly linearly, which turns out to not quite be the case in our experience. We increased our Cassandra cluster to 10 then 30 nodes while keeping TimescaleDB at a cool 5 nodes:</p><figure id=\"7d18\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*FmlUQsnrbsW1PdDV9mOTvw.jpeg\" data-width=\"1600\" data-height=\"942\" data-focus-x=\"50\" data-focus-y=\"39\" data-action=\"zoom\" data-action-value=\"1*FmlUQsnrbsW1PdDV9mOTvw.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*FmlUQsnrbsW1PdDV9mOTvw.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">30 Cassandra nodes performing 27% slower than 5 TimescaleDB nodes</strong></figcaption></figure><p id=\"2ab7\" class=\"graf graf--p graf-after--figure\"><strong class=\"markup--strong markup--p-strong\">Even a 30 node Cassandra cluster performs nearly <em class=\"markup--em markup--p-em\">27% slower</em> for inserts against a single TimescaleDB primary.</strong> With 3 TimescaleDB nodes — the maximum with TimescaleDB needed to provide the same availability as 30 node Cassandra with a Replication Factor of 2 — we now see that Cassandra needs well over <strong class=\"markup--strong markup--p-strong\">10x </strong>(probably closer to <strong class=\"markup--strong markup--p-strong\">15x</strong>) the resources as TimescaleDB to achieve similar write rates.</p><p id=\"229e\" class=\"graf graf--p graf-after--p\">For each node in these benchmarks, we paid for an Azure D8s v3 VM (<strong class=\"markup--strong markup--p-strong\">$616.85</strong>/month) as well as 4 attached 1TB SSDs (<strong class=\"markup--strong markup--p-strong\">$491.52</strong>/month). The minimum number of TimescaleDB nodes needed to achieve its write throughput and availability in the above chart is 3 (<strong class=\"markup--strong markup--p-strong\">$3,325.11</strong>/month), while the minimum number of Cassandra nodes required to achieve its highest write throughput and availability in the above chart is 30 (<strong class=\"markup--strong markup--p-strong\">$33,251.10</strong>/month). In other words, we paid <strong class=\"markup--strong markup--p-strong\">$29,925.99 </strong>more for Cassandra to get <strong class=\"markup--strong markup--p-strong\">73% </strong>as much write throughput as TimescaleDB.</p><p id=\"4b06\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Put another way, TimescaleDB exhibits higher inserts at 10% of the cost of Cassandra.</strong></p><figure id=\"599c\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*HJU1qaNPQnuYbPnHZPfReA.jpeg\" data-width=\"1600\" data-height=\"605\" data-action=\"zoom\" data-action-value=\"1*HJU1qaNPQnuYbPnHZPfReA.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*HJU1qaNPQnuYbPnHZPfReA.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">TimescaleDB: Higher inserts at 1/10 the cost of Cassandra</strong></figcaption></figure><h3 id=\"1735\" class=\"graf graf--h3 graf-after--figure\">Query Performance</h3><p id=\"0d71\" class=\"graf graf--p graf-after--h3\">Cassandra is admittedly less celebrated than SQL databases for its strength with analytical queries, but we felt it was worth diving into a few types of queries that come up frequently with time-series datasets. For all queries, we used 4 concurrent clients per node per query. We measured both the mean query times and the total read throughput in queries per second.</p><h4 id=\"037c\" class=\"graf graf--h4 graf-after--p\">Simple Rollups: TimescaleDB competitive (up to 4x faster)</h4><p id=\"de89\" class=\"graf graf--p graf-after--h4\">We’ll start with the mean query time on a few single rollup (i.e., groupby) queries on time. We ran these queries in 1000 different permutations (i.e., random time ranges and hosts).</p><figure id=\"14c2\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*sqjx0u4fUEGWuITVlJDKGQ.jpeg\" data-width=\"4489\" data-height=\"2071\" data-action=\"zoom\" data-action-value=\"1*sqjx0u4fUEGWuITVlJDKGQ.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*sqjx0u4fUEGWuITVlJDKGQ.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">LOWER query times = BETTER performance</strong></figcaption></figure><p id=\"600b\" class=\"graf graf--p graf-after--figure\">Cassandra holds its own here, but TimescaleDB is markedly better on 2 of the 3 simple rollup types and very competitive on the other. Even in simple time rollup queries where Cassandra’s clustering keys should really shine, TimescaleDB’s hypertables outperform.</p><p id=\"21b8\" class=\"graf graf--p graf-after--p\">Deducing the total read throughput of each database is fairly intuitive from the above chart, but let’s take a look at the recorded QPS of each queryset just to make sure there are no surprises:</p><figure id=\"d2f1\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*nli-uxPBy5HNnAHwNCtF9Q.jpeg\" data-width=\"4489\" data-height=\"2050\" data-action=\"zoom\" data-action-value=\"1*nli-uxPBy5HNnAHwNCtF9Q.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*nli-uxPBy5HNnAHwNCtF9Q.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">HIGHER queries per second = BETTER performance</strong></figcaption></figure><p id=\"f714\" class=\"graf graf--p graf-after--figure\">When it comes to read throughput, TimescaleDB maintains its markedly better performance here.</p><h4 id=\"0a37\" class=\"graf graf--h4 graf-after--p\">Rollups on Time and Device: TimescaleDB 10x-47x faster</h4><p id=\"e62e\" class=\"graf graf--p graf-after--h4\">Bringing multiple rollups (across both time and device) into the mix starts to make both databases sweat, but TimescaleDB has a huge advantage over Cassandra, especially when it comes to rolling up multiple metrics. Given the lengthy mean read times here, we only ran 100 for each query type.</p><figure id=\"283d\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*bTtAhCyCZ_OF3ho50r3h7w.jpeg\" data-width=\"4489\" data-height=\"1825\" data-action=\"zoom\" data-action-value=\"1*bTtAhCyCZ_OF3ho50r3h7w.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*bTtAhCyCZ_OF3ho50r3h7w.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">LOWER query times = BETTER performance</strong></figcaption></figure><p id=\"fdf0\" class=\"graf graf--p graf-after--figure\">We see a similar story for read throughput:</p><figure id=\"8b46\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*jdLVIUiLV3ZiFhbmseKWow.jpeg\" data-width=\"4489\" data-height=\"1782\" data-action=\"zoom\" data-action-value=\"1*jdLVIUiLV3ZiFhbmseKWow.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*jdLVIUiLV3ZiFhbmseKWow.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">HIGHER queries per seconds = BETTER performance</strong></figcaption></figure><h4 id=\"a4fd\" class=\"graf graf--h4 graf-after--figure\">Complex Analytical Queries: TimescaleDB 3100x-5800x faster</h4><p id=\"ac39\" class=\"graf graf--p graf-after--h4\">We also took a look at 2 slightly more complex queries that you commonly encounter in time-series analysis.</p><p id=\"d00e\" class=\"graf graf--p graf-after--p\">The first (‘<em class=\"markup--em markup--p-em\">lastpoint</em>’) is a query that retrieves the latest reading for every host in the dataset, even if you don’t a priori know <em class=\"markup--em markup--p-em\">when</em> it last communicated with the database³.</p><p id=\"14ad\" class=\"graf graf--p graf-after--p\">The second (‘<em class=\"markup--em markup--p-em\">groupby-orderby-limit</em>’) does a single rollup on time to get the MAX reading of a CPU metric on a per-minute basis for the last 5 intervals for which there are readings before a specified end time⁴.</p><p id=\"7f7e\" class=\"graf graf--p graf-after--p\">Each queryset was run 100 times.</p><figure id=\"605c\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*57Yab_EBXtmiXzEMev_-Tg.jpeg\" data-width=\"4489\" data-height=\"1825\" data-action=\"zoom\" data-action-value=\"1*57Yab_EBXtmiXzEMev_-Tg.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*57Yab_EBXtmiXzEMev_-Tg.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">LOWER query times = BETTER performance</strong></figcaption></figure><p id=\"ada1\" class=\"graf graf--p graf-after--figure\">And the read throughput:</p><figure id=\"9ed0\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*aMVMRYs2p0SXcPrpCraG-w.jpeg\" data-width=\"4489\" data-height=\"1825\" data-action=\"zoom\" data-action-value=\"1*aMVMRYs2p0SXcPrpCraG-w.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*aMVMRYs2p0SXcPrpCraG-w.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><strong class=\"markup--strong markup--figure-strong\">HIGHER queries per second = BETTER performance</strong></figcaption></figure><p id=\"7054\" class=\"graf graf--p graf-after--figure\"><strong class=\"markup--strong markup--p-strong\">For these queries, Cassandra is clearly not the right tool for the job.</strong> TimescaleDB can easily leverage hypertables to narrow the search space to a single chunk, using a per-chunk index on host and time to gather our data from there. Our multi-part primary key on Cassandra, on the other hand, provides no guarantee that all of the data in a given time range will even be on a single node. In practice, for queries like this that touch every host tag in the data set, we end up scanning most, if not all, of the nodes in a cluster and grouping on the client side.</p><h3 id=\"45bd\" class=\"graf graf--h3 graf-after--p\">Conclusion</h3><p id=\"3211\" class=\"graf graf--p graf-after--h3\"><strong class=\"markup--strong markup--p-strong\">As we see, 5 TimescaleDB nodes outperform a 30 node Cassandra cluster, with higher inserts, up to 5800x faster queries, 10% the cost, a much more flexible data model, and full SQL.</strong></p><p id=\"868b\" class=\"graf graf--p graf-after--p\">Cassandra’s turnkey write scalability comes at a steep cost. For all but the simplest rollup queries, our benchmarks show TimescaleDB with a large advantage, with average query times anywhere from 10 to 5,873 times faster for common time-series queries. While Cassandra’s clustered wide rows provide good performance for querying data for a single key, it quickly degrades for complex queries involving multiple rollups across many rows.</p><p id=\"2014\" class=\"graf graf--p graf-after--p\">Additionally, while Cassandra makes it easy to add nodes to increase write throughput, it turns out you often just don’t need to do that for TimescaleDB. With 10–15x the write throughput of Cassandra, a single TimescaleDB node with a couple of replicas for high availability is more than adequate for dealing with workloads that would require a 30+ node fleet of Cassandra instances to handle.</p><p id=\"d47f\" class=\"graf graf--p graf-after--p\">However, Cassandra’s scaling model does offer nearly limitless storage since adding more storage capacity is as simple as adding another node to the cluster. A single instance of TimescaleDB currently tops out around 50–100TB. If you need to store petabyte scale data and can’t take advantage of retention policies or rollups, then massively clustered Cassandra might be the solution for you. However, we’re actively working on a clustered version of TimescaleDB that will similarly allow users to add nodes to increase write throughput and storage capacity — so please stay tuned for more details.</p><p id=\"52f0\" class=\"graf graf--p graf-after--p\">Also, we don’t claim to be Cassandra experts. We’ve tried to be as open as we can about our data models, configurations, and methodologies so readers can raise any concerns they may have about our benchmarks and help us make them as accurate as possible.</p><p id=\"1422\" class=\"graf graf--p graf-after--p graf--trailing\">As a time-series database company we’ll always be quite interested in evaluating the performance of other solutions. Cassandra was a pleasure to work with in terms of scaling out write throughput. But attaining that at the cost of per-node performance, the vibrant PostgreSQL ecosystem, and the expressiveness of full SQL simply does not seem worth it.</p>",
        "created_at": "2018-09-07T14:43:31+0000",
        "updated_at": "2018-09-07T14:43:39+0000",
        "published_at": "2018-05-31T17:23:01+0000",
        "published_by": [
          "Lee Hampton"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 17,
        "domain_name": "blog.timescale.com",
        "preview_picture": "https://cdn-images-1.medium.com/focal/1200/632/51/23/1*r0qp_ot-pVnQP0HdC4UaRQ.jpeg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12081"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 93,
            "label": "data",
            "slug": "data"
          }
        ],
        "is_public": false,
        "id": 12078,
        "uid": null,
        "title": "A rock and a hard place: Between ScyllaDB and Cassandra",
        "url": "https://www.zdnet.com/article/a-rock-and-a-hard-place-between-scylladb-and-cassandra/",
        "content": "<p>Cassandra is a poster child of the NoSQL world. Originally an open source project sprung out of Facebook, it has been adopted by the Apache Foundation and backed by an enterprise, DataStax, that also offers DataStax Enterprise based on Cassandra. Cassandra is among the top 10 database solutions <a href=\"https://db-engines.com/en/ranking_trend\" target=\"_blank\" rel=\"noopener noreferrer\">according to DB-Engines</a>.</p><p>That is precisely why it now has a potentially dangerous rival in ScyllaDB. ScyllaDB is a new kid on the NoSQL block aiming to offer a solution that is open source and API-compatible with Cassandra, but performs much better. The goal is to be a drop-in replacement for Cassandra, and when we're talking about database #8 in the world, that's kind of a big deal.</p><h3>I, Cloudius</h3><p>Dor Laor and Avi Kivity did not set out with this grandiose plan back in 2013. It was not for lack of ambition, but this just was not their thing. They both have backgrounds in hypervisors and were <a href=\"http://www.eweek.com/cloud/how-did-kvm-virtualization-get-into-the-linux-kernel\" target=\"_blank\" rel=\"noopener noreferrer\">part of the team that built KVM and got acquired by Red Hat</a>. Leaving Red Hat, their initial plan was to write a unikernel that would displace Linux from cloud servers. So no lack of ambition there.</p><p>They founded a startup called Cloudius, found investors, assembled a team and started working hard. At some point however they realized that their potential would not be reached for a number of reasons, and decided to pivot. And pivot they did, to add another NoSQL database to the never ending list, one that would be able to do what Cassandra does and then some.</p><p>But why go for a NoSQL database, and why target Cassandra?</p><figure class=\"image  image-full-width shortcode-image\"><img src=\"https://zdnet1.cbsistatic.com/hub/i/r/2017/10/24/05b22e21-a725-4cf2-a42b-19683720c1ce/resize/770xauto/ed15875f35d515666d1a6365839b9a2c/scyllalogo2000px.png\" class=\"\" alt=\"scyllalogo2000px.png\" height=\"auto\" width=\"770\" /><figcaption>ScyllaDB did not start as a database at all, but having pivoted to one, it may prove a force to be reckoned with. Image: ScyllaDB.</figcaption></figure>\n<p>Part of Cloudius mission was to speed up server loads, with an emphasis on databases. Laor, ScyllaDB CEO, says that they had managed to boost Redis performance by 70 percent without actually doing anything Redis-specific. You may wonder how was that possible, and there is an answer, but for now let's stick to the fact that this triggered them to take that direction.</p><p>It was a combination of market and technical reasons that made Cloudius target Cassandra. Laor says Hadoop was in their list as well, but since that had already been done they decided to go for rewriting Cassandra: \"The world does not need another database format. Cassandra's format is good, and it is successful. Cassandra is the best high availability platform out there.\"</p>\n    <section class=\"sharethrough-top\" data-component=\"medusaContentRecommendation\" data-medusa-content-recommendation-options=\"{&quot;promo&quot;:&quot;promo_ZD_recommendation_sharethrough_top_in_article_desktop&quot;,&quot;spot&quot;:&quot;dfp-in-article&quot;}\">\n    </section><p>They say imitation is the sincerest form of flattery, and it's obvious from this that the ScyllaDB team found Cassandra worth imitating. But it's more complicated than that: \"Cassandra is everywhere in critical workloads. But when we targeted it for optimization, we ran against limitations tied to its JVM nature. In the end, Cassandra ends up competing with itself.</p><p>At that time, <a href=\"https://cloudplatform.googleblog.com/2014/03/cassandra-hits-one-million-writes-per-second-on-google-compute-engine.html\" target=\"_blank\" rel=\"noopener noreferrer\">Google had just published a benchmark detailing how they managed to get 1 million transactions on top of Cassandra</a> in their cloud using 300 virtual machines. This piqued our interest, and focusing our work on Cassandra we managed to get a record 1.6 million transactions on one virtual machine. This is how we got started.\"</p><h3>Enter ScyllaDB</h3><p>Cloudius pivoted and rebranded, but kept the same team and investors. Thus ScyllaDB was born. You may think it's cheeky to target \"the best high availability platform out there\" and aim to do better, but Laor says they are hoping to see history repeating. And the entirety of that quote, \"imitation is the sincerest form of flattery that mediocrity can pay to greatness,\" may not necessarily apply here.</p><p>\"When we entered the market with KVM, all the players were established -- VMWare, HyperV, Xen. We showed up last, but based on Avi's revolutionary design KVM now dominates. We think our differentiation this time around is even bigger,\" says Laor.</p><p>So what is this differentiation? ScyllaDB promises something simple, alluring, and hard to believe: keep your codebase, replace Cassandra with ScyllaDB, get up to 10 times boost in performance. There are benchmarks and references to back those claims, but how can this possibly work? It comes down to a number of things.</p><figure class=\"image  image-full-width shortcode-image\"><img src=\"https://zdnet1.cbsistatic.com/hub/i/r/2017/10/24/965711a0-fe02-4bf3-833e-6f02393defcc/resize/770xauto/71f840138022af60b4c2268d1481950b/scylladbthemes.png\" class=\"\" alt=\"scylladbthemes.png\" height=\"auto\" width=\"770\" /><figcaption>ScyllaDB has been focused on stability, performance and compatibility. Today the announcement of version 2.0 signifies a new phase. Image: ScyllaDB</figcaption></figure><p>First, different implementation language. ScyllaDB has been rewritten from scratch in C++, as opposed to Cassandra's Java-based codebase. The JVM adds an intermediate layer between source code and hardware, trading portability and ease of use for performance. JVMs have come a long way, but the proper use of a language closer to the low-level fundamentals may result in better performance.</p><p>But that's only part of ScyllaDB's secret sauce. An equally big part has to do with those underlying fundamentals, such as memory or socket allocation. The kind of nitty gritty details that are hard to get, program, and maintain, but can result in dramatic improvements. The kind of thing that you get to know intimately if you program, say, a hypervisor.</p><p>All those lessons learned through years of low level programming have been distilled in <a href=\"https://github.com/scylladb/seastar\" target=\"_blank\" rel=\"noopener noreferrer\">SeaStar</a>. SeaStar is an open source framework for high performance applications that ScyllaDB is built on, although there is nothing database-specific about it. SeaStar is event-driven and enables writing efficient non-blocking, asynchronous code. </p><p>The tradeoff? Complexity. Laor admits it's hard to program on top of SeaStar, but says the result is worth the effort. He mentions for example <a href=\"https://github.com/fastio/pedis\" target=\"_blank\" rel=\"noopener noreferrer\">Pedis</a>, a rewrite of Redis based on SeaStar done by Alibaba, which turbo-charges Redis. Besides, ScyllaDB promises, the average Cassandra user does not need to worry about that. </p><p>ScyllaDB aims to ease the complex task of configuring and tuning Cassandra deployments by offering auto-tuning capabilities. ScyllaDB has added improvements in both node management and network protocols with the goal of having clusters running optimally without requiring administrator intervention.</p><p>Laor compared this feature to <a href=\"https://www.zdnet.com/article/oracle-preps-autonomous-database-at-openworld-aims-to-cut-labor-admin-time/\" target=\"_blank\">Oracle's self tuning database</a>. There are however similar solutions for other platforms too, <a href=\"https://www.zdnet.com/article/spark-gets-automation-analyzing-code-and-tuning-clusters-in-production/\" target=\"_blank\">such as Spark</a>. For Spark, some approaches are based on using machine learning on datasets gathered from many operational clusters, some others on rules.</p><p>ScyllaDB has adopted the rule-based approach, as Laor does not believe datasets may be representative of all possible configurations. \"We use developer intelligence, not artificial intelligence,\" he says. Arguably, datasets from operational Cassandra clusters would be hard to come by for ScyllaDB anyway. Which brings us to an interesting point.</p><h3>A rock and a hard place</h3><p>On the one hand, the decision to build a new platform that is compatible with an existing one reduces friction and lowers the adoption barrier for organizations. ScyllaDB already has names such as Samsung, IBM, and Outbrain among its early <a href=\"http://www.scylladb.com/users/\" target=\"_blank\" rel=\"noopener noreferrer\">adopters using it in production</a>.</p><p>On the other hand, it induces friction with the platform the newcomer aims to displace: Cassandra. We've seen <a href=\"https://www.zdnet.com/article/towards-a-unifying-data-theory-and-practice-combining-operations-analytics-and-streaming/\" target=\"_blank\">similar examples in the Spark world</a>, but the difference is that Spark alternatives are still largely based on Spark so there can be cross-pollination and eventually perhaps convergence.</p><p>Here we're talking about a radical departure -- different implementation language, different low-level infrastructure, different network protocols. There really is no room for Cassandra and ScyllaDB to play side by side, as amply exemplified by the fact they cannot even coexist in a cluster.</p><figure class=\"image  image-full-width shortcode-image\"><img src=\"https://zdnet3.cbsistatic.com/hub/i/r/2017/10/24/8143b608-7510-4fb3-8c1f-ea35a654729e/resize/770xauto/3c34674788631b734ae6d73d4439dd21/scylladbbenchmark.png\" class=\"\" alt=\"scylladbbenchmark.png\" height=\"auto\" width=\"770\" /><figcaption>One of ScyllaDB's benchmarks, in which it is shown to outperform Cassandra. Image: ScyllaDB</figcaption></figure><p>Typically, Laor says, people set up a proof of concept ScyllaDB cluster working side by side with Cassandra until they feel confident enough to make the switch. \"We have different protocols. We considered supporting Cassandra protocols, but there are so many versions out there we decided against it. Plus, when things go wrong in a mixed cluster, whom will you blame?\"</p><p>Could that hurt adoption? \"We are not married to our databases, that's what people tell us,\" says Laor. \"It's a big investment, but they can change. Choosing Cassandra was a strategic decision for us. We started from scratch and rewrote everything. When you do that, you create antagonism. It touches many people, it's sensitive.</p><p>But the results speak for themselves. For example, an AdTech client of ours has managed to go from 100,000 timeouts per second with Cassandra to 100 per second with ScyllaDB. We have not been doing much in terms of collaboration, mostly because at the moment we are heads-down working on feature parity. But like KVM and Xen, where we had common interfaces, there may be potential for collaboration.\"</p><p>Laor mentions some areas in which they are contributing to the Cassandra community, such as ScyllaDB CTO presenting design choices at Cassandra next generation conference or contributing a driver for Go. He also emphasizes that ScyllaDB is an open source project and they try to document and disseminate design decisions and implementation and says they would like to work with Cassandra on certain features in the future.</p><p>ScyllaDB is a newcomer, but on paper at least it looks like it's got what it takes to displace a heavyweight such as Cassandra with DataStax's enterprise backing. The team has been there and done it before, feature parity is almost there, financials and organizational structure seem to be there as well.</p><p>ScyllaDB is well funded, with a total of $25 million, and has a team of 45 (mostly engineers) working together for years. On the technical front, it seems like ScyllaDB can give Cassandra a run for its money. But what does that \"hostile takeover\" mean for Cassandra, DataStax and the community? Will ScyllaDB be able to win hearts and minds?</p><p>It seems the Cassandra community is currently in somewhat of a turmoil anyway. There has been some <a href=\"https://www.theregister.co.uk/2016/11/14/datastax_versus_asf_staxeit/\" target=\"_blank\" rel=\"noopener noreferrer\">friction between DataStax and the Apache Foundation</a>, resulting in uncertainty about the project's future and direction. So to be a Cassandra user today may mean you are between a rock and a hard place.</p><figure class=\"image  image-full-width shortcode-image\"><img src=\"https://zdnet3.cbsistatic.com/hub/i/r/2017/10/24/8541e0a0-26dc-43b4-b9cc-16d10e67a501/resize/770xauto/b7ebbe39ddf7e4bbd0ea67a678f15a8d/scylladbcontributors.png\" class=\"\" alt=\"scylladbcontributors.png\" height=\"auto\" width=\"770\" /><figcaption>ScyllaDB plus SeaStar contributors are about as many as Cassandra contributors at this point, per ScyllaDB's accounts. Image: ScyllaDB</figcaption></figure><p>DataStax on its part did not reply to a request for comment. ScyllaDB on the other hand says their community is growing, despite the fact that the entry barrier is high due to the complex nature of their implementation, and that they have practically achieved feature parity.</p><p>ScyllaDB 2.0 is being announced today at <a href=\"http://www.scylladb.com/scylla-summit-2017/\" target=\"_blank\" rel=\"noopener noreferrer\">Scylla Summit</a>, bringing some highly sought after features such as counters and materialized views. According to Laor, full feature parity will be achieved in early 2018. Add to the mix the <a href=\"http://www.scylladb.com/press-release/scylladb-acquires-seastar-io-database-service-technology/\" target=\"_blank\" rel=\"noopener noreferrer\">recent acquisition of Seastar.io</a>, which will act as a catalyst for ScyllaDB to offer a managed cloud version, and you see why ScyllaDB is a name you may be hearing more in the future.</p><p>Speaking of names, what's with ScyllaDB's name anyway? Apparently its founders wanted to use a name from Greek mythology, as <a href=\"https://www.quora.com/Where-does-Apache-Cassandras-name-come-from#\" target=\"_blank\" rel=\"noopener noreferrer\">was the case for Cassandra</a>. According to them in some parts of the world \"Scylla\" is pronounced \"scale-ah,\" which alludes to scalability, and thus a name was born.</p><p>Ironically, Cassandra was an Oracle nobody would listen to. Scylla and Charybdis were <a href=\"https://history.howstuffworks.com/history-vs-myth/10-expressions-from-ancient-world4.htm\" target=\"_blank\" rel=\"noopener noreferrer\">a monster and a whirlpool guarding the strait of Messina</a>, making it impossible to navigate past them. To be between Scylla and Charybdis is to be between a rock and a hard place. But to be between ScyllaDB and Cassandra may turn out to be a good thing for the community, should it eventually steer clear of the antagonism.</p>",
        "created_at": "2018-09-06T20:56:18+0000",
        "updated_at": "2018-09-06T20:56:35+0000",
        "published_at": "2017-10-24T15:07:43+0000",
        "published_by": [
          "George Anadiotis"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 9,
        "domain_name": "www.zdnet.com",
        "preview_picture": "https://zdnet2.cbsistatic.com/hub/i/r/2017/10/24/55c43b25-b204-4c6d-b2f9-b47b149dc71b/thumbnail/770x578/f2bd628a85e063c4fc302708cd4dde80/ancient-expression-5.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12078"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 93,
            "label": "data",
            "slug": "data"
          }
        ],
        "is_public": false,
        "id": 12076,
        "uid": null,
        "title": "Cassandra Hits One Million Writes Per Second on Google Compute Engine",
        "url": "https://cloudplatform.googleblog.com/2014/03/cassandra-hits-one-million-writes-per-second-on-google-compute-engine.html",
        "content": "<div class=\"post-content\" itemprop=\"articleBody\">\n\n<noscript><div>\nGoogle is <a href=\"http://research.google.com/pubs/pub35290.html\">known for creating scalable high performance systems</a>. In a recent blog post, we demonstrated how Google Cloud Platform can rapidly provision and scale networking load to handle <a href=\"http://googlecloudplatform.blogspot.com/2013/11/compute-engine-load-balancing-hits-1-million-requests-per-second.html\">one million requests per second</a>. A fast front end without a fast backend has limited use, so we decided to demonstrate a backend serving infrastructure that could handle the same load. We looked at popular open source building blocks for cloud applications and choose <a href=\"http://cassandra.apache.org/\">Cassandra</a>, a NoSQL database designed for scale and simplicity.<p>Using 330 <a href=\"https://cloud.google.com/products/compute-engine\">Google Compute Engin</a>e virtual machines, 300 1TB Persistent Disk volumes, Debian Linux, and Datastax Cassandra 2.2, we were able to construct a setup that can:<br /></p><ul><li>sustain one million writes per second to Cassandra with a median latency of 10.3 ms and 95% completing under 23 ms</li>\n<li>sustain a loss of ⅓ of the instances and volumes and still maintain the 1 million writes per second (though with higher latency)</li>\n<li>scale up and down linearly so that the configuration described can be used to create a cost effective solution</li>\n<li>go from nothing in existence to a fully configured and deployed instances hitting 1 million writes per second took just 70 minutes. A configured environment can achieve the same throughput in 20 minutes.</li>\n</ul><ul>\n        <a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/dml/dml_config_consistency_c.html\">Cassandra Quorum commit</a>                                                      \n                                            \n<a href=\"https://3.bp.blogspot.com/-X7wn0kCzCx4/UypsTp0ms3I/AAAAAAAAAcQ/BymsEROARgs/s3200/diagram.png\"><img border=\"0\" height=\"396\" src=\"https://3.bp.blogspot.com/-X7wn0kCzCx4/UypsTp0ms3I/AAAAAAAAAcQ/BymsEROARgs/s3200/diagram.png\" width=\"640\" alt=\"image\" /></a></ul></div>\n<br /><div>\nYou can find the instructions on how to reproduce the results by following the </div><a href=\"https://gist.github.com/ivansmf/6ec2197b69d1b7b26153\">setup instructions</a><div>.</div><p><b>Results</b><br />\nWith 15,000 concurrent clients Cassandra was able to maintain 10.5ms median latency (8.3ms with 12,000 clients), and 95th latency percentile at 23ms. Here is how the solution scales as the number of concurrent clients grows:<br /><img alt=\"Cassandra Blog Post - latencies and throughput (3).png\" height=\"300px;\" src=\"https://lh5.googleusercontent.com/OmMHhyxMJDH_ye7RbhDuCthdymaBS4QgUFASVNSPVD6kXKc3mxGAOHqUdN28BGs0cJDtFEkYcZLfRHNlVEBEVDShDtpTloZISjBqrVVpc9Ta7VdLskrPQliHCftBFw\" width=\"624px;\" /><br />\nBelow we show a graph of the throughput versus 95th percentile latency which quickly achieves very good response times after Cassandra initializes its internal state, and Java warms up its heap and memory mapped files table. This test was run longer than the minimal time required to hit over 1M writes per second in order to show the sustained throughput:<br /><img alt=\"Cassandra Blog Post - Latency and Throughput superimposed.png\" height=\"301px;\" src=\"https://lh4.googleusercontent.com/unIDk9_mA2E9IIdcEqVgZoNvkJ_SBeoirC8ntNe7DCFWBOTQ0N0r-ASV2qcumLL6KYGBXwa1lUfxcnhMlooYRL18r_UUvOYYgT7BbwVJWuyRIPFaWmOZpXYoYK5TMg\" width=\"624px;\" /><br />\nIn addition to looking at top end performance we also looked at resiliency. We removed ⅓ of the cluster nodes and it remained functional and serving more than 1M writes per second. Median latency held at 13.5ms, 95th percentile at 61.8ms, and 994.9th percentile at 1,333.5ms. We consider those numbers very good for a cluster in distress, proving Compute Engine and Cassandra can handle both spiky workloads and failures.</p><p><b>Conclusion</b><br />\nTuning the workload costs $5 per hour (on a 3 node cluster), and the minimal test required to hit one million writes per second takes 1 hour and 10 minutes at a cost of $330 USD when run in March 2014. </p><p>Putting it all together, this means the Google Cloud Platform was able to sustain <b>one million Cassandra writes per second at a cost of $0.07 USD per million writes</b>.</p><p>-Posted by Ivan Santa Maria Filho, Performance Engineering Lead\n\n</p></noscript>\n</div>",
        "created_at": "2018-09-06T20:18:21+0000",
        "updated_at": "2018-09-06T20:19:04+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "cloudplatform.googleblog.com",
        "preview_picture": "http://3.bp.blogspot.com/-X7wn0kCzCx4/UypsTp0ms3I/AAAAAAAAAcQ/BymsEROARgs/s3200/diagram.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12076"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 44,
            "label": "json",
            "slug": "json"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1280,
            "label": "cql",
            "slug": "cql"
          }
        ],
        "is_public": false,
        "id": 12073,
        "uid": null,
        "title": "Storing Nested Objects in Cassandra with Composite Columns",
        "url": "https://pkghosh.wordpress.com/2013/07/14/storing-nested-objects-in-cassandra-composite_columns/",
        "content": "<div class=\"pd-rating\" id=\"pd_rating_holder_3023117_post_1013\"><br /><p>One of the popular features of <em>MongoDB</em> is the ability to store arbitrarily nested objects and be able to index on any nested field. In this post I will show how to store nested objects in Cassandra using composite columns. I have recently added this feature to my open source Cassandra project <a href=\"https://github.com/pranab/agiato\" target=\"_blank\"><em>agiato</em></a>. Since in Cassandra, like many other NOSQL databases, stored data is highly denormalized. The denormalized data  often manifests itself in the form of a nested object e.g., denormalizing one to many relations.</p>\n<p>In the  solution presented here, the object data is stored in column families with composite columns. An object will typically have some statically defined fields. Rest of the fields will be dynamic. It is possible to define the table or column family behind the object in CQL3, based on the statically known fields of the object.</p>\n<h2>Composite Column</h2>\n<p>Super columns are frowned upon in Cassandra. Composite columns are the preferred approach. <a href=\"http://www.datastax.com/dev/blog/schema-in-cassandra-1-1\" target=\"_blank\">Composite columns</a> can do what super columns could do and more. The underlying data model in a super column is a 3 level nested map(including row key, super column name or key and column name or key). Composite columns are more flexible, allowing you to create a data model with multi level nested map.</p>\n<p>Here is an example of data model based on composite columns. Consider a building HVAC system with the sensor data  stored in a column family with composite columns. Here are the fields.</p>\n<table><tbody><tr><td>day</td>\n<td>day as integer</td>\n<td>20130421</td>\n</tr><tr><td>city</td>\n<td>city name</td>\n<td>Los Angeles</td>\n</tr><tr><td>buildingID</td>\n<td>An integer ID for a building</td>\n<td>2658</td>\n</tr><tr><td>sensorID</td>\n<td>An integer ID for a sensor in a building</td>\n<td>295016</td>\n</tr><tr><td>time</td>\n<td>timestamp as long</td>\n<td>1361237156</td>\n</tr><tr><td>temp</td>\n<td>temarature as double</td>\n<td>71.2</td>\n</tr><tr><td>humidity</td>\n<td>relative humidity as double</td>\n<td>30.7</td>\n</tr></tbody></table><p>A record is uniquely identified with the tuple <em>(day, city, buildingID, sensorID, time)</em>. The corresponding column family definition with composite key will be as below</p>\n<pre>create column family buildingControl \nwith key_validation_class = 'IntegerType'\nand comparator = 'CompositeType(UTF8Type, IntegerType, IntegerType, LongType, UTF8Type)'\nand default_validation_class='DoubleType';</pre>\n<p>The composite column along with row key provide a 6 level nested map. The composite column has 5 components. The first 4 correspond to the value of the  tuple <em>(city, buildingID, sensorID, time)</em>.  The last element is the name of a non primary column. The storage layout of a record will look as below</p>\n<table><tbody><tr><td rowspan=\"2\">20130421</td>\n<td>Los Angeles:2658:295016:1361237156:temp</td>\n<td>Los Angeles:2658:295016:1361237156:humidity</td>\n</tr><tr><td>71.2</td>\n<td>30.7</td>\n</tr></tbody></table><p>The first 4 fields of the composite column name are the values of <em>(city, buildingID, sensorID, time) </em>values respectively, which are the clustering keys. The last component is the column name for a non primary key column. The value of the composite column is the corresponding non primary key column value.</p>\n<p>The composite column name is shown in a human readable form. Internally, the composite column name is stored with some byte encoding logic. As a nested Map,  a record, in this example can be represented as <em>Map&lt;Integer,Map&lt;String, Map&lt;Integer, Map&lt;Integer, Map&lt;Long, Map&gt;&gt;&gt;&gt;&gt; . </em>Strictly speaking all the inner maps are sorted maps. As we will see later, in terms of CQL, the outer map corresponds to the row key and all the inner maps except for the last one correspond to the clustering key.</p>\n<p>A record comprises of a set of columns. The set of columns is repeated in the row for different unique values of <em>(city, buildingID, sensorID, time)</em>. Essentially, a row will contain multiple records. Before composite key was introduced in Cassandra, people had resorted to using this pattern for multi level map by doing string concatenation for column values and names.</p>\n<h2>Primary Key Syntactic Sugar</h2>\n<p>In thrift, the interface to Cassandra is tightly coupled to the internal storage structure.  CQL introduces a layer of abstraction between the underlying storage structure and the logical model and makes the the data model  look like a familiar RDBMS table. The entity in our example has 7 attributes and as shown below, all 7 of them appear as columns in the CQL  table definition.</p>\n<pre>CREATE TABLE buildingControl (\n\tday int,\n\tcity text,\n\tbuildingID int,\n\tsensorID int,\n\ttime timestamp,\n\ttemp double,\n\thumidity double\n\tPRIMARY KEY (day, city, buildingID, sensorID, time)\n);</pre>\n<p>The primary key definition maps to the storage structure as follows. The first element <em>day</em> is the row key or the partitioning key. The remaining 4 elements constitute the clustering key. Internally, a record is stored with  2 physical columns, as we saw in the thrift data model definition. The 7 logical columns map to one row key and two composite columns.</p>\n<h2>Into the Object Land</h2>\n<p>So far in out example, data model had a flat record structure and we were able to define everything nicely with CQL.  What if we have multiple related entities in our use case and we wanted to denormalize the data model. Here is an example of an order object in an eCommerce application, represented with JSON.</p>\n<pre>{\n\t\"custID\" : '12736467',\n\t\"date\" : '2013-06-10',\n\t\"orderID\" : 19482065,\n\t\"amount\" : 216.28,\n\t\"status\" : 'picked',\n\t\"notes\" : 'on time',\n\t\"items\" : [\n\t\t{\n\t\t\t\"sku\" : 87482734,\n\t\t\t\"quantity\" : 4\n\t\t},\n\t\t{\n\t\t\t\"sku\" : 32851042,\n\t\t\t\"quantity\" : 2\n\t\t}\n\t],\n\t\"customer\" : {\n\t\t\"name\" : 'Joe Smith',\n\t\t\"tel\" : '231 456 7890',\n\t\t\"email\" : 'joe@yahoo.com'\n\t}\n}</pre>\n<p>As you can see the the order object contains a child customer object and a list of order line item objects. The embedded customer object is statically defined and we could include it’s attributes  as columns in CQL table definition. However, the embedded list of order line item objects can not be represented in CQL. Because, CQL can handle only list and map of primitives.</p>\n<p>Cassandra is a <em>schema optional</em> data base and you can have any of the following with respect to schema</p>\n<div>\n<ol><li><em>Completely static</em></li>\n<li><em>Completely dynamic</em></li>\n<li><em>Semi static</em></li>\n</ol></div>\n<p>The schema for our example falls into the third category, because we can not define all the fields in the schema.</p>\n<h2>Linearizing Object Hieararchy</h2>\n<p>If we a traverse an arbitrarily nested object, the leaf nodes will contain field name, primitive value pair.  To linearize an object, after the traversing the object hierarchy, a list of leaf node objects is generated. Each such node is saved as a composite column. For nested fields, the column name is generated by concatenating all fields names in the path from the leaf node to the root, separated by period.</p>\n<p>A CQL table along with primary key could be defined comprising of only the statically defined primitive fields of an object.  Here is the partial definition for the order object</p>\n<pre>CREATE TABLE orders (\n    custID text,\n    date text,\n    orderID int,\n    amount double,\n    status text,\n    notes text,\n    PRIMARY KEY (custID,date,orderID)\n);</pre>\n<p>The only advantage of creating the CQL schema, is that you could run CQL select query. However the query will only return the columns defined in the CQL schema, but not the dynamic attributes of the object.</p>\n<h2>Saving the Object</h2>\n<p>The object to be saved in passed to the agiato API as a <em>SimpleDynaBean</em> which is simplified implementation  of apache common <a href=\"http://commons.apache.org/proper/commons-beanutils/javadocs/v1.8.3/apidocs/org/apache/commons/beanutils/DynaBean.html\" target=\"_blank\"> <em>DynaBean</em></a> interface. With <em>SimpleDynaBean</em> you could define an arbitrarily nested object. The API also gets passed the  primary key definition.   The different options for object representation are as follows.</p>\n<div>\n<ol><li><em>SimpleDynaBean object</em></li>\n<li><em>JSON string</em></li>\n<li><em>Java bean object</em></li>\n</ol></div>\n<p>The following code snippet uses <em>SimpleDynaBean</em>  and saves the order object in the column family <em>orders</em>.</p>\n<p><em>SimpleDynaBean obj = new  <em>SimpleDynaBean</em>();</em><br /><em> //populate hierarchical object</em><br /><em> ……</em><br /><em> AgiatoContext.initialize(“/home/pranab/Projects/bin/agiato/test.json”);</em><br /><em> ColumnFamilyWriter writer = AgiatoContext.createWriter(“orders”);</em><br /><em>PrimaryKey primKey = new PrimaryKey(“custID”, “date”, “orderID”);<br /><em> writer.writeObject(obj, primKey, ConsistencyLevel.ONE);</em></em></p>\n<p>The object  is traversed in a depth first way and a list leaf nodes is generated. The leaf nodes will include the columns defined in CQL as well as all the dynamic fields.</p>\n<p>The  arguments  in the <em>PrimaryKey</em>  constructor are the primary key column names. Here only the first element constitutes the row key. If that was not the case  you had to call <em>setRowKeyElementCount()</em> to specify how many fields out of the primary key elements constitute  the row key.</p>\n<p>If after the depth first traversal, the primary key fields  appear in the beginning of the list,  you could call another<em> PrimaryKey</em> constructor and pass the number of primary key elements.</p>\n<p>The class <em><a href=\"https://github.com/pranab/agiato/blob/master/src/main/java/agiato/cassandra/data/ObjectSerDes.java\" target=\"_blank\">ObjectSerDes</a></em> hadles object traversal and mapping object fields to composite key columns. Here is the <em><a href=\"https://github.com/pranab/agiato/blob/master/src/main/java/agiato/cassandra/data/DataAccess.java\" target=\"_blank\">DataAccess</a></em> class for Cassandra data access through thrift. From the list of leaf nodes, the row key and the clustering key are identified.  After traversing an object, the flattened list of <i>NamedObject </i> is created as shown below</p>\n<table><tbody><tr><td>row key ObjectNode</td>\n<td colspan=\"2\">clustering key ObjectNodes</td>\n<td colspan=\"3\">other ObjectNodes</td>\n</tr></tbody></table><p>The first <em>NamedObject</em> value is converted to a row key. The clustering key <em>NamedObject</em> values makeup the prefix part of the composite key column name.  Then the columns are saved in a way that has the same effect as running a CQL insert query, except that it will store all the dynamic fields of the object, which were not part of the CQL definition.</p>\n<p>Field values are serialized based on introspection of the field data. There is no check made against table meta data. For the fields defined in CQL schema, if data with wrong type is passed in the object,  be prepared to get surprising results from CQL query.</p>\n<h2>Update and Delete</h2>\n<p>For update and delete, it’s necessary to pass a partially formed object, whether a dynamic object or JSON string, to the API, containing the primary key fields and other fields that are going to be updated or deleted. For example, to update the status of an order, it’s necessary for the order object to have the primary key fields and the <em>status</em> field.</p>\n<p>With  statically compiled java bean object,  it’s a difficult to define a  partially formed object. One way of doing it is to have all the fields as primitive wrapper objects, where null will imply absence of a field value.</p>\n<h2>Querying the Object</h2>\n<p>I don’t have the query method to return fully populated object implemented yet. I will be adding it soon. In the mean time we can use CQL and CLI. If we run a CQl query this is what we get. As expected it return only the columns it knows about from the table definition. It does not return any of the dynamic columns.</p>\n<pre> custid   | date       | orderid  | amount | notes   | status\n----------+------------+----------+--------+---------+--------\n 12736467 | 2013-06-10 | 19482065 | 216.28 | in time | picked</pre>\n<p>However, when we run a CLI query it returns every field that is physically stored in Cassandra as below. All the values are in bytes in the output</p>\n<pre>RowKey: 3132373336343637\n=&gt; (name=2013-06-10:19482065:amount, value=406b08f5c28f5c29, timestamp=13..)\n=&gt; (name=2013-06-10:19482065:customer.email, value=6a6f65407961686f6f2e636f6d, timestamp=13..)\n=&gt; (name=2013-06-10:19482065:customer.name, value=4a6f6520536d697468, timestamp=13..)\n=&gt; (name=2013-06-10:19482065:customer.tel, value=323331203435362037383930, timestamp=13..)\n=&gt; (name=2013-06-10:19482065:items.[0].quantity, value=00000004, timestamp=13..)\n=&gt; (name=2013-06-10:19482065:items.[0].sku, value=000000000536e16e, timestamp=13..)\n=&gt; (name=2013-06-10:19482065:items.[1].quantity, value=00000002, timestamp=13..)\n=&gt; (name=2013-06-10:19482065:items.[1].sku, value=0000000001f54462, timestamp=13..)\n=&gt; (name=2013-06-10:19482065:notes, value=696e2074696d65, timestamp=13..)\n=&gt; (name=2013-06-10:19482065:status, value=7069636b6564, timestamp=13..)</pre>\n<h2>Why not Just Serialize</h2>\n<p>You might ask why bother with mapping all the nested fields of an object to columns. Why not serialize all the dynamic fields of the object and store it as a JSON string and include that column as a text filed in CQL create table.  Granted, the column will be visible as a JSON string when you run CQl select query. If you are leaning in that direction, consider the following scenario</p>\n<div>\n<ul><li><em>You want to create secondary index on some nested field e.g., customer.zipCode. The field needs an independent existence as a column </em></li>\n<li><em>You want to treat such nested field  it as dimension in for aggregation and analytic queries involving nested objects</em></li>\n</ul></div>\n<p>The other advantage of having separate column for each nested field, is that CLI query results are more readable.</p>\n<h2>Next Steps</h2>\n<p>My first goal is to support object based query. A prototype object will be passed to agiato. The fields defining the query criteria will need to have the values defined in this object. These fields will be a subset of the primary key fields.</p>\n<p>The API after running the query will return one or more fully populated object. Other fields need not be passed as part of the query object. However, if some prototype value for other fields is passed, then the API will return typed value for the fields, otherwise it will return byte arrays for values.</p>\n<p>My next goal is to support aggregate and analytic queries. The user will define cube schema consisting dimensions and measures pointing to object attributes, including nested attributes. While storing an object aggregate values will be computed and stored in separate column families. Storm will be part of the aggregation engine ecosystem.</p>\n<p>For commercial support for this solution or other solutions in my <a href=\"https://github.com/pranab\" target=\"_blank\" rel=\"noopener\">github repositories</a>, please talk to <a href=\"https://thirdeyedata.io/data-science-services-pkghosh/\" target=\"_blank\" rel=\"noopener\">ThirdEye Data Science Services</a>. Support is available for <em>Hadoop</em> or Spark deployment on cloud including installation, configuration and testing,</p>\n<div id=\"atatags-370373-5b9144ebb5924\">\n        \n    </div>\t\t<div class=\"wpcnt\">\n\t\t\t<div class=\"wpa wpmrec\">\n\t\t\t\tAdvertisements\n\t\t\t\t<div class=\"u\">\t\t<div>\n\t\t<div id=\"atatags-26942-5b9144ebb594e\">\n\t\t\t\n\t\t</div></div>\t\t<div>\n\t\t<div id=\"atatags-114160-5b9144ebb5950\">\n\t\t\t\n\t\t</div></div></div>\n\t\t\t\t\n\t\t\t</div>\n\t\t</div>\t\t\t\t\t\t\t\t\t\t\t</div>",
        "created_at": "2018-09-06T15:16:59+0000",
        "updated_at": "2018-09-06T15:17:06+0000",
        "published_at": "2013-07-15T04:07:56+0000",
        "published_by": [
          "Pranab"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 11,
        "domain_name": "pkghosh.wordpress.com",
        "preview_picture": "https://s0.wp.com/i/blank.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12073"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 44,
            "label": "json",
            "slug": "json"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1280,
            "label": "cql",
            "slug": "cql"
          }
        ],
        "is_public": false,
        "id": 12072,
        "uid": null,
        "title": "Inserting JSON data into a table",
        "url": "https://docs.datastax.com/en/cql/3.3/cql/cql_using/useInsertJSON.html",
        "content": "<p class=\"shortdesc\">Inserting JSON data with the INSERT command for testing queries.</p><section class=\"section context\"><p class=\"p\">In a production database, inserting columns and column values programmatically is\n                more practical than using cqlsh, but often, testing queries using this SQL-like\n                shell is very convenient. For JSON data all values will be inserted as a string if\n                they are not a number, but will be stored using the column data type. For example,\n                the id below is inserted as a string, but is stored as a\n                    UUID. For more information, see <a class=\"xref\" href=\"https://www.datastax.com/dev/blog/whats-new-in-cassandra-2-2-json-support\" target=\"_blank\">What's New in Cassandra 2.2: JSON\n                Support</a>.</p></section><ul class=\"ul steps-unordered\" id=\"useInsertJSON__steps-unordered_qfb_nvs_sr\"><li class=\"li step stepexpand\">\n                To insert JSON data, add <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">JSON</code> to the  <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph language-cql\">INSERT</code> command.. Note the absence of the\n                    keyword <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph language-cql\">VALUES</code> and the list of\n                    columns that is present in other <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph language-cql\">INSERT</code> commands.\n                <div class=\"itemgroup stepxmp\"><pre>cqlsh&gt; INSERT INTO cycling.cyclist_category JSON '{\n  \"category\" : \"GC\", \n  \"points\" : 780, \n  \"id\" : \"829aa84a-4bba-411f-a4fb-38167a987cda\",\n  \"lastname\" : \"SUTHERLAND\" }';\n  </pre></div>\n            </li><li class=\"li step stepexpand\">\n                A null value will be entered if a defined column like\n                        lastname, is not inserted into a table using JSON\n                    format.\n                <div class=\"itemgroup stepxmp\"><pre>cqlsh&gt; INSERT INTO cycling.cyclist_category JSON '{\n  \"category\" : \"Sprint\", \n  \"points\" : 700, \n  \"id\" : \"829aa84a-4bba-411f-a4fb-38167a987cda\"\n}';</pre></div>\n                <div class=\"itemgroup stepresult\"><img class=\"image\" id=\"useInsertJSON__image_whs_xv1_yr\" src=\"https://docs.datastax.com/en/cql/3.3/cql/images/screenshots/useInsertJSON.png\" alt=\"image\" /></div>\n            </li></ul>",
        "created_at": "2018-09-06T15:16:03+0000",
        "updated_at": "2018-09-06T15:16:07+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "docs.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12072"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 44,
            "label": "json",
            "slug": "json"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1280,
            "label": "cql",
            "slug": "cql"
          }
        ],
        "is_public": false,
        "id": 12071,
        "uid": null,
        "title": "Inserting JSON formatted values",
        "url": "https://docs.datastax.com/en/dse/6.0/cql/cql/cql_using/useInsertJSON.html",
        "content": "<p class=\"shortdesc\">Inserting JSON data with the INSERT command for testing queries.</p><section class=\"section context\"><p class=\"p\">In a production database, inserting columns and column values programmatically is\n                more practical than using cqlsh. The CQL INSERT commands supports JSON to provide a\n                manual testing from the cqlsh command line utility.</p><div class=\"p\">Use the following\n                    syntax:<pre>INSERT INTO [<var class=\"keyword varname\">keyspace_name</var>.]<var class=\"keyword varname\">table_name</var> JSON '{\"column_name\": value [,…]}' [DEFAULT UNSET];</pre><p>Note: Enclose\n                    all values other than numbers in double quotes. Booleans, UUID, and other data\n                    types typically recognized in cqlsh must be in double quotes.</p></div></section><ul class=\"ul steps-unordered\" id=\"useInsertJSON__steps-unordered_qfb_nvs_sr\"><li class=\"li step stepexpand\">\n                To insert JSON data, add <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">JSON</code> to the <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph language-cql\">INSERT</code> command. \n                <div class=\"itemgroup stepxmp\"><pre>INSERT INTO cycling.cyclist_category JSON '{\n  \"category\" : \"GC\", \n  \"points\" : 780, \n  \"id\" : \"829aa84a-4bba-411f-a4fb-38167a987cda\",\n  \"lastname\" : \"SUTHERLAND\" }';\n  </pre></div>\n            </li><li class=\"li step stepexpand\">\n                When upserting data if any columns are missing from the JSON, the value in the\n                    missing column is overwritten with null (by default). The following removes the\n                        <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">lastname</code> value \"SUTHERLAND\" from the previous\n                    example:\n                <div class=\"itemgroup stepresult\"><pre>INSERT INTO cycling.cyclist_category JSON '{\n  \"category\" : \"Sprint\", \n  \"points\" : 780, \n  \"id\" : \"829aa84a-4bba-411f-a4fb-38167a987cda\" }';\n  </pre></div>\n            </li><li class=\"li step stepexpand\">\n                Use the DEFAULT UNSET option to only overwrite values found in the JSON\n                    string:\n                <div class=\"itemgroup stepresult\"><pre>INSERT INTO cycling.cyclist_category JSON '{\n  \"category\" : \"Sprint\", \n  \"points\" : 780, \n  \"id\" : \"829aa84a-4bba-411f-a4fb-38167a987cda\" }'\nDEFAULT UNSET;\n  </pre></div>\n            </li><li class=\"li step stepexpand\">\n                Only the PRIMARY KEY fields are required when inserting a new row, any other\n                    column not define in the JSON is set to null:\n                <div class=\"itemgroup stepxmp\"><pre>INSERT INTO cycling.cyclist_category JSON '{\n  \"category\" : \"Sprint\", \n  \"points\" : 700, \n  \"id\" : \"829aa84a-4bba-411f-a4fb-38167a987cda\"\n}';</pre></div>\n                <div class=\"itemgroup stepresult\"><img class=\"image\" id=\"useInsertJSON__image_hwc_3wc_mz\" src=\"https://docs.datastax.com/en/dse/6.0/cql/cql/images/screenshots/useInsertJSON.png\" alt=\"image\" /></div>\n            </li></ul>",
        "created_at": "2018-09-06T15:15:41+0000",
        "updated_at": "2018-09-06T15:15:45+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "docs.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12071"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 12069,
        "uid": null,
        "title": "Supported platforms",
        "url": "https://docs.datastax.com/en/landing_page/doc/landing_page/supportedPlatforms.html",
        "content": "<p class=\"shortdesc\">Supported operating systems, JRE/JDKs, and browsers for DataStax Enterprise, DataStax\n  Studio, and DSE OpsCenter.</p><section class=\"section\" id=\"supportedPlatforms__uniform-clusters\"><h2 class=\"title sectiontitle\">Requirement for Uniform Clusters</h2><p class=\"p\">All Nodes in each Cluster must be uniformly licensed to use the same Subscription. For\n    example, if a Cluster contains 5 Nodes, all 5 Nodes within that Cluster must be either DataStax\n    Basic, or all 5 Nodes must be DataStax Enterprise. Mixing different Subscriptions within a\n    Cluster is not permitted. “Cluster” means a collection of Nodes running the Software which\n    communicate with one another via Gossip, and “Gossip” means the mechanism within the Software\n    enabling related Nodes to communicate with one another. For more information, see <a class=\"xref\" href=\"https://www.datastax.com/enterprise-terms\" target=\"_blank\">Enterprise\n     Terms</a>.</p></section><section class=\"section\" id=\"supportedPlatforms__dse-supported-platforms-60\"><h2 class=\"title sectiontitle\">Supported platforms for DataStax Enterprise 6.0, 5.1, and 5.0</h2><table class=\"table frame-all\"><caption>Amazon Linux\n      Amazon Linux AMI 2016.09 and 2017.03.\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full (2016.09 only)\n        Package, tarball, DataStax Installer\n       \n      Debian\n      Debian 9.x\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      Debian 8.x\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      Debian 7.x\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      CentOS\n      CentOS 7.5\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      CentOS 7.4\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      CentOS 7.3\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      CentOS 7.2\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      CentOS 7.1\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      CentOS 6.9\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      CentOS 6.8\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full (5.0.2 and later)\n        Package, tarball, DataStax Installer\n       \n      CentOS 6.7\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full (5.0.2 and later)\n        Package, tarball, DataStax Installer\n       \n      Mac OS X\n      10.11\n       \n        Dev only\n        Tarball\n       \n       \n        Dev only\n        Tarball, DataStax Installer\n       \n       \n        Dev only\n        Tarball, DataStax Installer\n       \n      10.10\n       \n        Dev only\n        Tarball\n       \n       \n        Dev only\n        Tarball, DataStax Installer\n       \n       \n        Dev only\n        Tarball, DataStax Installer\n       \n      10.9\n       \n        Dev only\n        Tarball\n       \n       \n        Dev only\n        Tarball, DataStax Installer\n       \n       \n        Dev only\n        Tarball, DataStax Installer\n       \n      Red Hat Enterprise Linux\n      Red Hat Enterprise Linux 7.5\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      Red Hat Enterprise Linux 7.4\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      Red Hat Enterprise Linux 7.3\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      Red Hat Enterprise Linux 7.2\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      Red Hat Enterprise Linux 7.1\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      Red Hat Enterprise Linux 6.9\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      Red Hat Enterprise Linux 6.8\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      Red Hat Enterprise Linux 6.7\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full (5.0.2 and later)\n        Package, tarball, DataStax Installer\n       \n      SUSE Linux Enterprise\n        Server\n      SUSE Enterprise Linux 12\n       \n        Full\n        Tarball\n       \n       \n        Full\n        Tarball\n       \n       \n        Full\n        Tarball\n       \n      SUSE Enterprise Linux 11\n       \n        Full\n        Tarball\n       \n       \n        Full\n        Tarball\n       \n       \n        Full\n        Tarball\n       \n      Ubuntu (LTS releases only)\n      Ubuntu LTS 16.04\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      Ubuntu LTS 14.04\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      Ubuntu LTS 12.04\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n       \n        Full\n        Package, tarball, DataStax Installer\n       \n      </caption></table></section><section class=\"section\" id=\"supportedPlatforms__dse-supported-platforms-50\"><h2 class=\"title sectiontitle\">Supported platforms for OpsCenter 6.5, 6.1, and 6.0</h2><table class=\"table frame-all\"><caption>Amazon Linux \n      Amazon Linux AMI 2016.09 and 2017.03.\n       \n        Full\n        Package, tarball\n       \n       \n        Full (6.1.5 and later)\n        Package, tarball\n       \n       \n        LCM not supported\n        Package, tarball\n       \n      Debian\n      Debian 9.x\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      Debian 8.x\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      Debian 7.x\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      <strong class=\"ph b\">CentOS</strong>\n      CentOS 7.4\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      CentOS 7.3\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      CentOS 7.2\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      CentOS 7.1\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      CentOS 6.9\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       Not supported\n      CentOS 6.8\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full (6.0.2 and later)\n        Package, tarball\n       \n      CentOS 6.7\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full (6.0.2 and later)\n        Package, tarball\n       \n      Mac OS X\n      10.11\n       \n        Dev only\n        Tarball\n       \n       \n        Dev only\n        Tarball\n       \n       \n        Dev only\n        Tarball\n       \n      10.10\n       \n        Dev only\n        Tarball\n       \n       \n        Dev only\n        Tarball\n       \n       \n        Dev only\n        Tarball\n       \n      10.9\n       \n        Dev only\n        Tarball\n       \n       \n        Dev only\n        Tarball\n       \n       \n        Dev only\n        Tarball\n       \n      Red Hat Enterprise Linux\n      Red Hat Enterprise Linux 7.4\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      Red Hat Enterprise Linux 7.3\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      Red Hat Enterprise Linux 7.2\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      Red Hat Enterprise Linux 7.1\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      Red Hat Enterprise Linux 6.9\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       Not supported\n      Red Hat Enterprise Linux 6.8\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      Red Hat Enterprise Linux 6.7\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full (6.0.2 and later)\n        Package, tarball\n       \n      <strong class=\"ph b\">SUSE Linux Enterprise\n         Server</strong>\n      SUSE Enterprise Linux 12\n       \n        OpsCenter LCM not supported\n        Tarball\n       \n       \n        OpsCenter LCM not supported\n        Tarball\n       \n       \n        OpsCenter LCM not supported\n        Tarball\n       \n      SUSE Enterprise Linux 11\n       \n        OpsCenter LCM not supported\n        Tarball\n       \n       \n        OpsCenter LCM not supported\n        Tarball\n       \n       \n        OpsCenter LCM not supported\n        Tarball\n       \n      Ubuntu (LTS releases only)\n      Ubuntu LTS 16.04\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      Ubuntu LTS 14.04\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      Ubuntu LTS 12.04\n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n       \n        Full\n        Package, tarball\n       \n      </caption></table><p>Note: Starting with OpsCenter 6.1.3, LCM automatically performs an <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.1/opsc/LCM/opscLCMjobsOverview.html#opscLCMrunJobsOverview__OSplatformCheck\" target=\"_blank\">OS supported platform check</a> for the version of DSE being\n    installed.</p></section><section class=\"section\" id=\"supportedPlatforms__jreJdk\"><h2 class=\"title sectiontitle\">JREs and JDKs supported by DataStax products</h2><table class=\"table frame-all\"><caption>DSE 6.0\n       Oracle Java 8 (at least 1.8.0_151)\n       OpenJDK 8\n      DSE 5.1, 5.0\n       Oracle Java 8 (at least 1.8.0_40)\n       OpenJDK 8\n      DSE 4.8, 4.7\n       \n        Oracle Java 7 (at least 1.7.0_25)<br />Oracle Java 8 (at least 1.8.0_40)\n       \n       OpenJDK 7\n      OpsCenter 6.5, 6.1, 6.0\n       Oracle Java 8 (JDK or JRE)\n       OpenJDK 8\n      Studio 6.0 (Linux)\n       Oracle Java 8 (at least 1.8.0_151)\n       OpenJDK 8\n      Studio 2.0 (Linux)\n       Oracle Java 8 (at least 1.8.0_40)\n       OpenJDK 8\n      Studio 2.0 (Windows 7 and 10)\n       Oracle Java 8\n       \n      Studio 1.0\n       Oracle Java 8\n       \n      </caption></table><p class=\"p\" id=\"supportedPlatforms__oracle-jdk-rec\">* The latest build of the supported Oracle JRE or JDK 8 version is recommended.</p></section><section class=\"section\" id=\"supportedPlatforms__browser-support\"><h2 class=\"title sectiontitle\">Supported browsers for DSE OpsCenter and DataStax Studio</h2><table class=\"table frame-all\"><caption>Apple Safari\n       Yes\n       Mac OS X 10.11, 10.12, 10.13\n      Google Chrome\n       Yes\n       \n        Ubuntu 14.04 LTS, 16.04 LTS<br />Mac OS X 10.11,10.12, 10.13<br />Windows 7, 10<br />CentOS 7\n       \n      Internet Explorer\n       No\n       No\n      Microsoft Edge\n       No\n       No\n      Mozilla Firefox\n       Yes\n       \n        Ubuntu 14.04 LTS, 16.04 LTS<br />Mac OS X 10.11, 10.12, 10.13<br />Windows 7, 10<br />CentOS 6.8, 7\n       \n      </caption></table><p class=\"p\" id=\"supportedPlatforms__studio-tested\">** DataStax Studio is tested on the listed platforms (all 64-bit) with the latest versions of\n    the specified web browsers.</p></section><p></p><h2 class=\"title sectiontitle\">Supported platforms for DataStax Enterprise 4.6.x (EOSL) to 4.8.x (EOL)</h2>\n   \n   <table class=\"table frame-all\"><caption>Debian\n      Debian 6.x-7.1\n       x86\n       Dev only\n      Debian 6.x-7.1\n       x86_64\n       Full\n      CentOS\n      CentOs 7.3\n       x86_64\n       Full (4.8.12 and later)\n      CentOs 7.2\n       x86_64\n       Full (4.8.9 and later)\n      CentOS 7.1\n       x86_64\n       Full (4.8.4 and later)\n      CentOS 7.0\n       x86\n       Dev only\n      CentOS 7.0\n       x86_64\n       Full\n      CentOS 6.8\n       x86_64\n       Full (4.8.12 and later)\n      CentOS 6.7\n       x86_64\n       Full (4.8.5 and later)\n      CentOS 5.8-6.5\n       x86\n       Dev only\n      CentOS 5.8-6.5\n       x86_64\n       Full\n      Mac\n      OS X 10.6\n       x86, x86_64\n       Dev only\n      OS X 10.5\n       x86, x86_64\n       Dev only\n      Oracle\n      Oracle Linux 6\n       x86\n       Dev only\n      Oracle Linux 6\n       x86_64\n       Full\n      Red Hat Enterprise\n        Linux\n      Red Hat Enterprise Linux 7.3\n       x86_64\n       Full (4.8.12 and later)\n      Red Hat Enterprise Linux 7.2\n       x86_64\n       Full (4.8.9 and later)\n      Red Hat Enterprise Linux 7.0\n       x86\n       Dev only\n      Red Hat Enterprise Linux 7.0\n       x86_64\n       Full\n      Red Hat Enterprise Linux 6.7\n       x86_64\n       Full (4.8.5 and later)\n      Red Hat Enterprise Linux 5.8-6.5\n       x86\n       Dev only\n      Red Hat Enterprise Linux 5.8-6.5\n       x86_64\n       Full\n      SUSE Linux Enterprise\n        Server\n      SUSE Enterprise Linux 11.2\n       x86_64\n       Full (tarball only)\n      SUSE Enterprise Linux 11.4\n       x86_64\n       Full (tarball only)\n      Ubuntu\n      Ubuntu 11.x-13.04\n       x86\n       Dev only\n      Ubuntu 11.x-13.04\n       x86_64\n       Full\n      Ubuntu 14.04\n       x86\n       Dev only\n      Ubuntu 14.04\n       x86_64\n       Full\n      </caption></table><section class=\"section\"><p>Note: DataStax products do not support big-endian systems.</p></section>",
        "created_at": "2018-09-05T20:44:16+0000",
        "updated_at": "2018-09-05T20:44:20+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "docs.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12069"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 12067,
        "uid": null,
        "title": "Deploy DataStax Enterprise (DSE) on the AWS Cloud with New Quick Start",
        "url": "https://aws.amazon.com/about-aws/whats-new/2017/07/deploy-datastax-enterprise--dse--on-the-aws-cloud-with-new-quick-start/",
        "content": "<p>DSE is the always-on data platform for cloud applications that is powered by Apache Cassandra. DSE is designed to handle big data workloads across multiple nodes with no single point of failure, by employing a peer-to-peer distributed system for data across homogeneous nodes. Integrated within each node of DSE is powerful indexing, search, analytics, and graph functionality, provided by combining Cassandra with Apache Solr, Apache Spark, and DSE Graph.</p><p>This Quick Start uses AWS CloudFormation templates to deploy DSE into a virtual private cloud (VPC) in your AWS account. You can build a new VPC for DSE, or deploy the software into your existing VPC. The automated deployment provisions an Amazon Elastic Compute Cloud (Amazon EC2) instance running DSE OpsCenter, which is the web console for managing DSE clusters. The Quick Start also provisions EC2 instances for up to four DSE data centers to support workload separation, and sets up a configurable number of nodes in each data center. You can also customize your network resources, and modify the instance type and volume size for each node.</p><p>The Quick Start includes a guide with step-by-step deployment and configuration instructions. To get started, use the following resources:</p><ul><li>View the <a href=\"https://aws.amazon.com/quickstart/architecture/datastax-enterprise/\">architecture and details</a></li> \n           <li>View the <a href=\"https://fwd.aws/VP83q\" target=\"_blank\">deployment guide</a></li> \n           <li>Browse and launch other <a href=\"https://aws.amazon.com/quickstart/\">AWS Quick Start reference deployments</a></li> \n          </ul><p><i>About Quick Starts</i></p><p><i>Quick Starts are automated reference deployments for key workloads on the AWS Cloud. Each Quick Start launches, configures, and runs the AWS compute, network, storage, and other services required to deploy a specific workload on AWS, using AWS best practices for security and availability. This is the latest in a series of Quick Starts built by AWS in collaboration with AWS partners to automate the deployment of popular products and technologies on AWS.</i></p>",
        "created_at": "2018-09-05T20:41:31+0000",
        "updated_at": "2018-09-05T20:41:32+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 1,
        "domain_name": "aws.amazon.com",
        "preview_picture": "https://a0.awsstatic.com/main/images/logos/aws_logo_smile_1200x630.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12067"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          }
        ],
        "is_public": false,
        "id": 12040,
        "uid": null,
        "title": "Leaderboard @Dream11",
        "url": "https://medium.com/dream11-tech-blog/leaderboard-dream11-4efc6f93c23e",
        "content": "<div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><h1 id=\"9a9e\" class=\"graf graf--h3 graf--leading graf--title\"><strong class=\"markup--strong markup--h3-strong\">Leaderboard @Dream11</strong></h1><p id=\"93cd\" class=\"graf graf--p graf-after--h3\"><em class=\"markup--em markup--p-em\">Scale to serve 100+ million reads/writes using Spark and Cassandra</em></p><p id=\"11aa\" class=\"graf graf--p graf-after--p\">By <a href=\"https://www.linkedin.com/in/bharatvishwakarma/\" data-href=\"https://www.linkedin.com/in/bharatvishwakarma/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Bharat Vishwakarma</a>, <a href=\"https://www.linkedin.com/in/amit-mirchandani-679b2a23/\" data-href=\"https://www.linkedin.com/in/amit-mirchandani-679b2a23/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Amit Mirchandani</a>, <a href=\"https://www.linkedin.com/in/amit-sharma-a831635/\" data-href=\"https://www.linkedin.com/in/amit-sharma-a831635/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Amit Sharma</a></p><p id=\"46f7\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">Dream11 — Leading the Pack in Fantasy Sports</em></strong></p><p id=\"0feb\" class=\"graf graf--p graf-after--p\">Dream11 is India’s #1 Fantasy Sports platform with a growing user base of over 15 million Indians playing Fantasy Cricket, Football &amp; Kabaddi. To play on Dream11, users create a virtual team of real players, who score points based on their real-life performance in the upcoming match. Our ‘<strong class=\"markup--strong markup--p-strong\">Leaderboard</strong>’ feature<strong class=\"markup--strong markup--p-strong\"> </strong>enables all our users to see how their team ranks amongst all other competing teams, as their points are updated every minute during every match. The excitement our users feel while watching their chosen players score points for their team with every four, six, wicket, goal, etc. is the core of our entire user experience.</p><p id=\"9281\" class=\"graf graf--p graf-after--p\">Leaderboard generation starts by calculating total points earned by every one of up to 6 teams created by each of our 15 million users. These points are calculated as per the composition of up to 11 players selected from the squad and points scored by the players for every action. These teams are then grouped by the participating contest, sorted by total points and ranked as per competitive ranking to create the Leaderboard for a contest. Leaderboards are updated every minute across thousands of contests for multiple matches being played at the same time.</p><p id=\"910b\" class=\"graf graf--p graf-after--p\">The Leaderboard at Dream11 is internally powered by the point calculation engine, code named “<strong class=\"markup--strong markup--p-strong\">Aryabhata”</strong> and here’s what it looks like:</p><figure id=\"c743\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*QfYqxzvoIXw6AG8LHUHnwQ.jpeg\" data-width=\"2843\" data-height=\"1452\" data-action=\"zoom\" data-action-value=\"1*QfYqxzvoIXw6AG8LHUHnwQ.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*QfYqxzvoIXw6AG8LHUHnwQ.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><em class=\"markup--em markup--figure-em\">Pic1: Desktop view of Dream11 Leaderboard</em></figcaption></div></figure><p id=\"b3d7\" class=\"graf graf--p graf-after--figure\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">The Dream11 Leaderboard — scale, technology and reliability considerations that influenced our engineering decisions</em></strong></p><p id=\"bb20\" class=\"graf graf--p graf-after--p\">Our Leaderboard has already served 1 million requests per minute, serving 250k concurrent users at its peak. As our user base grows, we expected the request pattern to grow by a much larger magnitude. So, we needed to design a system that would linearly scale as our traffic increases. It should be able to crunch gigabytes of data using distributed sorting within a SLA (Service Level Agreement) of under a minute, while maintaining strong consistency of different user views across multiple platforms. The persistent systems used should support millions of input/output operations per second, while maintaining throughput and latency under the strict SLA desired. <strong class=\"markup--strong markup--p-strong\">This is done to ensure that our users have a seamless experience across platforms and devices at all times.</strong></p><p id=\"e814\" class=\"graf graf--p graf-after--p\">Let’s dive deeper into our system to understand the technical nuances and how we approached them to provide a solution.</p><ol class=\"postList\"><li id=\"c3d9\" class=\"graf graf--li graf-after--p\"><strong class=\"markup--strong markup--li-strong\"><em class=\"markup--em markup--li-em\">Calculating, Sorting, Ranking and Storing data within 1 min SLA using Apache Spark</em></strong></li></ol><p id=\"0909\" class=\"graf graf--p graf-after--li\">Apache Spark is a fast, distributed, in-memory highly scalable cluster computing framework. It provides support for low level java APIs as well as sql, for processing large amount of data within sub second latency. We leverage Spark for calculating, sorting and ranking of our data. So far, we’ve been able to use Spark to process Leaderboards of around a million contests, ranging from 2 teams to 400,000 teams in different contests for a single match, totalling up to <strong class=\"markup--strong markup--p-strong\">40 million records within our 60-second SLA</strong>.</p><p id=\"7ca6\" class=\"graf graf--p graf-after--p\">The point calculation engine loads data for all contests in Apache Spark’s memory which is distributed across the cluster machines, allowing us to scale up simply by adding more nodes to the Spark cluster. The distributed contest data is then processed concurrently to calculate new points for every participating user team. Once the new user team points are prepared, they are grouped by contest, sorted and ranked as per competitive ranking, thereby completing the generation of the new Leaderboard state. <strong class=\"markup--strong markup--p-strong\">By deploying Spark at a large scale for all our calculations, we make sure our users don’t miss a thing with the fastest of real-time updates at lightening speed.</strong></p><p id=\"4beb\" class=\"graf graf--p graf-after--p\">2.<strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\"> Providing consistent views to users using Data Isolation and Snapshots</em></strong></p><p id=\"bb45\" class=\"graf graf--p graf-after--p\">The contests that a user has joined and the Leaderboard for each of those contests is displayed in different view parts on the product (refer to the image above). The user can browse this data and also access the points/ranks/teams of other users participating in the same contest as him, once the deadline (1 hour prior to the start of match) hits. Every view needs to be consistent when points and Leaderboard are updated in the background. For eg: If our Leaderboard for a contest shows results for calculations done at an event (eg. 1st ball of over), our team view should also show our users their team points of the same calculation event, so that the displayed points are the same.</p><p id=\"02a9\" class=\"graf graf--p graf-after--p\">The point calculation engine concludes by updating every contest record in worst case scenario. If we fetch data from the same source that is being simultaneously updated, the breadth of the update operation will result in inevitable view inconsistency. Moreover, since all the updates are live to the user, rollbacks damage user trust and provides a bad user experience. Therefore, we need to isolate read and write operations at the data level. Snapshot Isolation in transactional system provides a way to manage concurrency by isolating data at which they operate, generally implemented by MVCC [multi version concurrency control]. Operating with multiple version of data helps to isolate concurrent operation and maintain consistency of database.</p><p id=\"f63c\" class=\"graf graf--p graf-after--p\">The point calculation engine extends the above idea and snapshots every write action resulting in immutable data set. This insert-only behavior and immutability allows concurrent processing of multiple game events which we can tag linearly to allow easy rollbacks. The user view can now be tagged to a snapshot version thereby achieving consistency across all user view, even when new snapshot is in process.</p><p id=\"de57\" class=\"graf graf--p graf-after--p\">3.<strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\"> Guaranteeing atomicity of Bulk Write/Calculation using Apache Spark</em></strong></p><p id=\"3e58\" class=\"graf graf--p graf-after--p\">Apache Spark provides fault tolerance out of the box. In case of failures in cluster machines, jobs are rescheduled on other available cluster machines. If rescheduling fails for certain attempts, Spark fails the whole job and shuts down the entire process. As long as your process is idempotent, such a feature allows building atomic bulk operation by retrying failed events to completion.</p><p id=\"fbe0\" class=\"graf graf--p graf-after--p\">The point calculation engine leverages this fault tolerant property of Spark and solves atomic bulk write problem by moving away from UPDATE to INSERT IF NOT EXISTS operation. Since we only perform insert operations, multiple inserts would only result in the same output thereby achieving idempotency in the system.</p><p id=\"8601\" class=\"graf graf--p graf-after--p\">4.<strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\"> Implementing bulk Write/Read With Horizontal Scaling and HA using Apache Cassandra</em></strong></p><p id=\"8201\" class=\"graf graf--p graf-after--p\">Based on the requirements of the point calculation engine we needed to choose a persistent system with the following properties:</p><ul class=\"postList\"><li id=\"4550\" class=\"graf graf--li graf-after--p\">Highly available and capable of handling loss of a few nodes</li><li id=\"bc5e\" class=\"graf graf--li graf-after--li\">Tunable consistency</li><li id=\"e39d\" class=\"graf graf--li graf-after--li\">Easy to scale up and down based on need</li><li id=\"7ce7\" class=\"graf graf--li graf-after--li\">Multi-Master to serve the traffic at the rate desired, which would be difficult in a master-slave setup due to replication lag</li><li id=\"9d59\" class=\"graf graf--li graf-after--li\">Optimized for very high write throughput</li><li id=\"7bde\" class=\"graf graf--li graf-after--li\">Strong community support</li></ul><p id=\"9e0c\" class=\"graf graf--p graf-after--li\">Apache Cassandra is a distributed, fault tolerant and highly available system that is specifically designed to handle huge amount of data. Primary features of Apache Cassandra includes decentralized architecture with no single point of failure, high availability with replication within cluster while performing write operation. This is in contrast to a master-slave system, where data is written to master and propagated to slave via binary logs which adds to replica lag. Apache Cassandra supports tunable consistency and has a strong community support.</p><p id=\"e1d2\" class=\"graf graf--p graf-after--p\">Dream11 Leaderboard is supported by a 10 node (c4.8x large Amazon ec2 instances) Cassandra cluster which has supported 5 Million read/write CPM (calls per minute) at peak.</p><p id=\"7bd3\" class=\"graf graf--p graf-after--p\">5.<strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\"> Supporting Flash Read and other Read optimizations using Redis as a local cache</em></strong></p><p id=\"fb6a\" class=\"graf graf--p graf-after--p\">Our Leaderboard use case demands bulk atomic writes as well as reads with very high throughput. It is also a well-known fact that Cassandra performs optimally best for pointed queries, i.e. queries with filter clause that identify partition and data residence efficiently. Hence, we needed to design the Data Model in such a way where both Bulk Writes and High Reads demands can be fulfilled to scale. We designed several views which were written by Spark jobs directly into Cassandra. The read queries to Cassandra directly fetched data from respective views with pointed queries and thus, became highly scaled.</p><p id=\"fa3c\" class=\"graf graf--p graf-after--p\">Also, since the Leaderboard data mutates every 60 seconds, we analyzed that a caching layer in the middle would further optimize reads. One possible option was to add a centralized Redis cluster. We had run into some issues at scale with this approach in the past because of hot/cold zones. Due to this, we ended up having to deploy a lot more nodes than needed. We also realized that since our point calculation data is immutable, it can be cached in a distributed fashion (Local redis at Web Server end) removing the need of a central redis cluster service thereby eliminating one more point of failure/bottleneck in the system. It can flush data every time a new event arrives and can achieve a low cache size. Running local redis servers proved to improve both latency and system availability since it reduces read load on Cassandra.</p><p id=\"2b4b\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">The Final Aryabhata Architecture</strong></p><p id=\"dbd7\" class=\"graf graf--p graf-after--p\">The image below shows our full Leaderboard architecture:</p><figure id=\"a61a\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*G3QgtcauLu_mRgKuZqpmyA.jpeg\" data-width=\"3322\" data-height=\"3231\" data-is-featured=\"true\" data-action=\"zoom\" data-action-value=\"1*G3QgtcauLu_mRgKuZqpmyA.jpeg\" src=\"https://cdn-images-1.medium.com/max/1600/1*G3QgtcauLu_mRgKuZqpmyA.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\"><em class=\"markup--em markup--figure-em\">Pic2: Aryabhata final architecture</em></figcaption></div></figure><ol class=\"postList\"><li id=\"3a9a\" class=\"graf graf--li graf-after--figure\">Dream11 API is an edge service that serves all requests</li><li id=\"08ad\" class=\"graf graf--li graf-after--li\">Resources consumed and created are saved to mysql database</li><li id=\"88f1\" class=\"graf graf--li graf-after--li\">After match deadline, data is fetched from mysql to s3 via Amazon Data pipeline</li><li id=\"235a\" class=\"graf graf--li graf-after--li\">Data Pipeline saves all contest related information to s3</li><li id=\"65e3\" class=\"graf graf--li graf-after--li\">Data pipeline sends an SNS notification on completion of data transfer</li><li id=\"c3a2\" class=\"graf graf--li graf-after--li\">SNS triggers a lambda function which makes an api call to Leaderboard api</li><li id=\"ff8d\" class=\"graf graf--li graf-after--li\">Leaderboard service schedules data loading spark job on spark cluster using Spark Job Server.</li><li id=\"a86c\" class=\"graf graf--li graf-after--li\">Spark reads contest data from S3 and saves it to cassandra after processing. Data is ready to serve</li><li id=\"2efb\" class=\"graf graf--li graf-after--li\">Leaderboard service connects to cassandra for showing contest detail views as explained earlier.</li></ol><p id=\"3609\" class=\"graf graf--p graf-after--li\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">Increasing the lead — how we’re moving forward</em></strong></p><ol class=\"postList\"><li id=\"2904\" class=\"graf graf--li graf-after--p\"><strong class=\"markup--strong markup--li-strong\"><em class=\"markup--em markup--li-em\">Distributing Cassandra Load</em></strong></li></ol><p id=\"47dd\" class=\"graf graf--p graf-after--li\">Cassandra stands at the heart of our Leaderboard system. It stores every mutation result and also supports real time queries. Although Cassandra is designed for no single point of failure; huge read/write loads sometimes limit throughput from a single partition. A simple solution to the problem could be separation of read and write. Cassandra supports multiple strategies for maintaining replication across two ring clusters. In the above strategy, one ring could only accept write, while reads could be served from the other ring. We would like to explore this strategy after conducting some experiments with consistency and latency.</p><p id=\"ce49\" class=\"graf graf--p graf-after--p\">2.<strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\"> Personalising Journey Graphs</em></strong></p><p id=\"62e8\" class=\"graf graf--p graf-after--p\">Leaderboard service persists every version of the Leaderboard state, each being a representation of a corresponding real world event. We can use this data to create a feature displaying a journey graph for a user for a particular match. Journey Graphs would be just personification of how user ranks travelled during game events. This will enable users to see a trend of their ranks during/after the match, adding transparency and help build trust.</p><p id=\"03a2\" class=\"graf graf--p graf-after--p\">3.<strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\"> Enhancing our Event Generation Model</em></strong></p><p id=\"5f9f\" class=\"graf graf--p graf-after--p\">Currently, the Leaderboard api works in pull mode where the clients constantly keep polling for new events to fetch latest data. To reduce latency and enhance user experience in the future, we would like to implement a push model. In this model, the Leaderboard service pushes events to client notifying new score update, greatly reducing the time between an actual event and corresponding update of points in the frontend. This will enable the view on the frontend to be auto-refreshed as soon as the new data is ready.</p><p id=\"961c\" class=\"graf graf--p graf-after--p\">4.<strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\"> Optimising Data Mining Processes</em></strong></p><p id=\"9203\" class=\"graf graf--p graf-after--p graf--trailing\">Data is the new gold and with Leaderboard architecture, we generate huge dataset that provides great insight about the match as well as user behaviour. The data science team hopes to analyze this data and run experiments to come up with informed product behaviour adding to richer user experience.</p></div></div>",
        "created_at": "2018-08-30T14:53:55+0000",
        "updated_at": "2018-08-30T14:54:05+0000",
        "published_at": "2017-10-24T09:57:55+0000",
        "published_by": [
          "Dream11 Engineering"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 9,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*G3QgtcauLu_mRgKuZqpmyA.jpeg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12040"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 962,
            "label": "lambda",
            "slug": "lambda"
          }
        ],
        "is_public": false,
        "id": 12038,
        "uid": null,
        "title": "Introduction - Setup lambda architecture example",
        "url": "https://skyhenryk.github.io/lae/1-introduction-for-lambda-architecture/",
        "content": "<p></p><h2>SETUP LAMBDA ARCHITECTURE EXAMPLE</h2>\n\t\t\t\t<h3>1. Introduction - Setup lambda architecture example</h3>\n\t\t\t\t\n\t\t\t\t24 March 2016<div><p>Let’s see how to build a lambda architecture example (LAE) for processing huge log data.</p><h2 id=\"what-is-the-lambda-architecture\">What is the Lambda Architecture?</h2><p>Nathan Marz came up with the term Lambda Architecture (LA) for a generic, scalable and fault-tolerant data processing architecture, based on his experience working on distributed data processing systems at Backtype and Twitter.</p><p>The LA aims to satisfy the needs for a robust system that is fault-tolerant, both against hardware failures and human mistakes, being able to serve a wide range of workloads and use cases, and in which low-latency reads and updates are required. The resulting system should be linearly scalable, and it should scale out rather than up.</p><p>Here’s how it looks like, from a high-level perspective:</p><p><img src=\"https://skyhenryk.github.io/images/lambda-architecture/1/lamdbda-architecture.png\" alt=\"LA screenshot\" /></p><p>The Lambda Architecture (cred. http://lambda-architecture.net/)</p><h2 id=\"lambda-architecture-example\">Lambda Architecture Example</h2><p><img src=\"https://skyhenryk.github.io/images/lambda-architecture/1.png\" alt=\"LA screenshot\" /></p><p><strong>lambda architecture example (LAE)</strong></p><p>Let’s build lambda architecture example (LAE) one by one with introduction and codes.</p><p>The most diffrent thing is that I put buffering layer for this example lambda architecture.</p><p>If the log data goes from api server to S3 or spark-streaming directly, it needs a long time to wait to send a data\nbecause I assume that there are <strong>quite many API server to send a log at the same time</strong> and threads of aws-cli(to send a data to s3) and spark-streaming are limited\nSo I will use kafka - distributed replicated cluster messaging system. It will help the api servers not to wait to send the log data.</p><p>In next post, We will learn about kafka and see how to setup for LAE.</p><h2 id=\"contents\">Contents</h2><ol><li>\n    <p>Intro to lambda architecture example (LAE) setup</p>\n  </li>\n  <li>\n    <p>Kafka - zookeeper (Distribution Layer)</p>\n    <p>2-1. Kafka - zookeeper single-server setup</p>\n    <p>2-2. Kafka - zookeeper multi-server setup</p>\n  </li>\n  <li>\n    <p>Spark streaming (Speed Layer)</p>\n    <p>3-1. Spark-streaming setup and connect with LAE</p>\n  </li>\n  <li>\n    <p>Cassandra (Query Layer)</p>\n    <p>4-1. NoSQL database performance comparison (canssandra, hbase, etc.)</p>\n    <p>4-2. Cassandra db setup and connect with LAE</p>\n  </li>\n  <li>\n    <p>Example API server setup and connect with LAE</p>\n  </li>\n  <li>\n    <p>S3 setup and connect with LAE (Batch Layer)</p>\n  </li>\n  <li>\n    <p>Spark, Zeppelin setup and connect with LAE (View Layer)</p>\n  </li>\n  <li>\n    <p>Docker setup for LAE</p>\n  </li>\n</ol><noscript><div>Please enable JavaScript to view the <a href=\"https://disqus.com/?ref_noscript\">comments powered by Disqus.</a></div></noscript></div>",
        "created_at": "2018-08-30T14:49:37+0000",
        "updated_at": "2018-08-30T14:50:45+0000",
        "published_at": "2016-03-24T00:00:00+0000",
        "published_by": [
          "yeo"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "skyhenryk.github.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12038"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1069,
            "label": "iot",
            "slug": "iot"
          }
        ],
        "is_public": false,
        "id": 12013,
        "uid": null,
        "title": "AN10 | IoT, Cassandra, Big Data",
        "url": "http://www.an10.io/technology/cassandra-and-iot-queries-are-they-a-good-match/",
        "content": "<h2 itemprop=\"name\" class=\"entry_title\">20 Jul Cassandra and IoT queries: Are they a good match?</h2><p><em>Cassandra is known to be an industry standard Database for storing Big Data (Volume, Velocity, and Variability), and an IoT platform is really a canonical representation of such a use case. We present here benchmarking results for some typical IoT query cases we were experiencing in the design of ORION, our IoT platform. The aim is to understand when, and perhaps more importantly, how Cassandra can play an important role for this important use case.</em></p><p><em>Authors: </em>Arbab Khalil, Affan Syed</p><h2>Introduction</h2><p>At AN10, we are developing an entire Internet of Things ecosystem, building the entire value chain of IoT products and services: from intelligent sensors and gateways at the edge, to our industrial IoT platform ORION, to high-performance applications and solution suites for specific industries.</p><p>The core of our entire product &amp; service offering is ORION, a Big Data capable, highly scalable and full-featured IoT platform. An IoT platform provides the capabilities of IoT device management, scalable data management &amp; analytics, and robust APIs on top. The idea is for the platform to do the heavy lifting, and provide applications with IoT capabilities through API calls. Building this platform has required reviewing, testing, and adapting quite a few data stacks, tools and technologies. As an open company, we want to share our experiences with using these technologies as part of a series of blogs. These will take the form of an evidence-based article, showing real-world results.</p><h2>The data-analytics stack for ORION</h2><p>One of the basic functions of an IoT platform is that it  meant to scale and receive data from million of devices every minute.  As such, we decided to stick with using a big data stack that matches, at the first cut,  our need to scale: thus we went with the SMACK (Spark, Mesos, Akka, Cassandra, Kafka) stack [1].  To summarize this stack, it allows horizontal scalability in every aspect of a Big Data stack. Cassandra scales the big data storage at high write speeds, Spark provides a distributed analytic engine, Mesos provides the cluster and resource monitoring and management capability, while Kafka and Akka enable a queue-based microservices architecture to build features around a real-time data pipeline.</p><p>We use  Cassandra, where the table schema has to be optimized (denormalized) for the type of queries we expect the platform to execute. This is a well known tradeoff (footnote: while there are a host of wonderful articles about how to build schema in Cassandra for IoT, this article is not it), but we wanted to better evaluate the different schema models for the type of queries, generally, we expect our platform to handle.</p><blockquote class=\"\"><h5 class=\"blockquote-text\">we wanted to better evaluate the different schema models for the type of queries, generally, we expect our platform to handle.</h5></blockquote><h3>A generic IoT query</h3><p>IoT application have a general nature where devices report sensor data on regular intervals (as well as irregular events, but we will leave those aside for the current discussion). This sampling rate implies a time-series data which demands windowed queries; however, IoT applications have another topological mapping that requires queries to range over a group of devices (lets assume uniquely identified by a device id) — this can for region specific or collocation related analysis.</p><p>Thus, typically, while the need to look at real-time data can be separated per device, analytical queries will demand queries that range over both time and device ids . This is the generic query we were experiencing being demanded by our customers in the IoT domain.</p><p>While using several best-practices for time-series schema design in Cassandra, there are a few other constraints that crop up that make no particular schema design a clear winner, and require detailed benchmarking (thus this effort). One such is the issue of ensuring that partition sizes are not exceptionally large, so we need a way to have a deterministic break down on a size, e.g. monthly basis. Another, concern is the issue of partitioning such that all data does not go to a single partition at the same time (e.g. hotspots due to partition key having ONLY date).  These all make the denormalization of data for Cassandra difficult, especially for the case where the range over devices and time cannot be restricted, either.</p><p>In order to evaluate, we came up with a few schema configurations, where we majorly change the PRIMARY_KEY (determining the performance of queries). We consider a simple device with a packet containing the dev_id, date and timestamp for a  single voltage sensor (different sensors will have different tables for now, we can also keep them in one, but again a distraction from the current evaluation).</p><div><pre>#### C* table1 ####</pre><pre>CREATE TABLE test_timeseries.timeseries_with_date1 (</pre><pre>    dev_id text,</pre><pre>    day date,</pre><pre>    rec_time timestamp,</pre><pre>    voltage float,</pre><pre>    <code>PRIMARY KEY ((dev_id, rec_time), day)</code></pre><pre>) WITH CLUSTERING ORDER BY (day ASC)</pre><pre>#### C* table2 ####</pre><pre>CREATE TABLE test_timeseries.timeseries_with_date2 (</pre><pre>    dev_id text,</pre><pre>    day date,</pre><pre>    rec_time timestamp,</pre><pre>    voltage float,</pre><pre>    <code>PRIMARY KEY (dev_id, day, rec_time)</code></pre><pre>) WITH CLUSTERING ORDER BY (day ASC, rec_time ASC)</pre><pre>#### C* table3 ####</pre><pre>CREATE TABLE test_timeseries.timeseries_with_date3 (</pre><pre>    day date,</pre><pre>    dev_id text,</pre><pre>    rec_time timestamp,</pre><pre>    voltage float,</pre><pre>    <code>PRIMARY KEY (day, dev_id, rec_time)</code></pre><pre>) WITH CLUSTERING ORDER BY (dev_id ASC, rec_time ASC)</pre><pre>#### C* table4 ####</pre><pre>CREATE TABLE test_timeseries.timeseries_with_date4 (</pre><pre>    day date,</pre><pre>    dev_id text,</pre><pre>    rec_time timestamp,</pre><pre>    voltage float,</pre><pre>   <code> PRIMARY KEY ((day, dev_id), rec_time)</code></pre><pre>) WITH CLUSTERING ORDER BY (rec_time ASC)</pre><pre>#### C* table5 ####</pre><pre>CREATE TABLE test_timeseries.timeseries_with_date5 (</pre><pre>    dev_id text,</pre><pre>    day date,</pre><pre>    rec_time timestamp,</pre><pre>    voltage float,</pre><pre>    <code>PRIMARY KEY ((dev_id, day), rec_time)</code></pre><p><code>) WITH CLUSTERING ORDER BY (rec_time ASC)</code></p></div><h2>Evaluation Framework:</h2><p>We are using a virtual machine with 8 vCPUs and 30 GB memory running Ubuntu Server 16.04. Both Cassandra and Spark are running on same machine with single node deployment. We are using Cassandra:3.11.2, Spark 2.2.0 and Spark-Cassandra-Connector:2.0.7-s_2.11 version.  We use Spark to enable any query optimization possible using its Catalyst engine.</p><h3>Testing Procedure</h3><p>Data-set contains data from 1000 devices, each sending one packet per minute over a total duration of two months (July and Aug 2017) or 62 days. Each table contains 89.28 million entries.</p><p>Initially we benchmarked these 5 tables with three different querying methods available in Spark. We ran a simple count query on each table for different number of devices over variable time duration in days.  We will discuss each method one by one:</p><p>1. where() function of DataFrame API: Conditions on day and dev_id are given through where() function followed by cunt() function on dataframe. This query is performed as follows:</p><div><pre>### Loading Table from Cassandra ###</pre><pre>table1_df = spark.read.format(\"org.apache.spark.sql.cassandra\")\\</pre><pre>    .options(keyspace = 'test_timeseries', table = 'timeseries_with_date').load()</pre><pre>#### Querying table ####</pre><pre>query_table1_df = table1_df.where((col(\"day\") &gt;= \"2017-07-01\") &amp; (col(\"day\") &lt;= \"2017-07-04\") &amp; \\</pre><pre>    (col(\"dev_id\").isin(devices))).count()</pre></div><p>2. Spark SQL Query using range queries: Here we are running queries using Spark SQL API and gives duration through range. It makes writing queries simple and improves code readability. This query is performed as follows:</p><div><pre>#### Loading Table from Cassandra and Registering Temp View ####</pre><pre>spark.read.format(\"org.apache.spark.sql.cassandra\")\\</pre><pre>    .options(keyspace = 'test_timeseries', table = 'timeseries_with_date')\\</pre><pre>    .load().createOrReplaceTempView(\"table1\")</pre><pre>#### Querying table where <code>devices </code><code>is a string having list of all dev_id's ####</code></pre><pre>query_table1 = spark.sql(\"SELECT COUNT(1) FROM table1 WHERE day &gt;= cast('2017-07-01' as date) AND \\</pre><pre>day &lt;= cast('2017-07-15' as date) AND dev_id  IN(\" + devices + \")\" </pre></div><p>3. Spark SQL Query using IN() : Here we are running queries using Spark SQL API and giving dates as list through IN(). This is to check weather range performs better or passing days through IN(). This query is performed as follows:</p><div><pre>query1 = \"SELECT COUNT(1) FROM table1 WHERE day IN \" + dates + \" AND dev_id  IN \" + devices</pre><pre>## <code>dates</code><code> is string containing all days and </code><code>devices</code><code> is string containing all dev_id's</code></pre><pre>query_table1 = spark.sql(query1)</pre></div><p>Results of these queries are shown in Without Year sheet in results sections. Time format is minutes:seconds.milliseconds (MM:SS.sss)</p><h3>Adding another field for performance</h3><p>After reading  Jon Haddad’s article [2] about “Cassandra Time Series Data Modeling For Massive Scale”, we added an extra field of year in partition keys of above four PRIMARY KEYs. Our new dataset contains data from 1000 devices, corresponding to sending one packet per minute over a total duration of four months (Nov 2017 to Feb 2018) or 120 days, thus ensuring two different partitions due to years. Now we have four additional  tables with the same schema but different primary keys:</p><ol><li>    PRIMARY KEY ((year, dev_id), day, rec_time)</li><li>    PRIMARY KEY ((year, day), dev_id, rec_time)</li><li>    PRIMARY KEY ((year, dev_id, day), rec_time)</li><li>    PRIMARY KEY ((year, day, dev_id), rec_time)</li></ol><h2>Results:</h2><p>We performed our experiments and the results were compiled in a publicly visible <a href=\"https://docs.google.com/spreadsheets/d/1Kk6Sgm_sfB34m8oKEOxspcwtZxxM_xwqmCQT6sT8H-Y/edit#gid=0\">sheet</a> shared here [6]. While we encourage the reader to look at it in details, here we replicate one table of the sheet for quick analysis.</p><table dir=\"ltr\" border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px;\"><colgroup><col width=\"58\" /><col width=\"46\" /><col width=\"117\" /><col width=\"101\" /><col width=\"102\" /><col width=\"118\" /><col width=\"111\" /><col width=\"99\" /><col width=\"100\" /><col width=\"108\" /><col width=\"106\" /><col width=\"100\" /></colgroup><tbody><tr><td colspan=\"2\" rowspan=\"1\" data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;Parameters&quot;}\">Parameters</td><td colspan=\"9\" rowspan=\"1\" data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;PRIMARY KEY&quot;}\">PRIMARY KEY</td><td></td></tr><tr><td colspan=\"2\" rowspan=\"1\"></td><td colspan=\"5\" rowspan=\"1\" data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;Results of Range Queries&quot;}\">Results of Range Queries</td><td colspan=\"4\" rowspan=\"1\" data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;Result of IN() Queries&quot;}\">Result of IN() Queries</td><td></td></tr><tr><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;Devices&quot;}\">Devices</td><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;Days&quot;}\">Days</td><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;((dev_id, rec_time), day)&quot;}\">((dev_id, rec_time), day)</td><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;(dev_id, day, rec_time)&quot;}\">(dev_id, day, rec_time)</td><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;(day, dev_id, rec_time)&quot;}\">(day, dev_id, rec_time)</td><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;((day, dev_id), rec_time)&quot;}\">((day, dev_id), rec_time)</td><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;((dev_id, day ), rec_time)&quot;}\">((dev_id, day ), rec_time)</td><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;(dev_id, day, rec_time)&quot;}\">(dev_id, day, rec_time)</td><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;(day, dev_id, rec_time)&quot;}\">(day, dev_id, rec_time)</td><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;((day, dev_id), rec_time)&quot;}\">((day, dev_id), rec_time)</td><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;((dev_id, day ), rec_time)&quot;}\">((dev_id, day ), rec_time)</td><td></td></tr><tr><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:1}\">1</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:7}\">7</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0033810465046296293}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">04:52.122</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0000026631828703703703}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:00.230</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.000034860300925925924}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:03.012</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003967326655092592}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:42.777</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003681333287037037}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:18.067</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.000038440127314814816}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:03.321</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0007922761921296297}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">01:08.453</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004947186435185186}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">07:07.437</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0046534990625}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:42.062</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:7.435185185185186e-8}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:00.006</td></tr><tr><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:1}\">1</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:15}\">15</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004337737326388889}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:14.781</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.000003232465277777778}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:00.279</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0000354265625}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:03.061</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003946980011574074}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:41.019</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003609857824074074}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:11.892</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.00003457298611111111}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:02.987</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0016276647222222224}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">02:20.630</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004781432928240741}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:53.116</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004341447083333333}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:15.101</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:9.082175925925925e-8}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:00.008</td></tr><tr><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:1}\">1</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:31}\">31</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.006014227534722223}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">08:39.629</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.000005120648148148148}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:00.442</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.00003518762731481482}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:03.040</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003977694305555555}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:43.673</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003711375648148148}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:20.663</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.00003422395833333333}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:02.957</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0031992222685185184}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">04:36.413</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004778606898148148}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:52.872</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004523499444444444}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:30.830</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:1.2386574074074073e-7}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:00.011</td></tr><tr><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:1}\">1</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:62}\">62</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.008353146516203704}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">12:01.712</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.000007675104166666666}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:00.663</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.00003424778935185185}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:02.959</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0039016963425925924}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:37.107</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0035999456944444445}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:11.035</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.00003884090277777777}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:03.356</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0070086818750000005}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">10:05.550</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.005190131238425926}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">07:28.427</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004680102743055556}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:44.361</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:1.364236111111111e-7}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:00.012</td></tr><tr><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:200}\">200</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:7}\">7</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003355122141203704}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">04:49.883</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.00014877582175925927}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:12.854</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.005958901122685186}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">08:34.849</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0038958906250000005}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:36.605</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003577375902777778}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:09.085</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0014223928935185185}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">02:02.895</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0007384906481481481}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">01:03.806</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004818106400462963}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:56.284</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004549877256944445}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:33.109</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0000022571180555555557}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:00.195</td></tr><tr><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:200}\">200</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:15}\">15</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004297555648148148}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:11.309</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.00030386327546296296}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:26.254</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.00585302525462963}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">08:25.701</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003847285798611111}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:32.405</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0035731719328703703}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:08.722</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0014106333680555555}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">02:01.879</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0016743773379629629}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">02:24.666</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0050338127777777774}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">07:14.921</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0044997042129629634}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:28.774</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.000004611481481481481}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:00.398</td></tr><tr><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:200}\">200</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:31}\">31</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0059676143981481486}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">08:35.602</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0006084985763888889}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:52.574</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0058990232407407405}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">08:29.676</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003912620474537037}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:38.050</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003633043587962963}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:13.895</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0014473511689814815}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">02:05.051</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0033170381597222224}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">04:46.592</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004918577534722223}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">07:04.965</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004384375324074074}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:18.810</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.000005602800925925926}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:00.484</td></tr><tr><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:200}\">200</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:62}\">62</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.008374208981481482}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">12:03.532</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0011038888425925926}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">01:35.376</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.006141329837962963}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">08:50.611</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004063637511574074}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:51.098</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0037832920370370372}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:26.876</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.001436457488425926}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">02:04.110</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.006365348981481481}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">09:09.966</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004726526493055556}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:48.372</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0044583690625}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:25.203</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.000022921412037037037}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:01.980</td></tr><tr><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;*All&quot;}\">*All</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:7}\">7</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003367778287037037}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">04:50.976</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0007369423032407408}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">01:03.672</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.005769388043981481}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">08:18.475</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003861304490740741}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:33.617</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003487727175925926}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:01.340</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.007753202800925925}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">11:09.877</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0007695971296296295}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">01:06.493</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004639710219907407}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:40.871</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004197842268518518}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:02.694</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0000073612384259259255}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:00.636</td></tr><tr><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;*All&quot;}\">*All</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:15}\">15</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004268437083333333}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:08.793</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0015203506481481482}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">02:11.358</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.005865909502314815}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">08:26.815</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0038259733564814814}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:30.564</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003527004664351852}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:04.733</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.007744230034722223}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">11:09.101</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0015461416435185184}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">02:13.587</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004838077141203704}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:58.010</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0048064288310185185}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:55.275</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.000012391597222222223}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:01.071</td></tr><tr><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;*All&quot;}\">*All</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:31}\">31</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.005901403564814814}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">08:29.881</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0030778950578703706}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">04:25.930</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.005901267731481481}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">08:29.870</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0038836484722222222}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:35.547</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003554246261574074}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:07.087</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.008092113958333333}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">11:39.159</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.003171625057870371}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">04:34.028</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004777974583333333}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:52.817</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0041682598379629635}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:00.138</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.00004259931712962963}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:03.681</td></tr><tr><td data-sheets-value=\"{&quot;1&quot;:2,&quot;2&quot;:&quot;*All&quot;}\">*All</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:62}\">62</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.008221841087962964}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">11:50.367</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.005406336770833333}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">07:47.107</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.005830159722222222}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">08:23.726</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0037742180208333335}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:26.092</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.0035038981944444446}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">05:02.737</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.007737928217592593}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">11:08.557</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.006104637106481482}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">08:47.441</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004867551064814815}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">07:00.556</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.004536799224537037}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">06:31.979</td><td data-sheets-value=\"{&quot;1&quot;:3,&quot;3&quot;:0.00007961810185185186}\" data-sheets-numberformat=\"[null,6,&quot;mm\\&quot;:\\&quot;ss\\&quot;.\\&quot;000&quot;,1]\">00:06.879</td></tr></tbody></table><h2>Evaluation of Results</h2><p>Here we will briefly discuss the behavior of each PRIMARY KEY with respect to query time and its effect on C* cluster.</p><ol><li>PRIMARY KEY ((dev_id, rec_time), day) :  Our queries are based on day and dev id not on rec_time, so having rec_time before day proved to be a worst case in most of cases.</li><li>PRIMARY KEY (dev_id, day, rec_time) : It seems to be the best case in terms of execution time in both range queries and IN() queries. This has a major problem of large partition size because all data of one device is stored in single partition.</li><li>PRIMARY KEY (day, dev_id, rec_time) : With range queries it didn’t performed well but with IN() queries the execution time is directly proportional to number of days irrespective of number of devices. This has problem of hotspot in C* cluster as single partition contains data of whole day which also results in large partition size.</li><li>PRIMARY KEY ((day, dev_id), rec_time) : The execution time is directly proportional to total table size, almost same for different number of days and number of devices. Although it solves the problem of large partitions and hotspot in C* cluster.</li><li>PRIMARY KEY ((dev_id, day), rec_time) : Its behaviour is almost similar to 4th one.</li><li>PRIMARY KEY ((year, dev_id), day, rec_time) : It is good for lower number of devices but worst cast for higher number of devices. It somehow tries to solve the problem of all data of single device in one partition as it created a partition over year and device id.</li><li>PRIMARY KEY ((year, day), dev_id, rec_time) : The execution time only depends on number of days only, irrespective of number of devices being queried but it is never the worst case. Moreover it has same hotspot issue as mentioned in 2nd PRIMARY KEY.</li><li>PRIMARY KEY ((year, dev_id, day), rec_time) : Its behavior is also to 4th PRIMARY KEY. Moreover adding year field just adds an additional cost as date is already there.</li><li>PRIMARY KEY ((year, day, dev_id), rec_time) : Its behavior is similar to 8th PRIMARY KEY.</li></ol><h3>Other Observations:</h3><p>We observed that having a multi-field partition key allows for fast querying only if the “=” is used going left to right. If an IN() (for specifying eg. range of time or list of devices) is used once that order, than any further usage of IN() removes any benefit (i.e. a near full table scan).</p><p>Another useful observation was that using the IN() to query for days is less useful than putting in a range query.</p><h2>Conclusion:</h2><blockquote class=\"\"><h5 class=\"blockquote-text\">As such, it seems that using Cassandra for serving OLTP and OLAP queries for an IoT use-case is not, perhaps, the best decision. This requires us to know combine Cassandra's industry strength robustness and performance, with another DB that enables fast queries for the typical IoT use case.</h5></blockquote><p>The main takeaways from this benchmarking is that we don’t have a single schema to answer our (IoT) use case without any drawbacks. Thus while the ((day, dev_id), rec_time) gives a constant response, it is dependent entirely on the total data size (full scan). On the other hand, (dev_id, day, rec_time) and its counterpart (day, dev_id, rec_time) provide acceptable results, we have the issue of very large partition space in the first, and hotspot while writing for the latter case.</p><p>As such, it seems that using Cassandra for serving OLTP and OLAP queries for an IoT use-case is not, perhaps, the best decision. This requires us to know combine Cassandra’s industry strength robustness and performance, with another DB that enables fast queries for the typical IoT use case. We are currently evaluating Druid [3], InfluxDB [4], and Kudu [5]  as companion database in this regard. We will keep everyone posted about the final decision and performance results.</p><h4><strong>About Authors</strong></h4><p><em>Arbab is Design Engineer at AN10. akhalil@an10.io<br /></em><em>Affan Syed is Head of Engineering at AN10. <strong><a href=\"https://www.linkedin.com/in/affan-ahmed-syed/\">LinkedIn</a></strong></em></p><h3>References[1] <a href=\"https://www.oreilly.com/ideas/the-smack-stack\">https://www.oreilly.com/ideas/the-smack-stack</a>[2] <a href=\"http://thelastpickle.com/blog/2017/08/02/time-series-data-modeling-massive-scale.html\">http://thelastpickle.com/blog/2017/08/02/time-series-data-modeling-massive-scale.html</a>[3] <a href=\"http://druid.io/\">http://druid.io/</a>[4] <a href=\"https://www.influxdata.com/time-series-platform/influxdb/\">https://www.influxdata.com/time-series-platform/influxdb/</a>[5] <a href=\"https://kudu.apache.org/\">https://kudu.apache.org/</a>[6] <a href=\"https://docs.google.com/spreadsheets/d/1Kk6Sgm_sfB34m8oKEOxspcwtZxxM_xwqmCQT6sT8H-Y/edit#gid=0\">https://docs.google.com/spreadsheets/d/1Kk6Sgm_sfB34m8oKEOxspcwtZxxM_xwqmCQT6sT8H-Y/edit#gid=0</a></h3>",
        "created_at": "2018-08-28T20:11:25+0000",
        "updated_at": "2018-08-28T20:11:37+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 12,
        "domain_name": "www.an10.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12013"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 190,
            "label": "sql",
            "slug": "sql"
          }
        ],
        "is_public": false,
        "id": 12010,
        "uid": null,
        "title": "Spark SQL Against Cassandra Example - DZone Database",
        "url": "https://dzone.com/articles/spark-sql-against-cassandra",
        "content": "<div class=\"content-html\" itemprop=\"text\"><p>Spark SQL is awesome.  It allows you to query any Resilient Distributed Dataset (RDD) using SQL (including data stored in Cassandra!).</p><p>First thing to do is to create a SQLContext from your SparkContext.  I'm using Java so... <br />(sorry -- I'm still not hip enough for Scala) </p>\n<div>\n <pre lang=\"text/x-java\">JavaSparkContext context =new JavaSparkContext(conf);\nJavaSQLContext sqlContext =new JavaSQLContext(context);</pre>\n</div>\n<p> Now you have a SQLContext, but you have no data.  Go ahead and create an RDD, just like you would in regular Spark:</p>\n<div>\n <pre lang=\"text/x-java\">JavaPairRDD&lt;Integer, Product&gt; productsRDD = \n  javaFunctions(context).cassandraTable(\"test_keyspace\", \"products\",\n    productReader).keyBy(new Function&lt;Product, Integer&gt;() {\n  @Override\n  public Integer call(Product product) throws Exception {\n    return product.getId();\n  }\n});</pre>\n <br /></div>\n<p> (The example above comes from the <a href=\"https://github.com/boneill42/spark-on-cassandra-quickstart\" rel=\"nofollow\">spark-on-cassandra-quickstart project</a>, as described in <a href=\"http://brianoneill.blogspot.com/2015/04/holy-momentum-batman-spark-and.html\" rel=\"nofollow\">my previous post</a>.)</p>\n<p> Now that we have a plain vanilla RDD,  we need to spice it up with a schema, and let the sqlContext know about it.  We can do that with the following lines:</p>\n<div>\n <pre lang=\"text/x-java\">JavaSchemaRDD schemaRDD =   sqlContext.applySchema(productsRDD.values(), Product.class);        \nsqlContext.registerRDDAsTable(schemaRDD, \"products\");   </pre>\n <br /></div>\n<p> Shazam.  Now your sqlContext is ready for querying.  Notice that it inferred the schema from the Java bean. (Product.class).  (Next blog post, I'll show how to do this dynamically)</p>\n<p> You can prime the pump with a:</p>\n<div>\n <pre lang=\"text/x-java\">System.out.println(\"Total Records = [\" + productsRDD.count() + \"]\");</pre>\n <br /></div>\n<p> The count operation forces Spark to load the data into memory, which makes queries like the following lightning fast:</p>\n<div>\n <pre lang=\"text/x-java\">JavaSchemaRDD result = sqlContext.sql(\"SELECT id from products WHERE price &lt; 0.50\");\nfor (Row row : result.collect()){\n  System.out.println(row);\n}</pre>\n</div>\n<p> That's it.  You're off to the SQL races.</p>\n<div>\n <div>\n  <p> <em>P.S.  If you try querying the sqlContext without applying a schema and/or without registering the RDD as a table, you may see something similar to this:</em></p>\n  <pre lang=\"text/x-java\">Exception in thread \"main\" org.apache.spark.sql.catalyst.errors.package$TreeNodeException: Unresolved attributes: 'id, tree:\n'Project ['id]\n 'Filter ('price &lt; 0.5)\n  NoRelation$\n</pre>\n </div>\n</div></div><div class=\"content-html\" itemprop=\"text\"><a>\n                        <img class=\"pub-image\" width=\"420\" itemprop=\"image\" src=\"src\" alt=\"image\" /></a></div>",
        "created_at": "2018-08-28T19:46:44+0000",
        "updated_at": "2018-08-28T19:47:00+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "dzone.com",
        "preview_picture": "https://dz2cdn2.dzone.com/storage/article-thumb/10877-thumb.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12010"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 1333,
            "label": "shell",
            "slug": "shell"
          }
        ],
        "is_public": false,
        "id": 12008,
        "uid": null,
        "title": "The Correct Way to Connect Spark Shell to Cassandra",
        "url": "https://mattilyra.github.io/2016/05/04/spark-shell-cassandra-connection.html",
        "content": "<p>tl;dr</p><ul><li>using the cassandra connector in the spark-shell is fairly straightforward</li>\n  <li>setting up the connection in a way that doens’t break the existing <code class=\"highlighter-rouge\">sc</code> is not documented anywhere</li>\n  <li><a href=\"#solution\">the correct solution</a> is to not call <code class=\"highlighter-rouge\">sc.stop</code> but provide the cassandra host on startup of the shell</li>\n</ul>\n<p>Apache Cassandra is a NoSQL distributed database that’s been gaining popularity recently. It’s also pretty high performance, scoring very high in a (not so) recent <a href=\"http://vldb.org/pvldb/vol5/p1724_tilmannrabl_vldb2012.pdf\" title=\"Solving Big Data Challenges for Enterprise Application Performance Management [Rabl et. al 2012] (PDF)\">comparison of key-value stores</a> (PDF) for different workloads. Among the contenders were HBase, Cassandra, Voldemort, Redis, VoltDB and MySQL, HBase tends to be the winner (by one to two orders of magnitude) when it comes to latency and Cassandra when it comes to throughput - depending on the number of nodes in cluster. A key-value store is nice, but it isn’t much use unless you have something doing reads and writes into it. That’s where <code class=\"highlighter-rouge\">spark</code> comes in.</p>\n<p>Every data scientist’s<sup><a href=\"https://en.wikipedia.org/wiki/Data_science\">[1]</a> <a href=\"https://www-01.ibm.com/software/data/infosphere/data-scientist/\">[2]</a><a href=\"#footnote\" title=\"I don't like the term either but that's what we seem to have settled for.\">[3]</a></sup> favourite new toy <code class=\"highlighter-rouge\">spark</code> is a distributed in-memory data processing framework. Cassandra very helpfully comes with a <code class=\"highlighter-rouge\">spark</code> connector that allows you to pull data into spark as <code class=\"highlighter-rouge\">RDD</code>s or <code class=\"highlighter-rouge\">DataFrame</code>s directly from Cassandra.</p>\n<p>Connecting to a Cassandra host from <code class=\"highlighter-rouge\">spark</code> isn’t all that complicated, just import the connector and tell <code class=\"highlighter-rouge\">SparkConf</code> where to find the Cassandra host from and you’re off to the races. The Cassandra connector <a href=\"https://github.com/datastax/spark-cassandra-connector/#documentation\">docs</a> cover the basic usage pretty well. Aside from the bazillion different versions of the connector getting everything up and running is fairly straightforward.</p>\n<p>Start the spark shell with the necessary Cassandra connector dependencies <code class=\"highlighter-rouge\">bin/spark-shell --packages datastax:spark-cassandra-connector:1.6.0-M2-s_2.10</code>.</p>\n<figure class=\"highlight\"><pre class=\"language-scala\" data-lang=\"scala\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n</pre><pre>import org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.SQLContext\nimport com.datastax.spark.connector._\n# connect to a local cassandra instance\nval conf = new SparkConf(true)\n    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\nval sc = new SparkContext(conf)\nval sqlContext = new SQLContext(sc)\n# read in some data as a DataFrame\nval df = sqlContext\n    .read\n    .format(\"org.apache.spark.sql.cassandra\")\n    .options(Map(\"table\" -&gt; \"fooTable\", \"keyspace\" -&gt; \"bar\")).load.cache()</pre></figure><p>Lovely, you now have a DataFrame that acts just like any other <code class=\"highlighter-rouge\">spark</code> DataFrame. So far so good. Now let’s say you wanted to test something in the <code class=\"highlighter-rouge\">spark-shell</code> and pull in data from Cassandra. No problem, just do what you did before, except that you need to stop the existing <code class=\"highlighter-rouge\">SparkContext</code> that is created automagically when the shell starts up, before you can create a new one. This isn’t really documented anywhere, except sporadically on <a href=\"https://stackoverflow.com/questions/25837436/how-to-load-spark-cassandra-connector-in-the-shell\">StackOverflow</a>. The accepted answer is actually the wrong way to do this.</p>\n<figure class=\"highlight\"><pre class=\"language-scala\" data-lang=\"scala\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n</pre><pre>// DO NOT DO THIS\nsc.stop // NOOOooo\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.apache.spark.sql.SQLContext\nimport com.datastax.spark.connector._\n// connect to a local cassandra instance\nval conf = new SparkConf(true)\n    .set(\"spark.cassandra.connection.host\", \"127.0.0.1\")\nval sc = new SparkContext(conf)\nval sqlContext = new SQLContext(sc)\n// read in some data as a DataFrame\nval df = sqlContext\n    .read\n    .format(\"org.apache.spark.sql.cassandra\")\n    .options(Map(\"table\" -&gt; \"fooTable\", \"keyspace\" -&gt; \"bar\")).load.cache()</pre></figure><p>The <code class=\"highlighter-rouge\">SparkContext</code> created above will not function like the old <code class=\"highlighter-rouge\">SparkContext</code> created when the shell started up. This doesn’t actually have anything to do the Cassandra connector perse, it’s just that the setup for the Cassandra connector brings up this issue. To see the problem consider the following simplified code without the Cassandra connector.</p>\n<figure class=\"highlight\"><pre class=\"language-scala\" data-lang=\"scala\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n</pre><pre>import org.apache.spark.SparkConf\nimport org.apache.spark.SparkContext\nsc.stop\nval conf = sc.getConf\nval sc = new SparkContext(conf)\nval rdd = sc.parallelize(Array(0, 1), (1, 10), (2, 15), (3, 7))\nval map = Map(0-&gt;3, 1-&gt;2, 2-&gt;1, 3-&gt;0)\nval BVmap = sc.broadcast(map)\nrdd.map( r =&gt; BVmap.value(r._1))</pre></figure><p>The above doesn’t do anything particularly worthwhile, but it illustrates the problem. Because the <code class=\"highlighter-rouge\">SparkContext</code> was recreated the code will fail in the shell, due to <code class=\"highlighter-rouge\">sc</code> being not serialisable anymore.</p>\n<p>The solution is extremely simple, but suprisingly difficult to find. Instead of calling <code class=\"highlighter-rouge\">sc.stop</code> and then recreating the <code class=\"highlighter-rouge\">conf</code> with the Cassandra host details added, just add the Cassandra host details to the configuration defaults in <code class=\"highlighter-rouge\">$SPARK_HOME/conf/spark-defaults.conf</code>. Should you not have access to the default conf you can also provide the connection host in the call to <code class=\"highlighter-rouge\">spark-shell</code></p>\n<p><code class=\"highlighter-rouge\">bin/spark-shell --packages datastax:spark-cassandra-connector:1.6.0-M2-s_2.10 --conf spark.cassandra.connection.host=127.0.0.1</code></p>\n<p>This not being included in the official Cassandra connector documentation is bizarre.</p>\n<p>[3] I don’t like the term either but that’s what we seem to have settled for.</p>\n<ul><li><a href=\"http://spark.apache.org\">Spark</a></li>\n  <li><a href=\"http://cassandra.apache.org\">Cassandra</a></li>\n  <li><a href=\"https://github.com/datastax/spark-cassandra-connector\">Spark Cassandra Connector</a></li>\n</ul>",
        "created_at": "2018-08-28T19:36:19+0000",
        "updated_at": "2018-08-28T19:36:45+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 4,
        "domain_name": "mattilyra.github.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12008"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1280,
            "label": "cql",
            "slug": "cql"
          }
        ],
        "is_public": false,
        "id": 12003,
        "uid": null,
        "title": "Using Apache Cassandra — A few things before you start",
        "url": "https://hackernoon.com/using-apache-cassandra-a-few-things-before-you-start-ac599926e4b8?gi=3cc903a7236c",
        "content": "<p id=\"8aee\" class=\"graf graf--p graf-after--h3\">The <a href=\"https://www.slideshare.net/planetcassandra/cassandra-summit-2014-cql-under-the-hood-39445761\" data-href=\"https://www.slideshare.net/planetcassandra/cassandra-summit-2014-cql-under-the-hood-39445761\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">CQL</a> — Cassandra Query language gives an almost SQL type interface to Apache Cassandra. I have found many times,that many who use this,do not know about some important points of Cassandra that makes it different from SQL databases like Postgres. Same is the case for operations team, there are some aspects related to storage, GC settings , that many are not aware of. I am not an expert in Cassandra internals and don’t aspire to be if I can avoid it.This is mostly a note to myself, and something which I can ask others to refer to instead of repeating over email a gazillion times. There are lot of other parts like repair etc which I have left out. The intention here is to make this as short as possible, but if you feel somethings are to be added, please comment.</p><h4 id=\"2e37\" class=\"graf graf--h4 graf-after--p\">Cassandra has Tune-able Consistency — not just eventual consistency</h4><p id=\"0963\" class=\"graf graf--p graf-after--h4\">Many considering Cassandra as a replacement for SQL database like Postgres, MySQL or Oracle, shy away thinking that eventual consistency of NoSQL does not meet their requirement. In Cassandra ,however consistency is configurable. This means that with some write and read speed sacrifice, you can have strong consistency as well as high availability. Cassandra can be used for small data as well as big data; depending on your use case you can tune the consistency per key-space or even per-operation.</p><blockquote id=\"86c1\" class=\"graf graf--blockquote graf-after--p\"><div>Cassandra values Availability and Partitioning tolerance (AP). Tradeoffs between consistency and latency are tunable in Cassandra. You can get strong consistency with Cassandra (with an increased latency).<br /><a href=\"https://wiki.apache.org/cassandra/ArchitectureOverview\" data-href=\"https://wiki.apache.org/cassandra/ArchitectureOverview\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\">https://wiki.apache.org/cassandra/ArchitectureOverview</a></div></blockquote><p id=\"712d\" class=\"graf graf--p graf-after--blockquote\">At this point it may be a good idea to have a short recap of CAP theorem as there is a lot of confusion translating the theoretical surmise to the practical world.</p><blockquote id=\"e767\" class=\"graf graf--blockquote graf-after--p\"><div>In 2000, Dr. Eric Brewer gave a keynote at the <em class=\"markup--em markup--blockquote-em\">Proceedings of the Annual ACM Symposium on Principles of Distributed Computing</em> in which he laid out his famous CAP Theorem: <em class=\"markup--em markup--blockquote-em\">a shared-data system can have at most two of the three following properties: </em><strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\">C</em></strong><em class=\"markup--em markup--blockquote-em\">onsistency, </em><strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\">A</em></strong><em class=\"markup--em markup--blockquote-em\">vailability, and tolerance to network </em><strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\">P</em></strong><em class=\"markup--em markup--blockquote-em\">artitions.</em></div></blockquote><p id=\"cb8b\" class=\"graf graf--p graf-after--blockquote\">This applies to any distributed data base, not just Cassandra.So Cassandra can provide C and A not P ? Is it a big problem ?</p><p id=\"c5f4\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Short answer — It is not. Skip the rest of the section if you are in a hurry.</strong></p><p id=\"b375\" class=\"graf graf--p graf-after--p\">Long answer read on.Here is the excerpt from Cassandra docs. (DataStax’s docs)</p><blockquote id=\"82aa\" class=\"graf graf--blockquote graf-after--p\"><div>… You can tune Cassandra’s consistency level per-operation, or set it globally for a cluster or datacenter. You can vary the consistency for individual read or write operations so that the data returned is more or less consistent, as required by the client application. This allows you to make Cassandra act more like a CP (consistent and partition tolerant) or AP (highly available and partition tolerant) system according to the CAP theorem, depending on the application requirements.</div></blockquote><blockquote id=\"8be4\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">Note:</strong> It is not possible to “tune” Cassandra into a completely CA system. See <a href=\"https://codahale.com/you-cant-sacrifice-partition-tolerance/\" data-href=\"https://codahale.com/you-cant-sacrifice-partition-tolerance/\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener\" target=\"_blank\">You Can’t Sacrifice Partition Tolerance</a> for a more detailed discussion. -<a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDataConsistency.html#dmlAboutDataConsistency__eventual-consistency\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDataConsistency.html#dmlAboutDataConsistency__eventual-consistency\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDataConsistency.html</a></div></blockquote><p id=\"f1c2\" class=\"graf graf--p graf-after--blockquote\">Here is an excerpt from the article linked in the DataStax’ s Cassandra documentation page.</p><blockquote id=\"15b8\" class=\"graf graf--blockquote graf-after--p\"><div>Of the CAP theorem’s Consistency, Availability, and Partition Tolerance, <strong class=\"markup--strong markup--blockquote-strong\">Partition Tolerance is mandatory in distributed systems. You cannot not choose it. </strong>Instead of CAP, you should think about your availability in terms of <em class=\"markup--em markup--blockquote-em\">yield</em> (percent of requests answered successfully) and <em class=\"markup--em markup--blockquote-em\">harvest</em> (percent of required data actually included in the responses) and which of these two your system will sacrifice when failures happen. -<a href=\"https://codahale.com/you-cant-sacrifice-partition-tolerance/\" data-href=\"https://codahale.com/you-cant-sacrifice-partition-tolerance/\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://codahale.com/you-cant-sacrifice-partition-tolerance/</a></div></blockquote><p id=\"c333\" class=\"graf graf--p graf-after--blockquote\">What the above article explains in depth is that Availability is tied to Network Partitioning or Partition Tolerance.Worst case scenario network partitions are quite rare inside a Data Center network. Also network partitions cannot be prevented from happening. It is ever present, though mostly transient and intermittent. The risk of network partitioning across many nodes in a cluster so as to disrupt Availability for a multi-node cluster is very less.</p><p id=\"ddd6\" class=\"graf graf--p graf-after--p\">So with Cassandra you can have as good a C and A system as practically possible.</p><h4 id=\"f386\" class=\"graf graf--h4 graf-after--p\"><strong class=\"markup--strong markup--h4-strong\">Give Importance to modelling the Partition key</strong></h4><p id=\"607e\" class=\"graf graf--p graf-after--h4\">If there is only one thing that you should read,maybe it is the link below</p><p><a href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" data-href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">The most important thing to know in Cassandra data modeling: The primary key</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Patrick is regarded as one of the foremost experts of Apache Cassandra and data modeling techniques. As the Chief…</em>www.datastax.com</a></p><p id=\"958c\" class=\"graf graf--p graf-after--mixtapeEmbed\">What is the Partition Key ? It is the first part of the Primary Key or the Primary key itself if Primary key is not composite</p><p id=\"5327\" class=\"graf graf--p graf-after--p\">Why is this most important part ?</p><p id=\"fbfc\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">To have balanced write of data to multiple Cassandra nodes in the cluster and subsequent balanced reads of the data.</strong></p><blockquote id=\"fe60\" class=\"graf graf--blockquote graf-after--p\"><div>When data is inserted into the cluster, the first step is to apply a hash function to the partition key. The output is used to determine what node (and replicas) will get the data.-<a href=\"https://docs.datastax.com/en/archived/cassandra/3.x/cassandra/architecture/archPartitionerM3P.html\" data-href=\"https://docs.datastax.com/en/archived/cassandra/3.x/cassandra/architecture/archPartitionerM3P.html\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.datastax.com/en/archived/cassandra/3.x/cassandra/architecture/archPartitionerM3P.html</a></div></blockquote><p id=\"1e3d\" class=\"graf graf--p graf-after--blockquote\">Here are two main goals to consider for modelling the data</p><p><a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" data-href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Basic Rules of Cassandra Data Modeling</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Learn more about Apache Cassandra and data modeling READ MORE DS:220 COURSE Picking the right data model is the hardest…</em>www.datastax.com</a></p><p id=\"1dab\" class=\"graf graf--p graf-after--mixtapeEmbed\">1. Spread data evenly around the cluster — Model Partition Key</p><p id=\"8ae6\" class=\"graf graf--p graf-after--p\">2. Minimize the number of partitions read -Model Partition Key and Clustering keys</p><p id=\"9ea6\" class=\"graf graf--p graf-after--p\">Let us take an example. Below is a initial modelling of table where the data is some events (say political rallies, speeches etc) that has occurred in a particular location, centered over latitude,longitude and say having a radius of 250 meters. Each location has an influential candidate of that area. Sometimes the same area can have multiple influential candidates. I have illustrated a slightly complex example so as to show the flexibility in data types present in Cassandra,and all the aspects to consider when modelling the key. The actual cell can be a telecom cell with multiple coverage by different operators or different technologies. The example I give here is a bit contrived and for illustration only.</p><figure id=\"716e\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"8775\" class=\"graf graf--p graf-after--figure\">Note that partition key is chosen assuming that events are distributed evenly across a city, the key will distribute the data from multiple locations evenly across available Cassandra nodes.</p><p id=\"9ffa\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">The partition query should be modeled for efficient retrieval of the data application needs</strong></p><p id=\"9633\" class=\"graf graf--p graf-after--p\">The modelling of Tables and thereby the partition key is primarily with consideration of efficient data retrieval.</p><p id=\"02b8\" class=\"graf graf--p graf-after--p\">Assume that we need to query all events happening in a location by a Candidate for a time interval.The queries will be a set of statements like</p><blockquote id=\"ec1c\" class=\"graf graf--blockquote graf-after--p\"><div>select * from demo_table where bin_cell_key = (1234, 222, ‘Candidate1’) and time_key &gt;= (‘06:00:00.000000000’, ‘06:00:00.000000000’) and time_key &lt; (‘07:30:00.000000000’, ‘07:30:00.000000000’)</div></blockquote><blockquote id=\"6b6b\" class=\"graf graf--blockquote graf-after--blockquote\"><div>select * from demo_table where bin_cell_key = (1234, 223, ‘Candidate1’) ..</div></blockquote><p id=\"feeb\" class=\"graf graf--p graf-after--blockquote\">But to compose this <em class=\"markup--em markup--p-em\">bin_cell_key</em> we need to know first which Candidates are there in which locations. For this we need to model helper tables. Note — Data duplication is okay in NoSQL data modelling and to have the same effect of JOINs this is needed. Some helper tables to get the bin_cell_key</p><blockquote id=\"42fa\" class=\"graf graf--blockquote graf-after--p\"><div>create table cell_bin (cell text, bin tuple&lt;int,int&gt;, PRIMARY KEY (cell,bin));</div></blockquote><p id=\"ac52\" class=\"graf graf--p graf-after--blockquote\">Example CQL:</p><blockquote id=\"011b\" class=\"graf graf--blockquote graf-after--p\"><div>select * from cell_bin where cell=’Candidate1’;</div></blockquote><blockquote id=\"f71c\" class=\"graf graf--blockquote graf-after--blockquote\"><div>cell | bin<br /> — — — — -+ — — — — —-<br />Candidate1| (1234, 222)<br />Candidate1| (1234, 223)</div></blockquote><p id=\"f849\" class=\"graf graf--p graf-after--blockquote\">And similarly for the other way round</p><blockquote id=\"caf0\" class=\"graf graf--blockquote graf-after--p\"><div>create table bin_cell (bin tuple&lt;int,int&gt;, cell text, PRIMARY KEY (bin,cell));</div></blockquote><p id=\"21a2\" class=\"graf graf--p graf-after--blockquote\">Example CQL:</p><blockquote id=\"0774\" class=\"graf graf--blockquote graf-after--p\"><div>cqlsh:demo_keyspace&gt; select * from bin_cell where bin = (1234, 222);<br /> bin | cell<br /> — — — — -+ — — — —— -<br /> (1234, 222) | Candidate1<br /> (1234, 222) | Candidate2</div></blockquote><p id=\"fca6\" class=\"graf graf--p graf-after--blockquote\">We can stop here. But if you are curious read on. What if we want to aggregate all events that has taken place in a region irrespective of the Candidate. For this we need to split the cell out. Why ? because in case of a composite partition key all the elements need to be specified in the query.</p><blockquote id=\"ebf9\" class=\"graf graf--blockquote graf-after--p\"><div>select * from demo_table where bin = (1234,222) and year=2017 and month=12 and day=1;</div></blockquote><p id=\"81aa\" class=\"graf graf--p graf-after--blockquote\">The table for below which also adds time to the partition key so that data from different days are distributed across available nodes are given below.</p><figure id=\"2100\" class=\"graf graf--figure graf--iframe graf-after--p\"><blockquote id=\"861f\" class=\"graf graf--blockquote graf-after--figure\"><div>create table demo_table( year int,month int,day int, bin tuple&lt;double,double&gt;, cell text, time_key tuple&lt;timestamp,timestamp&gt;,event text, PRIMARY KEY((bin,year,month,day),cell,time_key));</div></blockquote><h4 id=\"040b\" class=\"graf graf--h4 graf-after--blockquote\">Test and measure your reads &amp; write via nodetool cfstats</h4><p id=\"f60b\" class=\"graf graf--p graf-after--h4\">How do we know if our data model is distributing writes across nodes. How do we know if the write latency and read latency is distributed across nodes and if it is linearly scale able in proportional to the nodes added. The answer to all of this via node tool cfstats command</p><p><a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCFstats.html\" data-href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCFstats.html\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCFstats.html\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">nodetool cfstats</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Provides statistics about tables.</em>docs.datastax.com</a></p><p id=\"0c51\" class=\"graf graf--p graf-after--mixtapeEmbed\">You need to run long runs with write and then read operations using multiple nodes and everyday update a table like one below based on the output from cfstats. Soon you will know if your write and read are balanced. Actually adding more nodes should also decrease your read time linearly. This is really beautiful.</p><figure id=\"f30b\" class=\"graf graf--figure graf--iframe graf-after--p\"><h4 id=\"73a0\" class=\"graf graf--h4 graf-after--figure\">Do not use ALLOW FILTERING and IN Clause in CQL indiscriminately</h4><p id=\"1258\" class=\"graf graf--p graf-after--h4\">If you feel that there is no way out; then please read the section above. Most probably your table modelling has to be refactored.</p><blockquote id=\"3ad8\" class=\"graf graf--blockquote graf-after--p\"><div>When your query is rejected by Cassandra because it needs filtering, you should resist the urge to just add ALLOW FILTERING to it. You should think about your data, your model and what you are trying to do. -<a href=\"https://www.datastax.com/dev/blog/allow-filtering-explained-2\" data-href=\"https://www.datastax.com/dev/blog/allow-filtering-explained-2\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://www.datastax.com/dev/blog/allow-filtering-explained-2</a></div></blockquote><p id=\"820a\" class=\"graf graf--p graf-after--blockquote\"><strong class=\"markup--strong markup--p-strong\">Or for that matter IF clause</strong></p><p id=\"fba3\" class=\"graf graf--p graf-after--p\">The new versions of Cassandra supports light-weight transactions. In CQL this is done via the IF clause. <em class=\"markup--em markup--p-em\">Insert into table IF NOT EXISTS</em>.</p><blockquote id=\"8bee\" class=\"graf graf--blockquote graf-after--p\"><a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlLtwtTransactions.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlLtwtTransactions.html\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener\" target=\"_blank\">Lightweight transactions</a><div> should not be used casually, as the latency of operations increases fourfold due to the due to the round-trips necessary between the CAS coordinators. -</div><a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useInsertLWT.html\" data-href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useInsertLWT.html\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.datastax.com/en/cql/3.3/cql/cql_using/useInsertLWT.html</a></blockquote><h4 id=\"185f\" class=\"graf graf--h4 graf-after--blockquote\">Be aware of the JVM GC Suck-age as JVM Heap Increases</h4><p id=\"e711\" class=\"graf graf--p graf-after--h4\">Cassandra runs on the JVM and relies on the OS page cache for improving performance. <em class=\"markup--em markup--p-em\">There is no need to throw huge amounts of RAM at Cassandra</em>. Cassandra performance should increase by adding more low powered nodes.We have run our long runs for about 2 weeks with load on 2 GB JVM heap, and Cassandra had never once gone down.</p><p id=\"df9c\" class=\"graf graf--p graf-after--p\">JVM GC suckage is directly proportional to the JVM heap. This is true for any Java process and also for Cassandra</p><figure id=\"be45\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*HQhjWbOdml9VTeSYo18Pdw.png\" data-width=\"533\" data-height=\"400\" src=\"https://cdn-images-1.medium.com/max/1600/1*HQhjWbOdml9VTeSYo18Pdw.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">source-<a href=\"https://www.slideshare.net/mattdennis/cassandra-antipatterns\" data-href=\"https://www.slideshare.net/mattdennis/cassandra-antipatterns\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\">https://www.slideshare.net/mattdennis/cassandra-antipatterns</a></figcaption></figure><p id=\"da9f\" class=\"graf graf--p graf-after--figure\"><em class=\"markup--em markup--p-em\">Some illuminating quotes</em></p><p id=\"3659\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">Many users new to Cassandra are tempted to turn up Java heap size too high, which consumes the majority of the underlying system’s RAM. In most cases, increasing the Java heap size is actually </em><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">detrimental</em></strong><em class=\"markup--em markup--p-em\"> for these reasons:</em></p><p id=\"3569\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">1</em>. <em class=\"markup--em markup--p-em\">In most cases, the capability of Java to gracefully handle garbage collection above 8GB quickly diminishes.</em></strong></p><p id=\"0c46\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">2.Modern operating systems maintain the OS page cache for frequently accessed data and are very good at keeping this data in memory, but can be prevented from doing its job by an elevated Java heap size.</em></p><p id=\"f7a1\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">If you have more than 2GB of system memory, which is typical, keep the size of the Java heap relatively small to allow more memory for the page cache .</em></p><p><a href=\"http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html\" data-href=\"http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Tuning Java resources</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Consider tuning Java resources in the event of a performance degradation or high memory consumption.</em>docs.datastax.com</a></p><p id=\"4efa\" class=\"graf graf--p graf-after--mixtapeEmbed\">Database like <a href=\"http://www.scylladb.com/open-source/\" data-href=\"http://www.scylladb.com/open-source/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Scylla DB</a> have ported the Cassandra design on to C++ so as to avoid the GC pauses and other such problems related to JVM. But as long as you keep the JVM heap around 8 GB things should be fine.</p><p id=\"e23d\" class=\"graf graf--p graf-after--p\">An update here- While I was working with Cassandra G1GC was not deemed stable enough to be used. Now DataStax version of Cassandra used G1GC. G1GC can handle larger heaps; however Cassandra can use RAM heavily for page caches, filters etc, so all the above still makes sense. Limit JVM heap to the minimum you need and leave the rest of the memory for Cassandra process.</p><h4 id=\"a0f6\" class=\"graf graf--h4 graf-after--p\">Cassandra does not like SAN/Shared Storage</h4><p id=\"c898\" class=\"graf graf--p graf-after--h4\">Cassandra is designed for the low fetch times of spinning disks by only appending data to the end for writes and minimize disk seek time during read via application selected partition keys and thereby spreading reads across multiple nodes — Regarding storage a good slide to go through <a href=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" data-href=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" class=\"markup--anchor markup--p-anchor\" title=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" rel=\"noreferrer noopener noopener noopener\" target=\"_blank\">https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra</a></p><figure id=\"9e1c\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*SEX2I_8ohef5sG65VuX5ow.png\" data-width=\"613\" data-height=\"449\" data-is-featured=\"true\" src=\"https://cdn-images-1.medium.com/max/1600/1*SEX2I_8ohef5sG65VuX5ow.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">source-<a href=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" data-href=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" class=\"markup--anchor markup--figure-anchor\" title=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" rel=\"noreferrer noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra</a></figcaption></figure><blockquote id=\"5b27\" class=\"graf graf--blockquote graf-after--figure\"><div>Customer/User — “We have an awesome SAN and would like to use it for Cassandra.”<br />DataStax — “We don’t recommend shared storage for Cassandra.”<br />Customer/User — “Why not.”<br />DataStax — “Two reasons really. One — performance suffers. Two — shared storage introduces a single point of failure into the architecture.”<br />Customer/User — “Our SAN is awesome and has never had any down time and can preform a kagillion IOPS. So why exactly shouldn’t we use shared storage.”</div></blockquote><blockquote id=\"8db5\" class=\"graf graf--blockquote graf-after--blockquote\"><a href=\"https://www.datastax.com/dev/blog/impact-of-shared-storage-on-apache-cassandra\" data-href=\"https://www.datastax.com/dev/blog/impact-of-shared-storage-on-apache-cassandra\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\">https://www.datastax.com/dev/blog/impact-of-shared-storage-on-apache-cassandra</a></blockquote><h4 id=\"2539\" class=\"graf graf--h4 graf-after--blockquote\">Horizontal Scale-ability -Transparent Sharding vs Application Level Sharding</h4><p id=\"f4c4\" class=\"graf graf--p graf-after--h4\">Database sharding is a way of horizontally scaling database. Application level sharding means that the logic of partitioning data across multiple node is done at the application level. That is based on some key- classic example — East Coast vs West Coast or <a href=\"http://www.malinga.me/application-aware-sharding-for-a-mysql-database/\" data-href=\"http://www.malinga.me/application-aware-sharding-for-a-mysql-database/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">similar</a>; your DB write code will select a Database to connect , write and read. The problem is that the complexity of application level sharding quickly gets complex and it is not a good way to scale . Cassandra and most NoSQL databases does sharding transparently (as you have seen via the partition key). This is a pretty big advantage as without this horizontal scaling is a hard problem.</p><h4 id=\"db8e\" class=\"graf graf--h4 graf-after--p\">Open-Source and Commercial</h4><p id=\"a666\" class=\"graf graf--p graf-after--h4\">As of the time of writing this, relations between Apache Foundation and Datastax- which was one of the largest contributor to Cassandra ? have soured. There is an commercial version of Cassandra — Datastax Enterprise Edition and open source version is the Apache Cassandra. The Java driver of Cassandra has two version, the open source and DSE provided, and you cannot use commercial driver with open source Cassandra. For other languages like Go the driver is open source.</p><figure id=\"05af\" class=\"graf graf--figure graf-after--p graf--trailing\"><div class=\"aspectRatioPlaceholder is-locked\"><a href=\"https://goo.gl/w4Pbea\" data-href=\"https://goo.gl/w4Pbea\" class=\"graf-imageAnchor\" data-action=\"image-link\" data-action-observe-only=\"true\"><img class=\"graf-image\" data-image-id=\"1*PZjwR1Nbluff5IMI6Y1T6g@2x.png\" data-width=\"1400\" data-height=\"700\" src=\"https://cdn-images-1.medium.com/max/1600/1*PZjwR1Nbluff5IMI6Y1T6g@2x.png\" alt=\"image\" /></a></div></figure></figure></figure></figure>",
        "created_at": "2018-08-28T15:51:59+0000",
        "updated_at": "2018-08-28T15:52:00+0000",
        "published_at": "2018-02-12T17:06:39+0000",
        "published_by": [
          "Alex Punnen"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 11,
        "domain_name": "hackernoon.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*SEX2I_8ohef5sG65VuX5ow.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12003"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 12002,
        "uid": null,
        "title": "Apache Cassandra vs. Apache Ignite -- the winner is: Ignite!",
        "url": "https://www.gridgain.com/resources/blog/apache-cassandra-vs-apache-ignite-affinity-collocation-and-distributed-sql",
        "content": "<p>In the <a href=\"https://www.gridgain.com/resources/blog/apache-cassandra-or-apache-ignite-thoughts-simplified-architecture\" target=\"_blank\">previous article</a>, we reviewed and summarized pitfalls of the query-driven data modeling methodology (a.k.a. denormalized data modeling) utilized in Apache Cassandra. Turns out that the methodology prevents us from developing efficient applications without insight into what our <strong>queries</strong> will be like. In reality, an application architecture under this scenario will get more complicated, less maintainable and prone to notable data duplication.</p><p>Furthermore, it was discovered that the pitfalls are usually shielded with statements like <em>\"if you really want to be scalable, fast and highly-available then be ready to store duplicated data and sacrifice SQL along with strong consistency.\"</em> That might have been true a decade ago but it's a total fable nowadays!</p><p>Not going too far, we picked another Apache Software Foundation citizen -- <a href=\"https://ignite.apache.org\">Apache Ignite</a> -- and in this article will figure out how our Ignite based application's architecture would look like and what would be its maintenance costs.</p><p>The application of our choice tracks all the cars produced by a vendor and gives insights on production power for every single vendor. If you recall, it has the following relational model:</p><img alt=\"Cars and Vendors\" data-entity-type=\"file\" data-entity-uuid=\"06c8fa2e-626c-4b3b-a63a-9c359a453754\" src=\"https://www.gridgain.com/sites/default/files/inline-images/cars_vendors.png\" class=\"align-center\" /><p>Can we just go ahead and create those 3 tables in Ignite with the <a href=\"https://apacheignite-sql.readme.io/docs/create-table\" target=\"_blank\">CREATE TABLE</a> command and run our SQL-driven application immediately? Depends... If we were not supposed to join the data stored in different tables then it would be enough. But in the previous article, we put a requirement that the application has to support 2 queries that require JOINs:</p><ul><li><strong>Q1: get car models produced by a vendor within a particular time frame (newest first).</strong></li>\n<li><strong>Q2: get a number of cars of a specific model produced by a vendor.</strong></li>\n</ul><p>In the Cassandra case, we got away with JOINs by creating 2 tables for each of the queries. Do we go through the same experience with Ignite? Totally not. In fact, Ignite's <a href=\"https://apacheignite-sql.readme.io/docs/distributed-joins#non-collocated-joins\" target=\"_blank\">non-collocated JOINs</a> are already at our service and doesn't require anything else from us once the 3 tables are created. However, they are not as efficient and fast as their <a href=\"https://apacheignite-sql.readme.io/docs/distributed-joins#section-collocated-joins\" target=\"_blank\">collocated</a> counter-part. Therefore, let's learn more about <a href=\"https://apacheignite.readme.io/docs/affinity-collocation\" target=\"_blank\">affinity collocation</a> first and see how this concept is used in modern databases such as Apache Ignite.</p><p>Affinity collocation is a powerful concept of Ignite (and other distributed databases such as Google Spanner or MemSQL) that allows storing <strong>related</strong> data on the same cluster nodes. How do we know which data is related? It's simple, especially with the background of relational databases --  just identify a parent-child relationship between your business objects, specify <a href=\"https://apacheignite-sql.readme.io/docs/create-table\" target=\"_blank\">affinity keys in CREATE TABLE statements</a> and your job is done because Ignite will take care of the rest!</p><p>Taking the Cars and Vendors application as an example, it's reasonable to use Vendor as a parent entity and Car with Production as its children. For instance, once this happens and the affinity keys are set, all the Cars produced by specific Vendor will be stored on the same node like it's shown in the picture below:</p><img alt=\"Affinity Collocation\" data-entity-type=\"file\" data-entity-uuid=\"1a2bfb95-f25e-4282-bdf5-44e6ea6c9d84\" src=\"https://www.gridgain.com/sites/default/files/inline-images/affinity_collocation.png\" class=\"align-center\" /><p>Specifically, the picture shows that all the Cars produced by Toyota are stored on Ignite node 1 while the Cars produced by Ford Motors are stored on Ignite node 2. This is the affinity collocation in action -- Cars are stored on a node where an entry of a respective Vendor is.</p><p>It's time to define SQL tables to achieve such data distribution. Let's start with Vendor table:</p><pre class=\"language-java\">\nCREATE TABLE Vendor (\n    id INT PRIMARY KEY,\n    name VARCHAR,\n    address VARCHAR\n);\n</pre><p>The Vendors will be distributed randomly across the cluster nodes. Ignite uses the primary key column to calculate a node that will own a Vendor's entry (refer to the recording of <a href=\"https://www.gridgain.com/resources/webinars/in-memory-computing-essentials-architects-and-developers-part-1\" target=\"_blank\">Ignite essentials Part 1</a> webinar for more details).</p><p>The Car table goes next:</p><pre class=\"language-java\">\nCREATE TABLE Car (\n    id INT,\n    vendor_id INT,\n    model VARCHAR,\n    year INT,\n    price float,\n    PRIMARY KEY(id, vendor_id)\n) WITH \"affinityKey=vendor_id\";\n</pre><p>The table for Cars has <code class=\"language-java\">affinityKey</code> parameter set to <code class=\"language-java\">vendor_id</code> column. This key instructs Ignite to store a Car on the cluster node of its <code class=\"language-java\">vendor_id</code> (refer to the recording of <a href=\"https://www.gridgain.com/resources/webinars/in-memory-computing-essentials-architects-and-developers-part-2\" target=\"_blank\">Ignite essentials Part 2</a> webinar for more info).</p><p>Repeating the same procedure for Production table which entries will be stored on the cluster nodes identified by <code class=\"language-java\">vendor_id</code> as well:</p><pre class=\"language-java\">\nCREATE TABLE Production (\n    id INT,\n    car_id INT,\n    vendor_id INT,\n    country VARCHAR,\n    total INT,\n    year INT,\n    PRIMARY KEY(id, car_id, vendor_id)\n) WITH \"affinityKey=vendor_id\";\n</pre><p>That's it with the data modeling done right in Apache Ignite. The next step is to get down to our application code and make out all the required queries.</p><p>Ignite cluster can be queried with our old-good-friend SQL that supports distributed JOINs and secondary indexes. </p><p>There are two types of JOINs in Ignite -- <a href=\"https://apacheignite-sql.readme.io/docs/distributed-joins#section-collocated-joins\" target=\"_blank\">collocated</a> and <a href=\"https://apacheignite-sql.readme.io/docs/distributed-joins#section-non-collocated-joins\" target=\"_blank\">non-collocated</a>. The collocated JOINs omit data (that is needed to complete a JOIN) movement between cluster nodes expecting that the tables, that are being joined, are already collocated and all the data is available locally. That's the most efficient and performant JOINs you can get in distributed databases such as Ignite. The non-collocated JOINs as the name suggests is our a backup plan if there was no way to achieve affinity collocation for some tables but they still need to be joined. This type of JOINs is slower because they might involve data movement between cluster nodes at a JOIN time.</p><p>Previously we could collocate Vendor, Car and Production tables and now can benefit from the collocated JOINs preparing an SQL query for <strong>Q1 </strong>(<em>get car models produced by a vendor within a particular time frame (newest first</em>)):</p><pre class=\"language-java\">\nSELECT c.model, p.country, p.total, p.year FROM Vendor as v\nJOIN Production as p ON v.id = p.vendor_id\nJOIN Car as c ON c.id = p.car_id\nWHERE v.name = 'Ford Motor' and p.year &gt;= 2017\nORDER BY p.year;\n</pre><p>Is there any way to make it even faster? Sure, let's define secondary indexes for <code class=\"language-java\">Vendor.name</code> and <code class=\"language-java\">Production.year</code> columns:</p><pre class=\"language-java\">\nCREATE INDEX vendor_name_id ON Vendor (name);\nCREATE INDEX prod_year_id ON Production (year);\n</pre><p>The SQL query for required <strong>Q2</strong> (<em>get a number of cars of a specific model produced by a vendor</em>) does not require extra efforts from us:</p><pre class=\"language-java\">\nSELECT p.country, p.total, p.year FROM Vendor as v\nJOIN Production as p ON v.id = p.vendor_id\nJOIN Car as c ON c.id = p.car_id\nWHERE v.name = 'Ford Motor' and c.model = 'Explorer';\n</pre><p>Now, whenever a boss stops by our cubicle requesting to add a new feature we can quickly come up with a set of new SQL queries needed for it, configure secondary indexes and go to the kitchen to chat with teammates. Job done! In comparison, recall what it took us to support new query <strong>Q2 </strong> for the <a href=\"https://www.gridgain.com/resources/blog/apache-cassandra-or-apache-ignite-thoughts-simplified-architecture\" target=\"_blank\">Cassandra based architecture</a>.</p><p>Affinity collocation based on data modeling in Ignite has the following advantages over the query-driven denormalized modeling in Cassandra:</p><ul><li>Applications data layer is modeled in a familiar relational way and easy to maintain.</li>\n<li>Data is accessed using standard SQL syntax.</li>\n<li>Affinity collocation provides more benefits from modern distributed databases:\n<ul><li>Efficient and highly performant distributed JOINs</li>\n<li><a href=\"https://ignite.apache.org/collocatedprocessing.html\" target=\"_blank\">Collocated computations</a></li>\n<li>Watch a recording of this <a href=\"https://www.gridgain.com/resources/webinars/scale-out-and-conquer-architectural-decisions-behind-distributed-in-memory\" target=\"_blank\">webinar</a> to reveal more benefits of this technique.</li>\n</ul></li>\n</ul><p>The simplified software architecture we get with Ignite is not the only benefit if we consider it instead of Cassandra. Give me a couple of weeks to put together my thoughts on strong consistency and in-memory performance.</p>",
        "created_at": "2018-08-28T15:49:10+0000",
        "updated_at": "2018-08-28T15:49:15+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.gridgain.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12002"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1017,
            "label": "cosmos",
            "slug": "cosmos"
          }
        ],
        "is_public": false,
        "id": 12001,
        "uid": null,
        "title": "Introduction to the Azure Cosmos DB Cassandra API",
        "url": "https://docs.microsoft.com/en-us/azure/cosmos-db/cassandra-introduction",
        "content": "<ul class=\"metadata page-metadata\" data-bi-name=\"page info\" lang=\"en-us\" dir=\"ltr\">\n\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t<li class=\"contributors-holder\">&#13;\n\t\t\t\t\t\t\t\tContributors&#13;\n\t\t\t\t\t\t\t\t</li>\n\n\t\t\t\t\t\t</ul><p>Azure Cosmos DB provides the Cassandra API (preview) for applications that are written for Apache Cassandra that need premium capabilities like:</p>\n<ul><li><a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/partition-data\" data-linktype=\"relative-path\">Scalable storage size and throughput</a>.</li>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/distribute-data-globally\" data-linktype=\"relative-path\">Turn-key global distribution</a></li>\n<li>Single-digit millisecond latencies at the 99th percentile.</li>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels\" data-linktype=\"relative-path\">Five well-defined consistency levels</a></li>\n<li><a href=\"http://www.vldb.org/pvldb/vol8/p1668-shukla.pdf\" data-linktype=\"external\">Automatic indexing of data</a> without requiring you to deal with schema and index management. </li>\n<li>Guaranteed high availability, all backed by <a href=\"https://azure.microsoft.com/support/legal/sla/cosmos-db/\" data-linktype=\"external\">industry-leading SLAs</a></li>\n</ul><h2 id=\"what-is-the-azure-cosmos-db-apache-cassandra-api\">What is the Azure Cosmos DB Apache Cassandra API?</h2>\n<p>Azure Cosmos DB can be used as the data store for apps written for <a href=\"https://cassandra.apache.org/\" data-linktype=\"external\">Apache Cassandra</a>, by using the Apache Cassandra API. This means that by using existing <a href=\"https://cassandra.apache.org/doc/latest/getting_started/drivers.html?highlight=driver\" data-linktype=\"external\">Apache licensed drivers compliant with CQLv4</a>, your application written for Cassandra can now communicate with the Azure Cosmos DB Cassandra API. In many cases, you can switch from using Apache Cassandra to using Azure Cosmos DB 's Apache Cassandra API, by simply changing a connection string. Using this functionality, you can easily build and run Cassandra API database applications in the Azure cloud with Azure Cosmos DB's global distribution and <a href=\"https://azure.microsoft.com/support/legal/sla/cosmos-db\" data-linktype=\"external\">comprehensive industry-leading SLAs</a>, while continuing to use familiar skills and tools for Cassandra API.</p>\n<p><img src=\"https://docs.microsoft.com/en-us/azure/cosmos-db/media/cassandra-introduction/cosmosdb-cassandra.png\" alt=\"Azure Cosmos DB Cassandra API\" data-linktype=\"relative-path\" /></p>\n<p>The Cassandra API enables you to interact with data stored in Azure Cosmos DB using the Cassandra Query Language based tools (like CQLSH) and Cassandra client drivers you’re already familiar with. Learn more about it in this Microsoft Mechanics video with Principal Engineering Manager Kirill Gavrylyuk.</p>\n<h2 id=\"what-is-the-benefit-of-using-apache-cassandra-api-for-azure-cosmos-db\">What is the benefit of using Apache Cassandra API for Azure Cosmos DB?</h2>\n<p><strong>No operations management</strong>: As a true fully managed service, Azure Cosmos DB ensures that Cassandra API administrators do not have to worry about managing and monitoring a myriad settings across OS, JVM, and yaml files and their interplay. Azure Cosmos DB provides monitoring of throughput, latency, storage and availability, and configurable alerts. </p>\n<p><strong>Performance management</strong>: Azure Cosmos DB provides SLA backed low latency reads and writes for the 99th percentile. Users do not need to worry about lot of operational overhead to provide good read and write SLAs. These typically include scheduling compaction, managing tombstones, bloom filters setting, and replica lags. Azure Cosmos DB takes away the worry of managing these issues and lets you focus on the application deliverables.</p>\n<p><strong>Automatic indexing</strong>: Azure Cosmos DB automatically indexes all the columns of table in Cassandra API database. Azure Cosmos DB  does not require creation of secondary indices to speed up queries. It provides low latency read and write experience while doing automatic consistent indexing. </p>\n<p><strong>Ability to use existing code and tools</strong>: Azure Cosmos DB provides wire protocol level compatibility with existing SDKs and tools. This compatibility ensures you can use your existing codebase with Cassandra API of Azure Cosmos DB with trivial changes.</p>\n<p><strong>Throughput and storage elasticity</strong>: Azure Cosmos platform provides elasticity of guaranteed throughput across regions via simple portal, PowerShell, or CLI operations. You can elastically scale Azure Cosmos DB Tables with predictable performance seamlessly as your application grows. Azure Cosmos DB supports Cassandra API tables that can scale to virtually unlimited storage sizes. </p>\n<p><strong>Global distribution and availability</strong>: Azure Cosmos DB provides the ability to distribute data throughout Azure regions to enable  users with a low latency experience while ensuring availability. Azure Cosmos DB provides 99.99% availability within a region and 99.999% read availability across the regions with no operations overhead. Azure Cosmos DB is available in 30+ <a href=\"https://azure.microsoft.com/regions/services/\" data-linktype=\"external\">Azure Regions</a>. Learn more in <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/distribute-data-globally\" data-linktype=\"relative-path\">Distribute data globally</a>. </p>\n<p><strong>Choice of consistency</strong>: Azure Cosmos DB provides the choice of five well-defined consistency levels to achieve optimal trade-off between consistency and performance. These consistency levels are strong, bounded-staleness, session, consistent prefix, and eventual. These granular, well-defined consistency levels allow developer to make sound trade-offs between consistency, availability, and latency. Learn more in <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels\" data-linktype=\"relative-path\">Using consistency levels to maximize availability and performance</a>. </p>\n<p><strong>Enterprise grade</strong>: Azure cosmos DB provides <a href=\"https://www.microsoft.com/trustcenter\" data-linktype=\"external\">compliance certifications</a> to ensure users can use the platform securely. Azure Cosmos DB also provides encryption at rest and in motion, IP firewall, and audit logs for control plane activities.  </p>\n<p>If you already have an Azure subscription, you can sign up to join the Cassandra API (preview) program in the <a href=\"https://aka.ms/cosmosdb-cassandra-signup\" data-linktype=\"external\">Azure portal</a>.  If you’re new to Azure, sign up for a <a href=\"https://azure.microsoft.com/free\" data-linktype=\"external\">free trial</a> where you get 12 months of free access to Azure Cosmos DB. Complete the following steps to request access to the Cassandra API (preview) program.</p>\n<ol><li><p>In the <a href=\"https://portal.azure.com\" data-linktype=\"external\">Azure portal</a>, click <strong>Create a resource</strong> &gt; <strong>Databases</strong> &gt; <strong>Azure Cosmos DB</strong>. </p>\n</li>\n<li><p>In the New Account page, select <strong>Cassandra</strong> in the API box. </p>\n</li>\n<li><p>In the <strong>Subscription</strong> box, select the Azure subscription that you want to use for this account.</p>\n</li>\n<li><p>Click <strong>Sign up to preview today</strong>.</p>\n<p> <img src=\"https://docs.microsoft.com/en-us/azure/cosmos-db/media/cassandra-introduction/cassandra-sign-up.png\" alt=\"Azure Cosmos DB Cassandra API\" data-linktype=\"relative-path\" /></p>\n</li>\n<li><p>In the Sign up to preview today pane, click <strong>OK</strong>. </p>\n<p> Once you submit the request, the status changes to <strong>Pending approval</strong> in the New account pane. </p>\n</li>\n</ol><p>After you submit your request, wait for email notification that your request has been approved. Due to the high volume of requests, you should receive notification within a week. You do not need to create a support ticket to complete the request. Requests will be reviewed in the order in which they were received. </p>\n<h2 id=\"how-to-get-started\">How to get started</h2>\n<p>Once you've joined the preview program, follow the Cassandra API quickstarts to create an app by using the Cassandra API:</p>\n<ul><li><a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/create-cassandra-nodejs\" data-linktype=\"relative-path\">Quickstart: Build a Cassandra web app with Node.js and Azure Cosmos DB</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/create-cassandra-java\" data-linktype=\"relative-path\">Quickstart: Build a Cassandra web app with Java and Azure Cosmos DB</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/create-cassandra-dotnet\" data-linktype=\"relative-path\">Quickstart: Build a Cassandra web app with .NET and Azure Cosmos DB</a></li>\n<li><a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/create-cassandra-python\" data-linktype=\"relative-path\">Quickstart: Build a Cassandra web app with Python and Azure Cosmos DB</a></li>\n</ul><h2 id=\"next-steps\">Next steps</h2>\n<p>Information about the Azure Cosmos DB Cassandra API is integrated into the overall Azure Cosmos DB documentation, but here are a few pointers to get you started:</p>\n<ul><li>Follow the <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/create-cassandra-nodejs\" data-linktype=\"relative-path\">Quickstarts</a> to create an account and a new app by using a Git sample</li>\n<li>Follow the <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/tutorial-develop-cassandra-java\" data-linktype=\"relative-path\">Tutorial</a> to create a new app programmatically.</li>\n<li>Follow the <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/cassandra-import-data\" data-linktype=\"relative-path\">Import Cassandra data tutorial</a> to import your existing data into Azure Cosmos DB.</li>\n<li>Read the <a href=\"https://docs.microsoft.com/en-us/azure/cosmos-db/faq#cassandra\" data-linktype=\"relative-path\">FAQ</a>.</li>\n</ul>",
        "created_at": "2018-08-28T15:49:00+0000",
        "updated_at": "2018-08-28T15:49:05+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 5,
        "domain_name": "docs.microsoft.com",
        "preview_picture": "https://docs.microsoft.com/_themes/docs.theme/master/en-us/_themes/images/microsoft-header.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12001"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 190,
            "label": "sql",
            "slug": "sql"
          },
          {
            "id": 1280,
            "label": "cql",
            "slug": "cql"
          }
        ],
        "is_public": false,
        "id": 12000,
        "uid": null,
        "title": "Cassandra Query Language (CQL) vs SQL",
        "url": "https://medium.com/@alexbmeng/cassandra-query-language-cql-vs-sql-7f6ed7706b4c",
        "content": "<section class=\"section section--body section--middleCenter section-image--aspectRatioViewport is-imageBackgrounded is-backgrounded is-darkBackgrounded u-imageSpectrum section--first\"><div class=\"js-sectionPoint u-ignoreBlock u-textColorTransparentWhite u-absolute u-top0 u-left0\"><div class=\"section-background u-sizeFullWidth u-absolute u-top0 u-cursorDefault u-bottom0\" data-scroll=\"aspect-ratio-viewport\" contenteditable=\"false\" data-image-id=\"1*9gY7eT-NP2W2YIKQLqLSxw.png\" data-width=\"1415\" data-height=\"986\"><div class=\"section-backgroundImage u-absolute u-top0 u-left0 u-right0 u-backgroundCover u-bottom0\"><div class=\"u-ignoreBlock u-absolute0 u-borderRadius3\"><br /></div><div class=\"section-doubleWidthTable u-table u-ignoreBlock\"><div class=\"section-contentCell u-tableCell u-verticalAlignTop u-ignoreBlock\"><div class=\"section-aspectRatioViewportBottomSpacer u-ignoreBlock\"><div class=\"u-ignoreBlock\"></div><div class=\"section-doubleWidthTable u-table u-ignoreBlock\"><div class=\"section-contentCell u-tableCell u-ignoreBlock\"><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><h1 id=\"3dac\" class=\"graf graf--h3 graf--leading graf--title\">Cassandra Query Language</h1><h2 id=\"fd00\" class=\"graf graf--h2 graf-after--h3 graf--trailing\">CQL vs SQL</h2></div></div></div><div class=\"u-tableCell u-verticalAlignTop u-ignoreBlock\"><div class=\"section-aspectRatioViewportPlaceholder u-ignoreBlock\"><div class=\"u-ignoreBlock\"></div><div class=\"section-aspectRatioViewportBottomPlaceholder u-ignoreBlock\"><div class=\"u-ignoreBlock\"></div></div></div></div><div class=\"u-tableCell u-ignoreBlock\"><div class=\"section-aspectRatioViewportCropPlaceholder u-ignoreBlock\"><div class=\"u-ignoreBlock\"></div></div></div><div class=\"section-captionContent u-ignoreBlock\"></div></div></div></div></div></div></div></div></section><section class=\"section section--body section--last\"><div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"50fc\" class=\"graf graf--p graf--leading\">When Apache Cassandra was originally released, it featured a command line interface for dealing with <a href=\"https://thrift.apache.org\" data-href=\"https://thrift.apache.org\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">thrift</a>. Manipulating data this way was cumbersome and required learning the details of the API. This introduced an additional hurdle for new users coming from a relational database background. Exposing this low level interface also meant that language-specific drivers often needed to make significant updates from version to version. Clearly, an abstraction was needed.</p><p id=\"803a\" class=\"graf graf--p graf-after--p\">The Cassandra Query Language (CQL) was created to provide the necessary abstraction. CQL is purposefully similar to Structured Query Language (SQL) used in relational databases like MySQL and Postgres. This similarity lowers the barrier of entry for users familiar with relational databases. Many queries are very similar between the two. In fact, a lot of basic things are even exactly the same. The following is both valid CQL and SQL:</p><h3 id=\"8c14\" class=\"graf graf--h3 graf-after--p\">Valid CQL and SQL</h3><pre id=\"0b36\" class=\"graf graf--pre graf-after--h3\"><strong class=\"markup--strong markup--pre-strong\">USE </strong>myDatabase;</pre><pre id=\"7ca6\" class=\"graf graf--pre graf-after--pre\"><em class=\"markup--em markup--pre-em\">/* Creating Tables */</em><br /><strong class=\"markup--strong markup--pre-strong\">CREATE TABLE IF NOT EXISTS</strong> myTable (id <strong class=\"markup--strong markup--pre-strong\">INT PRIMARY KEY</strong>);</pre><pre id=\"5ed5\" class=\"graf graf--pre graf-after--pre\">/* Altering Tables /*<br /><strong class=\"markup--strong markup--pre-strong\">ALTER</strong> <strong class=\"markup--strong markup--pre-strong\">TABLE</strong> myTable <strong class=\"markup--strong markup--pre-strong\">ADD</strong> myField <strong class=\"markup--strong markup--pre-strong\">INT</strong>;</pre><pre id=\"e95b\" class=\"graf graf--pre graf-after--pre\"><em class=\"markup--em markup--pre-em\">/* Creating Indexes */</em><br /><strong class=\"markup--strong markup--pre-strong\">CREATE INDEX </strong>myIndex <strong class=\"markup--strong markup--pre-strong\">ON</strong> myTable (myField);</pre><pre id=\"0753\" class=\"graf graf--pre graf-after--pre\"><em class=\"markup--em markup--pre-em\">/* Inserting Data */</em><br /><strong class=\"markup--strong markup--pre-strong\">INSERT INTO</strong> myTable (id, myField) <strong class=\"markup--strong markup--pre-strong\">VALUES </strong>(1, 7);</pre><pre id=\"ea87\" class=\"graf graf--pre graf-after--pre\"><em class=\"markup--em markup--pre-em\">/* Selecting Data */</em><br /><strong class=\"markup--strong markup--pre-strong\">SELECT </strong>* <strong class=\"markup--strong markup--pre-strong\">FROM </strong>myTable <strong class=\"markup--strong markup--pre-strong\">WHERE </strong>myField = 7;</pre><pre id=\"eb4e\" class=\"graf graf--pre graf-after--pre\">/* Counting Data */<br /><strong class=\"markup--strong markup--pre-strong\">SELECT COUNT</strong>(*) <strong class=\"markup--strong markup--pre-strong\">FROM </strong>myTable;</pre><pre id=\"e4bd\" class=\"graf graf--pre graf-after--pre\"><em class=\"markup--em markup--pre-em\">/* Deleting Data */</em><br /><strong class=\"markup--strong markup--pre-strong\">DELETE FROM </strong>myTable <strong class=\"markup--strong markup--pre-strong\">WHERE </strong>myField = 7;</pre><p id=\"cc89\" class=\"graf graf--p graf-after--pre\">From this, its easy to think that the two languages work just the same. But there are quite a few differences once you get beyond the basics.</p><h3 id=\"e296\" class=\"graf graf--h3 graf-after--p\">Differences</h3><p id=\"f43a\" class=\"graf graf--p graf-after--h3\">Cassandra is a non-relational database, and so uses different concepts to store and retrieve data. Simplistically, a Cassandra <em class=\"markup--em markup--p-em\">keyspace</em> is a SQL <em class=\"markup--em markup--p-em\">database</em>, and a Cassandra <em class=\"markup--em markup--p-em\">column family </em>is a SQL <em class=\"markup--em markup--p-em\">table</em> (CQL allows you to interchange the words “TABLE” and “COLUMNFAMILY” for convenience). This difference necessitates a different syntax for creating and manipulating data:</p><h4 id=\"b957\" class=\"graf graf--h4 graf-after--p\">Creating databases:</h4><pre id=\"f37e\" class=\"graf graf--pre graf-after--h4\"><em class=\"markup--em markup--pre-em\">/* Create a new keyspace in CQL */</em><br /><strong class=\"markup--strong markup--pre-strong\">CREATE KEYSPACE</strong> myDatabase <strong class=\"markup--strong markup--pre-strong\">WITH </strong>replication = <br />   {'class': 'SimpleStrategy', 'replication_factor': 1};<em class=\"markup--em markup--pre-em\">/* Create a new database in SQL */</em><br /><strong class=\"markup--strong markup--pre-strong\">CREATE DATABASE</strong> myDatabase;</pre><p id=\"1fec\" class=\"graf graf--p graf-after--pre\">Cassandra’s keyspaces require more specifications than a standard relational database. Note that the example above is the simplest form. In a production environment, likely spread across several data-centers, a keyspace would be created with a different strategy and replication factor.</p><h4 id=\"55f0\" class=\"graf graf--h4 graf-after--p\">Organizing Data</h4><p id=\"b782\" class=\"graf graf--p graf-after--h4\">None of the standard relational stuff is going to work in CQL. There is no support for things like <strong class=\"markup--strong markup--p-strong\">JOIN</strong>,<strong class=\"markup--strong markup--p-strong\"> GROUP BY</strong>, or <strong class=\"markup--strong markup--p-strong\">FOREIGN KEY</strong>. Leaving these features out is important because it makes writing and retrieving data from Cassandra much more efficient.</p><p id=\"ce8c\" class=\"graf graf--p graf-after--p\">But sets of data tend to have relationships with one another. So without the relational tools of SQL, we need another strategy to represent these relationships. The problem is that reads in Cassandra tend to be more expensive than you might be used to when dealing with a relational database. But there is a filp side to this. And if you take only one thing away from this post, let it be this:</p><p id=\"4748\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">Writes are cheap. Write everything the way you want to read it.</em></strong></p><p id=\"8b6a\" class=\"graf graf--p graf-after--p\">In other words, any query you plan to do in the future should already be organized into a column family. You want to look up addresses that correspond to a person? Then make a column family that consists of people and addresses. You can still store them separately, but you should also store them together. Having a column family to represent every query makes reads much more efficient. You’re not just going to have to de-normalize, you’re going to have to forget about normalization all together.</p><h4 id=\"9cec\" class=\"graf graf--h4 graf-after--p\">Inserts vs Updates</h4><p id=\"10d8\" class=\"graf graf--p graf-after--h4\">The concept of cheap writes extends to updating data. This is because, unlike in SQL, a read is not performed during the update. The syntax is the same in CQL and SQL.</p><pre id=\"19c7\" class=\"graf graf--pre graf-after--p\"><em class=\"markup--em markup--pre-em\">/* Updating data */</em><strong class=\"markup--strong markup--pre-strong\"><br />UPDATE </strong>myTable <strong class=\"markup--strong markup--pre-strong\">SET </strong>myField = 2 <strong class=\"markup--strong markup--pre-strong\">WHERE </strong>id = 6;</pre><p id=\"9e08\" class=\"graf graf--p graf-after--pre\">However, if the row does not exist, it will still get created. Similarly as unintuitive, an <strong class=\"markup--strong markup--p-strong\">INSERT </strong>statement will actually replace data if it exists. Because again, CQL does not perform a read while inserting. Without a read, there is no way to know if the data being inserted is replacing an existing record. This means that both inserts and updates are extremely fast.</p><h4 id=\"1749\" class=\"graf graf--h4 graf-after--p\">Time to Live</h4><p id=\"3090\" class=\"graf graf--p graf-after--h4\">CQL enables you to set a TTL on a row. Meaning that you can set a row to expire 24 hours from the time it gets created. This is accomplished with the <strong class=\"markup--strong markup--p-strong\">USING TTL</strong> command (values are in seconds).</p><pre id=\"76d9\" class=\"graf graf--pre graf-after--p\"><em class=\"markup--em markup--pre-em\">/* Expiring Data in 24 Hours */</em><br /><strong class=\"markup--strong markup--pre-strong\">INSERT INTO </strong>myTable (id, myField) <strong class=\"markup--strong markup--pre-strong\">VALUES </strong>(2, 9) <strong class=\"markup--strong markup--pre-strong\">USING TTL</strong> 86400;</pre><p id=\"e4ca\" class=\"graf graf--p graf-after--pre\">Twenty-four hours after that query is executed, the data will be deleted. Well, actually…</p><h4 id=\"5cb3\" class=\"graf graf--h4 graf-after--p\">Deletions</h4><p id=\"f401\" class=\"graf graf--p graf-after--h4\">Executing a DELETE in CQL does not actually delete data. But deletions are a topic of their own. DataStax has a great <a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/dml/dml_about_deletes_c.html\" data-href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/dml/dml_about_deletes_c.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">introduction to Cassandra deletions</a> that I would definitely recommend reading.</p><h3 id=\"73fe\" class=\"graf graf--h3 graf-after--p\">Performance Issues</h3><p id=\"a811\" class=\"graf graf--p graf-after--h3\">Scaling Cassandra can be daunting the first time around. But when you inevitably run into performance issues, there are a few things you can look at, in addition to the things mentioned above.</p><h4 id=\"85c8\" class=\"graf graf--h4 graf-after--p\">Tracing Queries</h4><p id=\"57a4\" class=\"graf graf--p graf-after--h4\">There is a command in CQL that allows you to trace queries across nodes and data centers, showing some useful debugging information.</p><pre id=\"d671\" class=\"graf graf--pre graf-after--p\"><em class=\"markup--em markup--pre-em\">/* Enable query traces */</em><br /><strong class=\"markup--strong markup--pre-strong\">TRACING ON</strong>;</pre><pre id=\"c40a\" class=\"graf graf--pre graf-after--pre\">/* Disable query traces */<br /><strong class=\"markup--strong markup--pre-strong\">TRACING OFF</strong>;</pre><p id=\"b040\" class=\"graf graf--p graf-after--pre\">This will show the full network path the query takes, along with the latency at each step. This is helpful for verifying assertions about the replication factor or the connectivity between nodes and data centers. Note that tracing is expensive and should be used sparingly. You definitely do not want it on for all your production queries.</p><h4 id=\"1403\" class=\"graf graf--h4 graf-after--p\">Filtering</h4><p id=\"6a6d\" class=\"graf graf--p graf-after--h4\">If you attempt to run a SELECT across a large range of values, you might end up with an error message.</p><pre id=\"58df\" class=\"graf graf--pre graf-after--p\"><em class=\"markup--em markup--pre-em\">/* Select Data within a range */</em><br /><strong class=\"markup--strong markup--pre-strong\">SELECT </strong>* <strong class=\"markup--strong markup--pre-strong\">FROM </strong>myTable <strong class=\"markup--strong markup--pre-strong\">WHERE </strong>myField &gt; 5000 <strong class=\"markup--strong markup--pre-strong\">AND </strong>myField &lt; 100000;</pre><pre id=\"6b50\" class=\"graf graf--pre graf-after--pre\"><em class=\"markup--em markup--pre-em\">Bad Request: Cannot execute this query as it might involve data<br />filtering and thus may have unpredictable performance. If you want<br />to execute this query despite the performance unpredictability,<br />use ALLOW FILTERING.</em></pre><p id=\"15d7\" class=\"graf graf--p graf-after--pre\">I think the error message here speaks for itself. If you’re allowing filtering on your CQL queries, think above whether or not you really need to do that.</p><h3 id=\"be01\" class=\"graf graf--h3 graf-after--p\">Conclusion</h3><p id=\"92d8\" class=\"graf graf--p graf-after--h3\">The similarity between CQL and SQL is really a double edged sword. While you should enjoy the easy-to-learn syntax, take the time to understand the differences. There is a lot more to Cassandra than I could possibly cover here. If you’re writing production queries for the first time, I would recommend reading up on the query you’re writing, even if it is something as simple as a SELECT. The <a href=\"http://www.datastax.com/documentation/cql/3.1/cql/cql_intro_c.html\" data-href=\"http://www.datastax.com/documentation/cql/3.1/cql/cql_intro_c.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">DataStax CQL documentation</a> is particularly helpful in calling out caveats and providing helpful examples.</p><h4 id=\"5cf6\" class=\"graf graf--h4 graf-after--p\">Further Reading</h4><ul class=\"postList\"><li id=\"959b\" class=\"graf graf--li graf-after--h4\"><a href=\"http://planetcassandra.org/what-is-apache-cassandra/\" data-href=\"http://planetcassandra.org/what-is-apache-cassandra/\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">What is Apache Cassandra?</a></li><li id=\"a9c6\" class=\"graf graf--li graf-after--li\"><a href=\"http://www.ebaytechblog.com/2012/07/16/cassandra-data-modeling-best-practices-part-1\" data-href=\"http://www.ebaytechblog.com/2012/07/16/cassandra-data-modeling-best-practices-part-1\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Cassandra Data Modeling Best Practices</a></li><li id=\"d7bc\" class=\"graf graf--li graf-after--li\"><a href=\"http://www.slideshare.net/johnny15676/introduction-to-cql-and-data-modeling\" data-href=\"http://www.slideshare.net/johnny15676/introduction-to-cql-and-data-modeling\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Introduction to CQL and Data Modeling</a></li><li id=\"27f1\" class=\"graf graf--li graf-after--li graf--trailing\"><a href=\"http://techblog.netflix.com/search/label/Cassandra\" data-href=\"http://techblog.netflix.com/search/label/Cassandra\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Cassandra posts on the Netflix Tech Blog</a></li></ul></div></div></section>",
        "created_at": "2018-08-28T15:47:45+0000",
        "updated_at": "2018-08-28T15:47:53+0000",
        "published_at": "2014-11-24T02:10:31+0000",
        "published_by": [
          "Alex Meng"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 5,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*9gY7eT-NP2W2YIKQLqLSxw.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/12000"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 235,
            "label": "rest",
            "slug": "rest"
          }
        ],
        "is_public": false,
        "id": 11987,
        "uid": null,
        "title": "wikimedia/restbase",
        "url": "https://github.com/wikimedia/restbase",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>RESTBase was built to provide a <a href=\"http://rest.wikimedia.org/en.wikipedia.org/v1/?doc\" rel=\"nofollow\">low-latency &amp; high-throughput API for\nWikipedia / Wikimedia\ncontent</a>. It is basically\na storage proxy, which presents a coherent API powered by Swagger specs to the\noutside, and backs up many of these entry points with storage.  The default\n<strong>table storage</strong> backend is based on Cassandra, which helps it to perform\nwell at Wikimedia's scale without placing undue burden on operations.</p>\n<p>As a proxy, RESTBase does not perform any significant content processing\nitself. Instead, it requests content transformations from backend services\nwhen needed, and typically (depending on configuration) stores it back for\nlater retrieval. For high-volume static end points, most requests will be\nsatisfied directly from storage.</p>\n<p>The <em>table storage</em> backends conform to a RESTful <a href=\"https://github.com/wikimedia/restbase/blob/master/doc/TableStorageAPI.md\">table storage\nAPI</a>\nsimilar to <a href=\"http://aws.amazon.com/documentation/dynamodb/\" rel=\"nofollow\">Amazon DynamoDB</a>\nand <a href=\"https://developers.google.com/datastore/\" rel=\"nofollow\">Google DataStore</a>. The primary\nimplementation uses Apache Cassandra. Notable features include automatically\nmaintained secondary indexes and some lightweight transaction support. A\n<a href=\"https://github.com/wikimedia/restbase-mod-table-sqlite\">SQLite backend</a> is\nunder development.</p>\n<p>RESTBase systematically emits statsd metrics about storage and backend\nrequests. Specifically, the systematic metric production for backend services\nprovides a good baseline level of instrumentation for tracking performance\nand errors in a micro-service architecture.</p>\n<h2><a id=\"user-content-issue-tracking\" class=\"anchor\" aria-hidden=\"true\" href=\"#issue-tracking\"></a>Issue tracking</h2>\n<p>We use <a href=\"https://phabricator.wikimedia.org/maniphest/task/create/?projects=PHID-PROJ-mszihytuo3ij3fcxcxgm\" rel=\"nofollow\">Phabricator to track\nissues</a>. See the <a href=\"https://phabricator.wikimedia.org/tag/restbase/\" rel=\"nofollow\">list of current issues in RESTBase</a>.</p>\n<h2><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\" href=\"#installation\"></a>Installation</h2>\n<p>Make sure that you have node 6+:</p>\n<div class=\"highlight highlight-source-shell\"><pre>sudo apt-get install nodejs nodejs-legacy nodejs-dev npm</pre></div>\n<p>Note: if your distribution does not have a recent version of Node, you can\ninstall one via <a href=\"https://github.com/creationix/nvm\">nvm</a>.</p>\n<p>From the <em>restbase</em> project directory, install the Node dependencies:</p>\n<div class=\"highlight highlight-source-shell\"><pre>npm install</pre></div>\n<p>Start RESTBase:</p>\n<div class=\"highlight highlight-source-shell\"><pre>node server</pre></div>\n<p>The defaults without a config file should work for a local Cassandra\ninstallation with the default passwords. Restbase has been tested with\n<a href=\"http://wiki.apache.org/cassandra/DebianPackaging\" rel=\"nofollow\">Cassandra 2.2.6</a>.\nTo customize RESTBase's behaviour, copy the example config to its\ndefault location:</p>\n<div class=\"highlight highlight-source-shell\"><pre>cp config.example.yaml config.yaml</pre></div>\n<p>You can also pass in the path to another file with the <code>-c</code> commandline option\nto <code>server.js</code>. If you're running a single Cassandra instance (e.g. a local\ndevelopment environment), set <code>defaultConsistency</code> to <code>one</code> in\n<code>config.yaml</code>.</p>\n<h2><a id=\"user-content-usage\" class=\"anchor\" aria-hidden=\"true\" href=\"#usage\"></a>Usage</h2>\n<p>See the <a href=\"https://en.wikipedia.org/api/rest_v1/\" rel=\"nofollow\">Wikimedia REST content API sandbox</a>\nfor a fine example of what RESTBase can do.</p>\n<h2><a id=\"user-content-development\" class=\"anchor\" aria-hidden=\"true\" href=\"#development\"></a>Development</h2>\n<h3><a id=\"user-content-testing\" class=\"anchor\" aria-hidden=\"true\" href=\"#testing\"></a>Testing</h3>\n<p>To run all the tests from a clean slate, first make sure Cassandra is running locally, then fire up the tests with npm:</p>\n<pre>npm test\n</pre>\n<p>To run tests from a single file, e.g. <em>test/features/pagecontent/rerendering.js</em>, run mocha with the file as an argument:</p>\n<pre>mocha test/features/pagecontent/rerendering.js\n</pre>\n<p>Note that this might require some setup (e.g. creating the necessary domain and buckets), which is currently done by <em>test/buckets.js</em>.</p>\n<p>This also works for a directory, e.g. <em>test/features/pagecontent/</em>:</p>\n<pre>mocha test/features/pagecontent\n</pre>\n<h3><a id=\"user-content-coverage\" class=\"anchor\" aria-hidden=\"true\" href=\"#coverage\"></a>Coverage</h3>\n<p>To check the test coverage, use npm, then browse the report:</p>\n<pre>npm run-script coverage\n</pre>\n<p>The coverage report can now be found in <em>&lt;project&gt;/coverage/lcov-report/index.html</em>.</p>\n<h2><a id=\"user-content-design-docs\" class=\"anchor\" aria-hidden=\"true\" href=\"#design-docs\"></a>Design docs</h2>\n</article>",
        "created_at": "2018-08-25T16:38:52+0000",
        "updated_at": "2018-08-25T16:39:00+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/56668?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11987"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11986,
        "uid": null,
        "title": "RESTBase",
        "url": "https://www.mediawiki.org/wiki/RESTBase",
        "content": null,
        "created_at": "2018-08-25T16:38:01+0000",
        "updated_at": "2018-08-25T16:38:16+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "www.mediawiki.org",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11986"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 560,
            "label": "content",
            "slug": "content"
          }
        ],
        "is_public": false,
        "id": 11985,
        "uid": null,
        "title": "\"Wikimedia Content API: A Cassandra Use-case\" by Eric Evans",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=85e_TCf7FZg",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/85e_TCf7FZg?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-08-25T16:35:36+0000",
        "updated_at": "2018-08-25T16:35:45+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/85e_TCf7FZg/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11985"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 11981,
        "uid": null,
        "title": "Cassandra Schemas for Beginners (like me)",
        "url": "https://medium.com/@jochasinga/cassandra-schemas-for-beginners-like-me-9714cee9236a",
        "content": "<div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><figure id=\"b12d\" class=\"graf graf--figure graf--leading\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*vGcN8EHmsZZlPafVq6Qkew.jpeg\" data-width=\"700\" data-height=\"390\" src=\"https://cdn-images-1.medium.com/max/1600/1*vGcN8EHmsZZlPafVq6Qkew.jpeg\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Image from <a href=\"https://www.jisc.ac.uk/rd/projects/digging-into-data-challenge\" data-href=\"https://www.jisc.ac.uk/rd/projects/digging-into-data-challenge\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://www.jisc.ac.uk/rd/projects/digging-into-data-challenge</a></figcaption></div></figure><h1 id=\"0eb2\" class=\"graf graf--h3 graf-after--figure graf--title\">Cassandra Schemas for Beginners (like me)</h1><h2 id=\"e327\" class=\"graf graf--h4 graf-after--h3 graf--subtitle\">I was awe-struck when I first learned that Cassandra had tables. Then followed was another question along the line of how was it even a NoSQL database.</h2><p id=\"7ff4\" class=\"graf graf--p graf-after--h4\">Upon reading and playing around with it I began to realize that the rows and columns were just what’s on the surface. Cassandra is as NoSQL as any other databases.</p><p id=\"a05d\" class=\"graf graf--p graf-after--p\">I’ve been working on a project that utilizes greatly on using Cassandra as a JSON storage, and that required a great understanding than working with document-based solutions like MongoDB or CouchDB, which already provide ways to store JSON out of the box. The first idea was storing a JSON blob as a string value in a single column, but that was a pretty bad idea to start with and contradicts greatly to the very reason of using Cassandra, since it would require my application to parse that JSON string every time.</p><blockquote id=\"6caa\" class=\"graf graf--blockquote graf-after--p\"><div>Storing JSON in a Cassandra column as a text or []byte contradicts greatly to the very reason of using Cassandra.</div></blockquote><p id=\"e18b\" class=\"graf graf--p graf-after--blockquote\">Cassandra is more similar to key-value-based NoSQL databases like Redis or a hashtable. For someone coming from relational SQL world, the comfort will end at the CQL syntax and setting primary keys. Coming from NoSQL like MongoDB, however, one will have to get over the query language and schemas but once they pass the NoSQL mental model can be adapted to Cassandra very quickly.</p><p id=\"9e91\" class=\"graf graf--p graf-after--p\">This is my best attempt at relating Cassandra schema design to a more traditional key-value data format like JSON to better educate myself with the hope of somebody else getting something out of it too.</p><h4 id=\"4ccf\" class=\"graf graf--h4 graf-after--p\">NoSQL != Schemaless</h4><p id=\"0a72\" class=\"graf graf--p graf-after--h4\">NoSQL stands for <em class=\"markup--em markup--p-em\">Not Only SQL</em>. It does not mean no schemas. For many, using document- and key-value-based NoSQL databases can lead to this misconception. Even key-value data pairs like JSON has schema or structure. It’s just more flexible to changes than a relational table-based schema.</p><h4 id=\"59a3\" class=\"graf graf--h4 graf-after--p\">The Only Difference</h4><p id=\"8dc4\" class=\"graf graf--p graf-after--h4\">The only real distinction most, if not all, NoSQL databases have from SQL or relational databases is <strong class=\"markup--strong markup--p-strong\">the lack of relationship</strong> between two distinct data collections, tables, documents or whatever each database uses as the term to define a a set of related data. You kind of just query a table or a document, get the appropriate data, and then query another table to perform a cross-table query or namely the JOIN operation in the SQL world (This will lead to the argument number 2 below).</p><blockquote id=\"4f3c\" class=\"graf graf--blockquote graf-after--p\"><div>The only distinction NoSQL databases have from a relational SQL-based databases is the lack of inter-collection relationships.</div></blockquote><p id=\"3a88\" class=\"graf graf--p graf-after--blockquote\">The long-standing argument between the SQL and NoSQL camps sums up to:</p><ol class=\"postList\"><li id=\"9f60\" class=\"graf graf--li graf-after--p\">the fact that a relational database can never scale as easily as NoSQL.</li><li id=\"c3d5\" class=\"graf graf--li graf-after--li\">Relational databases provide more flexible and robust queries while for NoSQL, without knowing the structure or schema, one is forced to think hard about how an application will access the data. This is known as <strong class=\"markup--strong markup--li-strong\">query-driven design</strong>.</li></ol><p id=\"4886\" class=\"graf graf--p graf-after--li\">I will not talk about the topic of scalability since most NoSQL users should be well-aware of that, and it’s best discussed somewhere else. However, I will focus on the underlying structure of Cassandra for the hope of a better understanding that will lead to a better query-driven designed schemas.</p><h4 id=\"c639\" class=\"graf graf--h4 graf-after--p\">Cassandra as a Key-value Database</h4><p id=\"f453\" class=\"graf graf--p graf-after--h4\">Cassandra can be thought of as a key-value database. Under the hood and beyond its Cassandra Query Language (CQL) and schemas, it actually contains a lookup key for every data in the form of a <strong class=\"markup--strong markup--p-strong\">primary key</strong>.</p><p id=\"7a52\" class=\"graf graf--p graf-after--p\">Considering this <em class=\"markup--em markup--p-em\">user_tweets</em> table:</p></div><div class=\"section-inner sectionLayout--outsetColumn\"><figure id=\"a94f\" class=\"graf graf--figure graf--startsWithDoubleQuote graf--layoutOutsetCenter graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*FQxcoFtHTLP469zlxtSIMA.png\" data-width=\"1230\" data-height=\"360\" data-action=\"zoom\" data-action-value=\"1*FQxcoFtHTLP469zlxtSIMA.png\" src=\"https://cdn-images-1.medium.com/max/2000/1*FQxcoFtHTLP469zlxtSIMA.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">“user_tweets” table</figcaption></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"a6a3\" class=\"graf graf--p graf-after--figure\">The <em class=\"markup--em markup--p-em\">username</em> field acts as a first and only primary key, which in Cassandra’s speak is called the <strong class=\"markup--strong markup--p-strong\">partition key</strong>. A partition key is very important in Cassandra and it basically groups all the related rows together for efficient storage and lookup. This will become clearer once we have more than one tweet per username. The partition key hence can be seen as the “lookup” key similar to what you might have dealt with in any hash table, map, dictionary or other key-value structure.</p></div><div class=\"section-inner sectionLayout--outsetColumn\"><figure id=\"2d54\" class=\"graf graf--figure graf--layoutOutsetCenter graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*9G4Vw6BaaeM77G2JXENtbQ.png\" data-width=\"1453\" data-height=\"670\" data-action=\"zoom\" data-action-value=\"1*9G4Vw6BaaeM77G2JXENtbQ.png\" src=\"https://cdn-images-1.medium.com/max/2000/1*9G4Vw6BaaeM77G2JXENtbQ.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">key-value relationship of the “user_tweet” table</figcaption></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"4744\" class=\"graf graf--p graf-after--figure\">We can simplify it to a JSON structure (not entirely accurate, but useful as a mental model for someone coming from Redis or MongoDB).</p><figure id=\"c29e\" class=\"graf graf--figure graf--iframe graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/8e16adb780c4d577634bc7b5966722cd?postId=9714cee9236a\" data-media-id=\"8e16adb780c4d577634bc7b5966722cd\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"cd31\" class=\"graf graf--p graf-after--figure\">One thing that should be noted very cautiously is that in Cassandra, unlike what the JSON array might portray, each partition of grouped rows under a partition key is stored non-contiguously, possibly on different nodes, making it very costly to access each of them together. In the previous table, if you look at the key-value relationship diagram, you’ll see that each row is not related to one another at all and is stored apart from one another.</p><p id=\"cbc8\" class=\"graf graf--p graf-after--p\">This can never be overstated. We can never access the second-level data (for instance, the email of a user) without accessing the primary username key first. Think of it as a JSON array as portrayed previously. To get to my <em class=\"markup--em markup--p-em\">email</em>, the <em class=\"markup--em markup--p-em\">username</em> must be provided as the key beforehand.</p><pre id=\"3e21\" class=\"graf graf--pre graf-after--p\"><strong class=\"markup--strong markup--pre-strong\">var</strong> tweets = JSON.parse(tweet_data)<br /><strong class=\"markup--strong markup--pre-strong\">var</strong> my_email = tweets[0]['jochasinga']['email'];</pre><p id=\"7828\" class=\"graf graf--p graf-after--pre\">The CQL rough equivalence of the above would have been</p><pre id=\"2305\" class=\"graf graf--pre graf-after--p\"><strong class=\"markup--strong markup--pre-strong\">SELECT</strong> \"email\" <strong class=\"markup--strong markup--pre-strong\">FROM</strong> \"user_tweets\" <strong class=\"markup--strong markup--pre-strong\">WHERE</strong> \"username\" = 'jochasinga';</pre><p id=\"ab4e\" class=\"graf graf--p graf-after--pre\">We supply the primary key, or the “lookup” key to the WHERE clause, hence it’s very cheap to retrieve the <em class=\"markup--em markup--p-em\">email </em>value<em class=\"markup--em markup--p-em\"> </em>of that <em class=\"markup--em markup--p-em\">username</em>.</p><p id=\"7a56\" class=\"graf graf--p graf-after--p\">If we try to query a row by supplying another non-key column to the WHERE clause, we would be getting an error warning us that it’d be very unwise to do so.</p><pre id=\"0941\" class=\"graf graf--pre graf-after--p\"><strong class=\"markup--strong markup--pre-strong\">SELECT</strong> * <strong class=\"markup--strong markup--pre-strong\">FROM</strong> \"user_tweets\" <strong class=\"markup--strong markup--pre-strong\">WHERE</strong> \"email\" = 'jo.chasinga@gmail.com';</pre><p id=\"52ce\" class=\"graf graf--p graf-after--pre\">The above CQL query, reads “select all columns from <em class=\"markup--em markup--p-em\">user_tweets</em> table where the <em class=\"markup--em markup--p-em\">email</em> is ‘jo.chasinga@gmail.com’.”, would return an erratic warning:</p><pre id=\"a834\" class=\"graf graf--pre graf-after--p\">InvalidRequest: code=2200 [Invalid query] message=”Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING</pre><p id=\"56af\" class=\"graf graf--p graf-after--pre\">It makes sense if you think about it. In a table with many more rows, querying by a non-key column like <em class=\"markup--em markup--p-em\">email </em>tells Cassandra to iterate through every primary key<em class=\"markup--em markup--p-em\"> username </em>before hitting the right <em class=\"markup--em markup--p-em\">email</em> value and retrieving the row. Here’s the rough approximation of the query in Javascript querying the previous JSON:</p><pre id=\"3a30\" class=\"graf graf--pre graf-after--p\">tweets.<strong class=\"markup--strong markup--pre-strong\">forEach</strong>(function(elm, i, arr) {</pre><pre id=\"88d4\" class=\"graf graf--pre graf-after--pre\">    // Retrieve an array of all the keys<br /><strong class=\"markup--strong markup--pre-strong\">var</strong> keys = Object.keys[elm]<br /><strong class=\"markup--strong markup--pre-strong\">if </strong>elm[keys[0]]['email'] == \"jo.chasinga@gmail.com\" {<br /><strong class=\"markup--strong markup--pre-strong\">return</strong> elm<br />    }<br />}</pre><p id=\"ed01\" class=\"graf graf--p graf-after--pre\">If you have a million rows of tweets, or in the JSON version, a million objects, you would end up traversing through every one blindly hopefully hitting your luck early. Also remember that each partition is possibly stored on a separate node from one another. You are of course given an option to execute this query anyway by using <em class=\"markup--em markup--p-em\">ALLOW FILTERING</em> flag, but you’ve been warned.</p><p id=\"48e2\" class=\"graf graf--p graf-after--p\">Things start to become clearer when we introduce another <em class=\"markup--em markup--p-em\">tweet_id</em> column as a <strong class=\"markup--strong markup--p-strong\">clustering column</strong> for the tweet table. Let’s say I retweeted to @banksy tweet, making two tweets for me.</p></div><div class=\"section-inner sectionLayout--outsetColumn\"><figure id=\"f767\" class=\"graf graf--figure graf--startsWithDoubleQuote graf--layoutOutsetCenter graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*c9g_AHIOkGM-DDKC5mlAag.png\" data-width=\"1650\" data-height=\"436\" data-action=\"zoom\" data-action-value=\"1*c9g_AHIOkGM-DDKC5mlAag.png\" src=\"https://cdn-images-1.medium.com/max/2000/1*c9g_AHIOkGM-DDKC5mlAag.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">“user_tweets” table after I have retweeted to @banksy</figcaption></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"af72\" class=\"graf graf--p graf-after--figure\">The <em class=\"markup--em markup--p-em\">tweet_id</em> is a clustering column with <em class=\"markup--em markup--p-em\">time_uuid </em>type, ordering rows under <em class=\"markup--em markup--p-em\">jochasinga</em> partition key in an time-ascending order. The <em class=\"markup--em markup--p-em\">email</em> field is being declared as STATIC meaning it is consistent for all the tweets for a user and there’s no need of duplicates. The mental model will be similar to this:</p></div><div class=\"section-inner sectionLayout--outsetColumn\"><figure id=\"969d\" class=\"graf graf--figure graf--layoutOutsetCenter graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*SNNH3D7s2HeT8wKOM9Dysw.png\" data-width=\"1875\" data-height=\"777\" data-action=\"zoom\" data-action-value=\"1*SNNH3D7s2HeT8wKOM9Dysw.png\" src=\"https://cdn-images-1.medium.com/max/2000/1*SNNH3D7s2HeT8wKOM9Dysw.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">Key-value relationship of the “user_tweets” table</figcaption></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"d8f5\" class=\"graf graf--p graf-after--figure\">And the approximate JSON-style representation would be something like</p><figure id=\"077e\" class=\"graf graf--figure graf--iframe graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe width=\"700\" height=\"250\" src=\"https://medium.com/media/70d873a7f58cf6ae30bc092c467052d8?postId=9714cee9236a\" data-media-id=\"70d873a7f58cf6ae30bc092c467052d8\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"4f8a\" class=\"graf graf--p graf-after--figure\">Note that <em class=\"markup--em markup--p-em\">tweet_id</em> value is simplified to string instead of <em class=\"markup--em markup--p-em\">time_uuid</em> type for just for brevity, and there was no such field as <em class=\"markup--em markup--p-em\">row_data</em> in the table<em class=\"markup--em markup--p-em\">. </em>It’s just how JSON needs a “key” for every value, and that how <em class=\"markup--em markup--p-em\">tweet_id </em>column<em class=\"markup--em markup--p-em\"> </em>orders the row data within a username’s partition is similar to how an array store data. However, in Cassandra, the <em class=\"markup--em markup--p-em\">tweet_id </em>encapsulates the time information used in the ordering of each row. With this JSON representation, the index of the array has nothing to do with the <em class=\"markup--em markup--p-em\">tweet_id</em> value.</p><p id=\"6f65\" class=\"graf graf--p graf-after--p\">Now with ‘jochasinga’ having more than one tweet, to query a specific one, you either have to do the following:</p><ol class=\"postList\"><li id=\"9fa2\" class=\"graf graf--li graf-after--p\">Query directly using a partition key and a unique primary key, in this case, the <em class=\"markup--em markup--li-em\">tweet_id</em>.</li></ol><pre id=\"b15d\" class=\"graf graf--pre graf-after--li\"><strong class=\"markup--strong markup--pre-strong\">SELECT</strong> * <strong class=\"markup--strong markup--pre-strong\">FROM</strong> \"user_tweets\" <strong class=\"markup--strong markup--pre-strong\">WHERE </strong>\"username\" = 'jochasinga'<strong class=\"markup--strong markup--pre-strong\"> AND</strong> \"id\" = bd48ac00-8310-11e5-985d-dd516b67e698;</pre><p id=\"c9b8\" class=\"graf graf--p graf-after--pre\">2. Query using a partition key and another unique primary key column OR non-key column, like the tweet body.</p><pre id=\"876d\" class=\"graf graf--pre graf-after--p\"><strong class=\"markup--strong markup--pre-strong\">SELECT</strong> * <strong class=\"markup--strong markup--pre-strong\">FROM</strong> \"user_tweets\" <strong class=\"markup--strong markup--pre-strong\">WHERE</strong> \"username\" = 'jochasinga' <strong class=\"markup--strong markup--pre-strong\">AND </strong>\"tweet\" = '@banksy thanks I'll try to check it out!'</pre><p id=\"ff9d\" class=\"graf graf--p graf-after--pre\">And in return, this is the row we get</p></div><div class=\"section-inner sectionLayout--outsetColumn\"><figure id=\"e367\" class=\"graf graf--figure graf--layoutOutsetCenter graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*eCVfUJGoCxnmh5OGWV_fZw.png\" data-width=\"1650\" data-height=\"211\" data-action=\"zoom\" data-action-value=\"1*eCVfUJGoCxnmh5OGWV_fZw.png\" src=\"https://cdn-images-1.medium.com/max/2000/1*eCVfUJGoCxnmh5OGWV_fZw.png\" alt=\"image\" /></div></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><h4 id=\"46fe\" class=\"graf graf--h4 graf-after--figure\">Secondary Indexes</h4><p id=\"9e35\" class=\"graf graf--p graf-after--h4\">Secondary indexes are sort of a “hack” to promote a non-key column (that is, a column that is not a primary key) to a secondary “key” that you can query against just like a primary key. This is just like a reverse lookup. For instance, querying with a <em class=\"markup--em markup--p-em\">username</em> you<em class=\"markup--em markup--p-em\"> </em>can get the <em class=\"markup--em markup--p-em\">email</em> of the user.</p><pre id=\"97f7\" class=\"graf graf--pre graf-after--p\"><strong class=\"markup--strong markup--pre-strong\">SELECT</strong> \"email\" <strong class=\"markup--strong markup--pre-strong\">FROM </strong>\"user_tweets\" <strong class=\"markup--strong markup--pre-strong\">WHERE </strong>\"username\" = 'jochasinga';</pre><p id=\"5ed5\" class=\"graf graf--p graf-after--pre\">In some case, we may want to query a username based on the email. By creating an index on <em class=\"markup--em markup--p-em\">email</em>, you can perform that kind of reverse lookup.</p><blockquote id=\"f931\" class=\"graf graf--blockquote graf-after--p\"><div>Note: Right now it is not possible to create an index on a static column, though in theory that can be done. See <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-8103\" data-href=\"https://issues.apache.org/jira/browse/CASSANDRA-8103\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">this thread</a> on how Apache is planning to include this feature in the next versions.</div></blockquote><p id=\"9a75\" class=\"graf graf--p graf-after--blockquote\">We will add another column of type <em class=\"markup--em markup--p-em\">list&lt;text&gt; </em>to store hashtags in a tweet. It makes sense considering how Twitter displays tweets with the corresponding hashtag in the search results. For instance, when a user search for all the tweets with hashtag <em class=\"markup--em markup--p-em\">#funnycats,</em> Twitter could have queried it this way:</p><pre id=\"46f9\" class=\"graf graf--pre graf-after--p\"><strong class=\"markup--strong markup--pre-strong\">SELECT </strong>* <strong class=\"markup--strong markup--pre-strong\">FROM</strong> \"user_tweets\" <strong class=\"markup--strong markup--pre-strong\">WHERE</strong> \"hashtags\" <strong class=\"markup--strong markup--pre-strong\">CONTAINS</strong> \"funnycats\";</pre><p id=\"0525\" class=\"graf graf--p graf-after--pre\">Pretty self-descriptive.</p><p id=\"cca3\" class=\"graf graf--p graf-after--p\">But without “marking” the <em class=\"markup--em markup--p-em\">hashtags</em> column, it would return a complaint just like how we tried querying with a non-key column value.</p><p id=\"48e4\" class=\"graf graf--p graf-after--p\">Let’s add the hashtags column and see how that goes.</p><pre id=\"76aa\" class=\"graf graf--pre graf-after--p\"><strong class=\"markup--strong markup--pre-strong\">ALTER</strong> <strong class=\"markup--strong markup--pre-strong\">TABLE</strong> \"user_tweets\" <strong class=\"markup--strong markup--pre-strong\">ADD</strong> \"hashtags\" list&lt;text&gt;;</pre><p id=\"c6e1\" class=\"graf graf--p graf-after--pre\">At this point, we will have to alter the data of the previous tweets’ <em class=\"markup--em markup--p-em\">hashtags</em> column, since the column we have just added contains nothing on each row.</p><pre id=\"0979\" class=\"graf graf--pre graf-after--p\"><strong class=\"markup--strong markup--pre-strong\">UPDATE</strong> \"user_tweets\" SET \"hashtags\" = ['art', 'graffiti', 'nyc'] <strong class=\"markup--strong markup--pre-strong\">WHERE</strong> \"username\" = 'jochasinga' <strong class=\"markup--strong markup--pre-strong\">AND</strong> \"id\" = bd48ac00-8310-11e5-985d-dd516b67e698;</pre><pre id=\"fe8f\" class=\"graf graf--pre graf-after--pre\"><strong class=\"markup--strong markup--pre-strong\">UPDATE</strong> \"user_tweets\" SET \"hashtags\" = ['rad'] <strong class=\"markup--strong markup--pre-strong\">WHERE</strong> \"username\" = 'banksy' <strong class=\"markup--strong markup--pre-strong\">AND</strong> \"id\" = 76e7a4d0-e796-11e3-90ce-5f98e903bf02;</pre><pre id=\"6c62\" class=\"graf graf--pre graf-after--pre\"><strong class=\"markup--strong markup--pre-strong\">UPDATE</strong> \"user_tweets\" SET \"hashtags\" = ['sad', 'HopeForUkraine'] <strong class=\"markup--strong markup--pre-strong\">WHERE</strong> \"username\" = 'random_may' <strong class=\"markup--strong markup--pre-strong\">AND</strong> \"id\" = fa507380-8310-11e5-985d-dd516b67e698;</pre><p id=\"e8d6\" class=\"graf graf--p graf-after--pre\">Then create a secondary index on the <em class=\"markup--em markup--p-em\">hashtags</em> column</p><pre id=\"6c02\" class=\"graf graf--pre graf-after--p\"><strong class=\"markup--strong markup--pre-strong\">CREATE INDEX ON</strong> \"user_tweets\" (\"hashtags\")</pre><p id=\"e590\" class=\"graf graf--p graf-after--pre\">We will turn up with a new version of user_tweets that looks like this:</p></div><div class=\"section-inner sectionLayout--outsetColumn\"><figure id=\"87d1\" class=\"graf graf--figure graf--layoutOutsetCenter graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*Fmu-Q_iiGJgrvJky2q7ViA.png\" data-width=\"2070\" data-height=\"436\" data-action=\"zoom\" data-action-value=\"1*Fmu-Q_iiGJgrvJky2q7ViA.png\" src=\"https://cdn-images-1.medium.com/max/2000/1*Fmu-Q_iiGJgrvJky2q7ViA.png\" alt=\"image\" /></div></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"9e5a\" class=\"graf graf--p graf-after--figure\">Now we will be able to query against the hashtags like Twitter search would have done it.</p><pre id=\"7417\" class=\"graf graf--pre graf-after--p\"><strong class=\"markup--strong markup--pre-strong\">SELECT</strong> * <strong class=\"markup--strong markup--pre-strong\">FROM</strong> \"user_tweets\" <strong class=\"markup--strong markup--pre-strong\">WHERE</strong> \"hashtags\" <strong class=\"markup--strong markup--pre-strong\">CONTAINS </strong>'art';</pre><p id=\"2d1a\" class=\"graf graf--p graf-after--pre\">Guess which row will be returned. (my first tweet, of course!)</p><p id=\"6234\" class=\"graf graf--p graf-after--p\">I said secondary indexes are “hacks” because they don’t scale well and should be used sparingly especially on columns with high-cardinality, meaning data are mostly distinct. It is more suitable for columns with low-cardinality like, say, static columns (scroll above to see why it isn’t possible yet to create an index on static columns). <a href=\"https://pantheon.io/blog/cassandra-scale-problem-secondary-indexes\" data-href=\"https://pantheon.io/blog/cassandra-scale-problem-secondary-indexes\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">This article </a>is great at elaborating the problem that comes with Cassandra’s secondary indexes.</p><h4 id=\"3463\" class=\"graf graf--h4 graf-after--p\">Conclusion</h4><p id=\"d780\" class=\"graf graf--p graf-after--h4\">It is best to think of Cassandra as a structure of sorted key-value pairs of documents than cells of data, and this was proved to be very useful for my process of designing the schema around what I needed. Partition keys and clustering columns are almost the only two most important friends of yours.</p><p id=\"bbbc\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">DON’T RESTRICT YOURSELF WITH SCHEMAS.</strong></p><p id=\"aaa0\" class=\"graf graf--p graf-after--p\">Here is a few rules you can remember</p><ul class=\"postList\"><li id=\"961c\" class=\"graf graf--li graf-after--p\">To partition is to separate two things from each other. So if you create your table like this</li></ul><pre id=\"081c\" class=\"graf graf--pre graf-after--li\"><strong class=\"markup--strong markup--pre-strong\">CREATE TABLE</strong> \"my_users\" (<br />    id uuid,<br />    email text,<br />    password blob,<br />    country text,<br /><strong class=\"markup--strong markup--pre-strong\">PRIMARY KEY</strong> (country, id)<br />);</pre><p id=\"1823\" class=\"graf graf--p graf-after--pre\">You are making users from two different countries will be stored separately. Rows in a partition will be ordered according to the unique id.</p><pre id=\"ccc1\" class=\"graf graf--pre graf-after--p\"><strong class=\"markup--strong markup--pre-strong\">CREATE TABLE</strong> \"my_users\" (<br />    id uuid,<br />    email text,<br />    password blob,<br />    city text,<br />    country text,<br /><strong class=\"markup--strong markup--pre-strong\">PRIMARY KEY</strong> ((country, city), id)<br />);</pre><p id=\"f271\" class=\"graf graf--p graf-after--pre\">Users from the same country, but in different cities will be stored separately. Rows in a partition will be ordered by id.</p><ul class=\"postList\"><li id=\"9730\" class=\"graf graf--li graf-after--p\">Clustering column, or the second primary key, decide how you want the database to be ordered. (Either ascending or descending)</li><li id=\"3cce\" class=\"graf graf--li graf-after--li\">Secondary indexes avoid <strong class=\"markup--strong markup--li-strong\">denormalization</strong>, or the process of creating redundant data to create effective single-partition reads, but they are quite costly and should be used on medium-traffic queries.</li><li id=\"5e6b\" class=\"graf graf--li graf-after--li graf--trailing\">Add columns at will. Cassandra is flexible and rows without data do not take memories (there’s no NULL pointer value for missing value).</li></ul></div></div>",
        "created_at": "2018-08-23T16:49:09+0000",
        "updated_at": "2018-08-23T16:49:24+0000",
        "published_at": "2015-11-13T18:54:30+0000",
        "published_by": [
          "Panisuan Joe Chasinga"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 10,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*vGcN8EHmsZZlPafVq6Qkew.jpeg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11981"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 228,
            "label": "dotnet",
            "slug": "net"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 11954,
        "uid": null,
        "title": "DSE6 + .NET v?",
        "url": "https://compositecode.blog/2018/08/05/dse6-net-v/",
        "content": "<p>Project Repo: <a href=\"https://github.com/Adron/InteroperabilityBlackBox\" target=\"_blank\">Interoperability Black Box</a></p>\n<p>First steps. Let’s get .NET installed and setup. I’m running Ubuntu 18.04 for this setup and start of project. To install .NET on Ubuntu one needs to go through a multi-command process of keys and some other stuff, fortunately Microsoft’s teams have made this almost easy by providing the commands for the various Linux distributions here. The commands I ran are as follows to get all this initial setup done.</p>\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">\nwget -qO- https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; microsoft.asc.gpg\nsudo mv microsoft.asc.gpg /etc/apt/trusted.gpg.d/\nwget -q https://packages.microsoft.com/config/ubuntu/18.04/prod.list\nsudo mv prod.list /etc/apt/sources.list.d/microsoft-prod.list\nsudo chown root:root /etc/apt/trusted.gpg.d/microsoft.asc.gpg\nsudo chown root:root /etc/apt/sources.list.d/microsoft-prod.list\n</pre>\n<p>After all this I could then install the .NET SDK. It’s been so long since I actually installed .NET on anything that I wasn’t sure if I just needed the runtime, the SDK, or what I’d actually need. I just assumed it would be safe to install the SDK and then install the runtime too.</p>\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">\nsudo apt-get install apt-transport-https\nsudo apt-get update\nsudo apt-get install dotnet-sdk-2.1\n</pre>\n<p>Then the runtime.</p>\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">\nsudo apt-get install aspnetcore-runtime-2.1\n</pre>\n<p><img data-attachment-id=\"14246\" data-permalink=\"https://compositecode.blog/2018/08/05/dse6-net-v/logo/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/06/logo.png?w=102&amp;h=102\" data-orig-size=\"2000,2000\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"logo\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/06/logo.png?w=102&amp;h=102?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/06/logo.png?w=102&amp;h=102?w=788\" class=\"wp-image-14246 alignright\" src=\"https://compositecode.files.wordpress.com/2018/06/logo.png?w=102&amp;h=102\" alt=\"logo\" width=\"102\" height=\"102\" srcset=\"https://compositecode.files.wordpress.com/2018/06/logo.png?w=102&amp;h=102 102w, https://compositecode.files.wordpress.com/2018/06/logo.png?w=204&amp;h=204 204w, https://compositecode.files.wordpress.com/2018/06/logo.png?w=150&amp;h=150 150w\" />Alright. Now with this installed, I wanted to also see if <a href=\"http://www.jetbrains.com/rider/\" target=\"_blank\" rel=\"noopener\">Jetbrains Rider</a> would detect – or at least what would I have to do – to have the IDE detect that .NET is now installed. So I opened up the IDE to see what the results would be. Over the left hand side of the new solution dialog, if anything isn’t installed Rider usually will display a message that X whatever needs installed. But it looked like everything is showing up as installed, “<em>yay for things working (at this point)!</em>”</p>\n<p><img data-attachment-id=\"14248\" data-permalink=\"https://compositecode.blog/2018/08/05/dse6-net-v/rider-01/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=788\" data-orig-size=\"799,536\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"rider-01\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=788?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=788?w=788\" class=\"alignnone size-full wp-image-14248\" src=\"https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=788\" alt=\"rider-01\" srcset=\"https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=788 788w, https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=150 150w, https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=300 300w, https://compositecode.files.wordpress.com/2018/06/rider-01.png?w=768 768w, https://compositecode.files.wordpress.com/2018/06/rider-01.png 799w\" /></p>\n<p>Next up is to get a solution started with the pertinent projects for what I want to build.</p>\n<p><img data-attachment-id=\"14249\" data-permalink=\"https://compositecode.blog/2018/08/05/dse6-net-v/dse2-2/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/06/dse2.png?w=788\" data-orig-size=\"800,139\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"dse2\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/06/dse2.png?w=788?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/06/dse2.png?w=788?w=788\" class=\"alignnone size-full wp-image-14249\" src=\"https://compositecode.files.wordpress.com/2018/06/dse2.png?w=788\" alt=\"dse2\" srcset=\"https://compositecode.files.wordpress.com/2018/06/dse2.png?w=788 788w, https://compositecode.files.wordpress.com/2018/06/dse2.png?w=150 150w, https://compositecode.files.wordpress.com/2018/06/dse2.png?w=300 300w, https://compositecode.files.wordpress.com/2018/06/dse2.png?w=768 768w, https://compositecode.files.wordpress.com/2018/06/dse2.png 800w\" /></p>\n<p><img data-attachment-id=\"14250\" data-permalink=\"https://compositecode.blog/2018/08/05/dse6-net-v/kazam_screenshot_00001/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=788\" data-orig-size=\"1264,364\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"Kazam_screenshot_00001\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=788?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=788?w=788\" class=\"alignnone size-full wp-image-14250\" src=\"https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=788\" alt=\"Kazam_screenshot_00001\" srcset=\"https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=788 788w, https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=150 150w, https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=300 300w, https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=768 768w, https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png?w=1024 1024w, https://compositecode.files.wordpress.com/2018/06/kazam_screenshot_00001.png 1264w\" /></p>\n<p>For the next stage I created three projects.</p>\n<ol><li>InteroperationalBlackBox – A basic class library that will be used by a console application or whatever other application or service that may need access to the specific business logic or what not.</li>\n<li>InteroperationalBlackBox.Tests – An <a href=\"https://xunit.github.io/\" target=\"_blank\" rel=\"noopener\">xunit</a> testing project for testing anything that might need some good ole’ testing.</li>\n<li>InteroperationalBlackBox.Cli – A console application (CLI) that I’ll use to interact with the class library and add capabilities going forward.</li>\n</ol><p>Alright, now that all the basic projects are setup in the solution, I’ll go out and see about the <a href=\"https://docs.datastax.com/en/developer/csharp-driver-dse/2.3/\" target=\"_blank\" rel=\"noopener\">.NET DataStax Enterprise driver</a>. Inside Jetbrains Rider I can right click on a particular project that I want to add or manage dependencies for. I did that and then put “<em>dse</em>” in the search box. The dialog pops up from the bottom of the IDE and you can add it by clicking on the bottom right plus sign in the description box to the right. Once you click the plus sign, once installed, it becomes a little red x.</p>\n<p><img data-attachment-id=\"14252\" data-permalink=\"https://compositecode.blog/2018/08/05/dse6-net-v/dse-adding-package/\" data-orig-file=\"https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=788\" data-orig-size=\"1398,594\" data-comments-opened=\"1\" data-image-meta=\"{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;,&quot;orientation&quot;:&quot;0&quot;}\" data-image-title=\"dse-adding-package\" data-image-description=\"\" data-medium-file=\"https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=788?w=300\" data-large-file=\"https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=788?w=788\" class=\"alignnone size-full wp-image-14252\" src=\"https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=788\" alt=\"dse-adding-package\" srcset=\"https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=788 788w, https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=150 150w, https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=300 300w, https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=768 768w, https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png?w=1024 1024w, https://compositecode.files.wordpress.com/2018/06/dse-adding-package.png 1398w\" /></p>\n<p>Alright. Now it’s <em>almost</em> time to get some code working. We need ourselves a database first however. I’m going to setup a cluster in <a href=\"https://cloud.google.com/\" target=\"_blank\" rel=\"noopener\">Google Cloud Platform</a> (GCP), but feel free to use whatever cluster you’ve got. These instructions will basically be reusable across wherever you’ve got your cluster setup. I wrote up a walk through and instructions for the GCP Marketplace a few weeks ago. I used the same offering to get this example cluster up and running to use. So, now back to getting the first snippets of code working.</p>\n<p>Let’s write a test first.</p>\n<pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\n[Fact]\npublic void ConfirmDatabase_Connects_False()\n{\n    var box = new BlackBox();\n    Assert.Equal(false, box.ConfirmConnection());\n}\n</pre>\n<p>In this test, I named the class called BlackBox and am planning to have a parameterless constructor. But as things go tests are very fluid, or ought to be, and I may change it in the next iteration. I’m thinking, at least to get started, that I’ll have a method to test and confirm a connection for the CLI. I’ve named it ConfirmConnection for that purpose. Initially I’m going to test for false, but that’s primarily just to get started. Now, time to implement.</p>\n<pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\nnamespace InteroperabilityBlackBox\nusing System;\nusing Dse;\nusing Dse.Auth;\n\nnamespace InteroperabilityBlackBox\n{\n    public class BlackBox\n    {\n        public BlackBox()\n        {}\n\n        public bool ConfirmConnection()\n        {\n            return false;\n        }\n    }\n}\n</pre>\n<p>That gives a passing test and I move forward. For more of the run through of moving from this first step to the finished code session check out this</p>\n\n<div class=\"jetpack-video-wrapper\"><iframe class=\"youtube-player\" width=\"788\" height=\"444\" src=\"https://www.youtube.com/embed/2a6_oDV5Dqs?version=3&amp;rel=1&amp;fs=1&amp;autohide=2&amp;showsearch=0&amp;showinfo=1&amp;iv_load_policy=1&amp;wmode=transparent\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe></div>\n<p>By the end of the coding session I had a few tests.</p>\n<pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\nusing Xunit;\n\nnamespace InteroperabilityBlackBox.Tests\n{\n    public class MakingSureItWorksIntegrationTests\n    {\n        [Fact]\n        public void ConfirmDatabase_Connects_False()\n        {\n            var box = new BlackBox();\n            Assert.Equal(false, box.ConfirmConnection());\n        }\n\n        [Fact]\n        public void ConfirmDatabase_PassedValuesConnects_True()\n        {\n            var box = new BlackBox(\"cassandra\", \"\", \"\");\n            Assert.Equal(false, box.ConfirmConnection());\n        }\n\n        [Fact]\n        public void ConfirmDatabase_PassedValuesConnects_False()\n        {\n            var box = new BlackBox(\"cassandra\", \"notThePassword\", \"\");\n            Assert.Equal(false, box.ConfirmConnection());\n        }\n    }\n}\n</pre>\n<p>The respective code for connecting to the database cluster, per the walk through I wrote about here, at session end looked like this.</p>\n<pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\nusing System;\nusing Dse;\nusing Dse.Auth;\n\nnamespace InteroperabilityBlackBox\n{\n    public class BlackBox : IBoxConnection\n    {\n        public BlackBox(string username, string password, string contactPoint)\n        {\n            UserName = username;\n            Password = password;\n            ContactPoint = contactPoint;\n        }\n\n        public BlackBox()\n        {\n            UserName = \"ConfigValueFromSecretsVault\";\n            Password = \"ConfigValueFromSecretsVault\";\n            ContactPoint = \"ConfigValue\";\n        }\n\n        public string ContactPoint { get; set; }\n        public string UserName { get; set; }\n        public string Password { get; set; }\n\n        public bool ConfirmConnection()\n        {\n            IDseCluster cluster = DseCluster.Builder()\n                .AddContactPoint(ContactPoint)\n                .WithAuthProvider(new DsePlainTextAuthProvider(UserName, Password))\n                .Build();\n\n            try\n            {\n                cluster.Connect();\n                return true;\n            }\n            catch (Exception e)\n            {\n                Console.WriteLine(e);\n                return false;\n            }\n\n        }\n    }\n}\n</pre>\n<p>With my interface providing the contract to meet.</p>\n<pre class=\"brush: csharp; title: ; notranslate\" title=\"\">\nnamespace InteroperabilityBlackBox\n{\n    public interface IBoxConnection\n    {\n        string ContactPoint { get; set; }\n        string UserName { get; set; }\n        string Password { get; set; }\n        bool ConfirmConnection();\n    }\n}\n</pre>\n<h2>Conclusions &amp; Next Steps</h2>\n<p>After I wrapped up the session two things stood out that needed fixed for the next session. I’ll be sure to add these as objectives for the next coding session at 3pm PST on Thursday.</p>\n<ol><li>The tests really needed to more resiliently confirm the integrations that I was working to prove out. My plan at this point is to add some Docker images that would provide the development integration tests a point to work against. This would alleviate the need for something outside of the actual project in the repository to exist. Removing that fragility.</li>\n<li>The application, in its “Black Box”, should do something. For the next session we’ll write up some feature requests we’d want, or maybe someone has some suggestions of functionality they’d like to see implemented in a CLI using .NET Core working against a DataStax Enterprise Cassandra Database Cluster? Feel free to leave a comment or three about a feature, I’ll work on adding it during the next session.</li>\n</ol><ul><li>Project Repo: <a href=\"https://github.com/Adron/InteroperabilityBlackBoxhttps://github.com/Adron/InteroperabilityBlackBox\" target=\"_blank\" rel=\"noopener\">https://github.com/Adron/InteroperabilityBlackBox</a></li>\n<li>File an Feature Request: <a href=\"https://github.com/Adron/InteroperabilityBlackBox/issues/new?template=feature_request.md\" target=\"_blank\" rel=\"noopener\">https://github.com/Adron/InteroperabilityBlackBox/issues/new?template=feature_request.md</a></li>\n</ul>",
        "created_at": "2018-08-14T13:52:57+0000",
        "updated_at": "2018-08-14T13:52:59+0000",
        "published_at": "2018-08-05T21:05:51+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "compositecode.blog",
        "preview_picture": "https://compositecode.files.wordpress.com/2018/06/datastax_and_rider.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11954"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1272,
            "label": "graph.database",
            "slug": "graph-database"
          },
          {
            "id": 1330,
            "label": "gremlin",
            "slug": "gremlin"
          },
          {
            "id": 1331,
            "label": "tinkerpop",
            "slug": "tinkerpop"
          }
        ],
        "is_public": false,
        "id": 11952,
        "uid": null,
        "title": "elubow/titan-gremlin",
        "url": "https://github.com/elubow/titan-gremlin",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p style=\"text-align: center;\"><a target=\"_blank\" href=\"https://raw.githubusercontent.com/elubow/titan-gremlin/master/titan-docker-logo.png\"><img src=\"https://raw.githubusercontent.com/elubow/titan-gremlin/master/titan-docker-logo.png\" width=\"250\" alt=\"image\" /></a></p>\n<p><a href=\"http://titandb.io/\" rel=\"nofollow\">Titan</a> is a free, open source database that is capable of processing\nextremely large graphs and it supports a variety of indexing and storage backends,\nwhich makes it easier to extend than some popular NoSQL Graph databases.</p>\n<p>This docker image instantiaties a Titan graph database that is capable of\nintegrating with an ElasticSearch container (Indexing) and a Cassandra container (Storage).</p>\n<p>The default distribution of Titan runs on a single node, so I thought it would be helpful\nif there was a modular way at runtime to hook up Titan to its dependencies.</p>\n<p>Enter Docker. Now it is possible to run Titan and it's dependencies in separate Docker containers.</p>\n<h2><a id=\"user-content-titan\" class=\"anchor\" aria-hidden=\"true\" href=\"#titan\"></a>Titan</h2>\n<p>This container is using Titan 1.0.0. Please refer to\nits <a href=\"https://github.com/thinkaurelius/titan/wiki/Downloads\">page</a> for more information.</p>\n<h2><a id=\"user-content-tinkerpop-and-gremlin\" class=\"anchor\" aria-hidden=\"true\" href=\"#tinkerpop-and-gremlin\"></a>TinkerPop and Gremlin</h2>\n<p><a href=\"http://www.tinkerpop.com/\" rel=\"nofollow\">TinkerPop</a> is a vendor-independent API specification for\nmanipulating and access Graph databases. This is using TinkerPop 3.0.1.</p>\n<h2><a id=\"user-content-running\" class=\"anchor\" aria-hidden=\"true\" href=\"#running\"></a>Running</h2>\n<p>The minimum system requirements for this stack is 1 GB with 2 cores.</p>\n<pre>docker run -d --name es1 elasticsearch\ndocker run -d --name cas1 elubow/cassandra\ndocker run -d -P --name titan1 --link es1:elasticsearch --link cas1:cassandra elubow/titan-gremlin\n</pre>\n<p>I run with a 3 node Cassandra cluster and some local ports exported, like so:</p>\n<pre>docker run -d --name cas1 -p 7000:7000 -p 7001:7001 -p 7199:7199 -p 9160:9160 -p 9042:9042 elubow/cassandra\ndocker run -d --name cas2 --link cas1:cassandra elubow/cassandra start docker inspect --format '{{ .NetworkSettings.IPAddress }}' cas1\ndocker run -d --name cas3 --link cas1:cassandra elubow/cassandra start docker inspect --format '{{ .NetworkSettings.IPAddress }}' cas1\ndocker run -d --name es1 --link cas1:cassandra -p 9200:9200 elasticsearch\ndocker run -d --name titan1 --link es1:elasticsearch --link cas1:cassandra -p 8182:8182 -p 8184:8184 elubow/titan-gremlin\n</pre>\n<h2><a id=\"user-content-connecting-with-gremlin-client\" class=\"anchor\" aria-hidden=\"true\" href=\"#connecting-with-gremlin-client\"></a>Connecting with Gremlin Client</h2>\n<p>If you want to connect from a Gremlin client, download <a href=\"http://s3.thinkaurelius.com/downloads/titan/titan-1.0.0-hadoop1.zip\" rel=\"nofollow\">Titan</a>.\nThen create a properties file that looks like this where the <code>storage.hostname</code> is the hostname or IP of docker.</p>\n<pre>storage.backend=cassandrathrift\nstorage.hostname=192.168.99.100\n</pre>\n<p>Then start the gremlin server by doing <code>bin/gremlin.sh</code> and run the following commands inside the Gremlin console:</p>\n<pre>gremlin&gt; graph = TitanFactory.open('/Users/elubow/tmp/local-gremlin.properties')\n==&gt;standardtitangraph[cassandrathrift:[192.168.99.100]]\ngremlin&gt; g = graph.traversal()\n==&gt;graphtraversalsource[standardtitangraph[cassandrathrift:[192.168.99.100]], standard]\ngremlin&gt; g.V()\n==&gt;v[4168]\n</pre>\n<p>NOTE: This will not use the elasticsearch backend.</p>\n<h3><a id=\"user-content-ports\" class=\"anchor\" aria-hidden=\"true\" href=\"#ports\"></a>Ports</h3>\n<p>8182: HTTP port for REST API\n8184: JMX Port (You won't need to use this, probably)</p>\n<p>To test out the REST API (over Boot2docker):</p>\n<pre>curl \"http://192.168.99.100:8182?gremlin=100-1\"\ncurl \"http://192.168.99.100:8182?gremlin=g.addV('Name','Eric')\"\ncurl \"http://192.168.99.100:8182?gremlin=g.V()\"\n</pre>\n<h2><a id=\"user-content-dependencies\" class=\"anchor\" aria-hidden=\"true\" href=\"#dependencies\"></a>Dependencies</h2>\n<p>I've tested this container with the following containers:</p>\n<pre>- elubow/cassandra: This is the Cassandra Storage backend for Titan. It scales well for large datasets. Also forces Cassandra 2.1 as that's compatible with Titan.\n- elasticsearch: This is the ElasticSearch Indexing backend for Titan. It provides search capabilities for Titan graph datasets.\n</pre>\n<h2><a id=\"user-content-roadmap\" class=\"anchor\" aria-hidden=\"true\" href=\"#roadmap\"></a>Roadmap</h2>\n<p>In the near future, I'd like to add support for:</p>\n<pre>- Scaling/Clustering Cassandra and ElasticSearch backends.\n- External volumes for persistent data.\n- Security between Titan and its backends.\n- Example application stack integrating with Titan.\n</pre>\n</article>",
        "created_at": "2018-08-13T14:57:11+0000",
        "updated_at": "2018-08-13T14:57:29+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/82815?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11952"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          },
          {
            "id": 1280,
            "label": "cql",
            "slug": "cql"
          }
        ],
        "is_public": false,
        "id": 11951,
        "uid": null,
        "title": "elubow/cql-vim",
        "url": "https://github.com/elubow/cql-vim",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Highlight CQL syntaxes inside the vim editor.</p>\n<h2><a id=\"user-content-install-with-plugin-manager\" class=\"anchor\" aria-hidden=\"true\" href=\"#install-with-plugin-manager\"></a>Install with plugin manager</h2>\n<pre>\" Plug (https://github.com/junegunn/vim-plug)\nPlug 'elubow/cql-vim'\n\" Vundle (https://github.com/VundleVim/Vundle.vim)\nPlugin 'elubow/cql-vim'\n</pre>\n<h2><a id=\"user-content-install-with-pathogen\" class=\"anchor\" aria-hidden=\"true\" href=\"#install-with-pathogen\"></a>Install with pathogen</h2>\n<pre>cd ~/.vim/\ngit clone git@github.com:elubow/cql-vim.git bundle/cql-vim\n</pre>\n<h2><a id=\"user-content-manual-install\" class=\"anchor\" aria-hidden=\"true\" href=\"#manual-install\"></a>Manual install</h2>\n<pre>cd ~/.local/src\ngit clone git@github.com:elubow/cql-vim.git \ncp -R cql-vim/syntax/* ~/.vim/syntax/\ncp -R cql-vim/ftdetect/* ~/.vim/ftdetect/\n</pre>\n<h2><a id=\"user-content-alternative-plugin-collection\" class=\"anchor\" aria-hidden=\"true\" href=\"#alternative-plugin-collection\"></a>Alternative plugin collection</h2>\n<p>cql-vim is also bundled in <a href=\"https://github.com/sheerun/vim-polyglot\">https://github.com/sheerun/vim-polyglot</a></p>\n</article>",
        "created_at": "2018-08-13T14:56:20+0000",
        "updated_at": "2018-08-13T14:56:27+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/82815?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11951"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1329,
            "label": "gossip.protocol",
            "slug": "gossip-protocol"
          }
        ],
        "is_public": false,
        "id": 11950,
        "uid": null,
        "title": "The Gossip Protocol - Inside Apache Cassandra.",
        "url": "https://www.linkedin.com/pulse/gossip-protocol-inside-apache-cassandra-soham-saha/",
        "content": "<p>The Gossip protocol is the <em>internal</em> communication technique for nodes in a cluster to talk to each other. Gossip is an efficient, lightweight, reliable inter-nodal broadcast protocol for diffusing data. It's decentralized, <em>\"epidemic\"</em>, fault tolerant and a peer-to-peer communication protocol. Cassandra uses gossiping for peer discovery and metadata propagation.</p><div class=\"slate-resizable-image-embed slate-image-embed__resize-middle\"><img data-media-urn=\"urn:li:digitalmediaAsset:C5612AQGRg4ztrilMrQ\" data-li-src=\"https://media.licdn.com/dms/image/C5612AQGRg4ztrilMrQ/article-inline_image-shrink_1000_1488/0?e=1539820800&amp;v=beta&amp;t=tlB2x1ZliULUkuJdHgSjHFgCAdgDyMjvfx-nCZIcHFs\" src=\"https://:0/\" alt=\"image\" /></div> \n<p>The gossip process runs every second for every node and exchange state messages with up to three other nodes in the cluster. Since the whole process is decentralized, there is nothing or no one that coordinates each node to gossip. Each node independently will always select one to three peers to gossip with. It will always select a live peer (if any) in the cluster, it will probabilistically pick a seed node from the cluster or maybe it will probabilistically select an unavailable node.</p> \n<div class=\"slate-resizable-image-embed slate-image-embed__resize-full-width\"><img data-media-urn=\"urn:li:digitalmediaAsset:C4E12AQHDjuit_L71uA\" data-li-src=\"https://media.licdn.com/dms/image/C4E12AQHDjuit_L71uA/article-inline_image-shrink_400_744/0?e=1539820800&amp;v=beta&amp;t=mfvG4KO0gLxazt24hjVbc1FH-cfWCthVrNP_xdkOstI\" src=\"https://:0/\" alt=\"image\" /></div> \n<p>The Gossip messaging is very similar to the TCP three-way handshake. With a regular broadcast protocol, there could only have been one message per round, and the data can be allowed to gradually spread through the cluster. But with the gossip protocol, having three messages for each round of gossip adds a degree of <em>anti-entropy</em>. This process allows obtaining \"convergence\" of data shared between the two interacting nodes much faster.</p> \n<p>SYN: The node initiating the round of gossip sends the SYN message which contains a compendium of the nodes in the cluster. It contains tuples of the IP address of a node in the cluster, the generation and the heartbeat version of the node.</p> \n<p>ACK: The peer after receiving SYN message compares its own metadata information with the one sent by the initiator and produces a diff. ACK contains two kinds of data. One part consists of updated metadata information (AppStates) that the peer has but the initiator doesn't, and the other part consists of digest of nodes the initiator has that the peer doesn't.</p> \n<p>ACK2: The initiator receives the ACK from peer and updates its metadata from the AppStates and sends back ACK2 containing the metadata information the peer has requested for. The peer receives ACK2, updates its metadata and the round of gossip concludes.</p> \n<p>An important note here is that this messaging protocol will cause only a constant amount of network traffic. Since the broadcasting of the initial digest is limited to three nodes and data convergence occurs through a pretty constant ACK and ACK2, there will not be much of network spike. Although, if a node gets UP, all the nodes might want to send data to that peer, causing the <em>Gossip Storm</em>.</p> \n<p>So how does a new node get the idea of whom to start gossiping with? Well, Cassandra has many seed provider implementations that provide a list of seed addresses to the new node and starts gossiping with one of them right away. After its first round of gossip, it will now possess cluster membership information about all the other nodes in the cluster and can then gossip with the rest of them.</p> \n<p>Well, how do we get to know if a node is UP/DOWN? The Failure Detector is the only component inside Cassandra(only the primary gossip class can mark a node UP besides) to do so. It is a heartbeat listener and marks down the timestamps and keeps backlogs of intervals at which it receives heartbeat updates from each peer. Based on the reported data, it determines whether a peer is UP/DOWN.</p> \n<p>How does a node being UP/DOWN affect the cluster? The write operations stay unaffected. If a node does not get an acknowledgment for a write to a peer, it simply stores it up as a <em>hint. </em>The nodes will stop sending read requests to a peer in DOWN state and probabilistically gossiping can be tried upon since its an unavailable node, as we have already discussed early on. All repair, stream sessions are closed as well when an unavailable node is involved.</p> \n<p>What if a peer is responding very slowly or timing out? Cassandra has another component called the Dynamic Snitch, which records and analyses latencies of read requests to peer nodes. It ranks latencies of peers in a rolling window and recalculates it every 100ms and resets the scores every 10mins to allow for any other events(Eg: Garbage Collection) delaying the response time of a peer. In this way, the Dynamic Snitch helps you identify the <em>slow </em>nodes and avoid them when indulging in Gossip.</p> \n<p>This article gives you the necessary overview and understanding of how the nodes in clusters in Apache Cassandra behaves via the gossip protocol. For a detailed information about APIs involved and other functional aspects, please visit the links given below in the references.</p> \n<p><strong>References:</strong></p> \n<ul><li>Apple Inc.: Cassandra Internals — Understanding Gossip</li> \n</ul><ul><li>https://docs.datastax.com/en/cassandra/2.1/cassandra/architecture/architectureGossipAbout_c.html</li> \n</ul><ul><li>https://wiki.apache.org/cassandra/ArchitectureGossip</li> \n</ul>",
        "created_at": "2018-08-13T05:35:18+0000",
        "updated_at": "2018-09-27T15:47:15+0000",
        "published_at": "2017-01-27T00:00:00+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.linkedin.com",
        "preview_picture": "https://media.licdn.com/mpr/mpr/shrinknp_400_400/gcrc/dms/image/C5612AQHxNfUXXB2gKA/article-cover_image-shrink_600_2000/0?e=1539820800&v=beta&t=71kFl-RMhBMaQnreGYauzjs22I21CSRgBXkaFpGFevo",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11950"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 17,
            "label": "development",
            "slug": "development"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11859,
        "uid": null,
        "title": "rackerlabs/cdeploy",
        "url": "https://github.com/rackerlabs/cdeploy",
        "content": "<p>cdeploy is a simple tool to manage your Cassandra schema migrations in the style of <a href=\"http://dbdeploy.com/\" rel=\"nofollow\">dbdeploy</a></p><p><code>pip install cdeploy</code></p><p>By default, cdeploy will look for migrations in './migrations'. This can be overridden by passing the path to your migrations directory as the first argument:</p><pre>cdeploy db/migrations\n</pre><p>The migrations directory should contain CQL scripts using the following naming convention: [version]_migration_description.cql</p><p>For example:</p><pre>migrations/\n    001_create_orders_table.cql\n    002_create_customers_table.cql\n</pre><p>Version numbers should begin from 1. Migration scripts may contain multiple semicolon terminated CQL statements. Comment lines can begin with either \"--\" or \"//\".</p><p>Migrations can also specify how to revert the changes by including additional statements following a line containing \"--//@UNDO\". For example:</p><pre>CREATE TABLE orders(\n    order_id uuid PRIMARY KEY,\n    price text\n);\n--//@UNDO\nDROP TABLE orders;\n</pre><p>To undo the most recently applied migration, run:</p><pre>cdeploy --undo\n</pre><div class=\"highlight highlight-source-python\"><pre>    from cdeploy import migrator\n    schema_migrator = migrator.Migrator('/path/to/migrations/directory', cassandra_session)\n    schema_migrator.run_migrations()</pre></div><p>cdeploy will look in your migrations directory for a configuration file named cassandra.yml, in a subdirectory named config:</p><pre>migrations/\n   config/\n       cassandra.yml\n</pre><p>The configuration file specifies the hosts to connect to and the keyspace name, and supports multiple environments. For example:</p><pre>development:\n    hosts: [host1]\n    keyspace: keyspace_name\nproduction:\n    hosts: [host1, host2, host3]\n    keyspace: keyspace_name\n</pre><p>The environment can be set via the ENV shell variable, and defaults to development if not specified:</p><pre>ENV=production cdeploy\n</pre><p>Additional configuration parameters for Cassandra are available:</p><pre>development:\n    hosts: [host1]\n    keyspace: keyspace_name\n    auth_enabled: true\n    auth_username: username\n    auth_password: password\n    port: 9042\n    ssl_enabled: true\n    ssl_ca_certs: /path/to/ca/certs\n    consistency_level: ALL\n    timeout: 10\n</pre>",
        "created_at": "2018-08-07T22:33:17+0000",
        "updated_at": "2018-08-07T22:33:45+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/84293?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11859"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 24,
            "label": "node",
            "slug": "node-js"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 235,
            "label": "rest",
            "slug": "rest"
          },
          {
            "id": 867,
            "label": "express",
            "slug": "express"
          }
        ],
        "is_public": false,
        "id": 11853,
        "uid": null,
        "title": "NodeJS & Express 4 With Cassandra  - Part 4",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=fLOR_7upQp0",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/fLOR_7upQp0?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-08-07T22:16:49+0000",
        "updated_at": "2018-08-07T22:16:50+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/fLOR_7upQp0/hqdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11853"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11852,
        "uid": null,
        "title": "Let’s play with Cassandra… (Part 3/3)",
        "url": "https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-33/",
        "content": "<div class=\"sharify-container\"><ul><li class=\"sharify-btn-twitter\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Tweet on Twitter\" href=\"https://twitter.com/intent/tweet?text=Let%26%238217%3Bs+play+with+Cassandra%26%238230%3B+%28Part+3%2F3%29: https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-33/\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-twitter\">&#13;\n\t\t\t\t\t\t\t\t\tTweet&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li><li class=\"sharify-btn-facebook\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Share on Facebook\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fblog.octo.com%2Fen%2Fnosql-lets-play-with-cassandra-part-33%2F\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-facebook\">&#13;\n\t\t\t\t\t\t\t\t\tShare&#13;\n\t\t\t\t\t\t\t\t\t0&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li><li class=\"sharify-btn-gplus\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Share on Google+\" href=\"http://plus.google.com/share?url=https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-33/\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-gplus\">&#13;\n\t\t\t\t\t\t\t\t\t+1&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li><li class=\"sharify-btn-linkedin\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Share on Linkedin\" href=\"https://www.linkedin.com/shareArticle?mini=true&amp;url=https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-33/&amp;title=Let’s play with Cassandra… (Part 3/3)\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-linkedin\">&#13;\n\t\t\t\t\t\t\t\t\tLinkedIn&#13;\n\t\t\t\t\t\t\t\t\t0&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li></ul></div><p>In this part, we will see a lot of Java code (the API exists in several other languages) and look at the client part of Cassandra. </p>\n<h2>Use Case #0: Open and close a connection to any node of your Cluster</h2>\n<p>Cassandra is now accessed using Thrift. The following code opens a connection to the specified node. </p>\n<pre><code class=\"java\">TTransport tr = new TSocket(\"192.168.216.128\", 9160);&#13;\nTProtocol proto = new TBinaryProtocol(tr);&#13;\ntr.open();&#13;\nCassandra.Client cassandraClient = new Cassandra.Client(proto);&#13;\n...&#13;\ntr.close();</code></pre>\n<p>As I <a href=\"https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-13\">told previously</a>, the default API does not provide any pool connections mechanisms that would have (1) the capacity to close and reopen connections in case a node has failed, (2) the capacity to load-balance requests among all the nodes of the cluster and (3) the capacity to automatically requesting another node in case the first attempt fails.</p>\n<h2>Use Case #1: Insert a customer </h2>\n<p>The following code insert a customer in the storage space (note that the object aCustomer  is the object you want to persist)</p>\n<pre><code class=\"java\">Map&lt;String , List&lt; ColumnOrSuperColumn &gt; &gt; insertClientDataMap = new HashMap&lt; string ,List&lt;ColumnOrSuperColumn &gt; &gt;();&#13;\nList&lt; ColumnOrSuperColumn &gt; clientRowData = new ArrayList&lt; ColumnOrSuperColumn &gt;();&#13;\n&#13;\nColumnOrSuperColumn columnOrSuperColumn = new ColumnOrSuperColumn();&#13;\ncolumnOrSuperColumn.setColumn(new Column(\"fullName\".getBytes(UTF8),  &#13;\naCustomer.getName().getBytes(UTF8), timestamp));&#13;\nclientRowData.add(columnOrSuperColumn);&#13;\n&#13;\ncolumnOrSuperColumn = new ColumnOrSuperColumn();&#13;\ncolumnOrSuperColumn.setColumn(new Column(\"age\".getBytes(UTF8),  &#13;\naCustomer.getAge().toString().getBytes(UTF8), timestamp));&#13;\nclientRowData.add(columnOrSuperColumn);&#13;\n&#13;\ncolumnOrSuperColumn = new ColumnOrSuperColumn();&#13;\ncolumnOrSuperColumn.setColumn(new Column(\"accountIds\".getBytes(UTF8),  &#13;\naCustomer.getAccountIds().getBytes(UTF8), timestamp));&#13;\nclientRowData.add(columnOrSuperColumn);</code></pre>\n<p>As you can read, the first line is in fact a Java representation of the structure: a map in which a row is identified by its key, and the value is a list of columns.  The rest of the code only create and append ColumnOrSuperColumn objects. Here, the columns have the following names: fullName, age, accountIds. You will also notice that when you create the column, you specify the timestamp the column is created. Remember that this timestamp will be used for “read-repair” and so that all your clients must be synchronized (using a NTP for instance)</p>\n<pre><code class=\"java\">insertClientDataMap.put(\"customers\", clientRowData);</code></pre>\n<p>The above lines put the list of Columns into the ColumnFamily named customers (so you can add several ColumnFamily in one time with the batch_insert method). Then, the following line inserts the customer into the Cassandra Storage. You need so to specify the keyspace, the row key (here the customer name), the Column family you want to insert and the Consistency Level you have chosen for this data.</p>\n<pre><code class=\"java\">cassandraClient.batch_insert(\"myBank\", aCustomer.getName(), insertClientDataMap,  ConsistencyLevel.DCQUORUM);</code></pre>\n<h2>Use Case #2: Insert operations for an account</h2>\n<p>Inserting an operation is almost the same code instead we are using SuperColumn. </p>\n<pre><code class=\"java\">Map&lt; string , List&lt; ColumnOrSuperColumn &gt; &gt; insertOperationDataMap = new HashMap&lt; string , List&lt; ColumnOrSuperColumn &gt; &gt;();&#13;\nList&lt; ColumnOrSuperColumn&gt; operationRowData = new ArrayList&lt; ColumnOrSuperColumn &gt;();&#13;\nList&lt; Column &gt; columns = new ArrayList&lt; Column &gt;();&#13;\n&#13;\n// THESE ARE THE SUPERCOLUMN COLUMNS&#13;\ncolumns.add(new Column(\"amount\".getBytes(UTF8),  &#13;\naBankOperation.getAmount().getBytes(UTF8), timestamp));&#13;\ncolumns.add(new Column(\"label\".getBytes(UTF8),  &#13;\naBankOperation.getLabel().getBytes(UTF8), timestamp));&#13;\nif (aBankOperation.getType() != null) {&#13;\n\tcolumns.add(new Column(\"type\".getBytes(UTF8),  &#13;\naBankOperation.getType().getBytes(UTF8), timestamp));&#13;\n}&#13;\nFor now, there is nothing new. A list of Columns is created with three columns: amount, label and type (withdrawal, transfer...). &#13;\n// here is a superColumn&#13;\nSuperColumn superColumn = new  &#13;\nSuperColumn(CassandraUUIDHelper.asByteArray(CassandraUUIDHelper.getTimeUUID()),  &#13;\ncolumns);&#13;\nColumnOrSuperColumn columnOrSuperColumn = new ColumnOrSuperColumn();&#13;\ncolumnOrSuperColumn.setSuper_column(superColumn);&#13;\noperationRowData.add(columnOrSuperColumn);</code></pre>\n<p>This case is different from the previous one. Instead of adding the previously defined Columns to the row, we create a SuperColumn with a dynamic (and time-based UUID) name…Quite dynamic isn’t it? Then, the three columns are added to the super column itself.<br />\nThe end of the code is similar to the previous one. The row is added to the ColumnFamily named operations and then associated to the current customer account id. </p>\n<pre><code class=\"java\">// put row data dans la columnFamily operations&#13;\ninsertOperationDataMap.put(\"operations\", operationRowData);&#13;\ncassandraClient.batch_insert(\"myBank\", aCustomer.getAccountIds(),  &#13;\ninsertOperationDataMap, ConsistencyLevel.ONE);</code></pre>\n<p>Here is what you get when reading the operations for the accounId<br /><a href=\"https://blog.octo.com/wp-content/uploads/2010/06/exit.png\"><img src=\"https://blog.octo.com/wp-content/uploads/2010/06/exit.png\" alt=\"\" title=\"exit\" width=\"564\" height=\"319\" class=\"aligncenter size-full wp-image-11797\" srcset=\"https://blog.octo.com/wp-content/uploads/2010/06/exit.png 564w, https://blog.octo.com/wp-content/uploads/2010/06/exit-300x170.png 300w, https://blog.octo.com/wp-content/uploads/2010/06/exit-160x90.png 160w\" /></a></p>\n<h2>Use Case #3 : Removing an item</h2>\n<p>Removing a complete row is – in terms of API – as simple as the rest of the API. </p>\n<pre><code class=\"java\">cassandraClient.remove(\"myBank\", myAccountId, new ColumnPath(\"operations\"),  timestamp, ConsistencyLevel.ONE);</code></pre>\n<p>It is yet a little more complex when you are looking<a href=\"http://spyced.blogspot.com/2010/02/distributed-deletes-in-cassandra.html\"> inside</a>. In brief, in a distributed system where node failure will occur, you can’t simply physically delete the record. So you replace it by a tombstone and the “mark as deleted” record will be effectively deleted once the tombstone will be considered enough old. At least, you can still use “logical deletion” and write a code that do not use these flagged records. </p>\n<h2>To (quickly) conclude this series of articles</h2>\n<p>I really like Cassandra which looks like a ready to use tools (even if NoSQL is plenty of great tools) and a way to achieve high performance system at “low” (at least lower) cost than with commercial tools. There are still concerns I hope I will be able to discuss like security (Cassandra provides authentication mechanisms…), searching (or at least getting ranges of datas), monitoring (and how to monitor all the nodes of your cluster into a unique tools like <a href=\"http://ganglia.sourceforge.net/\">Ganglia</a>, <a href=\"http://www.nagios.org/\">Nagios</a> or <a href=\"http://graphite.wikidot.com/start\">Graphite</a> or even how to use <a href=\"http://hadoop.apache.org/\">Hadoop</a> above Cassandra. </p>\n<p>To be continued…</p>\n<div class=\"sharify-container\"><ul><li class=\"sharify-btn-twitter\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Tweet on Twitter\" href=\"https://twitter.com/intent/tweet?text=Let%26%238217%3Bs+play+with+Cassandra%26%238230%3B+%28Part+3%2F3%29: https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-33/\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-twitter\">&#13;\n\t\t\t\t\t\t\t\t\tTweet&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li><li class=\"sharify-btn-facebook\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Share on Facebook\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fblog.octo.com%2Fen%2Fnosql-lets-play-with-cassandra-part-33%2F\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-facebook\">&#13;\n\t\t\t\t\t\t\t\t\tShare&#13;\n\t\t\t\t\t\t\t\t\t0&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li><li class=\"sharify-btn-gplus\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Share on Google+\" href=\"http://plus.google.com/share?url=https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-33/\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-gplus\">&#13;\n\t\t\t\t\t\t\t\t\t+1&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li><li class=\"sharify-btn-linkedin\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Share on Linkedin\" href=\"https://www.linkedin.com/shareArticle?mini=true&amp;url=https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-33/&amp;title=Let’s play with Cassandra… (Part 3/3)\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-linkedin\">&#13;\n\t\t\t\t\t\t\t\t\tLinkedIn&#13;\n\t\t\t\t\t\t\t\t\t0&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li></ul></div><div class=\"yarpp-related\">\n<h3>Related posts:</h3><ol><li><a href=\"https://blog.octo.com/en/nosqleu-and-nosql-what%e2%80%99s-the-deal/\" rel=\"bookmark\" title=\"no:sql(eu) and NoSQL: What’s the deal?\">no:sql(eu) and NoSQL: What’s the deal? </a></li>\n<li><a href=\"https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-13/\" rel=\"bookmark\" title=\"Let’s play with Cassandra… (Part 1/3)\">Let’s play with Cassandra… (Part 1/3) </a></li>\n<li><a href=\"https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-23/\" rel=\"bookmark\" title=\"Let’s play with Cassandra…(Part 2/3)\">Let’s play with Cassandra…(Part 2/3) </a></li>\n</ol></div>",
        "created_at": "2018-08-07T22:09:56+0000",
        "updated_at": "2018-08-07T22:10:03+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 5,
        "domain_name": "blog.octo.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11852"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11851,
        "uid": null,
        "title": "Let’s play with Cassandra…(Part 2/3)",
        "url": "https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-23/",
        "content": "<div class=\"sharify-container\"><ul><li class=\"sharify-btn-twitter\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Tweet on Twitter\" href=\"https://twitter.com/intent/tweet?text=Let%26%238217%3Bs+play+with+Cassandra%26%238230%3B%28Part+2%2F3%29: https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-23/\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-twitter\">&#13;\n\t\t\t\t\t\t\t\t\tTweet&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li><li class=\"sharify-btn-facebook\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Share on Facebook\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fblog.octo.com%2Fen%2Fnosql-lets-play-with-cassandra-part-23%2F\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-facebook\">&#13;\n\t\t\t\t\t\t\t\t\tShare&#13;\n\t\t\t\t\t\t\t\t\t0&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li><li class=\"sharify-btn-gplus\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Share on Google+\" href=\"http://plus.google.com/share?url=https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-23/\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-gplus\">&#13;\n\t\t\t\t\t\t\t\t\t+1&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li><li class=\"sharify-btn-linkedin\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Share on Linkedin\" href=\"https://www.linkedin.com/shareArticle?mini=true&amp;url=https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-23/&amp;title=Let’s play with Cassandra…(Part 2/3)\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-linkedin\">&#13;\n\t\t\t\t\t\t\t\t\tLinkedIn&#13;\n\t\t\t\t\t\t\t\t\t0&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li></ul></div><p>In this part, we will work in more details and closer to the code with Cassandra. The idea is to provide a kind of simplified current account system where a user has an account and the account has a balance…<br />\nThis system will so manipulate the following concepts:<br />\n– A client has different kind of properties defining his identity<br />\n– A client has one account<br />\n– The account has a list of operations (withdrawal, transfer are all kind of operations)<br />\nHere is the way it would have been modelized in the relational world (or at least UML world)<br /><a href=\"https://blog.octo.com/wp-content/uploads/2010/06/uml1.png\"><img src=\"https://blog.octo.com/wp-content/uploads/2010/06/uml1.png\" alt=\"\" title=\"uml\" width=\"530\" height=\"172\" class=\"aligncenter size-full wp-image-11785\" srcset=\"https://blog.octo.com/wp-content/uploads/2010/06/uml1.png 530w, https://blog.octo.com/wp-content/uploads/2010/06/uml1-300x97.png 300w, https://blog.octo.com/wp-content/uploads/2010/06/uml1-160x52.png 160w\" /></a><br /></p>\n<h2>The Cassandra set up</h2>\n<p>I will not drive deep into the details of a Cassandra set up. <a href=\"http://www.sodeso.nl/?p=80\">This article explains</a> it in details but here are the main points. </p>\n<h3>Define your cluster</h3>\n<p>Each nodes of the cluster has a configuration file called storage-conf.xml where are defined the following main sections<br />\n– <strong>Cluster and Keyspace definition</strong>. The cluster is made of several nodes (the Seed) which store all the Keyspaces you will define (and of course data).<br />\nAs we talked about in the previous part [add a link], you define the Keyspace that will contain all the ColumnFamily. </p>\n<pre><code class=\"xml\">&lt;storage&gt;&#13;\n  &lt;clustername&gt;Test Cluster&lt;/clustername&gt;&#13;\n  &lt;autobootstrap&gt;false&lt;/autobootstrap&gt;&#13;\n&lt;keyspaces&gt;&#13;\n    &lt;keyspace Name=\"myBank\"&gt;&#13;\n...&#13;\n&lt;/keyspace&gt;&lt;/keyspaces&gt;&#13;\n...&#13;\n&lt;partitioner&gt;org.apache.cassandra.dht.RandomPartitioner&lt;/partitioner&gt;&#13;\n&lt;initialtoken&gt;&lt;/initialtoken&gt;&#13;\n &lt;seeds&gt;&#13;\n    &lt;seed&gt;192.168.216.129&lt;/seed&gt;&#13;\n    &lt;seed&gt;192.168.216.130&lt;/seed&gt;&#13;\n  &lt;/seeds&gt;&#13;\n&lt;/storage&gt;</code></pre>\n<p>Then, you define the IP address of all the nodes (ie. The seeds) that will compose your cluster. During the startup phase, all the nodes will communicate to each other (using Gossip protocol), thus detecting starting or node failures.  You can go further in the definition of your cluster topology and group (as far as I know in another conf file) IPs by datacenters. </p>\n<p>The InitialToken, if not defined, will automatically be set by Cassandra (based on the cluster topology and following the Consistent Hashing algorithm). The documentation gives more details about <a href=\"http://wiki.apache.org/cassandra/Operations\">ring management</a><br />\nThe partitioner is a much more tricky and Cassandra provides, by default, <a href=\"http://spyced.blogspot.com/2009/05/consistent-hashing-vs-order-preserving.html\">two partitioners</a> : the <a href=\"http://ria101.wordpress.com/2010/02/22/cassandra-randompartitioner-vs-orderpreservingpartitioner/\">RandomPartitioner and the OrderPreservingPartitioner</a>. In the first case, the data will be partitioned using a row key hash (typically md5). In the second case, the data will be partitioned in their natural order and thus facilitates the range queries. So once again, the choice you made (and you cannot change it during your cluster life) is depending on the way your data is manipulated.</p>\n<p>– <strong>Node access</strong></p>\n<pre><code class=\"xml\">&lt;listenaddress&gt;192.168.216.128&lt;/listenaddress&gt;&#13;\n  &lt;storageport&gt;7000&lt;/storageport&gt;&#13;\n&#13;\n  &lt;thriftaddress&gt;192.168.216.128&lt;/thriftaddress&gt;&#13;\n  &lt;!-- Thrift RPC port (the port clients connect to). --&gt;&#13;\n  &lt;thriftport&gt;9160&lt;/thriftport&gt;&#13;\n...</code></pre>\n<p>The <code>ListenAddress</code> and <code>ThriftAddress</code> enable to define the current IP and listening port for the current node. The first IP is used by all the nodes to gossip each others. The second address is the one used by thrift clients to connect to the node and insert, delete or update data.</p>\n<h3>Define your data models</h3>\n<p>In our example, we will define two ColumnFamily. The first one will store all the customers. The second one all the operations. </p>\n<pre><code class=\"xml\">&lt;keyspace Name=\"myBank\"&gt;&#13;\n&lt;columnfamily CompareWith=\"UTF8Type\" Name=\"customers\"/&gt;&#13;\n&lt;columnfamily CompareWith=\"TimeUUIDType\" Name=\"operations\" ColumnType=\"Super\" CompareSubcolumnsWith=\"UTF8Type\"/&gt;&#13;\n&#13;\n&lt;!-- Number of replicas of the data --&gt;&#13;\n&lt;replicationfactor&gt;2&lt;/replicationfactor&gt;&#13;\n       &lt;replicaplacementstrategy&gt;org.apache.cassandra.locator.RackUnawareStrategy&lt;/replicaplacementstrategy&gt;&#13;\n&lt;keyscachedfraction&gt;0.01&lt;/keyscachedfraction&gt;&#13;\n      &lt;endpointsnitch&gt;org.apache.cassandra.locator.EndPointSnitch&lt;/endpointsnitch&gt;&#13;\n&lt;/keyspace&gt;</code></pre>\n<p>To begin with the simplest things, the replicationFactor defines the number of nodes the data will be replicated. Then let’s talk about the ColumnFamily. First, you will notice that the schema for each ColumnFamily is not defined (whereas the actual 0.6 version of Cassandra does not allow dynamically adding or removing ColumnFamily, the 0.7 should provide this feature) and you only know that customers and operations will be stored.<br />\nData modeling<br />\nIf you look at the UML diagram representing the different concepts, you will notice that there is a “one-to-many” relationship between an account and the operations on this account. An easy way to model this in Cassandra is by using the <a href=\"https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-13\">SuperColumn</a>. Thus the operation has the following structure: </p>\n<p><a href=\"https://blog.octo.com/wp-content/uploads/2010/06/column-oriented-model.png\"><img src=\"https://blog.octo.com/wp-content/uploads/2010/06/column-oriented-model.png\" alt=\"\" title=\"column-oriented model\" width=\"553\" height=\"271\" class=\"aligncenter size-full wp-image-11802\" srcset=\"https://blog.octo.com/wp-content/uploads/2010/06/column-oriented-model.png 553w, https://blog.octo.com/wp-content/uploads/2010/06/column-oriented-model-300x147.png 300w, https://blog.octo.com/wp-content/uploads/2010/06/column-oriented-model-160x78.png 160w\" /></a></p>\n<p>Thus:<br />\n–\tThe key is the account Id<br />\n–\tThe “Value” is a SuperColumn which stores all the operations for this account (the limitation of this model is the number of operations you could have…). Thus the operation is a list of columns (type, amount, date…) ordered by a <a href=\"http://wiki.apache.org/cassandra/FAQ\">time-based UUID</a> inside the SuperColumn. The CompareWith tells Cassandra how to sort the columns (remember the column are sorted, within a row, by their name. In our examples, I want my operations (whose name is a time-based UUID) to be chronologically sorted. That’s what I specify to Cassandra with the CompareWith attribute. The CompareSubcolumnsWith attribute will be responsible for sorting the Column included in the SuperColumn…<br />\nHere is what you get using the Cassandra-cli tools<br /><a href=\"https://blog.octo.com/wp-content/uploads/2010/06/exit.png\"><img src=\"https://blog.octo.com/wp-content/uploads/2010/06/exit.png\" alt=\"\" title=\"exit\" width=\"564\" height=\"319\" class=\"aligncenter size-full wp-image-11797\" srcset=\"https://blog.octo.com/wp-content/uploads/2010/06/exit.png 564w, https://blog.octo.com/wp-content/uploads/2010/06/exit-300x170.png 300w, https://blog.octo.com/wp-content/uploads/2010/06/exit-160x90.png 160w\" /></a></p>\n<div class=\"sharify-container\"><ul><li class=\"sharify-btn-twitter\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Tweet on Twitter\" href=\"https://twitter.com/intent/tweet?text=Let%26%238217%3Bs+play+with+Cassandra%26%238230%3B%28Part+2%2F3%29: https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-23/\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-twitter\">&#13;\n\t\t\t\t\t\t\t\t\tTweet&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li><li class=\"sharify-btn-facebook\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Share on Facebook\" href=\"http://www.facebook.com/sharer.php?u=https%3A%2F%2Fblog.octo.com%2Fen%2Fnosql-lets-play-with-cassandra-part-23%2F\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-facebook\">&#13;\n\t\t\t\t\t\t\t\t\tShare&#13;\n\t\t\t\t\t\t\t\t\t0&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li><li class=\"sharify-btn-gplus\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Share on Google+\" href=\"http://plus.google.com/share?url=https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-23/\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-gplus\">&#13;\n\t\t\t\t\t\t\t\t\t+1&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li><li class=\"sharify-btn-linkedin\">&#13;\n\t\t\t\t\t\t\t\t<a title=\"Share on Linkedin\" href=\"https://www.linkedin.com/shareArticle?mini=true&amp;url=https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-23/&amp;title=Let’s play with Cassandra…(Part 2/3)\">&#13;\n\t\t\t\t\t\t\t\t\t<i class=\"sharify sharify-linkedin\">&#13;\n\t\t\t\t\t\t\t\t\tLinkedIn&#13;\n\t\t\t\t\t\t\t\t\t0&#13;\n\t\t\t\t\t\t\t\t</i></a>&#13;\n\t\t\t\t\t\t\t</li></ul></div><div class=\"yarpp-related\">\n<h3>Related posts:</h3><ol><li><a href=\"https://blog.octo.com/en/nosqleu-and-nosql-what%e2%80%99s-the-deal/\" rel=\"bookmark\" title=\"no:sql(eu) and NoSQL: What’s the deal?\">no:sql(eu) and NoSQL: What’s the deal? </a></li>\n<li><a href=\"https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-13/\" rel=\"bookmark\" title=\"Let’s play with Cassandra… (Part 1/3)\">Let’s play with Cassandra… (Part 1/3) </a></li>\n<li><a href=\"https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-33/\" rel=\"bookmark\" title=\"Let’s play with Cassandra… (Part 3/3)\">Let’s play with Cassandra… (Part 3/3) </a></li>\n</ol></div>",
        "created_at": "2018-08-07T22:09:38+0000",
        "updated_at": "2018-08-07T22:09:42+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 5,
        "domain_name": "blog.octo.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11851"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11850,
        "uid": null,
        "title": "Let’s play with Cassandra… (Part 1/3)",
        "url": "https://blog.octo.com/en/nosql-lets-play-with-cassandra-part-13/",
        "content": "<p>I have already talked about it but <a href=\"https://blog.octo.com/en/nosqleu-and-nosql-what&#x2019;s-the-deal/\">NoSQL</a> is about diversity and includes various different tools and even kind of tools. Cassandra is one of these tools and is certainly and currently one of the most popular in the NoSQL ecosystem. <strong>Built by Facebook and currently in production at web giants like Digg, Twitter, Cassandra is a hybrid solution between <a href=\"http://www.allthingsdistributed.com/2007/10/amazons_dynamo.html\">Dynamo</a> and </strong><strong><a href=\"http://static.googleusercontent.com/external_content/untrusted_dlcp/labs.google.com/fr//papers/bigtable-osdi06.pdf\">BigTable</a></strong>. </p>\n<p>Hybrid firstly because <a href=\"http://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf\">Cassandra </a>uses a <strong>column-oriented way of modeling data</strong> (inspired by the BigTable) and permit to use <strong>Hadoop Map/Reduce jobs</strong> and secondly because it uses patterns inspired by Dynamo like <strong>Eventually Consistent, Gossip protocols, a master-master way of serving both read and write requests</strong>…</p>\n<p>Another DNA of Cassandra (and in fact a lot of NoSQL solutions) is that <strong>Cassandra has been built to be fully decentralized, designed for failure and Datacenter aware </strong>(in a sense you can configure Cassandra to ensure data replication between several Datacenter…). Hence, Cassandra is currently used between the <a href=\"http://spyced.blogspot.com/2010/04/cassandra-fact-vs-fiction.html\">Facebook US west and east coast datacenters</a> and stored (around two years ago) <a href=\"http://www.cs.cornell.edu/projects/ladis2009/papers/lakshman-ladis2009.pdf\">50+ TB of data on a 150 node cluster</a>.<br /></p>\n<h2>Data Modeling</h2>\n<p>The column oriented model is quite more complex to understand than the <a href=\"https://blog.octo.com/amazon-simpledb-le-harry-potter-de-voldemort/\">Key/Value model</a>. In a Key/value model, you have a key that uniquely identify a value and this value can be structured (based on a JSON format for instance) or completely unstructured (typically a BLOB). Therefore and from the basis, the simplest way to understand the column-oriented model is to begin with a Key/Value model and imagine that the Value is a collection of other Key/Value elements. In brief, this is a kind of structure where Hashmaps are included in another Hashmap…</p>\n<p>Completely lost? Here are a the main elements Cassandra defined (this <a href=\"http://arin.me/blog/wtf-is-a-supercolumn-cassandra-data-model\">article</a> or the <a href=\"http://wiki.apache.org/cassandra/DataModel\">Cassandra documentation</a> provide a more in depth view of the different types of structures you can have in Cassandra)<br />– <strong>Column</strong>. The basic element which is a tuple composed of a timestamp, a column name and a column value. The timestamp is set by the client and this has an architectural impact on clients’ clocks synchronization.<br />– <strong>SuperColumn</strong>. This structure is a little more complex. You can imagine it as a column in which can store a dynamic list of Columns.<br />– <strong>ColumnFamily</strong>: A set of columns. You can compare a ColumnFamily to a table in the relational world except the number and even (I am not sure this is the best idea but anyway) the names of columns can vary from a row to another. More important, the number of columns may vary during time (in the case for instance your schemas need to be upgraded etc…). Cassandra will not force any limitation in that case but your code will have to deal with these different schemas.<br />– <strong>KeySpaces</strong>: A set of ColumnFamily. Hence, the Keyspace configuration only defines (from the data modeling concern) the included ColumnFamily. The notion of “Row” does not exist by itself: this is a list of Columns or SuperColumns identified by a row key. </p>\n<h2>Data Partitioning and replication</h2>\n<p>Data Partitioning is one of tricky point. Depending on the studied tools, partitioning can be done either by the client library or by any node of the cluster and can be calculated using different algorithms (one of the most popular and reliable being the <a href=\"https://blog.octo.com/consistent-hashing-ou-l%E2%80%99art-de-distribuer-les-donnees/\">Consistent Hashing</a>.<br />Cassandra lets the nodes of the cluster (and not the client) partitioning the data based on the row key. Out of the box, Cassandra can nevertheless use <a href=\"http://ria101.wordpress.com/2010/02/22/cassandra-randompartitioner-vs-orderpreservingpartitioner/\">two different algorithms to distribute data over the nodes</a>. The first one is the RandomPartitionner and it gives you an equally and hash-based distribution. The second one is the OrderPreservingPartitioner and guarantees the Key are organized in a natural way. Thus, the latter facilitates the range queries since you need to hit fewer nodes to get all your ranges of data whereas the former has a better load balancing since the keys are more equally partitioned across the different nodes.<br />Then, each row (so all the Columns) is stored on the same physical node and columns are sorted based on their name. </p>\n<p>Moreover, and this is more linked to data replication, Cassandra natively enables replication across multiple datacenters and you can – by <a href=\"http://wiki.apache.org/cassandra/Operations\">configuration </a>– specify which nodes are in which Data Center. Hence, Cassandra would take care of replicating the data on at least one node in the distant Data Center (the partition-tolerant property of the CAP Theorem is so meaning full and I guess simpler to understand…). </p>\n<h2>Consistency management, conflict resolution and atomicity</h2>\n<p>NoSQL solutions like <a href=\"https://blog.octo.com/amazon-simpledb-le-harry-potter-de-voldemort/\">Voldemort </a>or <a href=\"http://incubator.apache.org/cassandra\">Cassandra </a>have chosen <a href=\"http://en.wikipedia.org/wiki/CAP_theorem\">Availability and Partition-tolerance over Consistency</a>. Thus, different strategy have been setting up: “Eventually Consistent”. </p>\n<p>Dealing with consistency is so a matter of dealing with data criticism and being able to define –for each data – the consistency level you need (based on the trade-offs the CAP Theorem imply). <strong>Cassandra defines different <a href=\"http://wiki.apache.org/cassandra/API\">levels of consistency</a></strong> and I will not go into further details but here are a couple of them:<br />– <strong>ONE</strong>. Cassandra ensures the data is written to at least one node’s commit log and memory table before responding to the client. During read, the data will be returned from the first node where it is found. In that case, you must accept stale state because you have no warranty the node you hit to read the data has the last version of the data.<br />– <strong>QUORUM</strong>. In that case, Cassandra will write the data on /2 + 1 nodes before responding to the client (the Replication factor is the number of nodes the data will be replicated and is defined for a Keyspace). For the read, the data will be read on /2 + 1 nodes before returning the data. In that case, you are sure to get a consistent data (because N is smaller than R+W where N is the total number of nodes where the data is replicated, R the number of nodes where this data is being read and W the number of nodes the data is being written)<br />– <strong>ALL</strong>. In that case, Cassandra will write and read the data from all the nodes.</p>\n<p>Of course, at a given time, chances are high that each node has its own version of the data. Conflict resolution is made during the read requests (called <a href=\"http://wiki.apache.org/cassandra/ReadRepair\">read-repair</a>) and the current version of Cassandra does not provide a Vector Clock conflict resolution mechanisms (should be available in the version 0.7). <strong>Conflict resolution is so based on timestamp</strong> (the one set when you insert the row or the column): the higher timestamp win and the node you are reading the data is responsible for that.<br />This is an important point because the timestamp is specified by the client, at the moment the column is inserted.<strong> Thus, all Cassandra clients’ need to be synchronized </strong>(based on an NTP for instance) in order to ensure the resolution conflict be reliable.</p>\n<p>Atomicity is also weaker than what we are used to in the relational world. <strong>Cassandra guarantees atomicity</strong> within a <a href=\"http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Cassandra-guarantees-reads-and-writes-to-be-atomic-within-a-single-ColumnFamily-td4288141.html\">ColumnFamily</a> s<strong>o for all the columns of a row</strong>. </p>\n<h2>Elasticity</h2>\n<p>“Cassandra is liquid” would have written any marketer and to be honest, a lot of NoSQL solutions have been built upon this DNA. <strong>First of all, elasticity is at the data modeling level. Your data will live longer than your business rules and softness in the way your data schemas can evolve across the time is an interesting point</strong>. </p>\n<p><strong>But elasticity is also about infrastructure and cluster sizing</strong>. Adding a new node to <a href=\"http://wiki.apache.org/cassandra/Operations\">Cassandra </a>is simple. Just turn on the AutoBootstrap property and specify at least one Seed of the current cluster. The node will hence be detected, added to the cluster and the data will be relocated (the needed time depends on the amount of data to transfer). Decommissioning a node is almost as simple as adding a node except you need to use the <a href=\"http://wiki.apache.org/cassandra/NodeProbe\">nodetool utility</a> (which provides more options to visualize the streams between the nodes…) or a JMX command. </p>\n<h2>Cassandra monitoring</h2>\n<p>Cassandra runs on a JVM and exposes JMX properties. You can thus collecting monitoring information using jConsole or any JMX compliant tool. </p>\n<p>For instance, you can monitor<br />– Your nodes (which are part of the cluster, which are dead…)<br /><img src=\"https://blog.octo.com/wp-content/uploads/2010/06/cassandra-jmonitoring.png\" alt=\"\" title=\"cassandra-jmonitoring\" width=\"609\" height=\"422\" class=\"aligncenter size-full wp-image-11659\" srcset=\"https://blog.octo.com/wp-content/uploads/2010/06/cassandra-jmonitoring.png 609w, https://blog.octo.com/wp-content/uploads/2010/06/cassandra-jmonitoring-300x208.png 300w, https://blog.octo.com/wp-content/uploads/2010/06/cassandra-jmonitoring-160x111.png 160w\" /></p>\n<p>– the streams between your nodes (especially in the case you added or removed nodes)<br />– Or stats. For instance, per-Column Family basic stats would be: Read Count, Read Latency, Write Count and Write Latency etc…</p>\n<p>More JMX plugins are available <a href=\"http://github.com/jbellis/cassandra-munin-plugins\">here</a> (provides graphs…) and <a href=\"http://nosql.mypopescu.com/post/611576467/cassandra-web-console\">some guys are developing web console</a>.</p>\n<h2>Access protocol</h2>\n<p>Cassandra is called using <a href=\"http://incubator.apache.org/thrift\">Thrift</a> (even if the 0.6 version introduced <a href=\"http://avro.apache.org\">Avro</a> protocol). As we told previously, Cassandra is responsible for routing the request to the proper node so you can reach any node of your cluster to serve your request. Nonetheless, <strong>the default thrift client API does not provide any load-balancing or connection pool mechanisms</strong>.<br />Concerning the connection pool, the main lacks, in my opinion, are (1) the capacity to close and reopen connections in case a node has failed, (2) the capacity to load-balance requests among all the nodes of the cluster and (3) the capacity to automatically request another node when the first attempt fails.<br />A higher level of load-balancing could be setup on the service layer</p>\n<p><img src=\"https://blog.octo.com/wp-content/uploads/2010/06/load-balancing.png\" alt=\"\" title=\"load-balancing\" width=\"639\" height=\"312\" class=\"aligncenter size-full wp-image-11660\" srcset=\"https://blog.octo.com/wp-content/uploads/2010/06/load-balancing.png 639w, https://blog.octo.com/wp-content/uploads/2010/06/load-balancing-300x146.png 300w, https://blog.octo.com/wp-content/uploads/2010/06/load-balancing-160x78.png 160w\" /></p>\n<p>Certain library (for instance <a href=\"http://prettyprint.me/2010/03/03/load-balancing-and-improved-failover-in-hector/\">Hector</a> in the Java World) provides connections pooling mechanisms and even kind of round-robin load balancing… </p>\n<p>In the next part, we will play in more details with Cassandra…</p>",
        "created_at": "2018-08-07T22:09:19+0000",
        "updated_at": "2018-08-07T22:10:11+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 7,
        "domain_name": "blog.octo.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11850"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 24,
            "label": "node",
            "slug": "node-js"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 235,
            "label": "rest",
            "slug": "rest"
          },
          {
            "id": 867,
            "label": "express",
            "slug": "express"
          }
        ],
        "is_public": false,
        "id": 11841,
        "uid": null,
        "title": "NodeJS & Express 4 With Cassandra  - Part 2",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=tOAmF0qlzWI",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/tOAmF0qlzWI?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-08-07T22:02:01+0000",
        "updated_at": "2018-08-07T22:02:02+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/tOAmF0qlzWI/hqdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11841"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 24,
            "label": "node",
            "slug": "node-js"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 235,
            "label": "rest",
            "slug": "rest"
          },
          {
            "id": 867,
            "label": "express",
            "slug": "express"
          }
        ],
        "is_public": false,
        "id": 11840,
        "uid": null,
        "title": "NodeJS & Express 4 With Cassandra  - Part 3",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=wEAzZ95hWCg",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/wEAzZ95hWCg?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-08-07T22:01:47+0000",
        "updated_at": "2018-08-07T22:01:58+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/wEAzZ95hWCg/hqdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11840"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 24,
            "label": "node",
            "slug": "node-js"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 235,
            "label": "rest",
            "slug": "rest"
          },
          {
            "id": 867,
            "label": "express",
            "slug": "express"
          }
        ],
        "is_public": false,
        "id": 11838,
        "uid": null,
        "title": "NodeJS & Express 4 With Cassandra - Part 1",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=ivstVNUKAW8",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/ivstVNUKAW8?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-08-07T21:59:08+0000",
        "updated_at": "2018-08-07T22:03:48+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/ivstVNUKAW8/hqdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11838"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 24,
            "label": "node",
            "slug": "node-js"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 867,
            "label": "express",
            "slug": "express"
          }
        ],
        "is_public": false,
        "id": 11837,
        "uid": null,
        "title": "bradtraversy/mysubscribers",
        "url": "https://github.com/bradtraversy/mysubscribers",
        "content": "<p><strong>MySubscribers</strong> is a very simple application (Start of an application) which allows you to create, read, update and delete users/subscribers. This application was only created to aid the YouTube course</p><p>##Supported Browsers##</p><ul><li>Chrome (stable and canary channel)</li>\n<li>Firefox</li>\n<li>IE 9 and 10</li>\n<li>Opera</li>\n<li>Safari</li>\n</ul><p>##License##</p><ul><li>MIT | <a href=\"http://opensource.org/licenses/MIT\" rel=\"nofollow\">http://opensource.org/licenses/MIT</a></li>\n<li>Feel Free to do what you want with it</li>\n</ul><p>##Authors##\nBrad Traversy | <a href=\"http://www.codeskillet.com\" rel=\"nofollow\">Codeskillet.com</a></p><ul><li><a href=\"http://twitter.com/bradtraversy\" rel=\"nofollow\">http://twitter.com/bradtraversy</a></li>\n<li><a href=\"http://www.linkedin.com/in/bradtraversy\" rel=\"nofollow\">http://www.linkedin.com/in/bradtraversy</a></li>\n</ul>",
        "created_at": "2018-08-07T21:58:22+0000",
        "updated_at": "2018-08-07T22:20:45+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/5550850?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11837"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 235,
            "label": "rest",
            "slug": "rest"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 1129,
            "label": "play",
            "slug": "play"
          }
        ],
        "is_public": false,
        "id": 11836,
        "uid": null,
        "title": "tuplejump/play-cassandra",
        "url": "https://github.com/tuplejump/play-cassandra",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>A Play Plugin for using Cassandra</p>\n<p>The Plugin initializes a Cassandra Session on startup and provides access to session\nand other properties through the <code>Cassandra</code> object.</p>\n<p>The Plugin also provides play-evolutions like functionality if it is not disabled.</p>\n<p>####Usage\nIn library dependencies, include</p>\n<pre>\"com.tuplejump\" %% \"play-cassandra\" % \"1.0.0-SNAPSHOT\"\n</pre>\n<p>Now, Cassandra host, port, cluster and session can be accessed through the API exposed by the Plugin.\nIn addition to that, a method <code>loadCQLFile</code> is also available. The API is documented at TODO</p>\n<p>Some examples,</p>\n<ul><li>executing a query</li>\n</ul><div class=\"highlight highlight-source-scala\"><pre>val query = QueryBuilder.select(\"title\")\n      .from(\"music\", \"playlists\")\n      .where(QueryBuilder.eq(\"id\", playlistId))\n    val queryResult = Cassandra.session.execute(query).toIterable</pre></div>\n<ul><li>executing a batch statement</li>\n</ul><div class=\"highlight highlight-source-scala\"><pre>val insertQuery: String = \"INSERT INTO music.playlists (id, song_id, title, artist, album) VALUES (?, ?, ?, ?, ?)\"\n    val ps = Cassandra.session.prepare(insertQuery)\n    var batch = new BatchStatement()\n    songs.foreach {\n      s =&gt;\n        batch.add(ps.bind(playlistId, s.id, s.title, s.artist, s.album))\n    }\n    Cassandra.session.execute(batch)</pre></div>\n<ul><li>loading a CQL statements from a file</li>\n</ul><div class=\"highlight highlight-source-scala\"><pre>Cassandra.loadCQLFile(\"init.CQL\")</pre></div>\n<p><strong>Note:The cluster and session exposed are closed by the Plugin when the application is stopped.</strong></p>\n<p>#####Evolution</p>\n<p>Evolution is enabled by default and the file names are expected to be integers in order,\nsimilar to play-evolutions for SQL or SQL-like databases.</p>\n<p><strong>The configuration property <code>cassandraPlugin.appName</code> should be set when evolution is enabled.\nThe plugin adds an entry for each appName and the default value is <code>appWithCassandraPlugin</code></strong></p>\n<p>To disable evolution, add the following to <code>conf/application.conf</code>,</p>\n<pre>cassandraPlugin.evolution.enabled=false\n</pre>\n<p><strong>Note: The plugin loads before <code>GlobalSettings</code>, so it is accessible in a custom <code>Global</code> object.</strong></p>\n<p>####Configuration\nThe default configuration is,</p>\n<pre>cassandraPlugin {\n  //host and port of where Cassandra is running\n  host = \"127.0.0.1\"    \n  port = 9042           \n  \n  evolution {\n    enabled = true\n    directory = \"evolutions/cassandra/\" //directory within conf to look for CQL files\n  }\n  \n  appName = \"appWithCassandraPlugin\" // appName to be saved in DB when using evolutions\n}\n</pre>\n</article>",
        "created_at": "2018-08-07T21:58:06+0000",
        "updated_at": "2018-08-07T21:58:14+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/3493976?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11836"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 901,
            "label": "security",
            "slug": "security"
          }
        ],
        "is_public": false,
        "id": 11827,
        "uid": null,
        "title": "Securing Apache Cassandra with Application Level Encryption - Instaclustr",
        "url": "https://www.instaclustr.com/securing-apache-cassandra-with-application-level-encryption/",
        "content": "<p>One topic that commonly comes up when discussing Cassandra with large enterprise clients is whether Cassandra can match feature X (audit logging, encryption at rest, column level security, etc) that is supported by Oracle, SQL Server or some other common enterprise RDBMS technology. In many cases, the answer to this question is no – Cassandra does not necessarily boast the same range of features (or, more unkindly, the same feature bloat) as leading RDBMS products.</p><p>However, enterprise security standards exist for good reasons and so it is necessary to present a solution which addresses these standards as part of the overall application solution. A key part of our answer to this question and best practice security in our view is to encrypt data as close to the point of collection and at latest, in the application layer.</p><p>We’re not alone in this viewpoint, Werner Vogels, CTO of <a href=\"https://aws.amazon.com/\">AWS</a> has said “We’ve got quite a few customers who’ve moved to 100% encryption. We really want to move our customers to a world where they own the keys and as such, they are the only ones who decide who has access to the data, not anybody else, not us as a provider.” <a href=\"https://www.businessinsider.com/amazon-cto-werner-vogels-speaks-out-in-support-of-encryption-aws-apple-security-zero-knowledge-2016-3#5Kc318BRLzq2GhE5.99\">(Business Insider)</a></p><p>Encrypting data in the application layer of your application allows you to meet many typical enterprise database security standards while maintaining a horizontally scalable and highly available architecture. For example:</p><ul><li>Encryption at rest – data is encrypted before it’s even received by the database and so by definition will be encrypted at rest.</li> <li>Authorization and enterprise I&amp;AM integration – regardless of database level integration, applications will likely need to be integrated with enterprise I&amp;AM security providers to meet functional requirements. Once the application holds the keys to unlock the data, this integration can be leveraged to implement authorization requirements at whatever level of granularity is required. Data can even be encrypted using a key or password controlled by the end user, providing a very high guarantee of access restriction on the data.</li> <li>Access logging – applications are free to implement whatever access logging is required, along with potentially much richer context than is typically available at the database layer.</li> </ul><p>For those coming from a relational database background, encrypting the data in the application may seem like it comes with a functional cost that is hard to bear. However, in the context of the more restricted query model of Cassandra, the functional cost of encrypting at the application is very limited. Consider the following:</p><ul><li>Partition keys (the subset of the primary key column that is used to determine the distribution of data amongst the nodes): these keys are actually translated into a hash value by Cassandra and can therefore only be used for equality operators. Equality can be evaluated between encrypted values just as well as unencrypted so there is no impact here.</li> <li>Clustering keys (the remaining subset of the primary key after the partition key columns are taken out): clustering key columns impact the ordering of data on disk and can be used for range queries (&gt; and &lt;). Encrypting these columns can therefore reduce available query functionality (as values would need to be decrypted before being evaluated). However, by far the most common use case for range queries is querying on date ranges (for a series of events) and it is hard to think of many situations where dates themselves are sensitive once any associated identifying data is encrypted.</li> <li>Non-key values: Cassandra does not (except with “allow filtering” which is generally not recommended) allow filtering on non-key columns. Non-key data can therefore be encrypted without any real loss in functionality. The one possible loss is if you want to use Cassandra aggregation to; for example, calculate a sum of values. Again, it’s hard to think of a situation where the value you are going to calculate a sum on is sensitive once identifying data is encrypted. The exception to this, which requires closer examination, is if you are planning on using secondary index search technology such as the <a href=\"https://lucene.apache.org/core/\">Lucene index</a> or <a href=\"http://spark.apache.org/\">Apache Spark</a> to access your data. In many cases, careful consideration of application design and what to encrypt can resolve these limitations.</li> </ul><p>There are many ways you can implement this encryption – including standard encryption libraries (<a href=\"http://commons.apache.org/proper/commons-crypto/\">Apache Commons Crypto</a>) called by wrapper classes in your code or a driver that supports encryption such as that provided by our partner <a href=\"http://www.baffle.io\">baffle.io</a>.</p><p>In summary, we believe that encrypting as close as possible to the point of data collection, rather than trying to protect it at many points in your application stack is definitely the best approach to protect your data. With <a href=\"https://www.instaclustr.com/managed/cassandra/\">Apache Cassandra</a>, the cost you pay for implementing this encryption may not be as significant as it first seems.</p>",
        "created_at": "2018-08-04T17:11:55+0000",
        "updated_at": "2018-09-13T14:58:01+0000",
        "published_at": "2017-04-28T05:13:22+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11827"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 1227,
            "label": "stream",
            "slug": "stream"
          }
        ],
        "is_public": false,
        "id": 11826,
        "uid": null,
        "title": "SnappyData, MemSQL-Spark & Cassandra-Spark: A Performance Benchmark",
        "url": "https://www.snappydata.io/blog/snappydata-memsql-cassandra-a-performance-benchmark",
        "content": "<p>There is a repo associated with this blog post <a href=\"https://github.com/SnappyDataInc/snappy-poc\">here</a></p><p>There is a blog post that explains the Ad Analytics code example used below <a href=\"http://www.snappydata.io/blog/ad-analytics-using-spark-streaming-and-snappydata-code-example\">here</a></p><h2>Introduction</h2><p>We recently released a mixed workload example for Ad Analytics (<a href=\"https://github.com/SnappyDataInc/snappy-poc\">description and source code on github</a>) to illustrate the simplicity of development using SQL or the Spark Dataframe API in SnappyData. By mixed workload we mean a system that that is continuously ingesting a high velocity stream of ad impressions while concurrently serving ad-hoc interactive queries.  In addition to demonstrating SnappyData’s simplicity, we wanted to also benchmark it against alternatives. Here we try to understand SnappyData’s performance compared to alternate “lambda” stacks -  <a href=\"https://github.com/datastax/spark-cassandra-connector\">Cassandra + Spark</a> and <a href=\"https://github.com/memsql/memsql-spark-connector\">MemSQL + Spark</a> for both stream ingestion rate and interactive analytic query performance.</p><p>Our summarized findings:<br />- Snappydata can ingest data <strong>twice as fast</strong> as Cassandra and <strong>1.5 times faster</strong> than MemSQL.<br />- While ingesting data, SnappyData can concurrently execute analytics class queries <strong>45 times faster</strong> than Cassandra and <strong>3 times faster</strong> than MemSQL.</p><p>Note that these numbers represent queries over exact datasets; they do not include SnappyData’s <a href=\"https://github.com/SnappyDataInc/snappydata/blob/master/docs/aqp.md\">Approximate Query Processing techniques</a>. When these techniques are used (and the data volumes have grown), query latency becomes multiple orders of magnitude faster. The source code used for these tests is available on <a href=\"https://github.com/SnappyDataInc/snappy-poc\">Github</a>. Below, we will describe the benchmark:</p><h2>Machine Configuration</h2><p>Five ‘c4.2xlarge’ Compute optimized EC2 instances with<br />-       8 Cores<br />-       15GB RAM<br />-       Dedicated EBS bandwidth of 1000 Mbps</p><p>Product versions used:<br />MemSQL<br />-       MemSQL Ops-4.1.11 Community Edition<br />-       Spark-MemSQL Connector 2.10_1.3.2<br />-       Spark 1.5.2<br />Cassandra<br />-       Cassandra 3.5<br />-       Spark-Cassandra Connector 2.10_1.6.0-M2<br />-       Spark 1.6.0<br />-       SnappyData<br />-       SnappyData 0.3-PREVIEW; with Spark 1.6.1 is bundled<br />-       Apache Kafka 2.10_0.8.2.2.</p><h2>Architecture</h2><p>One machine was used for the Spark Master while the other four were used as Workers. A single Kafka producer process was executed producing ad impressions (asynchronously) over 16 threads. Four Kafka brokers were collocated on the Spark Worker nodes. We used 32 Kafka partitions to maximize concurrent processing. The kafka producer uses Avro Java objects to represent AdImpressions. Each AdImpression, when serialized, has a payload size of 63 bytes. To allow for concurrent analytical queries we configured spark so enough cores were available for query processing.</p><p>Finally, each tested system used the highest version of Spark it supported: 1.6.1 for SnappyData, 1.6.0 for Cassandra and 1.5.2 for MemSQL.</p><p>For each store that supported columnar storage, we used a column store to support fast scans and aggregations. Data is ingested using the Kafka direct receiver available with Spark Streaming and ingested in parallel on each partition.</p><p>The figures below depict the benchmark architecture for all 3 product configurations. The only difference with SnappyData is that its store is fully collocated with the spark executors as depicted in the figure <a href=\"https://github.com/SnappyDataInc/snappydata/blob/master/docs/GettingStarted.md#snappydata-cluster-explanation\">here</a>. SnappyData completely avoids the need to shuffle data as its column store defaults to the partitioning used by the upstream RDD (Spark DStream in this case) resulting in better speed of ingestion and query performance.</p><h4>SnappyData</h4><p>We ran the locator and the lead node (which runs the Spark Driver and is the cluster master) on one server and used the remaining four to parallel ingest from kafka and store into the column table (both in-memory and on disk). </p><p class=\"block-img\"><img alt=\"\" src=\"https://snappyblog.cdn.prismic.io/snappyblog/8d7eb75c15c29dbbc4a314a9a49f27d157068399_screen-shot-2016-06-20-at-9.32.23-am.png\" width=\"623\" height=\"251\" /></p><h4>Spark-Cassandra</h4><p>Similar to SnappyData we run the spark master/Driver on one node and so the ingestion process on the remaining 4 AWS nodes.</p><p class=\"block-img\"><img alt=\"\" src=\"https://snappyblog.cdn.prismic.io/snappyblog/7e6edd43fd86117512f23b7772a6b7d8d7802673_screen-shot-2016-06-20-at-9.33.50-am.png\" width=\"652\" height=\"259\" /></p><h4>Spark-MemSQL</h4><p>Matching what we did for SnappyData we start the spark master/Driver along with the MemSQL aggregator on one node and the remaining nodes parallely ingest and store into a local column table in MemSQL.</p><p class=\"block-img\"><img alt=\"\" src=\"https://snappyblog.cdn.prismic.io/snappyblog/1349f4125003c60731d1bf1fc3177571449619de_screen-shot-2016-06-20-at-9.36.21-am.png\" width=\"637\" height=\"277\" /></p><h4>Cassandra Tuning</h4><p>To tune writes, the cassandra connector batches incoming writes and exposes several <a href=\"https://github.com/datastax/spark-cassandra-connector/blob/master/doc/reference.md#write-tuning-parameters\">tuning parameters</a>. The ones we adjusted are listed above. Given the number of cores in use, adjusting the concurrent writer count gave us the maximum throughput. By default, the connector retains up to 1000 batches per single Spark task in memory before writing to Cassandra. This configuration is inappropriate in a production scenario unless checkpointing is enabled in Spark Streaming. While we didn’t tune any memory setting in the Cassandra servers we ensured that enough memory was available on each node to ensure all the data will be cached in the OS page cache.</p><h4>MemSQL Tuning</h4><p>We used defaults for the MemSQL configuration.</p><h4>SnappyData Tuning</h4><p>We used defaults for  SnappyData configuration.</p><h2>Ingestion Performance</h2><p>To ingest data we used the Spark streaming Kafka direct receiver. This is the most optimal way to fetch from kafka – batches from the topic are directly processed by the stream without any additional buffering in the Spark layer.</p><p>For all three products, each incoming Avro message is first decoded and then turned into a Spark Row object before it is stored into their respective Column table (in-memory + disk persistent). Below is the code snippet used to store streaming data into a column table in SnappyData.</p><p>In the Cassandra case, we ingest into a table with no replication (i.e. replication factor 1) so it is equivalent to Snappydata.</p><p>In the MemSQL case, we created the equivalent column table: </p><p>And stored the incoming streaming data into the coulmn table:</p><p>The chart below shows the ingestion per second throughput after some warm up period.</p><p class=\"block-img\"><img alt=\"\" src=\"https://snappyblog.cdn.prismic.io/snappyblog/ebb7f21a126788e0cec4567624f3f56a57677008_screen-shot-2016-06-20-at-10.34.20-am.png\" width=\"615\" height=\"422\" /></p><p>In short, on 4 nodes with 8 cores and a capped CPU (to leave space for concurrent OLAP queries), <strong>SnappyData outperformed Spark-Cassandra by roughly 2x</strong> with 670,000 events per second versus 322,000 <strong>and outperformed MemSQL by roughly 1.5x</strong> with 670,000 vs 480,000.</p><h2>Concurrent Query workload</h2><p>We concurrently execute simple aggregation queries using the Spark SQL Driver program when the ingested data reaches 30, 60 and 90 million records. We logged the query execution time to a file. For example, you can see the driver program for Cassandra <a href=\"https://github.com/SnappyDataInc/snappy-poc/blob/master/spark-cassandra/src/main/scala/io/snappydata/benchmark/CassandraQueryPerf.scala\">here</a>. We used following three aggregation queries to measure OLAP performance.</p><p><strong>Q1 </strong>: Show top 20 Geos with the highest ad impressions<br /><em>select count(*) AS adCount, geo from adImpressions group by geo order by adCount desc limit 20;</em></p><p><strong>Q2</strong>: Geos with the highest total bid<br /><em>select sum (bid) as max_bid, geo from adImpressions group by geo order by max_bid desc limit 20;</em></p><p><strong>Q3</strong>: Top 20 publishers with the highest bid total<br /><em>select sum (bid) as max_bid, publisher from adImpressions group by publisher order by max_bid desc limit 20;  \t</em></p><p>The following chart shows the latency numbers for each query by product and row count.</p><p class=\"block-img\"><img alt=\"\" src=\"https://snappyblog.cdn.prismic.io/snappyblog/599e09a972a440ddd78296f4f3e9ecc89992d308_screen-shot-2016-06-20-at-10.48.42-am.png\" width=\"674\" height=\"482\" /></p><p>As you can see SnappyData outperforms Spark-Cassandra by roughly <strong>45 times</strong> when executing queries while also ingesting data and outperforms MemSQL by roughly <strong>3 times</strong> on the same setup.</p><h2>Analysis</h2><p>The Cassandra connector suffers from two problems: <br />- The full data has to be serialized and copied to the spark cluster including going through format conversions for each query and <br />- The data has to be shuffled across multiple partitions.</p><p>These issues add latency to both ingestion and query performance.</p><p>The MemSQL connector, on the other hand, attempts to push as much of the query as possible to MemSQL which provides significant savings in having to move large quantities of data into Spark for each query. Go through <a href=\"https://github.com/memsql/memsql-spark-connector/blob/master/connectorLib/src/main/scala/com/memsql/spark/pushdown/MemSQLPushdownStrategy.scala\">MemSQLPushdownStrategy</a> and <a href=\"https://github.com/memsql/memsql-spark-connector/blob/master/connectorLib/src/main/scala/com/memsql/spark/pushdown/MemSQLPhysicalRDD.scala\">MemSQLPhysicalRDD</a> for details. The other optimization in MemSQL is that it always attempts to collocate its partition to the parent, so, kafka partitions, queuing, and ingestion all occur without ever having to shuffle any records.</p><p>SnappyData embeds its column store alongside the Spark executors providing reference level access to rows. Similar to MemSQL, it also ensures that each partition in the store layer, by default, uses the parent’s partitioning method. i.e. there is no need to shuffle during storing. When queried, the data is column compressed and formatted in the same format as Spark (when cached or persisted). These features provide a significant performance boost in ingestion and query latency.</p><h2>Interactive Query response with AQP</h2><p>We also benchmarked SnappyData’s <a href=\"https://github.com/SnappyDataInc/snappydata/blob/master/docs/aqp.md\">Approximate Query Processing</a> techniques and compared this to “exact” queries against our in-memory column tables. While these sampling techniques require large data volumes for higher accuracy, we compensated by creating a relatively large <a href=\"http://snappydatainc.github.io/snappydata/aqp/#approximations-technique-2-sampling\">stratified sample</a> of 3% to work with smaller data sets in this example. We created a <a href=\"https://github.com/SnappyDataInc/snappy-poc/blob/master/src/main/scala/io/snappydata/adanalytics/SnappySQLLogAggregator.scala#L99\">sample on the base column Table</a>. When queries are executed we append an error clause so the sample is automatically consulted (<em>‘select .. from aggrAdImpressions group by … <em><strong>with error 0.2’</strong></em>). Let’s compare our 3 queries over the exact table and sample table (In SnappyData) at 350m, 400m and 450m records with concurrent streaming. </em></p><p class=\"block-img\"><img alt=\"\" src=\"https://snappyblog.cdn.prismic.io/snappyblog/63c512965721478575757c303bbceb2057422a84_screen-shot-2016-06-21-at-8.59.03-am.png\" width=\"421\" height=\"315\" /></p><p>As expected, the sample table execution is much faster than its counterpart exact table. While the performance is high, usually with large data sets the sample fraction can be much smaller with a resulting performance difference that is 2 or even 3 orders of magnitude better than running against the base table. It is also interesting to note that, <strong>with increasing data volume, the query execution time on the sample stays fairly constant</strong>. While the sample size grows in proportion to the base table size, most of the query execution cost comes from other sources like error computation and query planning through the spark driver. Other performance related blogs:</p><p>SnappyData is <a href=\"https://www.snappydata.io/download?utm_source=blog&amp;utm_campaign=memsql2016blog\">available for download</a>.<br /></p><p>Learn more and chat about SnappyData on any of our community channels:</p><p><a href=\"http://stackoverflow.com/questions/tagged/snappydata\">Stackoverflow</a><br /><a href=\"http://snappydata-slackin.herokuapp.com/\">Slack</a><br /><a href=\"https://groups.google.com/forum/#!forum/snappydata-user\">Mailing List</a><br /><a href=\"https://gitter.im/SnappyDataInc/snappydata\">Gitter</a><br /><a href=\"https://www.reddit.com/r/snappydata/\">Reddit</a><br /><a href=\"https://jira.snappydata.io/projects/SNAP/issues/SNAP-893?filter=allopenissues\">JIRA</a></p><p>Other important links:</p><p><a href=\"https://github.com/SnappyDataInc/snappydata\">SnappyData source</a><br /><a href=\"http://snappydatainc.github.io/snappydata/\">SnappyData docs</a><br /><a href=\"https://twitter.com/snappydata\">SnappyData twitter</a><br /><a href=\"http://www.snappydata.io/snappy-industrial\">SnappyData technical paper</a><br /></p>",
        "created_at": "2018-08-03T19:28:40+0000",
        "updated_at": "2018-08-03T19:28:54+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 7,
        "domain_name": "www.snappydata.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11826"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 1227,
            "label": "stream",
            "slug": "stream"
          },
          {
            "id": 1269,
            "label": "hdfs",
            "slug": "hdfs"
          },
          {
            "id": 1285,
            "label": "parquet",
            "slug": "parquet"
          }
        ],
        "is_public": false,
        "id": 11825,
        "uid": null,
        "title": "SnappyDataInc/snappydata",
        "url": "https://github.com/SnappyDataInc/snappydata",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\"><h3><a id=\"user-content-snappydata-fuses-apache-spark-with-an-in-memory-database-to-deliver-a-data-engine-capable-of-processing-streams-transactions-and-interactive-analytics-in-a-single-cluster\" class=\"anchor\" aria-hidden=\"true\" href=\"#snappydata-fuses-apache-spark-with-an-in-memory-database-to-deliver-a-data-engine-capable-of-processing-streams-transactions-and-interactive-analytics-in-a-single-cluster\"></a>SnappyData fuses Apache Spark with an in-memory database to deliver a data engine capable of processing streams, transactions and interactive analytics in a single cluster.</h3>\n<h3><a id=\"user-content-the-challenge-with-spark-and-remote-data-sources\" class=\"anchor\" aria-hidden=\"true\" href=\"#the-challenge-with-spark-and-remote-data-sources\"></a>The Challenge with Spark and Remote Data Sources</h3>\n<p>Apache Spark is a general purpose parallel computational engine for analytics at scale. At its core, it has a batch design center and is capable of working with disparate data sources. While this provides rich unified access to data, this can also be quite inefficient and expensive. Analytic processing requires massive data sets to be repeatedly copied and data to be reformatted to suit Spark. In many cases, it ultimately fails to deliver the promise of interactive analytic performance.\nFor instance, each time an aggregation is run on a large Cassandra table, it necessitates streaming the entire table into Spark to do the aggregation. Caching within Spark is immutable and results in stale insight.</p>\n<h3><a id=\"user-content-the-snappydata-approach\" class=\"anchor\" aria-hidden=\"true\" href=\"#the-snappydata-approach\"></a>The SnappyData Approach</h3>\n<p>At SnappyData, we take a very different approach. SnappyData fuses a low latency, highly available in-memory transactional database (GemFireXD) into Spark with shared memory management and optimizations. Data in the highly available in-memory store is laid out using the same columnar format as Spark (Tungsten). All query engine operators are significantly more optimized through better vectorization and code generation. <br />The net effect is, an order of magnitude performance improvement when compared to native Spark caching, and more than two orders of magnitude better Spark performance when working with external data sources.</p>\n<p>Essentially, we turn Spark into an in-memory operational database capable of transactions, point reads, writes, working with Streams (Spark) and running analytic SQL queries. Or, it is an in-memory scale out Hybrid Database that can execute Spark code, SQL or even Objects.</p>\n<p>If you are already using Spark, experience 20x speed up for your query performance. Try out this <a href=\"https://github.com/SnappyDataInc/snappydata/blob/master/examples/quickstart/scripts/Quickstart.scala\">test</a>.</p>\n<h5><a id=\"user-content-snappy-architecture\" class=\"anchor\" aria-hidden=\"true\" href=\"#snappy-architecture\"></a>Snappy Architecture</h5>\n<p><a target=\"_blank\" href=\"https://github.com/SnappyDataInc/snappydata/blob/master/docs/Images/SnappyArchitecture.png\"><img src=\"https://github.com/SnappyDataInc/snappydata/raw/master/docs/Images/SnappyArchitecture.png\" alt=\"SnappyData Architecture\" /></a></p>\n<h2><a id=\"user-content-getting-started\" class=\"anchor\" aria-hidden=\"true\" href=\"#getting-started\"></a>Getting Started</h2>\n<p>We provide multiple options to get going with SnappyData. The easiest option is, if you are already using Spark 2.1.1.\nYou can simply get started by adding SnappyData as a package dependency. You can find more information on options for running SnappyData <a href=\"https://github.com/SnappyDataInc/snappydata/blob/master/docs/quickstart.md\">here</a>.</p>\n<h2><a id=\"user-content-downloading-and-installing-snappydata\" class=\"anchor\" aria-hidden=\"true\" href=\"#downloading-and-installing-snappydata\"></a>Downloading and Installing SnappyData</h2>\n<p>You can download and install the latest version of SnappyData from the <a href=\"https://www.snappydata.io/download\" rel=\"nofollow\">SnappyData Download Page</a>.\nRefer to the <a href=\"https://github.com/SnappyDataInc/snappydata/blob/master/docs/install.md\">documentation</a> for installation steps.</p>\n<p>If you would like to build SnappyData from source, refer to the <a href=\"https://github.com/SnappyDataInc/snappydata/blob/master/docs/install/building_from_source.md\">documentation on building from source</a>.</p>\n<h2><a id=\"user-content-snappydata-in-5-minutes\" class=\"anchor\" aria-hidden=\"true\" href=\"#snappydata-in-5-minutes\"></a>SnappyData in 5 Minutes!</h2>\n<p>Refer to the <a href=\"https://github.com/SnappyDataInc/snappydata/blob/master/docs/quickstart.md\">5 minutes guide</a> which is intended for both first time and experienced SnappyData users. It provides you with references and common examples to help you get started quickly!</p>\n<h2><a id=\"user-content-documentation\" class=\"anchor\" aria-hidden=\"true\" href=\"#documentation\"></a>Documentation</h2>\n<p>To understand SnappyData and its features refer to the <a href=\"http://snappydatainc.github.io/snappydata/\" rel=\"nofollow\">documentation</a>.</p>\n<h2><a id=\"user-content-community-support\" class=\"anchor\" aria-hidden=\"true\" href=\"#community-support\"></a>Community Support</h2>\n<p>We monitor channels listed below for comments/questions.</p>\n<p><a href=\"http://stackoverflow.com/questions/tagged/snappydata\" rel=\"nofollow\">Stackoverflow</a> <a target=\"_blank\" href=\"https://camo.githubusercontent.com/750a2e8ad8774b724603a73181b558cc4624a609/687474703a2f2f692e696d6775722e636f6d2f4c5049647031322e706e67\"><img src=\"https://camo.githubusercontent.com/750a2e8ad8774b724603a73181b558cc4624a609/687474703a2f2f692e696d6775722e636f6d2f4c5049647031322e706e67\" alt=\"Stackoverflow\" data-canonical-src=\"http://i.imgur.com/LPIdp12.png\" /></a>    <a href=\"http://snappydata-slackin.herokuapp.com/\" rel=\"nofollow\">Slack</a><a target=\"_blank\" href=\"https://camo.githubusercontent.com/0b96fa3579aec14a6b9c65eda01ad3b0b0afd808/687474703a2f2f692e696d6775722e636f6d2f6833736336474d2e706e67\"><img src=\"https://camo.githubusercontent.com/0b96fa3579aec14a6b9c65eda01ad3b0b0afd808/687474703a2f2f692e696d6775722e636f6d2f6833736336474d2e706e67\" alt=\"Slack\" data-canonical-src=\"http://i.imgur.com/h3sc6GM.png\" /></a>        <a href=\"https://gitter.im/SnappyDataInc/snappydata\" rel=\"nofollow\">Gitter</a> <a target=\"_blank\" href=\"https://camo.githubusercontent.com/2a1445dce582ee1b45b82572e895fe8c41d024a6/687474703a2f2f692e696d6775722e636f6d2f6a4e414a654f6e2e6a7067\"><img src=\"https://camo.githubusercontent.com/2a1445dce582ee1b45b82572e895fe8c41d024a6/687474703a2f2f692e696d6775722e636f6d2f6a4e414a654f6e2e6a7067\" alt=\"Gitter\" data-canonical-src=\"http://i.imgur.com/jNAJeOn.jpg\" /></a>          <a href=\"https://groups.google.com/forum/#!forum/snappydata-user\" rel=\"nofollow\">Mailing List</a> <a target=\"_blank\" href=\"https://camo.githubusercontent.com/464a18c1541143bf3a0688e97f0252f9aa0058e4/687474703a2f2f692e696d6775722e636f6d2f596f6d644834732e706e67\"><img src=\"https://camo.githubusercontent.com/464a18c1541143bf3a0688e97f0252f9aa0058e4/687474703a2f2f692e696d6775722e636f6d2f596f6d644834732e706e67\" alt=\"Mailing List\" data-canonical-src=\"http://i.imgur.com/YomdH4s.png\" /></a>             <a href=\"https://www.reddit.com/r/snappydata\" rel=\"nofollow\">Reddit</a> <a target=\"_blank\" href=\"https://camo.githubusercontent.com/ee591e54475a19de576c063fd157404661fa7488/687474703a2f2f692e696d6775722e636f6d2f4142336356746a2e706e67\"><img src=\"https://camo.githubusercontent.com/ee591e54475a19de576c063fd157404661fa7488/687474703a2f2f692e696d6775722e636f6d2f4142336356746a2e706e67\" alt=\"Reddit\" data-canonical-src=\"http://i.imgur.com/AB3cVtj.png\" /></a>          <a href=\"https://jira.snappydata.io/projects/SNAP/issues\" rel=\"nofollow\">JIRA</a> <a target=\"_blank\" href=\"https://camo.githubusercontent.com/27c61aad184f4c87eaa441973b94c33642820c5e/687474703a2f2f692e696d6775722e636f6d2f4539327a6e74412e706e67\"><img src=\"https://camo.githubusercontent.com/27c61aad184f4c87eaa441973b94c33642820c5e/687474703a2f2f692e696d6775722e636f6d2f4539327a6e74412e706e67\" alt=\"JIRA\" data-canonical-src=\"http://i.imgur.com/E92zntA.png\" /></a></p>\n<h2><a id=\"user-content-link-with-snappydata-distribution\" class=\"anchor\" aria-hidden=\"true\" href=\"#link-with-snappydata-distribution\"></a>Link with SnappyData Distribution</h2>\n<p><strong>Using Maven Dependency</strong></p>\n<p>SnappyData artifacts are hosted in Maven Central. You can add a Maven dependency with the following coordinates:</p>\n<pre>groupId: io.snappydata\nartifactId: snappydata-core_2.11\nversion: 1.0.1\ngroupId: io.snappydata\nartifactId: snappydata-cluster_2.11\nversion: 1.0.1\n</pre>\n<p><strong>Using SBT Dependency</strong></p>\n<p>If you are using SBT, add this line to your <strong>build.sbt</strong> for core SnappyData artifacts:</p>\n<p><code>libraryDependencies += \"io.snappydata\" % \"snappydata-core_2.11\" % \"1.0.1\"</code></p>\n<p>For additions related to SnappyData cluster, use:</p>\n<p><code>libraryDependencies += \"io.snappydata\" % \"snappydata-cluster_2.11\" % \"1.0.1\"</code></p>\n<p>You can find more specific SnappyData artifacts <a href=\"http://mvnrepository.com/artifact/io.snappydata\" rel=\"nofollow\">here</a></p>\n<p><strong>Note:</strong> If your project fails when resolving the above dependency (that is, it fails to download javax.ws.rs#javax.ws.rs-api;2.1), it may be due an issue with its pom file. <br />As a workaround, you can add the below code to your <strong>build.sbt</strong>:</p>\n<pre>val workaround = {\n  sys.props += \"packaging.type\" -&gt; \"jar\"\n  ()\n}\n</pre>\n<p>For more details, refer <a href=\"https://github.com/sbt/sbt/issues/3618\">https://github.com/sbt/sbt/issues/3618</a>.</p>\n<h2><a id=\"user-content-ad-analytics-using-snappydata\" class=\"anchor\" aria-hidden=\"true\" href=\"#ad-analytics-using-snappydata\"></a>Ad Analytics using SnappyData</h2>\n<p>Here is a stream + Transactions + Analytics use case example to illustrate the SQL as well as the Spark programming approaches in SnappyData - <a href=\"https://github.com/SnappyDataInc/snappy-poc\">Ad Analytics code example</a>. Here is a <a href=\"https://www.youtube.com/watch?v=bXofwFtmHjE\" rel=\"nofollow\">screencast</a> that showcases many useful features of SnappyData. The example also goes through a benchmark comparing SnappyData to a Hybrid in-memory database and Cassandra.</p>\n<h2><a id=\"user-content-contributing-to-snappydata\" class=\"anchor\" aria-hidden=\"true\" href=\"#contributing-to-snappydata\"></a>Contributing to SnappyData</h2>\n<p>If you are interested in contributing, please visit the <a href=\"http://www.snappydata.io/community\" rel=\"nofollow\">community page</a> for ways in which you can help.</p>\n</article>",
        "created_at": "2018-08-03T19:06:30+0000",
        "updated_at": "2018-08-03T19:06:50+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/11342682?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11825"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 253,
            "label": "analytics",
            "slug": "analytics"
          }
        ],
        "is_public": false,
        "id": 11824,
        "uid": null,
        "title": "Apache Cassandra for analytics: A performance and storage analysis",
        "url": "https://www.oreilly.com/ideas/apache-cassandra-for-analytics-a-performance-and-storage-analysis",
        "content": "<figure class=\"article-image right\"><img src=\"https://d3ucjech6zwjp8.cloudfront.net/360x240/8905201835_985a40fd38_k-718d5db32bdc43a0c4dc819817f8e726.jpg\" alt=\"Portland General Electric's Salem Smart Power Center includes a large-scale energy storage system.\" srcset=\"https://d3ucjech6zwjp8.cloudfront.net/360x240/8905201835_985a40fd38_k-718d5db32bdc43a0c4dc819817f8e726.jpg 360w, https://d3ucjech6zwjp8.cloudfront.net/720x480/8905201835_985a40fd38_k-718d5db32bdc43a0c4dc819817f8e726.jpg 720w, https://d3ucjech6zwjp8.cloudfront.net/1440x960/8905201835_985a40fd38_k-718d5db32bdc43a0c4dc819817f8e726.jpg 1440w\" class=\"\" /><figcaption>Portland General Electric's Salem Smart Power Center includes a large-scale energy storage system.\n          \n                  (source: <a class=\"secondary\" target=\"_blank\" href=\"https://www.flickr.com/photos/portlandgeneralelectric/8905201835\">Portland General Electric on Flickr</a>)\n        \n        \n      \n    \n    \n  <figcaption></figcaption></figcaption></figure><p>This post is about using <a href=\"http://cassandra.apache.org/\">Apache Cassandra</a> for analytics. Think time series, IoT, data warehousing, writing, and querying large swaths of data—not so much transactions or shopping carts. Users thinking of Cassandra as an event store and source/sink for machine learning / modeling / classification would also benefit greatly from this post.</p><p>Two key questions when considering analytics systems are:</p>\n<ol><li>How much storage do I need (to buy)?</li>\n\t<li>How fast can my questions get answered?</li>\n</ol><p>I conducted a performance study, comparing different storage layouts, caching, indexing, filtering, and other options in Cassandra (including <a href=\"https://github.com/tuplejump/FiloDB\">FiloDB</a>), plus <a href=\"https://parquet.apache.org/\">Apache Parquet</a>, the modern gold standard for analytics storage. All comparisons were done using Spark SQL. More importantly than determining data modeling versus storage format versus row cache or DeflateCompressor, I hope this post gives you a useful framework for predicting storage cost and query speeds for your own applications.</p>\n<p>I was initially going to title this post, “Cassandra vs Hadoop,” but honestly, this post is not about Hadoop or Parquet at all. Let me get this out of the way, however, because many people, in their evaluations of different technologies, are going to think about one technology stack versus another. Which is better for which use cases? Is it possible to lower total cost of ownership (TCO) by having just one stack for everything? Answering the storage and query cost questions are part of this analysis. To be transparent, I am the author of FiloDB. While I do have much more vested on one side of this debate, I will focus on the analysis and let you draw your own conclusions. However, I hope you will realize that Cassandra is not just a key-value store; it can be—and is being—used for big data analytics, and it can be very competitive in both query speeds and storage costs.</p>\n<h2>Wide spectrum of storage costs and query speeds</h2>\n<p>Figure 1 summarizes different Cassandra storage options, plus Parquet. Farther to the right denotes higher storage densities, and higher up the chart denotes faster query speeds. In general, you want to see something in the upper right corner.</p>\n<figure id=\"id-68jin\"><img alt=\"\" class=\"iimagessized_figure-1_storagecostsquerychartjpg\" src=\"https://d3ansictanv2wj.cloudfront.net/Sized_Figure-1_StorageCostsQueryChart-ba2a9cf05af2b129995fa229565184b3.jpg\" /><figcaption>Figure 1. Graphic summarizing storage cost vs. query speed in Cassandra and Parquet. Image courtesy of Evan Chan.</figcaption></figure><p>Here is a brief introduction to the different players used in the analysis:</p>\n<ul><li>Regular Cassandra 2.x CQL tables, in both narrow (one record per partition) and wide (both partition and clustering keys, many records per partition) configurations</li>\n\t<li>COMPACT STORAGE tables, the way all of us Cassandra old timers did it before CQL (0.6, baby!)</li>\n\t<li>Caching Cassandra tables in Spark SQL</li>\n\t<li>\n<a href=\"http://github.com/tuplejump/FiloDB\">FiloDB</a>, an analytical database built on C* and Spark</li>\n\t<li>Parquet, the reference gold standard</li>\n</ul><p>What you see in Figure 1 is a wide spectrum of storage efficiency and query speed, from CQL tables at the bottom to FiloDB, which is up to 5x faster in scan speeds than Parquet and almost as efficient storage-wise. Keep in mind that the chart above has a log scale on both axes. Also, while this article will go into the tradeoffs and details about different options in depth, we will not be covering the many other factors people will choose CQL tables for, such as support for modeling maps, sets, lists, custom types, and many other things.</p>\n<h2>Summary of methodology for analysis</h2>\n<p>Query speed was computed by averaging the response times for three different queries:</p>\n<pre>\ndf.select(count(“numarticles”)).show\n</pre>\n<pre>\nSELECT Actor1Name, AVG(AvgTone) as tone FROM gdelt GROUP BY Actor1Name ORDER BY tone DESC\n</pre>\n<pre>\nSELECT AVG(avgtone), MIN(avgtone), MAX(avgtone) FROM gdelt WHERE monthyear=198012\n</pre>\n<p>The first query is an all-table-scan simple count. The second query measures a grouping aggregation. And the third query is designed to test filtering performance with a record count of 43.4K items, or roughly 1% of the original data set. The data set used for each query is the <a href=\"http://www.gdeltproject.org/\">GDELT</a> public data set: 1979-1984, 57 columns x 4.16 million rows, recording geopolitical events worldwide. The source code for ingesting the Cassandra tables and instructions for reproducing the queries are available in my <a href=\"http://github.com/velvia/cassandra-gdelt\">cassandra-gdelt repo</a>.</p>\n<p>The storage cost for Cassandra tables is computed by running compaction first, then taking the size of all stable files in the data folder of the tables.</p>\n<p>To make the Cassandra CQL tables more performant, shorter column names were used (for example, <code>a2code</code> instead of <code>Actor2Code</code>).</p>\n<p>All tests were run on my Macbook Pro 15” Mid-2015, SSD / 16GB. Specifics are as follows:</p>\n<ul><li>Cassandra 2.1.6 installed using CCM</li>\n\t<li>Spark 1.4.0 except where noted, run with master = ‘local[1]’ and spark.sql.shuffle.partitions=4</li>\n\t<li>Spark-Cassandra-Connector 1.4.0-M3</li>\n</ul><p>Running all the tests essentially single threaded was done partly out of simplicity and partly to form a basis for modeling performance behavior (see the modeling query performance section below).</p>\n<h2>Scan speeds are dominated by storage format</h2>\n<p>OK, let’s dive into details! The key to analytics query performance is the <em>scan speed</em>, or how many records you can scan per unit time. This is true for whole table scans, and it is true when you filter data, as we’ll see later. Figure 2 below shows the data for all query times, which are whole table scans, with relative speed factors for easier digestion:</p>\n<figure id=\"FIG2\" class=\"full\"><img alt=\"query times with relative speed factors\" class=\"iimagesfigure-2_updatedjpg\" src=\"https://d3ansictanv2wj.cloudfront.net/Figure-2_updated-302cecf8e0282d0371fba4d8ebd01c8b.jpg\" /><figcaption>Figure 2. All query times with relative speed factors. All query times run on Spark 1.4/1.5 with local[1]; C* 2.1.6 w/ 512MB row cache. Image courtesy of Evan Chan.</figcaption></figure><p>NOTE: to get more accurate scan speeds, one needs to subtract the baseline latency in Spark, but this is left out for simplicity. This actually slightly disfavors the fastest contestants.</p>\n<p>Cassandra’s COMPACT STORAGE gains an order-of-magnitude improvement in scan speeds simply due to more efficient storage. FiloDB and Parquet gain another order of magnitude due to a columnar layout, which allows reading only the columns needed for analysis, plus more efficient columnar blob compression. Thus, storage format makes the biggest difference in scan speeds. More details are below, but for regular CQL tables, the scan speed should be inversely proportional to the number of columns in each record, assuming simple data types (not collections).</p>\n<p>Part of the speed advantage of FiloDB over Parquet has to do with the InMemory option. You could argue this is not fair; however, when you read Parquet files repeatedly, most of that file is most likely in the OS Cache anyway. Yes, having in-memory data is a bigger advantage for networked reads from Cassandra, but I think part of the speed increase is because FiloDB’s columnar format is optimized more for CPU efficiency, rather than compact size. Also, when you cache Parquet files, you are caching an entire file or blocks thereof, compressed and encoded; FiloDB relies on small chunks, which can be much more efficiently cached (on a per-column basis, and allows for updates). Folks at Databricks have repeatedly told me that caching Parquet files in-memory did not result in significant speed gains, and this makes sense due to the format and compression.</p>\n<p>Wide row CQL tables are actually less efficient than narrow row due to additional overhead of clustering column name prefixing. Spark’s cacheTable should be nearly as efficient as the other fast solutions, but suffers from partitioning issues.</p>\n<h2>Storage efficiency generally correlates with scan speed</h2>\n<p>In <a href=\"#FIG2\">Figure 2</a>, you can see that these technologies list in the same order for storage efficiency as for scan speeds, and that’s not an accident. Storing tables as COMPACT STORAGE and FiloDB yields a roughly 7-8.5x improvement in storage efficiency over regular CQL tables for this data set. Less I/O = faster scans!</p>\n<p>Cassandra CQL wide row tables are less efficient, and you’ll see why in a minute. Moving from LZ4 to Deflate compression reduces storage footprint by 38% for FiloDB and 50% for the wide row CQL tables, so it’s definitely worth considering. DeflateCompressor actually sped up wide row CQL scans by 15%, but slowed down the single partition query slightly.</p>\n<h3>Why Cassandra CQL tables are inefficient</h3>\n<p>Let’s say a Cassandra CQL table has a primary key that looks like (pk, ck1, ck2, ck3) and other columns designated c1, c2, c3, c4 for creativity. This is what the physical layout looks like for one partition (“physical row”):</p>\n<table><tbody><tr><td>Column header</td>\n\t\t\t<td>ck1:ck2:ck3:c1</td>\n\t\t\t<td>ck1:ck2:ck3:c2</td>\n\t\t\t<td>ck1:ck2:ck3:c3</td>\n\t\t\t<td>ck1:ck2:ck3:c4</td>\n\t\t\t<td>ck1:ck2:ck3a:c1</td>\n\t\t\t<td>ck1:ck2:ck3a:c2</td>\n\t\t\t<td>ck1:ck2:ck3a:c3</td>\n\t\t\t<td>ck1:ck2:ck3a:c4</td>\n\t\t</tr><tr><td>pk : value</td>\n\t\t\t<td>v1</td>\n\t\t\t<td>v2</td>\n\t\t\t<td>v3</td>\n\t\t\t<td>v4</td>\n\t\t\t<td>v1</td>\n\t\t\t<td>v2</td>\n\t\t\t<td>v3</td>\n\t\t\t<td>v4</td>\n\t\t</tr></tbody></table><p>Cassandra offers ultimate flexibility in terms of updating any part of a record, as well as inserting into collections, but the price paid is that each column of every record is stored in its own cell, with a very lengthy column header consisting of the entire clustering key, plus the name of each column. If you have 100 columns in your table (very common for data warehouse fact tables), then the clustering key <em>ck1:ck2:ck3</em> is repeated 100 times. It is true that compression helps a lot with this, but not enough. Cassandra 3.x has a new, trimmer storage engine that does away with many of these inefficiencies, at a reported space savings of up to 4x.</p>\n<p>COMPACT STORAGE is the way that most of us who used Cassandra prior to CQL stored our data: as one blob per record. It is extremely efficient. That model looks like this:</p>\n<table><tbody><tr><td>Column header</td>\n\t\t\t<td>ck1:ck2:ck3</td>\n\t\t\t<td>ck1:ck2:ck3a</td>\n\t\t</tr><tr><td>pk</td>\n\t\t\t<td>value1_blob</td>\n\t\t\t<td>value2_blob</td>\n\t\t</tr></tbody></table><p>You lose features such as secondary indexing, but you can still model your data for efficient lookups by partition key and range scans of clustering keys.</p>\n<p>FiloDB, on the other hand, stores data by grouping columns together, and then by clumping data from many rows into its own efficient blob format. The layout looks like this:</p>\n<table><tbody><tr><th> </th>\n\t\t\t<th colspan=\"2\">Column 1</th>\n\t\t\t<th colspan=\"2\">Column 2</th>\n\t\t</tr><tr><td>pk</td>\n\t\t\t<td>Chunk 1</td>\n\t\t\t<td>Chunk 2</td>\n\t\t\t<td>Chunk 1</td>\n\t\t\t<td>Chunk 2</td>\n\t\t</tr></tbody></table><p>Columnar formats minimize I/O for analytical queries, which select a small subset of the original data. They also tend to remain compact even in-memory. FiloDB’s internal format is designed for fast random access without the need to deserialize. On the other hand, Parquet is designed for very fast linear scans, but most encoding types require the entire page of data to be deserialized—thus, filtering will incur higher I/O costs.</p>\n<h2>A formula for modeling query performance</h2>\n<p>We can model the query time for a single query using a simple formula:</p>\n<p>Predicted queryTime = Expected number of records / (# cores * scan speed)</p>\n<p>Basically, the query time is proportional to how much data you are querying, and inversely proportional to your resources and raw scan speed. Note that the scan speed above is single-core scan speed, such as was measured using my benchmarking methodology. Keep this model in mind when thinking about storage formats, data modeling, filtering, and other effects.</p>\n<h2>Can caching help? A little bit.</h2>\n<p>If storage size leads partially to slow scan speeds, what about taking advantage of caching options to reduce I/O? Great idea. Let’s review the different options.</p>\n<ul><li>Cassandra row cache: I tried row cache of 512MB for the narrow CQL table use case—512MB was picked as it was a quarter of the size of the data set on disk. Most of the time your data won’t fit in cache. This increased scan speed for the narrow CQL table by 29%. If you tend to access data at the beginning of your partitions, row cache could be a huge win. What I like best about this option is that it’s really easy to use and brain-dead simple, and it works with your changing data.</li>\n\t<li>DSE has an <a href=\"https://docs.datastax.com/en/datastax_enterprise/4.8/datastax_enterprise/inmem/inmemTOC.html\">in-memory tables</a> feature. Think of it, basically, as keeping your SSTables in-memory instead of on disk. It seems to me to be slower than row cache (since you still have to decompress the tables), and I’ve been told it’s not useful for most people.</li>\n\t<li>Finally, in Spark SQL you can cache your tables (CACHE TABLE in spark-sql, sqlContext.cacheTable in spark-shell) in an on-heap, in-memory columnar format. It is really fast (44x speedup over base case above), but suffers from multiple problems: the entire table has to be cached, it cannot be updated, and it is not HA (once any executor or the app dies, ka-boom!). Furthermore, you have to decide what to cache, and the initial read from Cassandra is still really slow.</li>\n</ul><p>None of the above options are anywhere close to the wins that better storage format and effective data modeling will give you. As my analysis shows, FiloDB, without caching, is faster than all Cassandra caching options. Of course, if you are loading data from different data centers or constantly doing network shuffles, then caching can be a big boost, but most Spark on Cassandra setups are collocated.</p>\n<h2>The Future: Optimizing for CPU, not I/O</h2>\n<p>For Spark queries over regular Cassandra tables, I/O dominates CPU due to the storage format. This is why the storage format makes such a big difference, and also why technologies like SSD have dramatically boosted Cassandra performance. Due to the dominance of I/O costs over CPU, it may be worth it to compress data more. For formats like Parquet and FiloDB, which are already optimized for fast scans and minimized I/O, it is the opposite—the CPU cost of querying data actually dominates over I/O. That’s why the Spark folks are working on code-gen and <a href=\"https://databricks.com/blog/2015/04/28/project-tungsten-bringing-spark-closer-to-bare-metal.html\">Project Tungsten</a>.</p>\n<p>If you look at the latest trends, memory is getting cheaper; NVRAM, 3DRAM, and very cheap persistent DRAM technologies promise to make I/O bandwidth no longer an issue. This trend obliterates decades of database design based on the assumption that I/O is much, much slower than CPU, and instead favors CPU-efficient storage formats. With the increase in IOPs, optimizing for linear reads is no longer quite as important.</p>\n<h2>Filtering and data modeling</h2>\n<p>Remember our formula for predicting query performance:</p>\n<p>Predicted queryTime = Expected number of records / (# cores * scan speed)</p>\n<p>Correct data modeling in Cassandra deals with the first part of that equation—enabling fast lookups by reducing the number of records that need to be looked up. Denormalization, writing summaries instead of raw data, and being smart about data modeling all help reduce the number of records. Partition and clustering key filtering are definitely the most effective filtering mechanisms in Cassandra. Keep in mind, though, that scan speeds are still really important, even for filtered data—unless you are really doing just single-key lookups.</p>\n<p>Look back at <a href=\"#FIG2\">Figure 2</a>. What do you see? Using partition key filtering on wide row CQL tables proved very effective—100x faster than scanning the whole wide row table on 1% of the data (a direct plugin in the formula of reducing the number of records to 1% of original). However, since wide rows are a bit inefficient compared to narrow tables, some speed is lost. You can also see in Figure 2 that scan speeds still matter. FiloDB’s in-memory execution of that same filtered query was still 100x faster than the Cassandra CQL table version—taking only 30 <em>milliseconds</em> as opposed to nearly three seconds. Will this matter? For serving concurrent, Web-speed queries it will certainly matter.</p>\n<p>Note that I only modeled a very simple equals predicate, but in reality, many people need much more flexible predicate patterns. Due to the restrictive predicates available for partition keys (= only for all columns except last one, which can be IN), modeling with regular CQL tables will probably require multiple tables, one each to match different predicate patterns. (This is being addressed in C* 2.2 a bit, maybe more in 3.x) This needs to be accounted for in the storage cost and TOC analysis. One way around this is to store custom index tables, which allows application-side custom scan patterns. FiloDB uses this technique to provide arbitrary filtering of partition keys.</p>\n<p>Some notes on the filtering and data modeling aspect of my analysis:</p>\n<ul><li>The narrow rows layout in CQL is one record per partition key, thus partition key filtering does not apply. See discussion of secondary indices below.</li>\n\t<li>Cached tables in Spark SQL, as of Spark 1.5, only does whole table scans. There might be some improvements coming, though—see <a href=\"https://issues.apache.org/jira/browse/SPARK-4849\">SPARK-4849</a> in Spark 1.6.</li>\n\t<li>FiloDB has roughly the same filtering capabilities as Cassandra—by partition key and clustering key—but improvements on the partition key filtering capabilities of C are planned.</li>\n\t<li>It is possible to partition your Parquet files and selectively read them, and it is supposedly possible to sort your files to take advantage of intra-file filtering. That takes extra effort, and since I haven’t heard of anyone doing the intra-file sort, I deemed it outside the scope of this study. Even if you were to do this, the filtering would not be anywhere near as granular as is possible with Cassandra and FiloDB—of course, your comments and enlightenment are welcome here.</li>\n</ul><h2>Cassandra’s secondary indices usually not worth it</h2>\n<p>How do secondary indices in Cassandra perform? Let’s test that with two count queries with a WHERE clause on Actor1CountryCode, a low cardinality field with a hugely varying number of records in our portion of the GDELT data set:</p>\n<ul><li>WHERE Actor1CountryCode = ‘USA’: 378k records (9.1% of records)</li>\n\t<li>WHERE Actor1CountryCode = ‘ALB’: 5005 records (0.1% of records)</li>\n</ul><table><tbody><tr><th> </th>\n\t\t\t<th>Large country</th>\n\t\t\t<th>Small country</th>\n\t\t\t<th>2i Scan Rate</th>\n\t\t</tr><tr><td>Narrow CQL Table</td>\n\t\t\t<td>28s / 6.6x</td>\n\t\t\t<td>0.7s / 264x</td>\n\t\t\t<td>13.5k records/sec</td>\n\t\t</tr><tr><td>CQL Wide Rows</td>\n\t\t\t<td>143s / 1.9x</td>\n\t\t\t<td>2.7s / 103x</td>\n\t\t\t<td>2643 records/sec</td>\n\t\t</tr></tbody></table><p>If secondary indices were perfectly efficient, one would expect query times to reduce linearly with the drop in the number of records. Alas, this is not so. For the CountryCode = USA query, one would expect a speedup of around 11x, but secondary indices proved very inefficient, especially in the wide rows case. Why is that? Because for wide rows, Cassandra has to do a lot of point lookups on the same partition, which is very inefficient and results in only a small drop in the I/O required (in fact, much more random I/O), compared to a full table scan.</p>\n<p>Secondary indices work well only when the number of records is reduced to such a small amount that the inefficiencies do not matter and Cassandra can skip most partitions. There are also other operational issues with secondary indices, and they are not recommended for use when the cardinality goes above 50,000 items or so.</p>\n<h2>Predicting your own data’s query performance</h2>\n<p>How should you measure the performance of your own data and hardware? It’s really simple, actually:</p>\n<ol><li>Measure your scan speed for your base Cassandra CQL table. Number of records / time to query, single threaded</li>\n\t<li>Use the formula earlier—Predicted queryTime = Expected number of records / (# cores * scan speed)</li>\n\t<li>Use relative speed factors above for predictions</li>\n</ol><p>The relative factors above are based on the GDELT data set with 57 columns. The more columns you have (data warehousing applications commonly have hundreds of columns), the greater you can expect the scan speed boost for FiloDB and Parquet. (Again, this is because, unlike for regular CQL/row-oriented layouts, columnar layouts are generally insensitive to the number of columns.) It is true that concurrency (within a single query) leads to its own inefficiencies, but in my experience, that is more like a 2x slowdown, and not the order-of-magnitude differences we are modeling here.</p>\n<p>User concurrency can be modeled by dividing the number of available cores by the number of users. You can easily see that in FAIR scheduling mode, Spark will actually schedule multiple queries at the same time (but be sure to modify fair-scheduler.xml appropriately). Thus, the formula becomes:</p>\n<p>Predicted queryTime = Expected number of records * # users / (# cores * scan speed)</p>\n<p>There is an important case where the above formula needs to be modified, and that is for single-partition queries (for example, where you have a WHERE clause with an exact match for all partition keys, and Spark pushes down the predicate to Cassandra). The above formula assumes that the queries are spread over the number of nodes you have, but this is not true for single-partition queries. In that case, there are two possibilities:</p>\n<ol><li>The number of users is less than the number of available cores. Then, the query time = number_of_records / scan_speed.</li>\n\t<li>The number of users is &gt;= the number of available cores. In that case, the work is divided amongst each core, so the original query time formula works again.</li>\n</ol><h2>Conclusions</h2>\n<p>Apache Cassandra is one of the most widely used, proven, and robust distributed databases in the modern big data era. The good news is that there are multiple options for using it in an efficient manner for ad-hoc, batch, time-series analytics applications.</p>\n<p>For (multiple) order-of-magnitude improvements in query and storage performance, consider the storage format carefully, and model your data to take advantage of partition and clustering key filtering/predicate pushdowns. Both effects can be combined for maximum advantage—using FiloDB plus filtering data improved a three-minute CQL table scan to sub-100ms response times. Secondary indices are helpful only if they filter your data down to, say, 1% or less—and even then, consider them carefully. Row caching, compression, and other options offer smaller advantages up to about 2x.</p>\n<p>If you need a lot of individual record updates, or lookups by individual record, but don’t mind creating your own blob format, the COMPACT STORAGE / single column approach could work really well. If you need fast analytical query speeds with updates, fine-grained filtering and a Web-speed in-memory option, FiloDB could be a good bet. If the formula above shows that regular Cassandra tables, laid out with the best data modeling techniques applied, are good enough for your use case, kudos to you!</p>\n        \n                  \n        \n                  <div class=\"image-credit\">\n  Article image: Portland General Electric's Salem Smart Power Center includes a large-scale energy storage system.\n          \n                  (source: <a class=\"secondary\" target=\"_blank\" href=\"https://www.flickr.com/photos/portlandgeneralelectric/8905201835\">Portland General Electric on Flickr</a>).\n        \n        \n      \n    \n    \n</div>",
        "created_at": "2018-08-03T19:03:57+0000",
        "updated_at": "2018-08-03T19:04:03+0000",
        "published_at": "2016-02-10T11:00:00+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 18,
        "domain_name": "www.oreilly.com",
        "preview_picture": "https://d3ucjech6zwjp8.cloudfront.net/1400x933/8905201835_985a40fd38_k-718d5db32bdc43a0c4dc819817f8e726.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11824"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1279,
            "label": "go",
            "slug": "go"
          },
          {
            "id": 1280,
            "label": "cql",
            "slug": "cql"
          }
        ],
        "is_public": false,
        "id": 11804,
        "uid": null,
        "title": "instaclustr/gocql",
        "url": "https://github.com/instaclustr/gocql",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p><a href=\"https://gitter.im/gocql/gocql?utm_source=badge&amp;utm_medium=badge&amp;utm_campaign=pr-badge&amp;utm_content=badge\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/da2edb525cde1455a622c58c0effc3a90b9a181c/68747470733a2f2f6261646765732e6769747465722e696d2f4a6f696e253230436861742e737667\" alt=\"Join the chat at https://gitter.im/gocql/gocql\" data-canonical-src=\"https://badges.gitter.im/Join%20Chat.svg\" /></a>\n<a href=\"https://travis-ci.org/gocql/gocql\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c0fd4dc7ef4cba955be030a1b7ef73dc47fc408c/68747470733a2f2f7472617669732d63692e6f72672f676f63716c2f676f63716c2e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/gocql/gocql.svg?branch=master\" /></a>\n<a href=\"https://godoc.org/github.com/gocql/gocql\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c54f2091747081e9d4527f5efbcb06b704f46890/68747470733a2f2f676f646f632e6f72672f6769746875622e636f6d2f676f63716c2f676f63716c3f7374617475732e737667\" alt=\"GoDoc\" data-canonical-src=\"https://godoc.org/github.com/gocql/gocql?status.svg\" /></a></p>\n<p>Package gocql implements a fast and robust Cassandra client for the\nGo programming language.</p>\n<p>Project Website: <a href=\"https://gocql.github.io/\" rel=\"nofollow\">https://gocql.github.io/</a><br />API documentation: <a href=\"https://godoc.org/github.com/gocql/gocql\" rel=\"nofollow\">https://godoc.org/github.com/gocql/gocql</a><br />Discussions: <a href=\"https://groups.google.com/forum/#!forum/gocql\" rel=\"nofollow\">https://groups.google.com/forum/#!forum/gocql</a></p>\n<h2><a id=\"user-content-supported-versions\" class=\"anchor\" aria-hidden=\"true\" href=\"#supported-versions\"></a>Supported Versions</h2>\n<p>The following matrix shows the versions of Go and Cassandra that are tested with the integration test suite as part of the CI build:</p>\n<table><thead><tr><th>Go/Cassandra</th>\n<th>2.1.x</th>\n<th>2.2.x</th>\n<th>3.0.x</th>\n</tr></thead><tbody><tr><td>1.8</td>\n<td>yes</td>\n<td>yes</td>\n<td>yes</td>\n</tr><tr><td>1.9</td>\n<td>yes</td>\n<td>yes</td>\n<td>yes</td>\n</tr></tbody></table><p>Gocql has been tested in production against many different versions of Cassandra. Due to limits in our CI setup we only test against the latest 3 major releases, which coincide with the official support from the Apache project.</p>\n<h2><a id=\"user-content-sunsetting-model\" class=\"anchor\" aria-hidden=\"true\" href=\"#sunsetting-model\"></a>Sunsetting Model</h2>\n<p>In general, the gocql team will focus on supporting the current and previous versions of Go. gocql may still work with older versions of Go, but official support for these versions will have been sunset.</p>\n<h2><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\" href=\"#installation\"></a>Installation</h2>\n<pre>go get github.com/gocql/gocql\n</pre>\n<h2><a id=\"user-content-features\" class=\"anchor\" aria-hidden=\"true\" href=\"#features\"></a>Features</h2>\n<ul><li>Modern Cassandra client using the native transport</li>\n<li>Automatic type conversions between Cassandra and Go\n<ul><li>Support for all common types including sets, lists and maps</li>\n<li>Custom types can implement a <code>Marshaler</code> and <code>Unmarshaler</code> interface</li>\n<li>Strict type conversions without any loss of precision</li>\n<li>Built-In support for UUIDs (version 1 and 4)</li>\n</ul></li>\n<li>Support for logged, unlogged and counter batches</li>\n<li>Cluster management\n<ul><li>Automatic reconnect on connection failures with exponential falloff</li>\n<li>Round robin distribution of queries to different hosts</li>\n<li>Round robin distribution of queries to different connections on a host</li>\n<li>Each connection can execute up to n concurrent queries (whereby n is the limit set by the protocol version the client chooses to use)</li>\n<li>Optional automatic discovery of nodes</li>\n<li>Policy based connection pool with token aware and round-robin policy implementations</li>\n</ul></li>\n<li>Support for password authentication</li>\n<li>Iteration over paged results with configurable page size</li>\n<li>Support for TLS/SSL</li>\n<li>Optional frame compression (using snappy)</li>\n<li>Automatic query preparation</li>\n<li>Support for query tracing</li>\n<li>Support for Cassandra 2.1+ <a href=\"https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v3.spec\">binary protocol version 3</a>\n<ul><li>Support for up to 32768 streams</li>\n<li>Support for tuple types</li>\n<li>Support for client side timestamps by default</li>\n<li>Support for UDTs via a custom marshaller or struct tags</li>\n</ul></li>\n<li>Support for Cassandra 3.0+ <a href=\"https://github.com/apache/cassandra/blob/trunk/doc/native_protocol_v4.spec\">binary protocol version 4</a></li>\n<li>An API to access the schema metadata of a given keyspace</li>\n</ul><h2><a id=\"user-content-performance\" class=\"anchor\" aria-hidden=\"true\" href=\"#performance\"></a>Performance</h2>\n<p>While the driver strives to be highly performant, there are cases where it is difficult to test and verify. The driver is built\nwith maintainability and code readability in mind first and then performance and features, as such every now and then performance\nmay degrade, if this occurs please report and issue and it will be looked at and remedied. The only time the driver copies data from\nits read buffer is when it Unmarshal's data into supplied types.</p>\n<p>Some tips for getting more performance from the driver:</p>\n<ul><li>Use the TokenAware policy</li>\n<li>Use many goroutines when doing inserts, the driver is asynchronous but provides a synchronous API, it can execute many queries concurrently</li>\n<li>Tune query page size</li>\n<li>Reading data from the network to unmarshal will incur a large amount of allocations, this can adversely affect the garbage collector, tune <code>GOGC</code></li>\n<li>Close iterators after use to recycle byte buffers</li>\n</ul><h2><a id=\"user-content-important-default-keyspace-changes\" class=\"anchor\" aria-hidden=\"true\" href=\"#important-default-keyspace-changes\"></a>Important Default Keyspace Changes</h2>\n<p>gocql no longer supports executing \"use \" statements to simplify the library. The user still has the\nability to define the default keyspace for connections but now the keyspace can only be defined before a\nsession is created. Queries can still access keyspaces by indicating the keyspace in the query:</p>\n<div class=\"highlight highlight-source-sql\"><pre>SELECT * FROM example2.table;</pre></div>\n<p>Example of correct usage:</p>\n<div class=\"highlight highlight-source-go\"><pre>\tcluster := gocql.NewCluster(\"192.168.1.1\", \"192.168.1.2\", \"192.168.1.3\")\n\tcluster.Keyspace = \"example\"\n\t...\n\tsession, err := cluster.CreateSession()\n</pre></div>\n<p>Example of incorrect usage:</p>\n<div class=\"highlight highlight-source-go\"><pre>\tcluster := gocql.NewCluster(\"192.168.1.1\", \"192.168.1.2\", \"192.168.1.3\")\n\tcluster.Keyspace = \"example\"\n\t...\n\tsession, err := cluster.CreateSession()\n\tif err = session.Query(\"use example2\").Exec(); err != nil {\n\t\tlog.Fatal(err)\n\t}</pre></div>\n<p>This will result in an err being returned from the session.Query line as the user is trying to execute a \"use\"\nstatement.</p>\n<h2><a id=\"user-content-example\" class=\"anchor\" aria-hidden=\"true\" href=\"#example\"></a>Example</h2>\n<div class=\"highlight highlight-source-go\"><pre>/* Before you execute the program, Launch `cqlsh` and execute:\ncreate keyspace example with replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\ncreate table example.tweet(timeline text, id UUID, text text, PRIMARY KEY(id));\ncreate index on example.tweet(timeline);\n*/\npackage main\nimport (\n\t\"fmt\"\n\t\"log\"\n\t\"github.com/gocql/gocql\"\n)\nfunc main() {\n\t// connect to the cluster\n\tcluster := gocql.NewCluster(\"192.168.1.1\", \"192.168.1.2\", \"192.168.1.3\")\n\tcluster.Keyspace = \"example\"\n\tcluster.Consistency = gocql.Quorum\n\tsession, _ := cluster.CreateSession()\n\tdefer session.Close()\n\t// insert a tweet\n\tif err := session.Query(`INSERT INTO tweet (timeline, id, text) VALUES (?, ?, ?)`,\n\t\t\"me\", gocql.TimeUUID(), \"hello world\").Exec(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tvar id gocql.UUID\n\tvar text string\n\t/* Search for a specific set of records whose 'timeline' column matches\n\t * the value 'me'. The secondary index that we created earlier will be\n\t * used for optimizing the search */\n\tif err := session.Query(`SELECT id, text FROM tweet WHERE timeline = ? LIMIT 1`,\n\t\t\"me\").Consistency(gocql.One).Scan(&amp;id, &amp;text); err != nil {\n\t\tlog.Fatal(err)\n\t}\n\tfmt.Println(\"Tweet:\", id, text)\n\t// list all tweets\n\titer := session.Query(`SELECT id, text FROM tweet WHERE timeline = ?`, \"me\").Iter()\n\tfor iter.Scan(&amp;id, &amp;text) {\n\t\tfmt.Println(\"Tweet:\", id, text)\n\t}\n\tif err := iter.Close(); err != nil {\n\t\tlog.Fatal(err)\n\t}\n}</pre></div>\n<h2><a id=\"user-content-data-binding\" class=\"anchor\" aria-hidden=\"true\" href=\"#data-binding\"></a>Data Binding</h2>\n<p>There are various ways to bind application level data structures to CQL statements:</p>\n<ul><li>You can write the data binding by hand, as outlined in the Tweet example. This provides you with the greatest flexibility, but it does mean that you need to keep your application code in sync with your Cassandra schema.</li>\n<li>You can dynamically marshal an entire query result into an <code>[]map[string]interface{}</code> using the <code>SliceMap()</code> API. This returns a slice of row maps keyed by CQL column names. This method requires no special interaction with the gocql API, but it does require your application to be able to deal with a key value view of your data.</li>\n<li>As a refinement on the <code>SliceMap()</code> API you can also call <code>MapScan()</code> which returns <code>map[string]interface{}</code> instances in a row by row fashion.</li>\n<li>The <code>Bind()</code> API provides a client app with a low level mechanism to introspect query meta data and extract appropriate field values from application level data structures.</li>\n<li>The <a href=\"https://github.com/scylladb/gocqlx\">gocqlx</a> package is an idiomatic extension to gocql that provides usability features. With gocqlx you can bind the query parameters from maps and structs, use named query parameters (:identifier) and scan the query results into structs and slices. It comes with a fluent and flexible CQL query builder that supports full CQL spec, including BATCH statements and custom functions.</li>\n<li>Building on top of the gocql driver, <a href=\"https://github.com/relops/cqlr\">cqlr</a> adds the ability to auto-bind a CQL iterator to a struct or to bind a struct to an INSERT statement.</li>\n<li>Another external project that layers on top of gocql is <a href=\"http://relops.com/cqlc\" rel=\"nofollow\">cqlc</a> which generates gocql compliant code from your Cassandra schema so that you can write type safe CQL statements in Go with a natural query syntax.</li>\n<li><a href=\"https://github.com/hailocab/gocassa\">gocassa</a> is an external project that layers on top of gocql to provide convenient query building and data binding.</li>\n<li><a href=\"https://github.com/kristoiv/gocqltable\">gocqltable</a> provides an ORM-style convenience layer to make CRUD operations with gocql easier.</li>\n</ul><h2><a id=\"user-content-ecosystem\" class=\"anchor\" aria-hidden=\"true\" href=\"#ecosystem\"></a>Ecosystem</h2>\n<p>The following community maintained tools are known to integrate with gocql:</p>\n<ul><li><a href=\"https://github.com/scylladb/gocqlx\">gocqlx</a> is a gocql extension that automates data binding, adds named queries support, provides flexible query builders and plays well with gocql.</li>\n<li><a href=\"https://github.com/db-journey/journey\">journey</a> is a migration tool with Cassandra support.</li>\n<li><a href=\"https://github.com/mikebthun/negronicql\">negronicql</a> is gocql middleware for Negroni.</li>\n<li><a href=\"https://github.com/relops/cqlr\">cqlr</a> adds the ability to auto-bind a CQL iterator to a struct or to bind a struct to an INSERT statement.</li>\n<li><a href=\"http://relops.com/cqlc\" rel=\"nofollow\">cqlc</a> generates gocql compliant code from your Cassandra schema so that you can write type safe CQL statements in Go with a natural query syntax.</li>\n<li><a href=\"https://github.com/hailocab/gocassa\">gocassa</a> provides query building, adds data binding, and provides easy-to-use \"recipe\" tables for common query use-cases.</li>\n<li><a href=\"https://github.com/kristoiv/gocqltable\">gocqltable</a> is a wrapper around gocql that aims to simplify common operations.</li>\n<li><a href=\"https://github.com/willfaught/gockle\">gockle</a> provides simple, mockable interfaces that wrap gocql types</li>\n<li><a href=\"https://github.com/scylladb/scylla\">scylladb</a> is a fast Apache Cassandra-compatible NoSQL database</li>\n</ul><h2><a id=\"user-content-other-projects\" class=\"anchor\" aria-hidden=\"true\" href=\"#other-projects\"></a>Other Projects</h2>\n<ul><li><a href=\"https://github.com/tux21b/gocqldriver\">gocqldriver</a> is the predecessor of gocql based on Go's <code>database/sql</code> package. This project isn't maintained anymore, because Cassandra wasn't a good fit for the traditional <code>database/sql</code> API. Use this package instead.</li>\n</ul><h2><a id=\"user-content-seo\" class=\"anchor\" aria-hidden=\"true\" href=\"#seo\"></a>SEO</h2>\n<p>For some reason, when you Google <code>golang cassandra</code>, this project doesn't feature very highly in the result list. But if you Google <code>go cassandra</code>, then we're a bit higher up the list. So this is note to try to convince Google that golang is an alias for Go.</p>\n<h2><a id=\"user-content-license\" class=\"anchor\" aria-hidden=\"true\" href=\"#license\"></a>License</h2>\n<blockquote>\n<p>Copyright (c) 2012-2016 The gocql Authors. All rights reserved.\nUse of this source code is governed by a BSD-style\nlicense that can be found in the LICENSE file.</p>\n</blockquote>\n<h2><a id=\"user-content-further-information\" class=\"anchor\" aria-hidden=\"true\" href=\"#further-information\"></a>Further Information</h2>\n<ul><li>See Alex Lourie's blog <a href=\"https://www.instaclustr.com/query-idempotence-in-gocql-driver/\" rel=\"nofollow\">\"Query Idempotence in GoCql driver\"</a></li>\n</ul></article>",
        "created_at": "2018-08-03T00:27:18+0000",
        "updated_at": "2018-08-03T00:27:23+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/11550580?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11804"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1279,
            "label": "go",
            "slug": "go"
          },
          {
            "id": 1280,
            "label": "cql",
            "slug": "cql"
          }
        ],
        "is_public": false,
        "id": 11803,
        "uid": null,
        "title": "GoCQL",
        "url": "http://gocql.github.io/",
        "content": "<div class=\"row\"><div class=\"col-lg-6\"><img src=\"http://gocql.github.io/gocql.png\" alt=\"GoCQL logo\" class=\"pull-left logo\" /></div></div><p><strong>Under Development:</strong> The GoCQL package is currently actively\n\tdeveloped and the API may change in the future.</p><div class=\"row\"><div class=\"col-lg-3\"><h3>Cluster Management</h3><p>GoCQL automatically discovers all data centers, racks and hosts\n\t\tin your cluster, manages a pool of connections to them and distributes\n\t\tqueries in a reasonable and efficient way.</p></div><div class=\"col-lg-3\"><h3>Type Conversion</h3><p>Automatic and safe type conversion between Cassandra and Go without\n\t\tany loss of precision. Basic types, collections and UUIDs are supported\n\t\tby default and custom types can implement their own marshaling logic.</p></div><div class=\"col-lg-3\"><h3>Synchronous and Concurrent</h3><p>Synchronous API with an asynchronous and concurrent back-end. Each\n\t\tconnection can handle up to 128 concurrent queries and may receive\n\t\tserver side push events at any time.</p></div><div class=\"col-lg-3\"><h3>Failover Management</h3><p>TODO :(</p></div></div><div class=\"row\"><div class=\"col-lg-3\"><h3>Result Paging</h3><p>Iterate over large results sets and let GoCQL fetch one page after\n\t\tanother. The next page is automatically pre-fetched in the background\n\t\tonce the iterator has passed a certain threshold.</p></div><div class=\"col-lg-3\"><h3>Atomic Batches</h3><p>Execute a batch of related updates in a single query. GoCQL supports\n\t\tlogged, unlogged and counter batches.</p></div><div class=\"col-lg-3\"><h3>Query Tracing</h3><p>Trace queries to obtain a detailed output of all events that\n\t\thappened during the query execution from Cassandra. The output might\n\t\thelp to identify bugs and performance bottlenecks in your\n\t\tapplication.</p></div><div class=\"col-lg-3\"><h3>Frame Compression</h3><p>Speed up and reduce the network traffic by compressing the frames\n\t\tthat are sent to Cassandra.\n\t\t<a href=\"https://code.google.com/p/snappy/\">Snappy</a>, a\n\t\tcompression algorithm that aims for very high speeds and reasonable\n\t\tcompression, is enabled by default.</p></div></div><div class=\"row\"><div class=\"col-lg-3\"><h3>Multiple Cassandra Versions</h3><p>GoCQL supports multiple Cassandra version. Currently Cassandra 1.2\n\t\tand Cassandra 2.0 are fully supported.</p></div><div class=\"col-lg-3\"><h3>Thoroughly Tested</h3><p>TODO :(</p></div><div class=\"col-lg-3\"><h3>BSD License</h3><p>Completely open source. Browse the source on\n\t\t<a href=\"https://github.com/gocql/gocql\">GitHub</a> and start\n\t\tcontributing today.</p></div></div>",
        "created_at": "2018-08-03T00:27:04+0000",
        "updated_at": "2018-08-03T00:27:10+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "gocql.github.io",
        "preview_picture": "http://gocql.github.io/gocql.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11803"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1276,
            "label": "ldap",
            "slug": "ldap"
          },
          {
            "id": 1277,
            "label": "authentication",
            "slug": "authentication"
          }
        ],
        "is_public": false,
        "id": 11802,
        "uid": null,
        "title": "instaclustr/cassandra-ldap",
        "url": "https://github.com/instaclustr/cassandra-ldap",
        "content": "<p>This is a pluggable authentication implementation for Apache Cassandra, providing a way to authenticate and create users based on a configured LDAP server.\nThis implementation purely provides authentication only. Role management must be performed through the usual Cassandra role management, CassandraAuthorizer. See <strong>How it works</strong> for more details.</p><p>There are separate branches for each supported version of Apache Cassandra. Each branch should work with any corresponding minor version of the same branch in Cassandra.\nBuilding the jar requires Apache maven.\nClone and checkout the desired branch.</p><pre>git clone https://github.com/instaclustr/cassandra-ldap/\ngit checkout 3.11\n</pre><p>Compile and package</p><pre>mvn package\n</pre><p>From the git repo, fill in and copy the <code>ldap.properties</code> file to some location ($CASSANDRA_CONF is a good one) on each of the nodes. This file is used for telling the authenticator details about the LDAP server and connection.</p><p>Copy the created jar file to each node in your cluster, and append it to the CLASSPATH variable in the cassandra-env.sh/cassandra-env.ps1)</p><pre>CLASSPATH=\"$CLASSPATH:/path/to/cassandra-ldap-3.11.2.jar\"\n</pre><p>Also add the following option to the JVM options, pointing to the location of your ldap.properties file (on each node).</p><pre>JVM_OPTS=\"$JVM_OPTS -Dldap.properties.file=$CASSANDRA_CONF/ldap.properties\"\n</pre><p>In your <code>cassandra.yaml</code> configure the authenticator <strong>and authorizer</strong> like so:</p><pre>authenticator: com.instaclustr.cassandra.ldap.LDAPAuthenticator\nauthorizer: CassandraAuthorizer\n</pre><p>Configure credential caching parameters in <code>cassandra.yaml</code>.\n[Re]start Cassandra.</p><p><strong>WARNING</strong> - Doing this on a live cluster should be handled with care. If done in a rolling fashion from PasswordAuthenticator (or some other implementation) connections to an LDAP configured node using non-ldap credentials will fail if usernames and passwords don't match, that is, nodes running LDAPAuthenticator will not be able to access Cassandra users that are <em>not</em> in LDAP. Safest method would be to either support both mechanisms in your application (handle failure with C* users) or switch to AllowAllAuthenticator (in a rolling fashion) prior to switching to LDAPAuthenticator.</p><p>LDAPAuthenticator currently supports plain text authorisation requests only in the form of a username and password. This request is made to the LDAP server over plain text, so you should be using client encryption on the Cassandra side and secure ldap (ldaps) on the LDAP side.</p><p>Credentials are sent from your client to the Cassandra server and then tested against the LDAP server for authentication using a specified service account. This service account should be configured in the <code>ldap.properties</code> file using the <code>service_dn</code> and <code>service_password</code> properties. This can be excluded if you allow anonymous access to ldap (not recommended unless you know what you're doing!).</p><p>On successful authentication to LDAP a corresponding Cassandra user will be created (including for the service user who will be SUPERUSER). These users are never removed, as it is deemed cleanup is not necessary as long as auth is still handled by LDAP. Manual cleanup of users will work fine, and if they re-auth a replacement user will be created. Passwords are not stored in Cassandra, however on 3.11 and later will live in the credentials cache when used.</p><ul><li>See blog by Kurt Greaves <a href=\"https://www.instaclustr.com/apache-cassandra-ldap-authentication/\" rel=\"nofollow\">\"Apache Cassandra LDAP Authentication\"</a></li>\n</ul>",
        "created_at": "2018-08-03T00:25:09+0000",
        "updated_at": "2019-03-22T03:26:03+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/11550580?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11802"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11801,
        "uid": null,
        "title": "instaclustr/cassandra-docker",
        "url": "https://github.com/instaclustr/cassandra-docker",
        "content": "<p>This is the Instaclustr public docker image for Apache Cassandra.\nIt contains docker images for Cassandra 3.0 and 3.11.1.</p><p>It contains some of our best practices and lessons learnt from running Cassandra on docker for the last 4 years over 1000's of clusters.\nIt also supports configuration via environment variables in the same manner as the <a href=\"https://hub.docker.com/_/cassandra/\" rel=\"nofollow\">official docker cassandra image</a></p><p>Primary configuration of Cassandra should be done by creating a volume mount on the Cassandra config\ndirectory and providing configuration externally (manually, docker swarm, k8s), however support for\nbasic configuration via environment variables does exist. See below.</p><p>Current status: <strong>Beta</strong></p><h2>Start a <code>cassandra</code> server instance</h2><p>Prebuilt images can be fetched by running:</p><div class=\"highlight highlight-text-shell-session\"><pre>docker pull instaclustr/cassandra</pre></div><p>Starting a Cassandra instance is simple:</p><div class=\"highlight highlight-text-shell-session\"><pre>$ docker run --name some-cassandra -d cassandra:tag</pre></div><p>... where <code>some-cassandra</code> is the name you want to assign to your container and <code>tag</code> is the tag specifying the Cassandra version you want. See the list above for relevant tags.</p><h2>Where to Store Data</h2><p>Important note: There are several ways to store data used by applications that run in Docker containers. We encourage users of the <code>cassandra</code> images to familiarize themselves with the options available, including:</p><ul><li>Let Docker manage the storage of your database data <a href=\"https://docs.docker.com/engine/tutorials/dockervolumes/#adding-a-data-volume\" rel=\"nofollow\">by writing the database files to disk on the host system using its own internal volume management</a>. This is the default and is easy and fairly transparent to the user. The downside is that the files may be hard to locate for tools and applications that run directly on the host system, i.e. outside containers. Performance will also suffer, this is generally recommended for non production environments.</li>\n<li>Create a data directory on the host system (outside the container) and <a href=\"https://docs.docker.com/engine/tutorials/dockervolumes/#mount-a-host-directory-as-a-data-volume\" rel=\"nofollow\">mount this to a directory visible from inside the container</a>. This places the database files in a known location on the host system, and makes it easy for tools and applications on the host system to access the files. The downside is that the user needs to make sure that the directory exists, and that e.g. directory permissions and other security mechanisms on the host system are set up correctly.</li>\n</ul><p>The Docker documentation is a good starting point for understanding the different storage options and variations, and there are multiple blogs and forum postings that discuss and give advice in this area. We will simply show the basic procedure here for the latter option above:</p><ol><li>\n<p>Create a data directory on a suitable volume on your host system, e.g. <code>/my/own/datadir</code>.</p>\n</li>\n<li>\n<p>Start your <code>cassandra</code> container like this:</p>\n<div class=\"highlight highlight-text-shell-session\"><pre>$ docker run --name some-cassandra -v /my/own/datadir:/var/lib/cassandra -d cassandra:tag</pre></div>\n</li>\n</ol><p>The <code>-v /my/own/datadir:/var/lib/cassandra</code> part of the command mounts the <code>/my/own/datadir</code> directory from the underlying host system as <code>/var/lib/cassandra</code> inside the container, where Cassandra by default will write its data files.</p><p>Note that users on host systems with SELinux enabled may see issues with this. The current workaround is to assign the relevant SELinux policy type to the new data directory so that the container will be allowed to access it:</p><div class=\"highlight highlight-text-shell-session\"><pre>$ chcon -Rt svirt_sandbox_file_t /my/own/datadir</pre></div><p>The Cassandra configuration directory (/etc/cassandra) can be managed by docker as a docker volume as appropriate for you environment if you are not using environment variable based configuration.\nThe Cassandra data directory however should generally be a bind mount to a directory on the host with an appropriately configured file system\n(e.g. XFS with a readahead value of 8).</p><p>This docker file does not yet support block device passthrough via the device flag.</p><h2>Configuring the kernel</h2><p>To optimally run Cassandra, the kernal and a few other parameters for the process need to be tuned. Most of these can be done via the docker command being run:</p><div class=\"highlight highlight-text-shell-session\"><pre>--cap-add=IPC_LOCK --ulimit memlock=-1 --ulimit nofile=100000 --ulimit nproc=32768</pre></div><p>Some sysctl suggestions will need to be set at the docker host level as docker is limited in what tuneables it can modify.\nE.g. changes to the sysctl <code>vm.max_map_count</code></p><p>When utilisting this image with Kubernetes, you can create a privileged init container that will set up the correct sysctl properties\nfor the kubernetes node. Allowing the Cassandra to be run as a non privileged container whilst still being configured correctly.\nFor example:</p><h2>Injecting configuration</h2><p>To provide your own configuration for Cassandra, via a user provided cassandra.yaml, cassandra-env.sh, jvm.properties, rack-dc.properties file etc.\nYou can volume mount the configration directory or use some other configuration management capability (e.g. kubernetes configMaps)</p><div class=\"highlight highlight-text-shell-session\"><pre>$ docker run --name some-cassandra -v /my/own/configdir:/etc/cassandra -d cassandra:tag</pre></div><p>Configuring Cassandra in this manner is not compatible with legacy configuration via <code>CASSANDRA_ENV_OVERRIDES</code>.</p><h2>Legacy configuration</h2><p>This docker images supports configuration via environment variables as well similar to the docker-library image see <a href=\"https://github.com/instaclustr/cassandra-docker/blob/master/LEGACY.md\">legacy documentation</a></p><h2>Elassandra</h2><p>Instaclustr cassandra docker images currently support Elassandra via <a href=\"https://github.com/zegelin/cassandra-docker\">https://github.com/zegelin/cassandra-docker</a>. Elassandra images can be pulled from docker hub via <code>docker pull instaclustr/cassandra:elassandra-5.5.0.13</code>\nExamples can be found in the example directory. Elassandra does not support environment variable configuration.</p>",
        "created_at": "2018-08-03T00:24:59+0000",
        "updated_at": "2019-03-22T03:24:40+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/11550580?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11801"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11800,
        "uid": null,
        "title": "zegelin/cassandra-docker",
        "url": "https://github.com/zegelin/cassandra-docker",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Apache Cassandra and Elassandra docker images.</p>\n<h3><a id=\"user-content-building-docker-images\" class=\"anchor\" aria-hidden=\"true\" href=\"#building-docker-images\"></a>Building Docker Images</h3>\n<p>Run <code>build.sh</code> in the repository root. This first builds the <code>base-openjre</code> image,\nwhich is a Debian Strech image with OpenJRE 8 (headless) installed, and then\nbuilds the given distribution and version of cassandra.</p>\n<p>For Elassandra, invoke <code>build.sh</code>:</p>\n<p><code>./build.sh elassandra 5.5.0.13</code></p>\n<h3><a id=\"user-content-openshift\" class=\"anchor\" aria-hidden=\"true\" href=\"#openshift\"></a>Openshift</h3>\n<p>The following steps assume that an OpenShift project named <code>myproject</code> has already been created\nand that the OpenShift client (<code>oc</code>) has been configured to connect to the OpenShift cluster.</p>\n<p><em>Note</em>: The project name (k8s namespace) is used as part of DNS names specified in the Cassandra configuration files.\nIf you use an alternate project name, you'll need to replace all occurances of <code>myproject</code> with\nthe appropriate value.</p>\n<p>It is also assumed that the Docker images have already been built and pushed to the local\nOpenShift Docker registry. See <em>Building Docker Images</em> above.</p>\n<p>Create an <code>elassandra-config</code> ConfigMap containing the Cassandra/Elassandra configuration:</p>\n<p><code>oc create configmap elassandra-config --from-file=examples/elassandra/openshift/conf/</code></p>\n<p>Create an <code>elassandra</code> Service:</p>\n<p><code>oc apply -f examples/elassandra/openshift/elassandra-service.yaml</code></p>\n<p>Create the <code>elassandra</code> StatefulSet, which spawns 3 Elassandra nodes:</p>\n<p><code>oc create -f examples/elassandra/openshift/elassandra-statefulset.yaml</code></p>\n</article>",
        "created_at": "2018-08-03T00:24:41+0000",
        "updated_at": "2018-08-03T00:24:52+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/19296634?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11800"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 902,
            "label": "kubernetes",
            "slug": "kubernetes"
          }
        ],
        "is_public": false,
        "id": 11799,
        "uid": null,
        "title": "instaclustr/cassandra-operator",
        "url": "https://github.com/instaclustr/cassandra-operator",
        "content": "<h2>Cassandra operator</h2><p>Build <a href=\"https://circleci.com/gh/instaclustr/cassandra-operator/tree/master\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/e8818f6860bae206f9f5897efb222eb3b3d4e310/68747470733a2f2f636972636c6563692e636f6d2f67682f696e737461636c757374722f63617373616e6472612d6f70657261746f722f747265652f6d61737465722e7376673f7374796c653d737667\" alt=\"CircleCI\" data-canonical-src=\"https://circleci.com/gh/instaclustr/cassandra-operator/tree/master.svg?style=svg\" /></a></p><h3>Project status: pre-alpha</h3><p>Major planned features have yet to be completed and API changes are currently planned, we reserve the right to address bugs and API changes in a backwards incompatible way before the project is declared stable. See <a href=\"https://github.com/instaclustr/cassandra-operator/blob/master/doc/user/upgrade/upgrade_guide.md\">upgrade guide</a> for safe upgrade process.</p><p>Currently user facing Cassandra cluster objects are created as <a href=\"https://kubernetes.io/docs/tasks/access-kubernetes-api/extend-api-custom-resource-definitions/\" rel=\"nofollow\">Kubernetes Custom Resources</a>, however, taking advantage of <a href=\"https://github.com/kubernetes/community/blob/master/contributors/design-proposals/api-machinery/aggregated-api-servers.md\">User Aggregated API Servers</a> to improve reliability, validation and versioning may be undertaken. The use of Aggregated API should be minimally disruptive to existing users but may change what Kubernetes objects are created or how users deploy the  operator.</p><p>We expect to consider the Cassandra operator stable soon; backwards incompatible changes will not be made once the project reaches stability.</p><h2>Overview</h2><p>The Cassandra operator manages Cassandra clusters deployed to <a href=\"http://kubernetes.io\" rel=\"nofollow\">Kubernetes</a> and automates tasks related to operating an Cassandra cluster.</p><ul><li><a href=\"#deploy-cassandra-operator\">Install</a></li>\n<li><a href=\"#create-and-destroy-an-Cassandra-cluster\">Create and destroy</a></li>\n<li><a href=\"#resize-an-Cassandra-cluster\">Resize</a></li>\n<li><a href=\"#node-recovery\">Recover a node</a> - <em>TODO</em></li>\n<li><a href=\"#disaster-recovery\">Backup and restore a cluster</a> - <em>TODO</em></li>\n<li><a href=\"#upgrade-an-Cassandra-cluster\">Rolling upgrade</a> - <em>TODO</em></li>\n<li><a href=\"#limitations\">Limitations</a></li>\n</ul><p>There are <a href=\"https://github.com/instaclustr/cassandra-operator/blob/master/doc/spec_examples.md\">more spec examples</a> on setting up clusters with backup, restore, and other configurations.</p><p>Read <a href=\"https://github.com/instaclustr/cassandra-operator/blob/master/doc/best_practices.md\">Best Practices</a> for more information on how to better use Cassandra operator.</p><p>Read <a href=\"https://github.com/instaclustr/cassandra-operator/blob/master/doc/rbac.md\">RBAC docs</a> for how to setup RBAC rules for Cassandra operator if RBAC is in place.</p><p>Read <a href=\"https://github.com/instaclustr/cassandra-operator/blob/master/doc/developers.md\">Developer Guide</a> for setting up development environment if you want to contribute.</p><p>See the <a href=\"https://github.com/instaclustr/cassandra-operator/blob/master/doc/resources.md\">Resources and Labels</a> doc for an overview of the resources created by the Cassandra-operator.</p><h2>Requirements</h2><ul><li>Kubernetes 1.8+</li>\n<li>Cassandra 3.11+</li>\n</ul><h2>Deploy Cassandra operator</h2><p>See <a href=\"https://github.com/instaclustr/cassandra-operator/blob/master/doc/op_guide.md\">instructions on how to install/uninstall Cassandra operator</a> .</p><h2>Create and destroy an Cassandra cluster</h2><div class=\"highlight highlight-source-shell\"><pre>$ kubectl create -f example/common/test.yaml</pre></div><p>A 3 member Cassandra cluster will be created.</p><div class=\"highlight highlight-source-shell\"><pre>$ kubectl get pods\nNAME                                    READY     STATUS    RESTARTS   AGE\ncassandra-operator7-5d58bc7874-t85dt    1/1       Running   0          18h\ntest-dc-0                               1/1       Running   0          1m\ntest-dc-1                               1/1       Running   0          1m\ntest-dc-2                               1/1       Running   0          1m</pre></div><p>See <a href=\"https://github.com/instaclustr/cassandra-operator/blob/master/doc/user/client_service.md\">client service</a> for how to access Cassandra clusters created by operator.</p><p>Destroy Cassandra cluster:</p><div class=\"highlight highlight-source-shell\"><pre>$ kubectl delete -f example/common/test.yaml</pre></div><h2>Resize an Cassandra cluster</h2><p>Create an Cassandra cluster, if you haven't already:</p><div class=\"highlight highlight-source-shell\"><pre>$ kubectl apply -f example/common/test.yaml</pre></div><p>In <code>example/common/test.yaml</code> the initial cluster size is 3.\nModify the file and change <code>replicas</code> from 3 to 5.</p><div class=\"highlight highlight-source-yaml\"><pre>apiVersion: stable.instaclustr.com/v1\nkind: CassandraDataCenter\nmetadata:\n  name: test-dc\nspec:\n  replicas: 5\n  image: \"gcr.io/cassandra-operator/cassandra:latest\"</pre></div><p>Apply the size change to the cluster CR:</p><div class=\"highlight highlight-source-shell\"><pre>$ kubectl apply -f example/common/test.yaml</pre></div><p>The Cassandra cluster will scale to 5 members (5 pods):</p><div class=\"highlight highlight-source-shell\"><pre>$ kubectl get pods\nNAME                            READY     STATUS    RESTARTS   AGE\ntest-dc-0                       1/1       Running   0          1m\ntest-dc-1                       1/1       Running   0          1m\ntest-dc-2                       1/1       Running   0          1m\ntest-dc-3                       1/1       Running   0          1m\ntest-dc-4                       1/1       Running   0          1m</pre></div><p>Similarly we can decrease the size of cluster from 5 back to 3 by changing the size field again and reapplying the change.</p><div class=\"highlight highlight-source-yaml\"><pre>apiVersion: stable.instaclustr.com/v1\nkind: CassandraDataCenter\nmetadata:\n  name: test-dc\nspec:\n  replicas: 3\n  image: \"gcr.io/cassandra-operator/cassandra:latest\"</pre></div><p>Then apply the changes</p><div class=\"highlight highlight-source-shell\"><pre>$ kubectl apply -f example/common/test.yaml</pre></div><h2>Limitations</h2><ul><li>This operator is currently a work in progress and breaking changes are landing in master all the time until we reach our initial release. Here be dragons!</li>\n<li>The operator does not currently manage backups, a command line backup utility is included, tested and built, but not yet managed by the operator.</li>\n<li>Do not use this in production.</li>\n</ul>",
        "created_at": "2018-08-03T00:14:56+0000",
        "updated_at": "2018-09-13T15:09:11+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/11550580?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11799"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 16,
            "label": "starter.template",
            "slug": "starter.template"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          }
        ],
        "is_public": false,
        "id": 11798,
        "uid": null,
        "title": "instaclustr/sample-KafkaSparkCassandra",
        "url": "https://github.com/instaclustr/sample-KafkaSparkCassandra",
        "content": "<p>Introductory sample scala app using Apache Spark Streaming to accept data from Kafka and write a summary to Cassandra.</p><p>This sample has been built with the following versions:</p><ul><li>Scala 2.11.8</li>\n<li>Kafka 1.1</li>\n<li>Spark 2.1.1</li>\n<li>Spark Cassandra Connector 2.3.0</li>\n<li>Cassandra 3.11.2</li>\n</ul><p>For a detailed, step by step guide on setting up and running this sample see the tutorial: TBA</p>",
        "created_at": "2018-08-03T00:13:49+0000",
        "updated_at": "2019-03-22T03:24:28+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/11550580?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11798"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          },
          {
            "id": 1275,
            "label": "diagnostics",
            "slug": "diagnostics"
          }
        ],
        "is_public": false,
        "id": 11797,
        "uid": null,
        "title": "smartcat-labs/cassandra-diagnostics",
        "url": "https://github.com/smartcat-labs/cassandra-diagnostics",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Monitoring and audit power kit for Apache Cassandra.</p>\n<p><a href=\"https://travis-ci.org/smartcat-labs/cassandra-diagnostics\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/76a416d18123babf572d1379913c56645af9672f/68747470733a2f2f7472617669732d63692e6f72672f736d6172746361742d6c6162732f63617373616e6472612d646961676e6f73746963732e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/smartcat-labs/cassandra-diagnostics.svg?branch=master\" /></a>\n<a href=\"https://maven-badges.herokuapp.com/maven-central/io.smartcat/cassandra-diagnostics/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/b2f836e75fad32d5b5e32102a53017aae8c71a64/68747470733a2f2f6d6176656e2d6261646765732e6865726f6b756170702e636f6d2f6d6176656e2d63656e7472616c2f696f2e736d6172746361742f63617373616e6472612d646961676e6f73746963732f62616467652e737667\" alt=\"Maven Central\" data-canonical-src=\"https://maven-badges.herokuapp.com/maven-central/io.smartcat/cassandra-diagnostics/badge.svg\" /></a>\n<a href=\"https://bintray.com/smartcat-labs/maven/cassandra-diagnostics/_latestVersion\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c32ac0c09babdcfc567b7c56d8980e56ce5833f8/68747470733a2f2f6170692e62696e747261792e636f6d2f7061636b616765732f736d6172746361742d6c6162732f6d6176656e2f63617373616e6472612d646961676e6f73746963732f696d616765732f646f776e6c6f61642e737667\" alt=\"Download\" data-canonical-src=\"https://api.bintray.com/packages/smartcat-labs/maven/cassandra-diagnostics/images/download.svg\" /></a></p>\n<h2><a id=\"user-content-introduction\" class=\"anchor\" aria-hidden=\"true\" href=\"#introduction\"></a>Introduction</h2>\n<p>Cassandra Diagnostics is an extension for Apache Cassandra server node implemented as Java agent. It uses bytecode instrumentation to augment Cassandra node with additional functionalities. The following images depicts the position of Cassandra Diagnostics in a Apache Cassandra based system.</p>\n<p><a target=\"_blank\" href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/diagrams/cassandra-diagnostics.png?raw=true\"><img src=\"https://github.com/smartcat-labs/cassandra-diagnostics/raw/dev/diagrams/cassandra-diagnostics.png?raw=true\" alt=\"Placement diagram\" /></a></p>\n<p>Cassandra Diagnostics has a modular architecture. On one side it has connectors for different versions of Apache Cassandra nodes or Cassandra Java Driver and on the other it has various reporters to send measurement to different collecting/monitoring tools. In between lies the core with a set of metrics processing modules. Reusable code goes to commons.</p>\n<p><a target=\"_blank\" href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/diagrams/architecture-diagram.png?raw=true\"><img src=\"https://github.com/smartcat-labs/cassandra-diagnostics/raw/dev/diagrams/architecture-diagram.png?raw=true\" alt=\"Architecture diagram\" /></a></p>\n<h3><a id=\"user-content-cassandra-diagnostic-commons\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandra-diagnostic-commons\"></a>Cassandra Diagnostic Commons</h3>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-commons\">Cassandra Diagnostics Commons</a> holds interface for core, connector and reports and it provides signature all the modules need to confront to be able to work together.</p>\n<h3><a id=\"user-content-cassandra-connector\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandra-connector\"></a>Cassandra Connector</h3>\n<p>Connector is a module which hooks into the query path and extract information for diagnostics. Bytecode instrumentation is used to augment existing Cassandra code with additional functionality. It uses low priority threads to execute the diagnostics information extraction with minimal performance impact to the target code (Cassandra node or application/driver).</p>\n<p>Currently Cassandra Diagnostics implements the following connector implementation:</p>\n<ul><li>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-connector21\">Cassandra Connector 2.1</a> is a connector implementation for Cassandra node for Cassandra version 2.1.x.</p>\n</li>\n<li>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-connector30\">Cassandra Connector 3.0</a> is a connector implementation for Cassandra node for Cassandra version 3.0.x.</p>\n</li>\n<li>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-driver-connector\">Cassandra Driver Connector</a> is a connector implementation for Datastax's Cassandra driver for diagnostics on the application side.</p>\n</li>\n</ul><h3><a id=\"user-content-cassandra-core\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandra-core\"></a>Cassandra Core</h3>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core\">Cassandra Diagnostics Core</a> is glue between connector and reporters. It holds all the modules for diagnostics, it has business logic for measurement and it decides what will be measured and what would be skipped. Its job is to load provided configuration or to setup sensible defaults.</p>\n<h3><a id=\"user-content-modules\" class=\"anchor\" aria-hidden=\"true\" href=\"#modules\"></a>Modules</h3>\n<p>There are default module implementations which serve as core features. Modules use configured reporters to report their activity.</p>\n<p>Please read <a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md\">core modules README</a> for more information and configuraion options for the modules.Core module implementations:</p>\n<h4><a id=\"user-content-heartbeat-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#heartbeat-module\"></a>Heartbeat Module</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md#heartbeat-module\">Heartbeat Module</a> produces messages to provide feedback that the diagnostics agent is loaded and working. Typical usage is with Log Reporter where it produces INFO message in configured intervals.\nDefault reporting interval is 15 minutes.</p>\n<h4><a id=\"user-content-slow-query-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#slow-query-module\"></a>Slow Query Module</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md#slow-query-module\">Slow Query Module</a> is monitoring execution time of each query and if it is above configured threshold it reports the value and query type using configured reporters.\nDefault query execution time threshold is 25 milliseconds.</p>\n<h4><a id=\"user-content-request-rate-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#request-rate-module\"></a>Request Rate Module</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md#request-rate-module\">Request Rate Module</a> uses codahale metrics library to create rate measurement of executed queries. Rates are reported for configurable statement types and consistency levels using configured reporters in configured periods.\nDefault reporting interval is 1 second.</p>\n<h4><a id=\"user-content-metrics-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#metrics-module\"></a>Metrics Module</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md#metrics-module\">Metrics Module</a> collects Cassandra's metrics, which are exposed over JMX, and ships them using predefined reporters. Metrics package names configuration is the same as a default metrics config reporter uses.\nDefault reporting interval is 1 second.</p>\n<h4><a id=\"user-content-status-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#status-module\"></a>Status Module</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md#status-module\">Status Module</a> is used to report Cassandra information exposed over JMX. It reports compaction information as a single measurement.\nDefault reporting interval is 1 minute.</p>\n<h4><a id=\"user-content-cluster-health-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#cluster-health-module\"></a>Cluster Health Module</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/COREMODULES.md#cluster-health-module\">Cluster Health Module</a> is used to report the health status of the nodes such as which nodes are marked as DOWN by gossiper. It uses the information exposed over JMX.\nDefault reporting interval is 10 seconds.</p>\n<h4><a id=\"user-content-hiccup-module\" class=\"anchor\" aria-hidden=\"true\" href=\"#hiccup-module\"></a>Hiccup Module</h4>\n<p>Module based on <a href=\"https://github.com/giltene/jHiccup\">jHiccup</a> that logs and reports platform hiccups including JVM stalls. Default reporting period is 5 seconds and reporter values and percentiles from 90 to 100 and Mean and Max values.</p>\n<h3><a id=\"user-content-reporters\" class=\"anchor\" aria-hidden=\"true\" href=\"#reporters\"></a>Reporters</h3>\n<p>Reporters take measurement from core and wrap them up in implementation specific format so it can be sent to reporters target (i.e. Influx reporter transforms measurement to influx query and stores it to InfluxDB).</p>\n<p>Reporter implementations:</p>\n<h4><a id=\"user-content-log-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#log-reporter\"></a>Log Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/src/main/java/io/smartcat/cassandra/diagnostics/reporter/LogReporter.java\">LogReporter</a> uses the Cassandra logger system to report measurement (this is default reporter and part of core). Reports are logged at the <code>INFO</code> log level in the following pattern:</p>\n<pre>Measurement {} [time={}, value={}, tags={}, fields={}]\n</pre>\n<p>Values for <code>time</code> is given in milliseconds. <code>tags</code> are used to better specify measurement and provide additional searchable labels and fields is a placeholder for additional fields connected to this measurement. Example can be Slow Query measurement, where <code>value</code> is execution time of query, <code>tags</code> can be type of statement (UPDATE or SELECT) so you can differentiate and search easy and <code>fields</code> can hold actual statement, which is not something you want to search against but it is valuable metadata for measurement.</p>\n<h4><a id=\"user-content-riemann-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#riemann-reporter\"></a>Riemann Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-reporter-riemann/README.md\">RiemannReporter</a> sends measurements towards <a href=\"http://riemann.io/\" rel=\"nofollow\">Riemann server</a>.</p>\n<h4><a id=\"user-content-influx-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#influx-reporter\"></a>Influx Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-reporter-influx/README.md\">InfluxReporter</a> sends measurements towards <a href=\"https://www.influxdata.com/time-series-platform/influxdb/\" rel=\"nofollow\">Influx database</a>.</p>\n<h4><a id=\"user-content-telegraf-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#telegraf-reporter\"></a>Telegraf Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-reporter-telegraf/README.md\">Telegraf Reporter</a> sends measurements towards <a href=\"https://github.com/influxdata/telegraf\">Telegraf agent</a>.</p>\n<h4><a id=\"user-content-datadog-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#datadog-reporter\"></a>Datadog Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-reporter-datadog/README.md\">Datadog Reporter</a> sends measurements towards <a href=\"https://github.com/DataDog/dd-agent\">Datadog Agent</a> using UDP.</p>\n<h4><a id=\"user-content-kafka-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#kafka-reporter\"></a>Kafka Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-reporter-kafka/README.md\">Kafka Reporter</a> sends measurements towards <a href=\"https://kafka.apache.org/\" rel=\"nofollow\">Kafka</a>.</p>\n<h4><a id=\"user-content-prometheus-reporter\" class=\"anchor\" aria-hidden=\"true\" href=\"#prometheus-reporter\"></a>Prometheus Reporter</h4>\n<p><a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-reporter-prometheus/README.md\">Prometheus Reporter</a> exposes measurements to be scraped by <a href=\"https://prometheus.io\" rel=\"nofollow\">Prometheus server</a>.</p>\n<h2><a id=\"user-content-configuration\" class=\"anchor\" aria-hidden=\"true\" href=\"#configuration\"></a>Configuration</h2>\n<p>Cassandra Diagnostics uses an external configuration file in YAML format. You can see default configuration in <a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/cassandra-diagnostics-core/src/main/resources/cassandra-diagnostics-default.yml\">cassandra-diagnostics-default.yml</a>. The default name of the config file is <code>cassandra-diagnostics.yml</code> and it is expected to be found on the classpath. This can be changed using property <code>cassandra.diagnostics.config</code>.\nFor example, the configuration can be set explicitly by changing <code>cassandra-env.sh</code> and adding the following line:</p>\n<pre>JVM_OPTS=\"$JVM_OPTS -Dcassandra.diagnostics.config=some-other-cassandra-diagnostics-configuration.yml\"\n</pre>\n<p>The following is an example of the configuration file:</p>\n<pre>global:\n  systemName: \"smartcat-cassandra-cluster\"\nreporters:\n  - reporter: io.smartcat.cassandra.diagnostics.reporter.LogReporter\nmodules:\n  - module: io.smartcat.cassandra.diagnostics.module.slowquery.SlowQueryModule\n    measurement: queryReport\n    options:\n      slowQueryThresholdInMilliseconds: 1\n    reporters:\n      - io.smartcat.cassandra.diagnostics.reporter.LogReporter\n</pre>\n<p>Specific query reporter may require additional configuration options. Those options are specified using <code>options</code> property. The following example shows a configuration options in case of <code>RiemannReporter</code> and it shows how you can configure specific modules to use this reporter:</p>\n<pre>global:\n  systemName: \"smartcat-cassandra-cluster\"\n# Reporters\nreporters:\n  - reporter: io.smartcat.cassandra.diagnostics.reporter.LogReporter\n  - reporter: io.smartcat.cassandra.diagnostics.reporter.RiemannReporter\n    options:\n      riemannHost: 127.0.0.1\n      riemannPort: 5555 #Optional\n      batchEventSize: 50 #Optional\n# Modules\nmodules:\n  - module: io.smartcat.cassandra.diagnostics.module.requestrate.RequestRateModule\n    measurement: requestRate\n    options:\n      period: 1\n      timeunit: SECONDS\n    reporters:\n      - io.smartcat.cassandra.diagnostics.reporter.LogReporter\n      - io.smartcat.cassandra.diagnostics.reporter.RiemannReporter\n</pre>\n<p>By default all measurements are reported with hostname queried with <a href=\"http://docs.oracle.com/javase/7/docs/api/java/net/InetAddress.html\" rel=\"nofollow\">InetAddress</a> java class. If required, hostname can be set using a hostname variable in configuration file:</p>\n<pre>global:\n  systemName: \"smartcat-cassandra-cluster\"\n  hostname: \"test-hostname\"\nreporters:\netc...\n</pre>\n<p>It is important to name system under observation because measurements can be collected by various systems. Hostname is not enough, it is easy to imagine one host having Cassandra node and Kafka node both emitting measurement and we want to group those by system. By default \"cassandra-cluster\" will be used but it is advised to override this to have unique grouping of measurements:</p>\n<pre>global:\n  systemName: \"cassandra-cluster\"\n  hostname: \"test-hostname\"\n</pre>\n<h2><a id=\"user-content-information-provider\" class=\"anchor\" aria-hidden=\"true\" href=\"#information-provider\"></a>Information provider</h2>\n<p>Being deployed on the node itself, diagnostics connector should provide a connection to the node over JMX by wrapping the Cassandra's NodeProbe class with provides access to all actions and metrics exposed over JMX. This is configured in the <code>connector</code> part of the configuration which sits in the root of diagnostics config.</p>\n<pre>connector:\n  jmxHost: 127.0.0.1\n  jmxPort: 7199\n  jmxAuthEnabled: false #Optional\n  jmxUsername: username #Optional\n  jmxPassword: password #Optional\n</pre>\n<p>Status module uses information provided by connector in order to collect info data.</p>\n<h2><a id=\"user-content-control-and-configuration-api\" class=\"anchor\" aria-hidden=\"true\" href=\"#control-and-configuration-api\"></a>Control and Configuration API</h2>\n<p>Cassandra Diagnostics exposes a control and configuration API. This API currently offers the following operations:</p>\n<pre>- getVersion - returns the actual Cassandra Diagnostics version.\n- reload - reloads the configuration\n</pre>\n<p>This API is exposed over JMX and HTTP protocols.</p>\n<p>The Diagnostics API JMX MXBean could be found under the following object name:</p>\n<pre>package io.smartcat.cassandra.diagnostics.api:type=DiagnosticsApi\n</pre>\n<p>The HTTP API is controlled using the following options in the <code>global</code> section in the configuration file:</p>\n<pre>global:\n  # controls if HTTP API is turned on. 'true' by default.\n  httpApiEnabled: true\n  # specifies the host/address part for listening TCP socket. '127.0.0.1' by default.\n  httpApiHost: 127.0.0.1\n  # specifies the port number for the listening TCP socket. '8998' by default.\n  httpApiPort: 8998\n  # if API authorization is enabled, API key must be provided through the 'Authorization' header\n  httpApiAuthEnabled: false\n  # API access key\n  httpApiKey: \"diagnostics-api-key\"\n</pre>\n<p>It implements the following endpoints for mapping HTTP requests to API operations:</p>\n<ul><li><code>GET /version</code> for <code>getVersion</code></li>\n<li><code>POST /reload</code> for <code>reload</code></li>\n</ul><h2><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\" href=\"#installation\"></a>Installation</h2>\n<p>Script for automated installation is <a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/bash-installer/README.md\">also available</a>.</p>\n<p>Cassandra Diagnostics consists of the following three components:</p>\n<ul><li>Cassandra Diagnostics Core</li>\n<li>Cassandra Diagnostics Connector</li>\n<li>Cassandra Diagnostics Reporter</li>\n</ul><p>Every of these components is packaged into its own JAR file (accompanied with necessary dependencies). These JAR files are available for download on <a href=\"https://mvnrepository.com/artifact/io.smartcat\" rel=\"nofollow\">Maven Central</a> and need to be present on the classpath.</p>\n<p>Pay attention to the fact that Cassandra Diagnostics Connector has to be aligned with the used Cassandra version. For example, <code>cassandra-diagnostics-connector21</code> should be used with Cassandra 2.1.</p>\n<p>Also note that more than one Cassandra Diagnostics Reporter can be used at the same time. That means that all respective JAR files have to be put on the classpath. The only exception to this rule is in case of <code>LogReporter</code> that is built in Cassandra Diagnostics Core and no Reporter JAR has to be added explicitly.</p>\n<p>Place <code>cassandra-diagnostics-core-VERSION.jar</code>, <code>cassandra-diagnostics-connector21-VERSION.jar</code> and required Reporter JARs (e.g. <code>cassandra-diagnostics-reporter-influx-VERSION-all.jar</code>) into Cassandra <code>lib</code> directory.</p>\n<p>Create and place the configuration file <code>cassandra-diagnostics.yml</code> into Cassandra's <code>conf</code> directory.\nAdd the following line at the end of <code>conf/cassandra-env.sh</code>:</p>\n<pre>JVM_OPTS=\"$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/cassandra-diagnostics-core-VERSION.jar -Dcassandra.diagnostics.config=cassandra-diagnostics.yml\"\n</pre>\n<h2><a id=\"user-content-usage\" class=\"anchor\" aria-hidden=\"true\" href=\"#usage\"></a>Usage</h2>\n<p>Upon Cassandra node start, the Diagnostics agent kicks in and instrument necessary target classes to inject diagnostics additions.\n<code>LogReporter</code> repors slow queries in <code>logs/system.log</code> at <code>INFO</code> level.\nThe dynamic configuration could be inspected/changed using <code>jconsole</code> and connecting to <code>org.apache.cassandra.service.CassandraDaemon</code>.</p>\n<h2><a id=\"user-content-build-and-deploy\" class=\"anchor\" aria-hidden=\"true\" href=\"#build-and-deploy\"></a>Build and deploy</h2>\n<p>Build and deploy process is described <a href=\"https://github.com/smartcat-labs/cassandra-diagnostics/blob/dev/BUILDANDDEPLOY.md\">here</a>.</p>\n<h2><a id=\"user-content-license-and-development\" class=\"anchor\" aria-hidden=\"true\" href=\"#license-and-development\"></a>License and development</h2>\n<p>Cassandra Diagnostics is licensed under the liberal and business-friendly <a href=\"http://www.apache.org/licenses/LICENSE-2.0.html\" rel=\"nofollow\">Apache Licence, Version 2.0</a> and is freely available on GitHub. Cassandra Diagnostics is further released to the repositories of Maven Central and on JCenter. The project is built using <a href=\"http://maven.apache.org/\" rel=\"nofollow\">Maven</a>. From your shell, cloning and building the project would go something like this:</p>\n<pre>git clone https://github.com/smartcat-labs/cassandra-diagnostics.git\ncd cassandra-diagnostics\nmvn package\n</pre>\n</article>",
        "created_at": "2018-08-03T00:09:56+0000",
        "updated_at": "2018-08-03T00:10:02+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 8,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/12434092?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11797"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 956,
            "label": "streaming",
            "slug": "streaming"
          }
        ],
        "is_public": false,
        "id": 11794,
        "uid": null,
        "title": "Yannael/kafka-sparkstreaming-cassandra",
        "url": "https://github.com/Yannael/kafka-sparkstreaming-cassandra",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>This Dockerfile sets up a complete streaming environment for experimenting with Kafka, Spark streaming (PySpark), and Cassandra. It installs</p>\n<ul><li>Kafka 0.10.2.1</li>\n<li>Spark 2.1.1 for Scala 2.11</li>\n<li>Cassandra 3.7</li>\n</ul><p>It additionnally installs</p>\n<ul><li>Anaconda distribution 4.4.0 for Python 2.7.10</li>\n<li>Jupyter notebook for Python</li>\n</ul>\n<p>Run container using <a href=\"https://hub.docker.com/r/yannael/kafka-sparkstreaming-cassandra\" rel=\"nofollow\">DockerHub image</a></p>\n<pre>docker run -p 4040:4040 -p 8888:8888 -p 23:22 -ti --privileged yannael/kafka-sparkstreaming-cassandra\n</pre>\n<p>See following video for usage demo.\n<br /><a href=\"https://www.youtube.com/watch?v=XxCFo7BzNQ8\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/61da6654e141eb080dce4055dbd84d7c47231037/687474703a2f2f696d672e796f75747562652e636f6d2f76692f587843466f37427a4e51382f302e6a7067\" alt=\"Demo\" width=\"480\" height=\"360\" border=\"10\" data-canonical-src=\"http://img.youtube.com/vi/XxCFo7BzNQ8/0.jpg\" style=\"text-align: center;\" /></a></p>\n<p>Note that any changes you make in the notebook will be lost once you exit de container. In order to keep the changes, it is necessary put your notebooks in a folder on your host, that you share with the container, using for example</p>\n<pre>docker run -v `pwd`:/home/guest/host -p 4040:4040 -p 8888:8888 -p 23:22 -ti --privileged yannael/kafka-sparkstreaming-cassandra\n</pre>\n<p>Note:</p>\n<ul><li>The \"-v <code>pwd</code>:/home/guest/host\" shares the local folder (i.e. folder containing Dockerfile, ipynb files, etc...) on your computer - the 'host') with the container in the '/home/guest/host' folder.</li>\n<li>Port are shared as follows:\n<ul><li>4040 bridges to Spark UI</li>\n<li>8888 bridges to the Jupyter Notebook</li>\n<li>23 bridges to SSH</li>\n</ul></li>\n</ul><p>SSH allows to get a onnection to the container</p>\n<pre>ssh -p 23 guest@containerIP\n</pre>\n<p>where 'containerIP' is the IP of th container (127.0.0.1 on Linux). Password is 'guest'.</p>\n<h3><a id=\"user-content-start-services\" class=\"anchor\" aria-hidden=\"true\" href=\"#start-services\"></a>Start services</h3>\n<p>Once run, you are logged in as root in the container. Run the startup_script.sh (in /usr/bin) to start</p>\n<ul><li>SSH server. You can connect to the container using user 'guest' and password 'guest'</li>\n<li>Cassandra</li>\n<li>Zookeeper server</li>\n<li>Kafka server</li>\n</ul><pre>startup_script.sh\n</pre>\n<h3><a id=\"user-content-connect-create-cassandra-table-open-notebook-and-start-streaming\" class=\"anchor\" aria-hidden=\"true\" href=\"#connect-create-cassandra-table-open-notebook-and-start-streaming\"></a>Connect, create Cassandra table, open notebook and start streaming</h3>\n<p>Connect as user 'guest' and go to 'host' folder (shared with the host)</p>\n<pre>su guest\n</pre>\n<p>Start Jupyter notebook</p>\n<pre>notebook\n</pre>\n<p>and connect from your browser at port host:8888 (where 'host' is the IP for your host. If run locally on your computer, this should be 127.0.0.1 or 192.168.99.100, check Docker documentation)</p>\n<h4><a id=\"user-content-start-kafka-producer\" class=\"anchor\" aria-hidden=\"true\" href=\"#start-kafka-producer\"></a>Start Kafka producer</h4>\n<p>Open kafkaSendDataPy.ipynb and run all cells.</p>\n<h4><a id=\"user-content-start-kafka-receiver\" class=\"anchor\" aria-hidden=\"true\" href=\"#start-kafka-receiver\"></a>Start Kafka receiver</h4>\n<p>Open kafkaReceiveAndSaveToCassandraPy.ipynb and run cells up to start streaming. Check in subsequent cells that Cassandra collects data properly.</p>\n<h4><a id=\"user-content-connect-to-spark-ui\" class=\"anchor\" aria-hidden=\"true\" href=\"#connect-to-spark-ui\"></a>Connect to Spark UI</h4>\n<p>It is available in your browser at port 4040</p>\n<p>The container is based on CentOS 6 Linux distribution. The main steps of the building process are</p>\n<ul><li>Install some common Linux tools (wget, unzip, tar, ssh tools, ...), and Java (1.8)</li>\n<li>Create a guest user (UID important for sharing folders with host!, see below), and install Spark and sbt, Kafka, Anaconda and Jupyter notbooks for the guest user</li>\n<li>Go back to root user, and install startup script (for starting SSH and Cassandra services), sentenv.sh script to set up environment variables (JAVA, Kafka, Spark, ...), spark-default.conf, and Cassandra</li>\n</ul><h3><a id=\"user-content-user-uid\" class=\"anchor\" aria-hidden=\"true\" href=\"#user-uid\"></a>User UID</h3>\n<p>In the Dockerfile, the line</p>\n<pre>RUN useradd guest -u 1000\n</pre>\n<p>creates the user under which the container will be run as a guest user. The username is 'guest', with password 'guest', and the '-u' parameter sets the linux UID for that user.</p>\n<p>In order to make sharing of folders easier between the container and your host, <strong>make sure this UID matches your user UID on the host</strong>. You can see what your host UID is with</p>\n<pre>echo $UID\n</pre>\n<h3><a id=\"user-content-clone-this-repository\" class=\"anchor\" aria-hidden=\"true\" href=\"#clone-this-repository\"></a>Clone this repository</h3>\n<pre>git clone https://github.com/Yannael/kafka-sparkstreaming-cassandra\n</pre>\n<h3><a id=\"user-content-build\" class=\"anchor\" aria-hidden=\"true\" href=\"#build\"></a>Build</h3>\n<p>From Dockerfile folder, run</p>\n<pre>docker build -t kafka-sparkstreaming-cassandra .\n</pre>\n<p>It may take about 30 minutes to complete.</p>\n<h3><a id=\"user-content-run\" class=\"anchor\" aria-hidden=\"true\" href=\"#run\"></a>Run</h3>\n<pre>docker run -v `pwd`:/home/guest/host -p 4040:4040 -p 8888:8888 -p 23:22 -ti --privileged kafka-sparkstreaming-cassandra\n</pre>\n</article>",
        "created_at": "2018-08-03T00:07:38+0000",
        "updated_at": "2018-08-03T00:07:53+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/976414?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11794"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1269,
            "label": "hdfs",
            "slug": "hdfs"
          }
        ],
        "is_public": false,
        "id": 11776,
        "uid": null,
        "title": "tuplejump/snackfs-release",
        "url": "https://github.com/tuplejump/snackfs-release",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\"><p><a href=\"http://tuplejump.github.io/calliope/snackfs.html\" rel=\"nofollow\">SnackFS @ Calliope</a></p>\n<p>SnackFS is our bite-sized, lightweight HDFS compatible FileSystem built over Cassandra.\nWith it's unique fat driver design it requires no additional SysOps or setup on the Cassanndra Cluster. All you have to do is point to your Cassandra cluster and you are ready to go.</p>\n<p>As SnackFS was written as a dropin replacement for HDFS, your existing HDFS backed applications not only run as-is on SnackFS, but they also run faster!\nSnackFS cluster is also more resilient than a HDFS cluster as there is no SPOF like the NameNode.</p>\n<h2><a id=\"user-content-prerequisites\" class=\"anchor\" aria-hidden=\"true\" href=\"#prerequisites\"></a>Prerequisites</h2>\n<ol><li>\n<p>SBT : It can be set up from the instructions <a href=\"http://www.scala-sbt.org/release/docs/Getting-Started/Setup.html#installing-sbt\" rel=\"nofollow\">here</a>.</p>\n</li>\n<li>\n<p>Cassandra(v1.2.12) : Instructions can be found <a href=\"http://wiki.apache.org/cassandra/GettingStarted\" rel=\"nofollow\">here</a>. An easier alternative would be using <a href=\"https://github.com/pcmanus/ccm\">CCM</a></p>\n</li>\n</ol><h2><a id=\"user-content-using-snackfs\" class=\"anchor\" aria-hidden=\"true\" href=\"#using-snackfs\"></a>Using SnackFS</h2>\n<h3><a id=\"user-content-use-the-binary\" class=\"anchor\" aria-hidden=\"true\" href=\"#use-the-binary\"></a>Use the binary</h3>\n<ul><li>\n<p>You can download the SnackFS distribution built with <a href=\"http://bit.ly/1eKV1ae\" rel=\"nofollow\">Scala 2.9.x here</a> and <a href=\"http://bit.ly/1jI7vVw\" rel=\"nofollow\">Scala 2.10.x here</a></p>\n</li>\n<li>\n<p>To add SnackFS to your SBT project use,</p>\n</li>\n</ul><p>For SBT</p>\n<div class=\"highlight highlight-source-scala\"><pre>\"com.tuplejump\" %% \"snackfs\" % \"0.6.1-EA\"</pre></div>\n<ul><li>To add SnackFS to your Maven project use,\nwith Scala 2.9.3 use,</li>\n</ul><div class=\"highlight highlight-text-xml\"><pre>&lt;dependency&gt;\n  &lt;groupId&gt;com.tuplejump&lt;/groupId&gt;\n  &lt;artifactId&gt;snackfs_2.9.3&lt;/artifactId&gt;\n  &lt;version&gt;0.6.1-EA&lt;/version&gt;\n&lt;/dependency&gt;</pre></div>\n<p>And with Scala 2.10.3,</p>\n<div class=\"highlight highlight-text-xml\"><pre>&lt;dependency&gt;\n  &lt;groupId&gt;com.tuplejump&lt;/groupId&gt;\n  &lt;artifactId&gt;snackfs_2.10&lt;/artifactId&gt;\n  &lt;version&gt;0.6.1-EA&lt;/version&gt;\n&lt;/dependency&gt;</pre></div>\n<h3><a id=\"user-content-build-from-source\" class=\"anchor\" aria-hidden=\"true\" href=\"#build-from-source\"></a>Build from Source</h3>\n<ol><li>\n<p>Checkout the source from <a href=\"http://github.com/tuplejump/snackfs\">http://github.com/tuplejump/snackfs</a></p>\n</li>\n<li>\n<p>To build SnackFS distribution run sbt's dist command in the project directory</p>\n</li>\n</ol><pre>[snackfs]$ sbt dist\n</pre>\n<p>This will result in a \"snackfs-{version}.tgz\" file in the \"target\" directory of \"snackfs\".\nExtract \"snackfs-{version}.tgz\" to the desired location.</p>\n<ol start=\"3\"><li>\n<p>Start Cassandra (default setup for snackfs assumes its a cluster with 3 nodes)</p>\n</li>\n<li>\n<p>It is possible to configure the file system by updating core-site.xml.\nThe following properties can be added.</p>\n<ul><li>snackfs.cassandra.host (default 127.0.0.1)</li>\n<li>snackfs.cassandra.port (default 9160)</li>\n<li>snackfs.consistencyLevel.write (default QUORUM)</li>\n<li>snackfs.consistencyLevel.read (default QUORUM)</li>\n<li>snackfs.keyspace (default snackfs)</li>\n<li>snackfs.subblock.size (default 8 MB (8 * 1024 * 1024))</li>\n<li>snackfs.block.size (default 128 MB (128 * 1024 * 1024))</li>\n<li>snackfs.replicationFactor (default 3)</li>\n<li>snackfs.replicationStrategy (default org.apache.cassandra.locator.SimpleStrategy)</li>\n</ul></li>\n<li>\n<p>SnackFS Shell provides the fs commands similar to Hadoop Shell. For example to create a directory,</p>\n</li>\n</ol><pre>[Snackfs(extracted)]$bin/snackfs -mkdir snackfs:///random\n</pre>\n<p>###To build and use with Hadoop</p>\n<ol><li>\n<p>Setup Apache Hadoop v1.0.4.(<a href=\"http://hadoop.apache.org/#Getting+Started\" rel=\"nofollow\">http://hadoop.apache.org/#Getting+Started</a>). The base directory will be referred as 'hadoop-1.0.4' in the following steps.</p>\n</li>\n<li>\n<p>Execute the following commands in the snackfs project directory.</p>\n</li>\n</ol><pre>[snackfs]$ sbt package\n</pre>\n<p>This will result in a \"snackfs_&lt;scala_version&gt;-&lt;version&gt;.jar\" file in the \"target/scala-&lt;scala_version&gt;\" directory of \"snackfs\".\nCopy the jar to 'hadoop-1.0.4/lib'.</p>\n<ol start=\"3\"><li>\n<p>Copy all the jars in snackfs/lib_managed and scala-library-&lt;scala_version&gt;.jar\n(located at '~/.ivy2/cache/org.scala-lang/scala-library/jars') to 'hadoop-1.0.4/lib'.</p>\n</li>\n<li>\n<p>Copy snackfs/src/main/resources/core-site.xml to 'hadoop-1.0.4/conf'</p>\n</li>\n<li>\n<p>Start Cassandra (default setup for snackfs assumes its a cluster with 3 nodes)</p>\n</li>\n<li>\n<p>Hadoop fs commands can now be run using snackfs. For example,</p>\n</li>\n</ol><pre>[hadoop-1.0.4]$ bin/hadoop fs -mkdir snackfs:///random\n</pre>\n<p>###To configure logging,</p>\n<h4><a id=\"user-content-in-system-environment\" class=\"anchor\" aria-hidden=\"true\" href=\"#in-system-environment\"></a>In System Environment</h4>\n<p>Set SNACKFS_LOG_LEVEL in the Shell to one of the following Values</p>\n<ul><li>DEBUG</li>\n<li>INFO</li>\n<li>ERROR</li>\n<li>ALL</li>\n<li>OFF</li>\n</ul><p>Default value if not set if ERROR</p>\n<p>####In code (for further control/tuning)\nIf you want your logs in a File, update LogConfiguration.scala like below</p>\n<div class=\"highlight highlight-source-scala\"><pre>val config = new LoggerFactory(\"\", Option(Level.ALL), List(FileHandler(\"logs\")), true)</pre></div>\n<p>The arguments for LoggerFactory are</p>\n<ol><li>node - Name of the logging node. (\"\") is the top-level logger.</li>\n<li>level - Log level for this node. Leaving it None implies the parent logger's level.</li>\n<li>handlers - Where to send log messages.</li>\n<li>useParents - indicates if log messages are passed up to parent nodes.To stop at this node level, set it to false</li>\n</ol><p>Additional logging configuration details can be found <a href=\"https://github.com/twitter/util/tree/master/util-logging#configuring\">here</a></p>\n</article>",
        "created_at": "2018-08-02T23:27:09+0000",
        "updated_at": "2018-08-02T23:27:17+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/3493976?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11776"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 1041,
            "label": "smack",
            "slug": "smack"
          }
        ],
        "is_public": false,
        "id": 11770,
        "uid": null,
        "title": "killrweather/killrweather",
        "url": "https://github.com/killrweather/killrweather",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>KillrWeather is a reference application (which we are constantly improving) showing how to easily leverage and integrate <a href=\"http://spark.apache.org\" rel=\"nofollow\">Apache Spark</a>,\n<a href=\"http://cassandra.apache.org\" rel=\"nofollow\">Apache Cassandra</a>, and <a href=\"http://kafka.apache.org\" rel=\"nofollow\">Apache Kafka</a> for fast, streaming computations in asynchronous <a href=\"http://akka.io\" rel=\"nofollow\">Akka</a> event-driven environments. This application focuses on the use case of  <strong><a href=\"https://github.com/killrweather/killrweather/wiki/4.-Time-Series-Data-Model\">time series data</a></strong>.</p>\n<h2><a id=\"user-content-sample-use-case\" class=\"anchor\" aria-hidden=\"true\" href=\"#sample-use-case\"></a>Sample Use Case</h2>\n<p>I need fast access to historical data  on the fly for  predictive modeling  with real time data from the stream.</p>\n<h2><a id=\"user-content-basic-samples\" class=\"anchor\" aria-hidden=\"true\" href=\"#basic-samples\"></a>Basic Samples</h2>\n<p><a href=\"https://github.com/killrweather/killrweather/tree/master/killrweather-examples/src/main/scala/com/datastax/killrweather\">Basic Spark, Kafka, Cassandra Samples</a></p>\n<h2><a id=\"user-content-reference-application\" class=\"anchor\" aria-hidden=\"true\" href=\"#reference-application\"></a>Reference Application</h2>\n<p><a href=\"https://github.com/killrweather/killrweather/tree/master/killrweather-app/src/main/scala/com/datastax/killrweather\">KillrWeather Main App</a></p>\n<h2><a id=\"user-content-time-series-data\" class=\"anchor\" aria-hidden=\"true\" href=\"#time-series-data\"></a>Time Series Data</h2>\n<p>The use of time series data for business analysis is not new. What is new is the ability to collect and analyze massive volumes of data in sequence at extremely high velocity to get the clearest picture to predict and forecast future market changes, user behavior, environmental conditions, resource consumption, health trends and much, much more.</p>\n<p>Apache Cassandra is a NoSQL database platform particularly suited for these types of Big Data challenges. Cassandra’s data model is an excellent fit for handling data in sequence regardless of data type or size. When writing data to Cassandra, data is sorted and written sequentially to disk. When retrieving data by row key and then by range, you get a fast and efficient access pattern due to minimal disk seeks – time series data is an excellent fit for this type of pattern. Apache Cassandra allows businesses to identify meaningful characteristics in their time series data as fast as possible to make clear decisions about expected future outcomes.</p>\n<p>There are many flavors of time series data. Some can be windowed in the stream, others can not be windowed in the stream because queries are not by time slice but by specific year,month,day,hour. Spark Streaming lets you do both.</p>\n<h2><a id=\"user-content-start-here\" class=\"anchor\" aria-hidden=\"true\" href=\"#start-here\"></a>Start Here</h2>\n<h3><a id=\"user-content-clone-the-repo\" class=\"anchor\" aria-hidden=\"true\" href=\"#clone-the-repo\"></a>Clone the repo</h3>\n<pre>git clone https://github.com/killrweather/killrweather.git\ncd killrweather\n</pre>\n<h3><a id=\"user-content-build-the-code\" class=\"anchor\" aria-hidden=\"true\" href=\"#build-the-code\"></a>Build the code</h3>\n<p>If this is your first time running SBT, you will be downloading the internet.</p>\n<pre>cd killrweather\nsbt compile\n# For IntelliJ users, this creates Intellij project files, but as of\n# version 14x you should not need this, just import a new sbt project.\nsbt gen-idea\n</pre>\n<h3><a id=\"user-content-setup-for-linux--mac---3-steps\" class=\"anchor\" aria-hidden=\"true\" href=\"#setup-for-linux--mac---3-steps\"></a>Setup (for Linux &amp; Mac) - 3 Steps</h3>\n<p>1.<a href=\"http://cassandra.apache.org/download/\" rel=\"nofollow\">Download the latest Cassandra</a> and open the compressed file.</p>\n<p>2.Start Cassandra - you may need to prepend with sudo, or chown /var/lib/cassandra. On the command line:</p>\n<pre>./apache-cassandra-{version}/bin/cassandra -f\n</pre>\n<p>3.Run the setup cql scripts to create the schema and populate the weather stations table.\nOn the command line start a cqlsh shell:</p>\n<pre>cd /path/to/killrweather/data\npath/to/apache-cassandra-{version}/bin/cqlsh\n</pre>\n<h3><a id=\"user-content-setup-for-windows---3-steps\" class=\"anchor\" aria-hidden=\"true\" href=\"#setup-for-windows---3-steps\"></a>Setup (for Windows) - 3 Steps</h3>\n<ol><li>\n<p><a href=\"http://www.planetcassandra.org/cassandra\" rel=\"nofollow\">Download the latest Cassandra</a> and double click the installer.</p>\n</li>\n<li>\n<p>Chose to run the Cassandra automatically during start-up</p>\n</li>\n<li>\n<p>Run the setup cql scripts to create the schema and populate the weather stations table.\nOn the command line start a <code>cqlsh</code> shell:</p>\n</li>\n</ol><pre>    cd c:/path/to/killrweather\n    c:/pat/to/cassandara/bin/cqlsh\n</pre>\n<h3><a id=\"user-content-in-cql-shell\" class=\"anchor\" aria-hidden=\"true\" href=\"#in-cql-shell\"></a>In CQL Shell:</h3>\n<p>You should see:</p>\n<pre> Connected to Test Cluster at 127.0.0.1:9042.\n [cqlsh {latest.version} | Cassandra {latest.version} | CQL spec {latest.version} | Native protocol {latest.version}]\n Use HELP for help.\n cqlsh&gt;\n</pre>\n<p>Run the scripts, then keep the cql shell open querying once the apps are running:</p>\n<pre> cqlsh&gt; source 'create-timeseries.cql';\n cqlsh&gt; source 'load-timeseries.cql';\n</pre>\n<h3><a id=\"user-content-run\" class=\"anchor\" aria-hidden=\"true\" href=\"#run\"></a>Run</h3>\n<h4><a id=\"user-content-logging\" class=\"anchor\" aria-hidden=\"true\" href=\"#logging\"></a>Logging</h4>\n<p>You will see this in all 3 app shells because log4j has been explicitly taken off the classpath:</p>\n<pre>log4j:WARN No appenders could be found for logger (kafka.utils.VerifiableProperties).\nlog4j:WARN Please initialize the log4j system properly.\n</pre>\n<p>What we are really trying to isolate here is what is happening in the apps with regard to the event stream.\nYou can add log4j locally.</p>\n<p>To change any package log levels and see more activity, simply modify</p>\n<h4><a id=\"user-content-from-command-line\" class=\"anchor\" aria-hidden=\"true\" href=\"#from-command-line\"></a>From Command Line</h4>\n<p>1.Start <code>KillrWeather</code></p>\n<pre>cd /path/to/killrweather\nsbt app/run\n</pre>\n<p>As the <code>KillrWeather</code> app initializes, you will see Akka Cluster start, Zookeeper and the Kafka servers start.</p>\n<p>For all three apps in load-time you see the Akka Cluster node join and start metrics collection. In deployment with multiple nodes of each app\nthis would leverage the health of each node for load balancing as the rest of the cluster nodes join the cluster:</p>\n<p>2.Start the Kafka data feed app\nIn a second shell run:</p>\n<pre>sbt clients/run\n</pre>\n<p>You should see:</p>\n<pre>Multiple main classes detected, select one to run:\n[1] com.datastax.killrweather.KafkaDataIngestionApp\n[2] com.datastax.killrweather.KillrWeatherClientApp\n</pre>\n<p>Select <code>KafkaDataIngestionApp</code>, and watch the shells for activity. You can stop the data feed or let it keep running.\nAfter a few seconds you should see data by entering this in the cqlsh shell:</p>\n<pre>cqlsh&gt; select * from isd_weather_data.raw_weather_data;\n</pre>\n<p>This confirms that data from the ingestion app has published to Kafka, and that raw data is\nstreaming from Spark to Cassandra from the <code>KillrWeatherApp</code>.</p>\n<pre>cqlsh&gt; select * from isd_weather_data.daily_aggregate_precip;\n</pre>\n<p>Unfortunately the precips are mostly 0 in the samples (To Do).</p>\n<p>3.Open a third shell and again enter this but select <code>KillrWeatherClientApp</code>:</p>\n<pre>sbt clients/run\n</pre>\n<p>This api client runs queries against the raw and the aggregated data from the kafka stream.\nIt sends requests (for varying locations and dates/times) and for some, triggers further aggregations\nin compute time which are also saved to Cassandra:</p>\n<ul><li>current weather</li>\n<li>daily temperatures</li>\n<li>monthly temperatures</li>\n<li>monthly highs and low temperatures</li>\n<li>daily precipitations</li>\n<li>top-k precipitation</li>\n</ul><p>Next I will add some forecasting with ML :)</p>\n<p>Watch the app and client activity in request response of weather data and aggregation data.\nBecause the querying of the API triggers even further aggregation of data from the originally\naggregated daily roll ups, you can now see a new tier of temperature and precipitation aggregation:\nIn the cql shell:</p>\n<pre>cqlsh&gt; select * from isd_weather_data.daily_aggregate_temperature;\ncqlsh&gt; select * from isd_weather_data.daily_aggregate_precip;\n</pre>\n<h4><a id=\"user-content-from-an-ide\" class=\"anchor\" aria-hidden=\"true\" href=\"#from-an-ide\"></a>From an IDE</h4>\n<ol><li>Run the app <a href=\"https://github.com/killrweather/killrweather/blob/master/killrweather-app/src/main/scala/com/datastax/killrweather/KillrWeatherApp.scala\">com.datastax.killrweather.KillrWeatherApp</a></li>\n<li>Run the kafka data ingestion server <a href=\"https://github.com/killrweather/killrweather/blob/master/killrweather-clients/src/main/scala/com/datastax/killrweather/KafkaDataIngestionApp.scala\">com.datastax.killrweather.KafkaDataIngestionApp</a></li>\n<li>Run the API client <a href=\"https://github.com/killrweather/killrweather/blob/master/killrweather-clients/src/main/scala/com/datastax/killrweather/KillrWeatherClientApp.scala\">com.datastax.killrweather.KillrWeatherClientApp</a></li>\n</ol><p>To close the cql shell:</p>\n<pre>cqlsh&gt; quit;\n</pre>\n</article>",
        "created_at": "2018-08-02T01:51:28+0000",
        "updated_at": "2018-08-02T01:51:43+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/9054253?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11770"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 1041,
            "label": "smack",
            "slug": "smack"
          }
        ],
        "is_public": false,
        "id": 11768,
        "uid": null,
        "title": "A Brief History of the SMACK Stack",
        "url": "https://chiefscientist.org/a-brief-history-of-the-smack-stack-f382547e91fe?gi=d78bedf3f6f9",
        "content": "<p id=\"d291\" class=\"graf graf--p graf-after--h3\">The term SMACK Stack was widely popularized in the San Francisco/Dublin Scala/Spark/Reactive Systems meetups and By the Bay series of conferences (Scala and Data). Since it took a life of its own, this is an abridged chronology on how it came about. I’m surely missing something, since it’s a view from (by) the Bay — please comment with any corrections or suggestions, as well as updates. I’m not elaborating the technical merits of the SMACK Stack here, as they are covered at <a href=\"http://noetl.org\" data-href=\"http://noetl.org\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">NoETL.org</a>.</p><p id=\"3242\" class=\"graf graf--p graf-after--p\">I’ve first seen the phrase SMACK Stack in a <a href=\"https://twitter.com/jamie_allen/status/614045214018158592\" data-href=\"https://twitter.com/jamie_allen/status/614045214018158592\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">tweet by Jamie Allen</a>, attributed to Oliver White, on June 25, 2015. Jamie is a veteran Typesafe/Lightbend executive and Scala/Akka/Reactive practitioner, and Oliver is their Chief Storyteller. By that time, our long-established plan to run <a href=\"http://2015.bigdatascala.bythebay.io/pipeline.html\" data-href=\"http://2015.bigdatascala.bythebay.io/pipeline.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">the first end-to-end data pipeline training</a> at Scala By the Bay and Big Data Scala 2015 was in full smack. We brought together the actual five companies comprising the SMACK Stack with their own trainers, training materials, and top OSS contributors, having worked relentlessly to create the actual SMACK Stack training, defining the SMACK Stack by doing and teaching.</p><p id=\"5350\" class=\"graf graf--p graf-after--p\">The original <a href=\"https://chiefscientist.org/sfscala.org\" data-href=\"sfscala.org\" class=\"markup--anchor markup--p-anchor\" target=\"_blank\">SF Scala</a> announcement went out on December 23, 2014. Here it is:</p><blockquote id=\"fb08\" class=\"graf graf--blockquote graf-after--p\"><div>We are starting two new conferences in 2015: Big Data Scala for complete data pipelines and “big” data science, and Text By the Bay for applied NLP/text mining. Our flagship Scala By the Bay conference continues into its third year, growing to 500, and directly followed by Big Data Scala.</div></blockquote><blockquote id=\"e59b\" class=\"graf graf--blockquote graf-after--blockquote\"><a href=\"https://chiefscientist.org/text.bythebay.io\" data-href=\"text.bythebay.io\" class=\"markup--anchor markup--blockquote-anchor\" target=\"_blank\">text.bythebay.io</a><div><br /><a href=\"https://chiefscientist.org/scala.bythebay.io\" data-href=\"scala.bythebay.io\" class=\"markup--anchor markup--blockquote-anchor\" target=\"_blank\">scala.bythebay.io</a><br /><a href=\"https://chiefscientist.org/bigdatascala.org\" data-href=\"bigdatascala.org\" class=\"markup--anchor markup--blockquote-anchor\" target=\"_blank\">bigdatascala.org</a></div></blockquote><blockquote id=\"5dfd\" class=\"graf graf--blockquote graf-after--blockquote\"><div>Both previous installments of SBTB sold out completely.</div></blockquote><blockquote id=\"aad9\" class=\"graf graf--blockquote graf-after--blockquote\"><div>You can find the talks from the previous conferences and meetups at Functional.TV. The 2014 conference is now at</div></blockquote><blockquote id=\"e1df\" class=\"graf graf--blockquote graf-after--blockquote\"><div>2014.scala.bythebay.io (photos from 2014 SBTB)</div></blockquote><blockquote id=\"3b38\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">We’re also creating a new kind of an all-day tutorial — Complete Pipeline Training, where in one day we’ll go through an end-to-end datapipeline in Scala, running Akka APIs on Mesos and pumping data through Kafka into Spark. Each segment will be taught by the respective company driving the component — Mesosphere, Typesafe, Confluent, and Databricks</strong>.</div></blockquote><p id=\"98de\" class=\"graf graf--p graf-after--blockquote\">The acronym SMACK was used publicly in the official announcement of the training. Here it is in the context of collecting questions for the closing panel at Big Data Scala with Martin Odersky, creator of Scala and cofounder of Typesafe (now Lightbend), who keynoted it.</p><figure id=\"0a91\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*THEpX1HaETt6rw6w80-7Fg.png\" data-width=\"2632\" data-height=\"1678\" data-action=\"zoom\" data-action-value=\"1*THEpX1HaETt6rw6w80-7Fg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*THEpX1HaETt6rw6w80-7Fg.png\" alt=\"image\" /></div></figure><p id=\"4f8a\" class=\"graf graf--p graf-after--figure\">The C for Cassandra was added when we invited Ryan Knight and Evan Chan. Ryan transitioned from Typesafe to DataStax, and Evan combined Spark and Cassandra for fast OLAP.</p><p id=\"739c\" class=\"graf graf--p graf-after--p\">Here’s the final lineup as delivered on the Complete Pipeline Training day</p><p id=\"4b42\" class=\"graf graf--p graf-after--p\">— <strong class=\"markup--strong markup--p-strong\">S: Scala and Spark</strong> =&gt; Typesafe, Databricks; every trainer was a fairly well-known Scala developer. Chris Fregly represented Databricks and Advanced Spark. Nilanjan Raychaudhuri helped with the Akka presentation. For Spark, we used <a href=\"http://spark-notebook.io\" data-href=\"http://spark-notebook.io\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Spark Notebook</a> as the GUI, presented by its creator Andy Petrella.</p><p id=\"6e57\" class=\"graf graf--p graf-after--p\">— <strong class=\"markup--strong markup--p-strong\">M: Mesos</strong> =&gt; Mesosphere. Jason Swartz</p><p id=\"3fe6\" class=\"graf graf--p graf-after--p\">— <strong class=\"markup--strong markup--p-strong\">A: Akka</strong> =&gt; Typesafe, Duncan De Vore, Nilanjan with slides, Ryan Knight emeritus.</p><p id=\"91a9\" class=\"graf graf--p graf-after--p\">— <strong class=\"markup--strong markup--p-strong\">C: Cassandra</strong>: Ryan Knight, for a while of Typesafe and then of DataStax, helped across the stack. Evan Chan (of Spark Job Server and FiloDB fame) is recognized for marrying Spark and Cassandra and was also instrumental.</p><p id=\"f58b\" class=\"graf graf--p graf-after--p\">— <strong class=\"markup--strong markup--p-strong\">K: Kafka</strong> =&gt; Confluent. Jesse Anderson was fielded by Confluent as its official training provider. Ewen Cheslak-Postava helped with the Kafka segment in the docker.</p><p id=\"0fbb\" class=\"graf graf--p graf-after--p\">Here’s the github repo used in the training: <a href=\"https://github.com/bythebay/pipeline\" data-href=\"https://github.com/bythebay/pipeline\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">https://github.com/bythebay/pipeline</a>.</p><p id=\"3576\" class=\"graf graf--p graf-after--p\">It’s important to note that Helena Edelson wrote <a href=\"https://github.com/killrweather/killrweather\" data-href=\"https://github.com/killrweather/killrweather\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">killrweather</a>, a project implementing the SMACK Stack, earlier, and presented it at <a href=\"http://pnwscala.org/2014/index.html\" data-href=\"http://pnwscala.org/2014/index.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">PNWScala</a> 2014 (among other venues). When Ryan and Evan joined the training we considered using killrweather and would have loved to have Helena join us (but she deferred to Ryan). We ended up with a different codebase to simplify setup and allow for Spark Notebook as a Spark GUI. Helena is a pioneer — and now a lead innovator — of SMACK Stack.</p><p id=\"7ada\" class=\"graf graf--p graf-after--p\">Originally, we worked with Mesosphere to spin up a training cluster for every student. (Mesosphere was able to secure GCE credits, but it was not ready to run on GCE.) At that point, Mesosphere messaging was mostly on fully utilizing the datacenter. Immediately after Big Data Scala, <a href=\"https://mesosphere.com/blog/2015/08/20/mesosphere-infinity-youre-4-words-away-from-a-complete-big-data-system/\" data-href=\"https://mesosphere.com/blog/2015/08/20/mesosphere-infinity-youre-4-words-away-from-a-complete-big-data-system/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Mesosphere announced <strong class=\"markup--strong markup--p-strong\">dcos infinity</strong></a>, inspired by <a href=\"https://mesosphere.com/blog/2015/07/24/learn-everything-you-need-to-know-about-scala-and-big-data-in-oakland/\" data-href=\"https://mesosphere.com/blog/2015/07/24/learn-everything-you-need-to-know-about-scala-and-big-data-in-oakland/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">the SMACK training By the Bay</a>. Infinity spins up Kafka, Spark and Cassandra working together. The SMACK Stack training By the Bay was the catalyst that lead to <em class=\"markup--em markup--p-em\">dcos infinity</em> — I pitched it to Flo directly (my good friend Jason Swartz was a devmanager at Mesosphere and we discussed it over lunch on the deck), and Flo put us together with Matt Trifiro, the CMO, with whom we continue to collaborate. It is important to observe that the data pipeline offering encapsulated by <em class=\"markup--em markup--p-em\">dcos infinity</em> heralded a new direction for Mesosphere, from data center utilization to data pipeline integration as a product. They’ve not called it “<em class=\"markup--em markup--p-em\">dcos smack”</em> to allow for database vendor neutrality. (We’ve also discussed “<em class=\"markup--em markup--p-em\">dcos khrabrov</em>” as an easily googlable alias.:)</p><p id=\"62ce\" class=\"graf graf--p graf-after--p\">The training was an incredible success — we had a 100 people at Galvanize going over the whole stack in one day, using a fully dockerized setup we provided on USB. It took five sweaty guys in an AirBnB — that Andy Petrella rented — to finalize the night before, after which I took it home to replicate on a device running under Windows. Of course it failed, and I manually copied the USBs into the night. But thanks to Nitro docker genius Ben Rizkowski, it worked great. Ben even popped in a Slack support channel, on a Sunday, while house-hunting in the East Bay.</p><p id=\"f0ba\" class=\"graf graf--p graf-after--p\"><a href=\"http://2015.bigdatascala.bythebay.io\" data-href=\"http://2015.bigdatascala.bythebay.io\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Big Data Scala</a> was held for the first time together with the training. It was keynoted by Martin Odersky, the creator of Scala, Matei Zaharia, the creator of Spark, Jay Kreps, the creator of Kafka, and Mike Olson, the CSO of Cloudera. Here we had most of the creators of the SMACK stack in person, as well as its key proponents and integrators. We’ve unveiled <a href=\"http://noetl.org\" data-href=\"http://noetl.org\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">NoETL.org</a> there as a way to popularize the reactive, streaming approach to data, as opposed to batching and dumping/reparsing text from HDFS. NoETL is in fact another way to describe what SMACK is good for, as a new way to write data pipelines.</p><p id=\"b487\" class=\"graf graf--p graf-after--p\">Since all of original SMACK Stack trainers are public speakers and OSS leads, the notion spread fast. Notably, Evan Chan moved forward with <a href=\"https://github.com/tuplejump/FiloDB\" data-href=\"https://github.com/tuplejump/FiloDB\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">FiloDB</a>, combining Cassandra and Spark to support OLAP. Evan presented his work at SF Scala and SF Spark, as well as Scala By the Bay and Big Data Scala, and subsequently teamed up with Helena Edelson to advance FiloDB and SMACK Stack in industry. Evan ran a Q&amp;A with O’Reilly in the Fall 2015. At the same time we took SMACK Stack to Europe via the Nitro office, the home of the Dublin Spark meetup. We also regularly host Andy Petrella at there regularly who further spread the word. Dean Wampler, the Lightbend Big Data architect, will present SMACK Stack at the O’Reilly Architecture conference this Fall (2016) — see how <a href=\"https://twitter.com/deanwampler/status/752510307562586112\" data-href=\"https://twitter.com/deanwampler/status/752510307562586112\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">the SMACKing already began</a>!</p><p id=\"b52d\" class=\"graf graf--p graf-after--p\">The SMACK Stack concept had become such a hit that I and other Nitro practitioners were approached by publishers to put together a book describing it. I’m helping drive such a project, and we’re looking for co-authors — ping me if you’re interested.</p><p id=\"747f\" class=\"graf graf--p graf-after--p\">It’s a good time to remind all SMACK lovers that Scala By the Bay and Big Data Scala are held at Twitter this year, employing its own SMACK Stack that you can see every day in action — e.g. when you like a tweet and someone gets a notification immediately, for millions of tweets. We’re calling the whole conference Scale+Scala By the Bay, or simply <a href=\"http://scala.bythebay.io\" data-href=\"http://scala.bythebay.io\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Scalæ By the Bay</a>, and run three tracks over three days, all joined in harmony. The tracks are as follows:</p><p id=\"518f\" class=\"graf graf--p graf-after--p\">— Thoughtful Sofware Engineering with Types</p><p id=\"2ce3\" class=\"graf graf--p graf-after--p\">— Reactive Systems and Streaming</p><p id=\"39b3\" class=\"graf graf--p graf-after--p\">— Data Pipelines for Data Engineering and Machine Learning. This is SMACK Stack in action.</p><p id=\"0f17\" class=\"graf graf--p graf-after--p graf--trailing\">SMACK is a registered trademark of By the Bay, LLC (pending). It was first used in our SMACK training commercially, and nowhere else before it. We trained a hundred people to build a whole big data backend in a day, and they took this knowledge to the world. It’s important this history, however brief, is acknowledged when using the term SMACK stack.</p>",
        "created_at": "2018-08-02T01:44:33+0000",
        "updated_at": "2018-08-02T01:44:45+0000",
        "published_at": "2016-07-11T19:41:56+0000",
        "published_by": [
          "Alexy Khrabrov"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 7,
        "domain_name": "chiefscientist.org",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*THEpX1HaETt6rw6w80-7Fg.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11768"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          },
          {
            "id": 1263,
            "label": "bigtable",
            "slug": "bigtable"
          }
        ],
        "is_public": false,
        "id": 11766,
        "uid": null,
        "title": "heroic",
        "url": "https://spotify.github.io/heroic/#!/index",
        "content": null,
        "created_at": "2018-08-02T01:38:02+0000",
        "updated_at": "2018-08-02T01:38:17+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "spotify.github.io",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11766"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1262,
            "label": "time.series",
            "slug": "time-series"
          },
          {
            "id": 1263,
            "label": "bigtable",
            "slug": "bigtable"
          }
        ],
        "is_public": false,
        "id": 11765,
        "uid": null,
        "title": "spotify/heroic",
        "url": "https://github.com/spotify/heroic",
        "content": "<p><a href=\"https://travis-ci.org/spotify/heroic\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/1ff3a276d500da8dc03f2a43b9a1b0dff28420f8/68747470733a2f2f7472617669732d63692e6f72672f73706f746966792f6865726f69632e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/spotify/heroic.svg?branch=master\" /></a>\n<a href=\"https://codecov.io/gh/spotify/heroic\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/7cdbc66d3bc1891024a9ec3ce3fbba2cd9c17cf3/68747470733a2f2f696d672e736869656c64732e696f2f636f6465636f762f632f6769746875622f73706f746966792f6865726f69632e737667\" alt=\"Codecov\" data-canonical-src=\"https://img.shields.io/codecov/c/github/spotify/heroic.svg\" /></a>\n<a href=\"https://github.com/spotify/heroic/blob/master/LICENSE\"><img src=\"https://camo.githubusercontent.com/dd47e65cd53623e602a1491c03d88ff626f291c2/68747470733a2f2f696d672e736869656c64732e696f2f6769746875622f6c6963656e73652f73706f746966792f6865726f69632e737667\" alt=\"License\" data-canonical-src=\"https://img.shields.io/github/license/spotify/heroic.svg\" /></a>\n<a href=\"https://gitter.im/spotify/heroic\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0a833308629f72d089428d515a6d3887df94efd1/68747470733a2f2f6261646765732e6769747465722e696d2f73706f746966792f6865726f69632e737667\" alt=\"Join the chat at https://gitter.im/spotify/heroic\" data-canonical-src=\"https://badges.gitter.im/spotify/heroic.svg\" /></a></p><p>A scalable time series database based on Bigtable, Cassandra, and Elasticsearch.\nGo to <a href=\"https://spotify.github.io/heroic/\" rel=\"nofollow\">https://spotify.github.io/heroic/</a> for documentation, please join <code>#heroic at Freenode</code> if you need help or want to chat.</p><p>This project adheres to the <a href=\"https://github.com/spotify/code-of-conduct/blob/master/code-of-conduct.md\">Open Code of Conduct</a>.\nBy participating, you are expected to honor this code.</p><p><strong>Stability Disclaimer:</strong>\nHeroic is an evolving project, and should in its current state be considered <em>unstable</em>.\nDo not use in production unless you are willing to spend time with it, experiment and contribute.\nNot doing so might result in losing your data to goblins. It is currently not on a release schedule and is not versioned. At Spotify we rely on multiple <em>release forks</em> that we actively maintain and flip between.</p><h2>Building</h2><p>Java 8 is required.</p><p>There are some repackaged dependencies that you have to make available, you do\nthis by running <code>tools/install-repackaged</code>.</p><div class=\"highlight highlight-source-shell\"><pre>$ tools/install-repackaged\nInstalling repackaged/x\n...</pre></div><p>After this, the project is built using Maven:</p><div class=\"highlight highlight-source-shell\"><pre>$ mvn package</pre></div><p>This will cause the <code>heroic-dist</code> module to produce a shaded jar that contains\nall required dependencies.</p><h2>Running</h2><p>After building, the entry point of the service is\n<a href=\"https://github.com/spotify/heroic/blob/master/heroic-dist/src/main/java/com/spotify/heroic/HeroicService.java\"><code>com.spotify.heroic.HeroicService</code></a>.\nThe following is an example of how this can be run:</p><pre>$ java -cp $PWD/heroic-dist/target/heroic-dist-0.0.1-SNAPSHOT-shaded.jar com.spotify.heroic.HeroicService &lt;config&gt;\n</pre><p>For help on how to write a configuration file, see the <a href=\"http://spotify.github.io/heroic/#!/docs/config\" rel=\"nofollow\">Configuration Section</a> of the official documentation.</p><p>Heroic has been tested with the following services:</p><h4>Logging</h4><p>Logging is captured using <a href=\"http://www.slf4j.org/\" rel=\"nofollow\">SLF4J</a>, and forwarded to\n<a href=\"http://logging.apache.org/log4j/\" rel=\"nofollow\">Log4j</a>.</p><p>To configure logging, define the <code>-Dlog4j.configurationFile=&lt;path&gt;</code>\nparameter. You can use <a href=\"https://github.com/spotify/heroic/blob/master/docs/log4j2-file.xml\">docs/log4j2-file.xml</a> as a base.</p><h2>Testing</h2><p>We run unit tests with Maven:</p><pre>$ mvn test\n</pre><p>A more comprehensive test suite is enabled with the <code>environment=test</code>\nproperty.</p><pre>$ mvn -D environment=test verify\n</pre><p>This adds:</p><ul><li><a href=\"http://checkstyle.sourceforge.net/\" rel=\"nofollow\">Checkstyle</a></li>\n<li><a href=\"http://findbugs.sourceforge.net/\" rel=\"nofollow\">FindBugs</a></li>\n<li><a href=\"http://maven.apache.org/surefire/maven-failsafe-plugin/\" rel=\"nofollow\">Integration Tests with Maven Failsafe Plugin</a></li>\n<li><a href=\"http://eclemma.org/jacoco/\" rel=\"nofollow\">Coverage Reporting with Jacoco</a></li>\n</ul><p>It is strongly recommended that you run the full test suite before setting up a\npull request, otherwise it will be rejected by Travis.</p><h4>Remote Integration Tests</h4><p>Integration tests are configured to run remotely depending on a set of system\nproperties.</p><h5>Elasticsearch</h5><table><thead><tr><th>Property</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td><code>-D elasticsearch.version=&lt;version&gt;</code></td>\n<td>Use the given client version when building the project</td>\n</tr><tr><td><code>-D it.elasticsearch.remote=true</code></td>\n<td>Run Elasticsearch tests against a remote database</td>\n</tr><tr><td><code>-D it.elasticsearch.seed=&lt;seed&gt;</code></td>\n<td>Use the given seed (default: <code>localhost</code>)</td>\n</tr><tr><td><code>-D it.elasticsearch.clusterName=&lt;clusterName&gt;</code></td>\n<td>Use the given cluster name (default: <code>elasticsearch</code>)</td>\n</tr></tbody></table><h5>Datastax</h5><table><thead><tr><th>Property</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td><code>-D datastax.version=&lt;version&gt;</code></td>\n<td>Use the given client version when building the project</td>\n</tr><tr><td><code>-D it.datastax.remote=true</code></td>\n<td>Run Datastax tests against a remote database</td>\n</tr><tr><td><code>-D it.datastax.seed=&lt;seed&gt;</code></td>\n<td>Use the given seed (default: <code>localhost</code>)</td>\n</tr></tbody></table><h5>Bigtable</h5><table><thead><tr><th>Property</th>\n<th>Description</th>\n</tr></thead><tbody><tr><td><code>-D bigtable.version=&lt;version&gt;</code></td>\n<td>Use the given client version when building the project</td>\n</tr><tr><td><code>-D it.bigtable.remote=true</code></td>\n<td>Run Bigtable tests against a remote database</td>\n</tr><tr><td><code>-D it.bigtable.project=&lt;project&gt;</code></td>\n<td>Use the given project</td>\n</tr><tr><td><code>-D it.bigtable.zone=&lt;zone&gt;</code></td>\n<td>Use the given zone</td>\n</tr><tr><td><code>-D it.bigtable.instance=&lt;instance&gt;</code></td>\n<td>Use the given instance</td>\n</tr><tr><td><code>-D it.bigtable.credentials=&lt;credentials&gt;</code></td>\n<td>Use the given credentials file</td>\n</tr></tbody></table><p>The following is an example Elasticsearch remote integration test:</p><pre>$&gt; mvn -P integration-tests \\\n    -D elasticsearch.version=1.7.5 \\\n    -D it.elasticsearch.remote=true \\\n    clean verify\n</pre><h4>Full Cluster Tests</h4><p>Full cluster tests are defined in <a href=\"https://github.com/spotify/heroic/blob/master/heroic-dist/src/test/java\">heroic-dist/src/test/java</a>.</p><p>This way, they have access to all the modules and parts of Heroic.</p><p>The <a href=\"https://github.com/spotify/heroic/blob/master/rpc/jvm\">JVM RPC</a> module is specifically designed to allow for rapid\nexecution of integration tests. It allows multiple cores to be defined and\ncommunicate with each other in the same JVM instance.</p><h4>Coverage</h4><p><a href=\"https://codecov.io/gh/spotify/heroic/branch/master\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/ba63688333e04608fe9f9cfdadaec029dec32c58/68747470733a2f2f636f6465636f762e696f2f67682f73706f746966792f6865726f69632f6272616e63682f6d61737465722f6772617068732f696369636c652e737667\" alt=\"Coverage\" data-canonical-src=\"https://codecov.io/gh/spotify/heroic/branch/master/graphs/icicle.svg\" /></a></p><p>There's an ongoing project to improve test coverage.\nClicking the above graph will bring you to <a href=\"https://codecov.io/gh/spotify/heroic/branches/master\" rel=\"nofollow\">codecov.io</a>, where you can find areas to focus on.</p><h4>Speedy Building</h4><p>For a speedy build without tests and checks, you can run:</p><div class=\"highlight highlight-source-shell\"><pre>$ mvn -D maven.test.skip=true package</pre></div><h4>Building a Debian Package</h4><p>This project does not provide a single debian package, this is primarily\nbecause the current nature of the service (alpha state) does not mesh well with\nstable releases.</p><p>Instead, you are encouraged to build your own using the provided scripts in\nthis project.</p><p>First run the <code>prepare-sources</code> script:</p><div class=\"highlight highlight-source-shell\"><pre>$ debian/bin/prepare-sources myrel 1</pre></div><p><code>myrel</code> will be the name of your release, it will be part of your package name\n<code>debian-myrel</code>, it will also be suffixed to all helper tools (e.g.\n<code>heroic-myrel</code>).</p><p>For the next step you'll need a Debian environment:</p><div class=\"highlight highlight-source-shell\"><pre>$ dpkg-buildpackage -uc -us</pre></div><p>If you encounter problems, you can troubleshoot the build with <code>DH_VERBOSE</code>:</p><div class=\"highlight highlight-source-shell\"><pre>$ env DH_VERBOSE=1 dpkg-buildpackage -uc -us</pre></div><h2>Contributing</h2><p>Fork the code at <a href=\"https://github.com/spotify/heroic\">https://github.com/spotify/heroic</a></p><p>Make sure you format the code using the provided formatter in <a href=\"https://github.com/spotify/heroic/blob/master/idea\">idea</a>. Even if you disagree\nwith the way it is formatted, consistency is more important.\nFor special cases, see <a href=\"#bypassing-validation\">Bypassing Validation</a>.</p><p>If possible, limit your changes to one module per commit.\nIf you add new, or modify existing classes. Keep that change to a single commit while maintaing\nbackwards compatible behaviour. Deprecate any old APIs as appropriate with <code>@Deprecated</code> and\nadd documentation for how to use the new API.</p><p>The first line of the commit should be formatted with <code>[module1,module2] my message</code>.</p><p><code>module1</code> and <code>module2</code> are paths to the modules affected with any <code>heroic-</code> prefix stripped.\nSo if your change affects <code>heroic-core</code> and <code>metric/bigtable</code>, the message should say\n<code>[core,metric/bigtable] did x to y</code>.</p><p>If more than <em>3 modules</em> are affected by a commit, use <code>[all]</code>.\nFor other cases, adapt to the format of existing commit messages.</p><p>Before setting up a pull request, run the comprehensive test suite as specified in\n<a href=\"#testing\">Testing</a>.</p><h4>Module Orientation</h4><p>The Heroic project is split into a couple of modules.</p><p>The most critical one is <a href=\"https://github.com/spotify/heroic/blob/master/heroic-component\"><code>heroic-component</code></a>. It contains\ninterfaces, value objects, and the basic set of dependencies necessary to glue\ndifferent components together.</p><p>Submodules include <a href=\"https://github.com/spotify/heroic/blob/master/metric\"><code>metric</code></a>, <a href=\"https://github.com/spotify/heroic/blob/master/suggest\"><code>suggest</code></a>,\n<a href=\"https://github.com/spotify/heroic/blob/master/metadata\"><code>metadata</code></a>, and <a href=\"https://github.com/spotify/heroic/blob/master/aggregation\"><code>aggregation</code></a>. The first three\ncontain various implementations of the given backend type, while the latter\nprovides aggregation methods.</p><p><a href=\"https://github.com/spotify/heroic/blob/master/heroic-core\"><code>heroic-core</code></a> contains the\n<a href=\"https://github.com/spotify/heroic/blob/master/heroic-core/src/main/java/com/spotify/heroic/HeroicCore.java\"><code>com.spotify.heroic.HeroicCore</code></a>\nclass which is the central building block for setting up a Heroic instance.</p><p><a href=\"https://github.com/spotify/heroic/blob/master/heroic-elasticsearch-utils\"><code>heroic-elasticsearch-utils</code></a> is a collection of\nutilities for interacting with Elasticsearch. This is separate since we have\nmore than one backend that needs to talk with elasticsearch.</p><p><a href=\"https://github.com/spotify/heroic/blob/master/heroic-parser\"><code>heroic-parser</code></a> provides an Antlr4 implementation of\n<a href=\"https://github.com/spotify/heroic/blob/master/heroic-component/src/main/java/com/spotify/heroic/grammar/QueryParser.java\"><code>com.spotify.heroic.grammar.QueryParser</code></a>,\nwhich is used to parse the Heroic DSL.</p><p><a href=\"https://github.com/spotify/heroic/blob/master/heroic-shell\"><code>heroic-shell</code></a> contains\n<a href=\"https://github.com/spotify/heroic/blob/master/heroic-shell/src/main/java/com/spotify/heroic/HeroicShell.java\"><code>com.spotify.heroic.HeroicShell</code></a>,\na shell capable of either running a standalone, or connecting to an existing\nHeroic instance for administration.</p><p><a href=\"https://github.com/spotify/heroic/blob/master/heroic-all\"><code>heroic-all</code></a> contains dependencies and references to all modules\nthat makes up a Heroic distribution. This is also where profiles are defined\nsince they need to have access to all dependencies.</p><p>Anything in the <a href=\"https://github.com/spotify/heroic/blob/master/repackaged\"><code>repackaged</code></a> directory is dependencies that\ninclude one or more Java packages that must be relocated to avoid conflicts.\nThese are exported under the <code>com.spotify.heroic.repackaged</code> groupId.</p><p>Finally there is <a href=\"https://github.com/spotify/heroic/blob/master/heroic-dist\"><code>heroic-dist</code></a>, a small project that depends on\n<a href=\"https://github.com/spotify/heroic/blob/master/heroic-all\"><code>heroic-all</code></a>, <a href=\"https://github.com/spotify/heroic/blob/master/heroic-shell\"><code>heroic-shell</code></a>, and a logging\nimplementation. Here is where everything is bound together into a distribution\n— a shaded jar. It also provides the entry-point for services, namely\n<a href=\"https://github.com/spotify/heroic/blob/master/heroic-dist/src/main/java/com/spotify/heroic/HeroicService.java\"><code>com.spotify.heroic.HeroicService</code></a>.</p><h4>Bypassing Validation</h4><p>To bypass automatic formatting and checkstyle validation you can use the\nfollowing stanza:</p><div class=\"highlight highlight-source-java\"><pre>// @formatter:off\nfinal List&lt;String&gt; list = ImmutableList.of(\n   \"Welcome to...\",\n   \"... The Wild West\"\n);\n// @formatter:on</pre></div><p>To bypass a FindBugs error, you should use the <code>@SupressFBWarnings</code> annotation.</p><div class=\"highlight highlight-source-java\"><pre>@SupressFBWarnings(value=\"FINDBUGS_ERROR_CODE\", justification=\"I Know Better Than FindBugs\")\npublic class IKnowBetterThanFindbugs() {\n    // ...\n}</pre></div><h2>HeroicShell</h2><p>Heroic comes with a shell that contains many useful tasks, these can either\nbe run in a readline-based shell with some basic completions and history, or\nstandalone.</p><p>You can use the following helper script to run the shell directly from the\nproject.</p><div class=\"highlight highlight-source-shell\"><pre>$ tools/heroic-shell [opts]</pre></div><p>There are a few interesting options available, most notably is <code>--connect</code> that\nallows the shell to connect to a remote heroic instance.</p><p>See <code>-h</code> for a full listing of options.</p><p>You can run individual tasks in <em>standalone</em> mode, giving you a bit more\noptions (like redirecting output) through the following.</p><div class=\"highlight highlight-source-shell\"><pre>$ tools/heroic-shell &lt;heroic-options&gt; -- com.spotify.heroic.shell.task.&lt;task-name&gt; &lt;task-options&gt;</pre></div><p>There are also profiles that can be activated with the <code>-P &lt;profile&gt;</code> switch,\navailable profiles are listed in <code>--help</code>.</p><h2>Repackaged Dependencies</h2><p>These are third-party dependencies that has to be repackaged to avoid binary\nincompatibilities with dependencies.</p><p>Every time these are upgraded, they must be inspected for new conflicts.\nThe easiest way to do this, is to build the project and look at the warnings\nfor the shaded jar.</p><pre>$&gt; mvn clean package -D maven.test.skip=true\n...\n[WARNING] foo-3.5.jar, foo-4.5.jar define 10 overlapping classes:\n[WARNING]   - com.foo.ConflictingClass\n...\n</pre><p>This would indicate that there is a package called foo with overlapping\nclasses.</p><p>You can find the culprit using the <code>dependency</code> plugin.</p><pre>$&gt; mvn package dependency:tree\n</pre>",
        "created_at": "2018-08-02T01:36:20+0000",
        "updated_at": "2018-08-02T01:38:24+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/251374?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11765"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 207,
            "label": "system",
            "slug": "system"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 985,
            "label": "cluster",
            "slug": "cluster"
          }
        ],
        "is_public": false,
        "id": 11764,
        "uid": null,
        "title": "riptano/ccm",
        "url": "https://github.com/riptano/ccm",
        "content": "<p>A script/library to create, launch and remove an Apache Cassandra cluster on\nlocalhost.</p><p>The goal of ccm and ccmlib is to make it easy to create, manage and destroy a\nsmall Cassandra cluster on a local box. It is meant for testing a Cassandra cluster.</p><h2>Requirements</h2><ul><li>\n<p>A working python installation (tested to work with python 2.7).</p>\n</li>\n<li>\n<p>pyYAML (<a href=\"http://pyyaml.org/\" rel=\"nofollow\">http://pyyaml.org/</a> -- <code>sudo easy_install pyYaml</code>)</p>\n</li>\n<li>\n<p>six (<a href=\"https://pypi.org/project/six/\" rel=\"nofollow\">https://pypi.org/project/six/</a> -- <code>sudo easy_install six</code>)</p>\n</li>\n<li>\n<p>ant (<a href=\"http://ant.apache.org/\" rel=\"nofollow\">http://ant.apache.org/</a>, on Mac OS X, <code>brew install ant</code>)</p>\n</li>\n<li>\n<p>psutil (<a href=\"https://pypi.org/project/psutil/\" rel=\"nofollow\">https://pypi.org/project/psutil/</a>)</p>\n</li>\n<li>\n<p>Java (which version depends on the version of Cassandra you plan to use. If\nunsure, use Java 7 as it is known to work with current versions of Cassandra).</p>\n</li>\n<li>\n<p>If you want to create multiple node clusters, the simplest way is to use\nmultiple loopback aliases. On modern linux distributions you probably don't\nneed to do anything, but on Mac OS X, you will need to create the aliases with</p>\n<pre>sudo ifconfig lo0 alias 127.0.0.2 up\nsudo ifconfig lo0 alias 127.0.0.3 up\n...\n</pre>\n<p>Note that the usage section assumes that at least 127.0.0.1, 127.0.0.2 and\n127.0.0.3 are available.</p>\n</li>\n</ul><h3>Optional Requirements</h3><ul><li>Paramiko (<a href=\"http://www.paramiko.org/\" rel=\"nofollow\">http://www.paramiko.org/</a>): Paramiko adds the ability to execute CCM\nremotely; <code>pip install paramiko</code></li>\n</ul><p><strong>Note</strong>: The remote machine must be configured with an SSH server and a working\nCCM. When working with multiple nodes each exposed IP address must be\nin sequential order. For example, the last number in the 4th octet of\na IPv4 address must start with <code>1</code> (e.g. 192.168.33.11). See\n<a href=\"https://github.com/riptano/ccm/blob/master/misc/Vagrantfile\">Vagrantfile</a> for help with configuration of remote\nCCM machine.</p><h2>Known issues</h2><p>Windows only:</p><ul><li><code>node start</code> pops up a window, stealing focus.</li>\n<li>cqlsh started from ccm show incorrect prompts on command-prompt</li>\n<li>non nodetool-based command-line options fail (sstablesplit, scrub, etc)</li>\n<li>To install psutil, you must use the .msi from pypi. pip install psutil will not work</li>\n<li>You will need ant.bat in your PATH in order to build C* from source</li>\n<li>You must run with an Unrestricted Powershell Execution-Policy if using Cassandra 2.1.0+</li>\n<li>Ant installed via <a href=\"https://chocolatey.org/\" rel=\"nofollow\">chocolatey</a> will not be found by ccm, so you must create a symbolic\nlink in order to fix the issue (as administrator):\n<ul><li>cmd /c mklink C:\\ProgramData\\chocolatey\\bin\\ant.bat C:\\ProgramData\\chocolatey\\bin\\ant.exe</li>\n</ul></li>\n</ul><p>Remote Execution only:</p><ul><li>Using <code>--config-dir</code> and <code>--install-dir</code> with <code>create</code> may not work as\nexpected; since the configuration directory and the installation directory\ncontain lots of files they will not be copied over to the remote machine\nlike most other options for cluster and node operations</li>\n<li>cqlsh started from ccm using remote execution will not start\nproperly (e.g.<code>ccm --ssh-host 192.168.33.11 node1 cqlsh</code>); however\n<code>-x &lt;CMDS&gt;</code> or <code>--exec=CMDS</code> can still be used to execute a CQLSH command\non a remote node.</li>\n</ul><h2>Installation</h2><p>ccm uses python distutils so from the source directory run:</p><pre>sudo ./setup.py install\n</pre><p>ccm is available on the <a href=\"https://pypi.org/project/ccm/\" rel=\"nofollow\">Python Package Index</a>:</p><pre>pip install ccm\n</pre><p>There is also a <a href=\"https://github.com/Homebrew/homebrew/blob/master/Library/Formula/ccm.rb\">Homebrew package</a> available:</p><pre>brew install ccm\n</pre><h2>Usage</h2><p>Let's say you wanted to fire up a 3 node Cassandra cluster.</p><h3>Short version</h3><pre>ccm create test -v 2.0.5 -n 3 -s\n</pre><p>You will of course want to replace <code>2.0.5</code> by whichever version of Cassandra\nyou want to test.</p><h3>Longer version</h3><p>ccm works from a Cassandra source tree (not the jars). There are two ways to\ntell ccm how to find the sources:</p><ol><li>\n<p>If you have downloaded <em>and</em> compiled Cassandra sources, you can ask ccm\nto use those by initiating a new cluster with:</p>\n<p>ccm create test --install-dir=&lt;path/to/cassandra-sources&gt;</p>\n<p>or, from that source tree directory, simply</p>\n<pre> ccm create test\n</pre>\n</li>\n<li>\n<p>You can ask ccm to use a released version of Cassandra. For instance to\nuse Cassandra 2.0.5, run</p>\n<pre> ccm create test -v 2.0.5\n</pre>\n<p>ccm will download the binary (from <a href=\"http://archive.apache.org/dist/cassandra\" rel=\"nofollow\">http://archive.apache.org/dist/cassandra</a>),\nand set the new cluster to use it. This means\nthat this command can take a few minutes the first time you\ncreate a cluster for a given version. ccm saves the compiled\nsource in <code>~/.ccm/repository/</code>, so creating a cluster for that\nversion will be much faster the second time you run it\n(note however that if you create a lot of clusters with\ndifferent versions, this will take up disk space).</p>\n</li>\n</ol><p>Once the cluster is created, you can populate it with 3 nodes with:</p><pre>ccm populate -n 3\n</pre><p>For Mac OSX, create a new interface for every node besides the first, for example if you populated your cluster with 3 nodes, create interfaces for 127.0.0.2 and 127.0.0.3 like so:</p><pre>sudo ifconfig lo0 alias 127.0.0.2\nsudo ifconfig lo0 alias 127.0.0.3\n</pre><p>Note these aliases will disappear on reboot. For permanent network aliases on Mac OSX see <a target=\"_blank\" href=\"https://github.com/riptano/ccm/blob/master/NETWORK_ALIASES.md\"><img src=\"https://github.com/riptano/ccm/raw/master/NETWORK_ALIASES.md\" alt=\"Network Aliases\" /></a>.</p><p>After that execute:</p><pre>ccm start\n</pre><p>That will start 3 nodes on IP 127.0.0.[1, 2, 3] on port 9160 for thrift, port\n7000 for the internal cluster communication and ports 7100, 7200 and 7300 for JMX.\nYou can check that the cluster is correctly set up with</p><pre>ccm node1 ring\n</pre><p>You can then bootstrap a 4th node with</p><pre>ccm add node4 -i 127.0.0.4 -j 7400 -b\n</pre><p>(populate is just a shortcut for adding multiple nodes initially)</p><p>ccm provides a number of conveniences, like flushing all of the nodes of\nthe cluster:</p><pre>ccm flush\n</pre><p>or only one node:</p><pre>ccm node2 flush\n</pre><p>You can also easily look at the log file of a given node with:</p><pre>ccm node1 showlog\n</pre><p>Finally, you can get rid of the whole cluster (which will stop the node and\nremove all the data) with</p><pre>ccm remove\n</pre><p>The list of other provided commands is available through</p><pre>ccm\n</pre><p>Each command is then documented through the <code>-h</code> (or <code>--help</code>) flag. For\ninstance <code>ccm add -h</code> describes the options for <code>ccm add</code>.</p><h3>Remote Usage (SSH/Paramiko)</h3><p>All the usage examples above will work exactly the same for a remotely\nconfigured machine; however remote options are required in order to establish a\nconnection to the remote machine before executing the CCM commands:</p><table><thead><tr><th align=\"left\">Argument</th>\n<th align=\"left\">Value</th>\n<th align=\"left\">Description</th>\n</tr></thead><tbody><tr><td align=\"left\">--ssh-host</td>\n<td align=\"left\">string</td>\n<td align=\"left\">Hostname or IP address to use for SSH connection</td>\n</tr><tr><td align=\"left\">--ssh-port</td>\n<td align=\"left\">int</td>\n<td align=\"left\">Port to use for SSH connection<br />Default is 22</td>\n</tr><tr><td align=\"left\">--ssh-username</td>\n<td align=\"left\">string</td>\n<td align=\"left\">Username to use for username/password or public key authentication</td>\n</tr><tr><td align=\"left\">--ssh-password</td>\n<td align=\"left\">string</td>\n<td align=\"left\">Password to use for username/password or private key passphrase using public key authentication</td>\n</tr><tr><td align=\"left\">--ssh-private-key</td>\n<td align=\"left\">filename</td>\n<td align=\"left\">Private key to use for SSH connection</td>\n</tr></tbody></table><h4>Special Handling</h4><p>Some commands require files to be located on the remote server. Those commands\nare pre-processed, file transfers are initiated, and updates are made to the\nargument value for the remote execution of the CCM command:</p><table><thead><tr><th align=\"left\">Parameter</th>\n<th align=\"left\">Description</th>\n</tr></thead><tbody><tr><td align=\"left\"><code>--dse-credentials</code></td>\n<td align=\"left\">Copy local DSE credentials file to remote server</td>\n</tr><tr><td align=\"left\"><code>--node-ssl</code></td>\n<td align=\"left\">Recursively copy node SSL directory to remote server</td>\n</tr><tr><td align=\"left\"><code>--ssl</code></td>\n<td align=\"left\">Recursively copy SSL directory to remote server</td>\n</tr></tbody></table><h4>Short Version</h4><pre>ccm --ssh-host=192.168.33.11 --ssh-username=vagrant --ssh-password=vagrant create test -v 2.0.5 -n 3 -i 192.168.33.1 -s\n</pre><p><strong>Note</strong>: <code>-i</code> is used to add an IP prefix during the create process to ensure\nthat the nodes communicate using the proper IP address for their node</p><h3>Source Distribution</h3><p>If you'd like to use a source distribution instead of the default binary each time (for example, for Continuous Integration), you can prefix cassandra version with <code>source:</code>, for example:</p><pre>ccm create test -v source:2.0.5 -n 3 -s\n</pre><h3>Automatic Version Fallback</h3><p>If 'binary:' or 'source:' are not explicitly specified in your version string, then ccm will fallback to building the requested version from git if it cannot access the apache mirrors.</p><h3>Git and GitHub</h3><p>To use the latest version from the <a href=\"https://git-wip-us.apache.org/repos/asf?p=cassandra.git\" rel=\"nofollow\">canonical Apache Git repository</a>, use the version name <code>git:branch-name</code>, e.g.:</p><pre>ccm create trunk -v git:trunk -n 5\n</pre><p>and to download a branch from a GitHub fork of Cassandra, you can prefix the repository and branch with <code>github:</code>, e.g.:</p><pre>ccm create patched -v github:jbellis/trunk -n 1\n</pre><h3>Bash command-line completion</h3><p>ccm has many sub-commands for both cluster commands as well as node commands, and sometimes you don't quite remember the name of the sub-command you want to invoke. Also, command lines may be long due to long cluster or node names.</p><p>Leverage bash's <em>programmable completion</em> feature to make ccm use more pleasant. Copy <code>misc/ccm-completion.bash</code> to somewhere in your home directory (or /etc if you want to make it accessible to all users of your system) and source it in your <code>.bash_profile</code>:</p><pre>. ~/scripts/ccm-completion.bash\n</pre><p>Once set up, <code>ccm sw&lt;tab&gt;</code> expands to <code>ccm switch</code>, for example. The <code>switch</code> sub-command has extra completion logic to help complete the cluster name. So <code>ccm switch cl&lt;tab&gt;</code> would expand to <code>ccm switch cluster-58</code> if cluster-58 is the only cluster whose name starts with \"cl\". If there is ambiguity, hitting <code>&lt;tab&gt;</code> a second time shows the choices that match:</p><pre>$ ccm switch cl&lt;tab&gt;\n    ... becomes ...\n$ ccm switch cluster-\n    ... then hit tab twice ...\ncluster-56  cluster-85  cluster-96\n$ ccm switch cluster-8&lt;tab&gt;\n    ... becomes ...\n$ ccm switch cluster-85\n</pre><p>It dynamically determines available sub-commands based on the ccm being invoked. Thus, users running multiple ccm's (or a ccm that they are continuously updating with new commands) will automagically work.</p><p>The completion script relies on ccm having two hidden subcommands:</p><ul><li>show-cluster-cmds - emits the names of cluster sub-commands.</li>\n<li>show-node-cmds - emits the names of node sub-commands.</li>\n</ul><p>Thus, it will not work with sufficiently old versions of ccm.</p><h2>Remote debugging</h2><p>If you would like to connect to your Cassandra nodes with a remote debugger you have to pass the <code>-d</code> (or <code>--debug</code>) flag to the populate command:</p><pre>ccm populate -d -n 3\n</pre><p>That will populate 3 nodes on IP 127.0.0.[1, 2, 3] setting up the remote debugging on ports 2100, 2200 and 2300.\nThe main thread will not be suspended so you don't have to connect with a remote debugger to start a node.</p><p>Alternatively you can also specify a remote port with the <code>-r</code> (or <code>--remote-debug-port</code>) flag while adding a node</p><pre>ccm add node4 -r 5005 -i 127.0.0.4 -j 7400 -b\n</pre><h2>Where things are stored</h2><p>By default, ccm stores all the node data and configuration files under <code>~/.ccm/cluster_name/</code>.\nThis can be overridden using the <code>--config-dir</code> option with each command.</p><h2>DataStax Enterprise</h2><p>CCM 2.0 supports creating and interacting with DSE clusters. The --dse\noption must be used with the <code>ccm create</code> command. See the <code>ccm create -h</code>\nhelp for assistance.</p><h2>CCM Lib</h2><p>The ccm facilities are available programmatically through ccmlib. This could\nbe used to implement automated tests against Cassandra. A simple example of\nhow to use ccmlib follows:</p><pre>import ccmlib.cluster\nCLUSTER_PATH=\".\"\ncluster = ccmlib.cluster.Cluster(CLUSTER_PATH, 'test', cassandra_version='2.1.14')\ncluster.populate(3).start()\n[node1, node2, node3] = cluster.nodelist()\n# do some tests on the cluster/nodes. To connect to a node through thrift,\n# the host and port to a node is available through\n#   node.network_interfaces['thrift']\ncluster.flush()\nnode2.compact()\n# do some other tests\n# after the test, you can leave the cluster running, you can stop all nodes\n# using cluster.stop() but keep the data around (in CLUSTER_PATH/test), or\n# you can remove everything with cluster.remove()\n</pre><p>--\nSylvain Lebresne <a href=\"mailto:sylvain@datastax.com\">sylvain@datastax.com</a></p>",
        "created_at": "2018-08-02T01:32:06+0000",
        "updated_at": "2018-08-02T01:33:33+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 9,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/275430?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11764"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 10,
            "label": "api",
            "slug": "api"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 941,
            "label": "ruby",
            "slug": "ruby"
          }
        ],
        "is_public": false,
        "id": 11761,
        "uid": null,
        "title": "Cassandra-web by avalanche123",
        "url": "http://avalanche123.com/cassandra-web/",
        "content": "<p>A web interface to Apache Cassandra with AngularJS and server-sent events.</p><h2>\nInstallation</h2><div class=\"highlight highlight-bash\"><pre>$ gem install cassandra-web</pre></div><h2>\nUsage</h2><div class=\"highlight highlight-bash\"><pre>$ cassandra-web</pre></div><p>Run <code>cassandra-web -h</code> for futher help.</p><h2>\nHow it works</h2><p>Cassandra web consists of an HTTP API powered by <a href=\"https://github.com/sinatra/sinatra\">Sinatra</a> and a thin HTML5/JavaScript frontend powered by <a href=\"https://angularjs.org/\">AngularJS</a>.</p><p>When you run <code>cassandra-web</code> script, it starts a <a href=\"http://code.macournoyer.com/thin/\">Thin web server</a> on a specified address, which defaults to <code>localhost:3000</code>. Openning <code>http://localhost:3000</code>, or whatever address you've specified in the browser, loads the AngularJS application and it starts interacting with the HTTP API of <code>cassandra-web</code>. This api uses the <a href=\"http://datastax.github.io/ruby-driver/\">Ruby Driver</a> to communicate with an <a href=\"http://cassandra.apache.org/\">Apache Cassandra</a> cluster.</p><p>When the frontend has fully loaded, <a href=\"https://github.com/avalanche123/cassandra-web/blob/master/app/public/js/cassandra.js#L108\">it subscribes to <code>/events</code> API endpoint</a>, and begins receiving <a href=\"http://www.w3.org/TR/2012/WD-eventsource-20120426/\">Server Sent Events</a>. <a href=\"https://github.com/avalanche123/cassandra-web/blob/master/app/helpers/sse.rb#L43-L56\">The API uses an event listener, which is registered with the <code>Cluster</code> instance created by the Ruby Driver, to stream events</a> such as <a href=\"https://github.com/avalanche123/cassandra-web/blob/master/app/helpers/sse.rb#L29-L39\">schema</a> and <a href=\"https://github.com/avalanche123/cassandra-web/blob/master/app/helpers/sse.rb#L13-L27\">node status</a> changes to update the user interface without having to refresh the page.</p><p>You can see this feature in action by creating a keyspace using the execute button in the top-right corner of the UI and executing the following statement:</p><pre>CREATE KEYSPACE example WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}\n</pre><p>If the statement executed successfully, you should see a new keyspace show up on the left side of the UI.</p><p><img src=\"https://raw.githubusercontent.com/avalanche123/cassandra-web/master/animation.gif\" alt=\"Alt text\" title=\"Create Keyspace\" /></p><p>The web server, Thin, used by <code>cassandra-web</code> is asynchronous and uses only a single thread to handle requests. This enables efficient handling multiple of long running connections, which is a requirement for streaming and Server Sent Events, but also means that the application cannot perform blocking operations during request handling, since it would hang up all connections for the duration of the blocking operation. <code>cassandra-web</code> therefore uses Asynchronous Execution feature of the Ruby Driver to not block on statements execution. <a href=\"https://github.com/avalanche123/cassandra-web/blob/master/app.rb#L88\">The application executes statements asynchronously, receiving a future from the Ruby Driver</a>. <a href=\"https://github.com/avalanche123/cassandra-web/blob/master/app/helpers/async.rb#L7-L40\">It then registers future completion listeners to send a response (or error) whenever it becomes available</a>.</p><h2>\nCredits</h2><p>Cassandra web is possible because of the following awesome technologies (in no particular order):</p><ul><li><a href=\"http://cassandra.apache.org/\">Apache Cassandra</a></li>\n<li><a href=\"http://datastax.github.io/ruby-driver/\">DataStax Ruby Driver for Apache Cassandra</a></li>\n<li><a href=\"https://github.com/sinatra/sinatra\">Sinatra</a></li>\n<li><a href=\"https://angularjs.org/\">AngularJS</a></li>\n<li><a href=\"http://getbootstrap.com/\">Twitter Bootstrap</a></li>\n<li><a href=\"http://code.macournoyer.com/thin/\">Thin</a></li>\n<li><a href=\"http://www.w3.org/TR/2012/WD-eventsource-20120426/\">Server Sent Events</a></li>\n<li><a href=\"http://prismjs.com/\">PrismJS</a></li>\n<li><a href=\"http://codemirror.net/\">CodeMirror</a></li>\n<li>and many others...</li>\n</ul>",
        "created_at": "2018-08-02T00:25:15+0000",
        "updated_at": "2018-08-02T00:25:24+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "avalanche123.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11761"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          },
          {
            "id": 1025,
            "label": "grafana",
            "slug": "grafana"
          },
          {
            "id": 1108,
            "label": "graphite",
            "slug": "graphite"
          }
        ],
        "is_public": false,
        "id": 11740,
        "uid": null,
        "title": "SoftwareMill blog: Cassandra Monitoring - part II - Graphite/InfluxDB & Grafana on Docker",
        "url": "https://softwaremill.com/cassandra-monitoring-part-2/",
        "content": "<p><em>This is the second part of the Cassandra Monitoring miniseries, index of all parts below:</em></p><ol><li><em><a href=\"https://softwaremill.com/cassandra-monitoring-part-1/\">Cassandra Monitoring - part I - Introduction</a></em></li>\n<li><em><a href=\"https://softwaremill.com/cassandra-monitoring-part-2/\">Cassandra Monitoring - part II - Graphite/InfluxDB &amp; Grafana on Docker</a></em></li>\n</ol><p>In this blogpost we will continue exploring the topic of <a href=\"http://cassandra.apache.org/\">Cassandra</a> metric reporters mentioned in <a href=\"https://softwaremill.com/cassandra-monitoring-part-1/\">Part I</a>. Our goal is to configure a reporter that sends metrics to an external time series database. For visualization we will use <a href=\"http://grafana.org/\">Grafana</a>, which can read data directly from various time series databases. We are going to heavily leverage <a href=\"https://www.docker.com/\">Docker</a>, so that we can omit the irrelevant setup details of various projects. To make it easier to set up a full working example, we have prepared a <a href=\"https://www.docker.com/products/docker-compose\">Docker Compose</a> script in our <a href=\"https://github.com/softwaremill/cassandra-monitoring\">GitHub repository</a>.</p><p>As a prerequisite for following this post, please install <a href=\"https://docs.docker.com/engine/installation/\">Docker</a> and <a href=\"https://docs.docker.com/compose/\">Docker Compose</a> on your machine. For instructions please see the linked docs.</p><p>We are going to describe two configuration variants - Cassandra-Graphite-Grafana and Cassandra-InfluxDB-Grafana over the Graphite protocol.</p><h2 id=\"network\">Network</h2><p>Both variants will require 3 containers each. Cassandra and Grafana will need access to the time series store. Links between Docker containers are currently a <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/default_network/dockerlinks/\">legacy</a> feature, so we are going to use <a href=\"https://docs.docker.com/v1.11/engine/userguide/networking/dockernetworks/\">Docker networking</a>. We are going to create just one network and attach all containers to it. Each of the containers will be accessible in our network under a hostname identical to their name.</p><p>To create the network you need to run:</p><pre>docker network create monitoring-network\n</pre><p>Note:\nIn our Docker Compose example scripts we do not set up a network explicitly, because it generates one <a href=\"https://docs.docker.com/v1.11/compose/networking/\">automatically</a> for the application.</p><p>Let's start with setting up a time series database. We will now describe two mutually exclusive configuration options. Remember that you only need one of them to get the monitoring working.</p><h3 id=\"graphite\">Graphite</h3><p>Unfortunately there is no official <a href=\"https://graphiteapp.org/\">Graphite</a> Docker image, so we have to use one of the non-official ones from <a href=\"https://hub.docker.com/r/sitespeedio/graphite/\">Docker Hub</a>.</p><p>In order to run the selected image, execute:</p><pre>docker run -d  -p 8080:80 -p 2003:2003 --net monitoring-network \\\n    --name graphite sitespeedio/graphite:0.9.14\n</pre><p>This means that we are running the <code>sitespeedio/graphite:0.9.14</code> image, creating a container named <code>graphite</code>, attached to network <code>monitoring-network</code>, mapping the container’s internal ports to local <code>8080</code> and <code>2003</code>. Everything is going to work in the background (<code>-d</code>).</p><p>The <code>http://localhost:8080</code> presents a simple web interface. Port <code>2003</code> is for the API. Default user and password is <code>guest</code> and <code>guest</code>.</p><p>Graphite installation without Docker is a bit complicated, so one of the Graphite developers has created a separate <a href=\"https://github.com/obfuscurity/synthesize\">project</a> purely for making the installation easier.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-graphite.71ca7b7e.png\" alt=\"Graphite Web UI\" /></p><p>Although the UI is a bit outdated, it is still capable of drawing graphs; moreover, Graphite itself also provides a graph rendering API. Graphite is a very <a href=\"http://graphite.readthedocs.io/en/latest/overview.html\">powerful</a> tool and it <a href=\"https://graphite.readthedocs.io/en/0.9.15/functions.html\">implements numerous functions</a> which can be applied to the data.</p><p>Messages sent to Graphite have a very simple format: <code>metric_path value timestamp\\n</code>. This means that a single metric path can have only a single value for a given timestamp.</p><h3 id=\"influxdb\">InfluxDB</h3><p><a href=\"https://influxdata.com/time-series-platform/influxdb/\">InfluxDB</a> is a relatively new <a href=\"https://github.com/influxdata/influxdb\">open source</a> time series database written in <a href=\"https://golang.org/\">Go</a>. The 1.0 release is planned for summer 2016. Official InfluxDB Docker images are available on <a href=\"https://hub.docker.com/_/influxdb/\">Docker Hub</a>. In this post we are using InfluxDB 0.13.0.</p><p>In our demonstration we are going to report data from Cassandra using the Graphite format, so we need to enable InfluxDB support for receiving data in this format. In order to do that, we have to pass one additional environment variable when creating the container:</p><pre>docker run -d -p 8083:8083 -p 8086:8086 \\\n    -e INFLUXDB_GRAPHITE_ENABLED=true \\\n    --net monitoring-network  \\\n    --name influxdb  \\\n    influxdb:0.13.0\n</pre><p>The <code>http://localhost:8083</code> presents a simple web UI. Port <code>8086</code> is for the API. Default user and password is <code>admin</code> and <code>admin</code>.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-influxdb.38631e4c.png\" alt=\"InfluxDB Web UI\" /></p><p>InfluxDB <a href=\"https://docs.influxdata.com/influxdb/v0.13/concepts/key_concepts/\">data model</a> can be more complex than Graphite. For a given timestamp and measurement name, it is possible to store multiple fields and, additionally, every measurement can be described using multiple tags. Tags are indexed, so it is possible to query measurements efficiently, e.g. for a specific node or environment. However, in this blog post we are configuring InfluxDB to receive data using the Graphite protocol, so it does not leverage all these features. A single measurement for a given timestamp only contains one column named <code>value</code>.</p><p>Querying data stored in InfluxDB is possible via <a href=\"https://docs.influxdata.com/influxdb/v0.13/query_language/data_exploration/\">InfluxQL</a>, which is an SQL-like language.</p><h2 id=\"cassandra\">Cassandra</h2><p>Official Cassandra images exist on <a href=\"https://hub.docker.com/_/cassandra/\">Docker Hub</a> - however,  they do not satisfy our needs. There are 3 things which need to be done in order to report metrics from Cassandra to an external system:</p><ol><li>Cassandra distribution is minimalistic and does not include the JAR used for Graphite reporting. For Cassandra &gt;= 2.2 the <a href=\"http://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-graphite/3.1.0/metrics-graphite-3.1.0.jar\"><code>metrics-graphite-3.1.0.jar</code></a> is required and for &lt;= 2.1 the <a href=\"http://repo1.maven.org/maven2/com/yammer/metrics/metrics-graphite/2.2.0/metrics-graphite-2.2.0.jar\"><code>metrics-graphite-2.2.0.jar</code></a>.</li>\n<li>Reporting <a href=\"https://github.com/apache/cassandra/blob/trunk/conf/metrics-reporter-config-sample.yaml\">configuration</a> needs to be created.</li>\n<li>Edit <code>cassandra-env.sh</code> in order to add the following line: <code>JVM_OPTS=\"$JVM_OPTS -Dcassandra.metricsReporterConfigFile=&lt;reporting-configuration&gt;.yaml\"</code> with the path to the configuration created in the previous step.</li>\n</ol><p><br />In this example we are going to use Cassandra 3.7.</p><p>Adding required files is pretty simple when you use a standalone Cassandra distribution; however, for Docker we need to create a new image. We will go with a \"minimalistic\" approach and extend the official one. To build the image, you need to create a directory in which we will place a <code>Dockerfile</code>, reporting configuration and the appropriate <code>.jar</code>.</p><h3 id=\"cassandra-graphite-grafana-\">Cassandra-Graphite-Grafana:</h3><p><code>Dockerfile</code>:</p><pre>FROM cassandra:3.7\nCOPY graphite.yaml /etc/cassandra/\nRUN echo \"JVM_OPTS=\\\"\\$JVM_OPTS -Dcassandra.metricsReporterConfigFile=graphite.yaml\\\"\" \\\n    &gt;&gt; /etc/cassandra/cassandra-env.sh\nCOPY metrics-graphite-3.1.0.jar /usr/share/cassandra/lib/\n</pre><p><code>graphite.yaml</code>:</p><pre>graphite:\n-\n  period: 60\n  timeunit: 'SECONDS'\n  prefix: 'Node1'\n  hosts:\n  - host: 'graphite'\n    port: 2003\n  predicate:\n    color: \"white\"\n    useQualifiedName: true\n    patterns:\n    - \".*\"\n</pre><h3 id=\"cassandra-influxdb-grafana-over-graphite-protocol-\">Cassandra-InfluxDB-Grafana over Graphite protocol:</h3><p><code>Dockerfile</code>:</p><pre>FROM cassandra:3.7\nCOPY influxdb.yaml /etc/cassandra/\nRUN echo \"JVM_OPTS=\\\"\\$JVM_OPTS -Dcassandra.metricsReporterConfigFile=influxdb.yaml\\\"\" \\\n    &gt;&gt; /etc/cassandra/cassandra-env.sh\nCOPY metrics-graphite-3.1.0.jar /usr/share/cassandra/lib/\n</pre><p><code>influxdb.yaml</code>:</p><pre>graphite:\n-\n  period: 60\n  timeunit: 'SECONDS'\n  prefix: 'Node1'\n  hosts:\n  - host: 'influxdb'\n    port: 2003\n  predicate:\n    color: \"white\"\n    useQualifiedName: true\n    patterns:\n    - \".*\"\n</pre><h3 id=\"differences\">Differences</h3><p>Both of these setups use the Graphite protocol, so they are almost identical. The only difference is the hostname of the time series database. For Graphite, we reference the <code>graphite</code> container via its hostname and, for InfluxDB, the <code>influxdb</code> hostname. The <code>prefix</code> value will be prepended to every metric name - this way you can make metrics coming from different nodes distinguishable.</p><p>Additionally, in both cases you need to put the <a href=\"http://repo1.maven.org/maven2/io/dropwizard/metrics/metrics-graphite/3.1.0/metrics-graphite-3.1.0.jar\"><code>metrics-graphite-3.1.0.jar</code></a> in the same directory with the <code>Dockerfile</code> and reporting configuration.</p><p>Both of these Cassandra scenarios are available on GitHub\nfor <a href=\"https://github.com/softwaremill/cassandra-monitoring/tree/master/part-II-cassandra-graphite-grafana/cassandra-graphite\">Graphite</a> and for <a href=\"https://github.com/softwaremill/cassandra-monitoring/tree/master/part-II-cassandra-influx-via-graphite-grafana/cassandra-influx-via-graphite\">InfluxDB over Graphite protocol</a>.</p><h3 id=\"running\">Running</h3><p>In order to build the image, you have to execute the following command in the directory with the <code>Dockerfile</code>:</p><pre>docker build -t cassandra-graphite .\n</pre><p>Then create the container and attach it to our network:</p><pre>docker run -d -p 9042:9042 --net monitoring-network \\\n    --name cassandra-graphite cassandra-graphite\n</pre><p>Please watch Cassandra logs for errors with <code>docker logs -f cassandra-graphite</code>. Every 60 seconds Cassandra should report its metrics to the corresponding time series database. You can check it in the web UI or using the CLI.</p><p>For Graphite, open <code>http://localhost:8080</code> and look at the tree on the left side:</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-graphite-with-metrics.4770ac5d.png\" alt=\"Graphite with Metics\" /></p><p>For InfluxDB, open <code>http://localhost:8083</code>, in the right upper corner set the <code>graphite</code> database (the database created by InfluxDB-Graphite integration is by default called <code>graphite</code>) and execute <code>SHOW MEASUREMENTS</code> query:</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-influx-with-metrics.1db87f9e.png\" alt=\"InfluxDB with Metics\" /></p><h2 id=\"grafana\">Grafana</h2><p>Fortunately for Grafana there are official images on <a href=\"https://hub.docker.com/r/grafana/grafana/\">Docker Hub</a>. In this post we are using Grafana 3.1.0. We already have Cassandra running, metrics are being stored in Graphite or InfluxDB, so as the last step we need to visualize them.</p><p>In order to run Grafana execute:</p><pre>docker run -d -p 3000:3000 --net monitoring-network --name grafana grafana/grafana:3.1.0\n</pre><p>Grafana UI should be working under <code>http://localhost:3000</code>. Default user and password is <code>admin</code> and <code>admin</code>.  If you haven't used Grafana before see the <a href=\"http://docs.grafana.org/guides/gettingstarted/\">Getting started</a> guide for UI walkthrough.</p><p>In order to draw a graph you first need to create a Data Source referring to the specific time series database. You can click it through Grafana UI or just execute the API e.g. using <code>curl</code>:</p><h3>Graphite</h3><pre>curl 'http://admin:admin@127.0.0.1:3000/api/datasources' -X POST \\\n-H 'Content-Type: application/json;charset=UTF-8' \\\n--data-binary '{\"name\":\"graphite\",\"type\":\"graphite\",\"url\":\"http://graphite:80\",\n\"access\":\"proxy\",\"isDefault\":true,\n\"basicAuth\":true,\"basicAuthUser\":\"guest\",\"basicAuthPassword\":\"guest\"}'\n</pre><p>Url <code>http://graphite:80</code> refers to Graphite container hostname.</p><h3>InfluxDB</h3><pre>curl 'http://admin:admin@127.0.0.1:3000/api/datasources' -X POST \\\n-H 'Content-Type: application/json;charset=UTF-8' \\\n--data-binary '{\"name\":\"influx\",\"type\":\"influxdb\",\"url\":\"http://influxdb:8086\",\n\"access\":\"proxy\",\"isDefault\":true,\"database\":\"graphite\",\"user\":\"admin\",\"password\":\"admin\"}'\n</pre><p>Url <code>http://influxdb:8086</code> refers to InfluxDB container hostname.</p><h3 id=\"graphs\">Graphs</h3><p>Query editors in Grafana differ among various data sources. As you can see below, for Graphite the editor does not display the whole metric list at once, but allows to browse it in a tree-like manner, similarly to the Graphite UI. It is possible to apply a function to a chosen metric. In contrast, for the InfluxDB data source Grafana offers a more SQL-like editor, where all metrics for all nodes are shown in a single list.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-grafana-graphite.de2257ab.png\" alt=\"Graphite Graphing\" />\nGraphite</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-grafana-influx.4141607b.png\" alt=\"InfluxDB Graphing\" />\nInfluxDB</p><p>We will focus more on graphing in one of the next parts of this blog series.</p><p>This post does not present a full, production ready configuration. When using Docker, it is important to remember that the data stored inside the container might be lost e.g. when a new version of an application is deployed. That is why it is recommended to use <a href=\"https://docs.docker.com/engine/tutorials/dockervolumes/\">Docker Volumes</a> for storing data. For more details please also read the docs of the specific Docker images.</p><p>Additionally be aware that by default Docker <a href=\"http://blog.viktorpetersson.com/post/101707677489/the-dangers-of-ufw-docker\">adds its own <code>iptables</code> rules</a> potentially making the ports exposed by a container accessible from remote hosts.</p><p>In production you must never use default passwords, always remember to change them!</p><p>Monitoring Cassandra using Graphite or InfluxDB (with data sent over Graphite protocol) is very similar. There are differences on the Grafana level, where Graphite integration seems to be a bit more mature. However, Graphite development was practically <a href=\"https://news.ycombinator.com/item?id=8740021\">dead</a> over the last years. Grafana creators have recently posted <a href=\"http://grafana.org/blog/2016/07/06/dear-graphite-users-and-developers.html\">a piece of information</a> that they want to revive Graphite. In contrast, InfluxDB undergoes a very intensive development process, which e.g. improves performance with almost every release.</p><p>If you'd like to quickly run one of the described variants, see our <a href=\"https://github.com/softwaremill/cassandra-monitoring\">GitHub</a> leveraging <a href=\"https://www.docker.com/products/docker-compose\">Docker Compose</a>.</p><p>In the next part of this series we will focus on the Cassandra-InfluxDB-Grafana stack. We will show improvements over Cassandra-InfluxDB reporting (omitting Graphite protocol) and the resulting advantages on the Grafana level.</p>",
        "created_at": "2018-08-01T01:13:43+0000",
        "updated_at": "2018-08-01T01:13:58+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 9,
        "domain_name": "softwaremill.com",
        "preview_picture": "https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-2b-graphite.71ca7b7e.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11740"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          }
        ],
        "is_public": false,
        "id": 11739,
        "uid": null,
        "title": "SoftwareMill blog: Cassandra Monitoring - part I - Introduction",
        "url": "https://softwaremill.com/cassandra-monitoring-part-1/",
        "content": "<p><em>This is the first part of the Cassandra Monitoring miniseries, index of all parts below:</em></p><ol><li><em><a href=\"https://softwaremill.com/cassandra-monitoring-part-1/\">Cassandra Monitoring - part I - Introduction</a></em></li>\n<li><em><a href=\"https://softwaremill.com/cassandra-monitoring-part-2/\">Cassandra Monitoring - part II - Graphite/InfluxDB &amp; Grafana on Docker</a></em></li>\n</ol><p>In this series we would like to focus on the Cassandra NoSQL database monitoring. If you would like to read more about general metric collection then you can find a great post on the <a href=\"https://www.datadoghq.com/blog/monitoring-101-collecting-data/\">DataDog Blog</a>. Here, we are not going to focus on <strong>what</strong> specifically you can gather from Cassandra, but rather <strong>how</strong>. Again, for details about different Cassandra metrics see the <a href=\"https://www.datadoghq.com/blog/how-to-monitor-cassandra-performance-metrics/\">another DataDog  blogpost</a>.\nIn the upcoming parts we are also going to present our open source contributions which make Cassandra monitoring easier and more effective.</p><p>Everybody who uses Cassandra knows <a href=\"http://docs.datastax.com/en/cassandra/3.x/cassandra/tools/toolsNodetool.html\"><code>nodetool</code></a>. It is a basic tool, bundled in the Cassandra distribution, for node management and <a href=\"http://docs.datastax.com/en/cassandra/3.0/cassandra/operations/opsMonitoring.html?scroll=opsMonitoring__opsMonitoringNodetool\">statistics gathering</a>. Under the hood it is just a Python console application. Nodetool shows <a href=\"https://www.datadoghq.com/blog/how-to-monitor-cassandra-performance-metrics/\">cluster status, compactions, bootstrap streams and much more</a>. It is a very important source of information, but it's just a CLI tool without any storage or visualization capabilities. For comfortable monitoring, and to get a better understanding of what hides behind all these numbers, we need something more, preferably with a GUI.</p><p>It is worth noting that Cassandra commiters <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-11939\">find it important</a> to not change output structure of <code>nodetool</code>, because people might have scripts based on them.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-1-nodetool.b6516be0.png\" alt=\"Nodetool\" /></p><h2 id=\"jmx-reporters\">JMX &amp; Reporters</h2><p>Cassandra exposes all its metrics via JMX (by default on port <code>7199</code>).</p><p>JMX can be read e.g. with <code>jconsole</code> or <code>jvisualvm</code> with <code>VisualVM-MBeans plugin</code> (both tools bundled in JDK distributions).\nThe JMX interface also offers some management features! For example under <code>org.apache.cassandra.db.StorageService</code> you can find operations related to node removal, drain, table snapshoting and more.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-1-jmx.21763bc4.png\" alt=\"JMX operations\" /></p><p>Note: by default remote JMX is disabled. If you really need it, you can enable it in <code>cassandra-env.sh</code>.</p><p>For metrics gathering Cassandra internally leverages <a href=\"http://metrics.dropwizard.io/\"><code>io.dropwizard.metrics</code></a>  (only from version <code>2.2</code>, previously library was named <code>com.yammer.metrics</code> and to be more confusing  <code>io.dropwizard.metrics</code> uses <code>com.codahale.metrics</code> package names). Those are the metrics presented via JMX. However, it is possible to access them in a different way. Cassandra 2.0.2 and up allows to configure reporters, so that every configured period Cassandra forwards those metrics e.g. to <a href=\"https://graphiteapp.org/\">Graphite</a>. This is implemented by <a href=\"https://github.com/addthis/metrics-reporter-config\"><code>metrics-reporter-config</code></a> library  (see <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-4430\">CASSANDRA-4430</a>) and provides a nice automatic way to process metrics in different systems, store and display them or check for alarms.</p><p>We will cover the concept of reporters in more detail, in the next part of this blogpost series.</p><h2 id=\"datastax-opscenter\">DataStax OpsCenter</h2><p><a href=\"http://www.datastax.com/products/datastax-opscenter\">OpsCenter</a> is a monitoring and management solution. It is also capable of system monitoring. Every node needs to have an OpsCenter agent installed, which sends data to the main OpsCenter service, which in turn stores them in a Cassandra keyspace. It is recommended to have a separate Cassandra cluster for storing OpsCenter data, so that OpsCenter activity won't be seen among the presented metrics. The application is also able to manage the cluster, add/remove nodes and more. However, the \"free\" OpsCenter is compatible with the open source Cassandra up to version 2.1. The new OpsCenter 6.0 is available only for DataStax Enterprise 4.7+ (based on Cassandra 2.1) and 5.0 (based on Cassandra 3.0). <a href=\"http://docs.datastax.com/en/landing_page/doc/landing_page/compatibility.html?scroll=compatibilityDocument__opsc-compatibility\">The Documentation</a> shows more detailed compatibility matrix.</p><p>In other words if your cluster uses open source Cassandra 2.2 or 3.x then OpsCenter is not for you.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-1-opscenter.2a9791e8.jpg\" alt=\"OpsCenter\" /><a href=\"http://www.datastax.com/wp-content/themes/datastax-2014-08/images/products/OpsCenter-Screenshot-VisualMonitoringandTuning.jpg\">Source</a></p><h2 id=\"datadog\">DataDog</h2><p>In contrast, <a href=\"https://www.datadoghq.com/\">DataDog</a> is a SaaS solution. It is capable of monitoring, but also supports a lot of other databases and services. However it is <a href=\"https://www.datadoghq.com/pricing/\">free</a> for only no more than 5 hosts with major <a href=\"https://www.datadoghq.com/pricing/\">limitations</a>.</p><p>DataDog requires an agent installed on every Cassandra node. It reads Cassandra <a href=\"https://github.com/DataDog/dd-agent/blob/master/dogstream/cassandra.py\">logs</a> and metrics using JMX. The agent is <a href=\"https://github.com/DataDog/dd-agent\">open source</a> so you can check what exactly it’s doing.</p><p><img src=\"https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-1-datadog.acf7c047.png\" alt=\"DataDog\" /><a href=\"https://www.datadoghq.com/blog/how-to-monitor-cassandra-performance-metrics/\">Source</a></p><h2 id=\"conclusions\">Conclusions</h2><p>There are a lot of options for Cassandra monitoring (and management), however none of them are perfect. If you are still using open source Cassandra 2.1 or below, or DataStax Enterprise, then you can use OpsCenter. If you are open to Cloud and SaaS then DataDog monitoring might be for you. Otherwise, you might be interested in Cassandra reporters and solutions based on <a href=\"https://graphiteapp.org/\">Graphite</a> or <a href=\"https://influxdata.com/time-series-platform/influxdb/\">InfluxDB</a> and <a href=\"http://grafana.org/\">Grafana</a> which we will describe in the <a href=\"https://softwaremill.com/cassandra-monitoring-part-2/\">next parts</a> of this blog series. We will compare the different options and show how to configure them for different Cassandra versions.</p><p>If you want to dive deeper into the topic of metrics, then these links might be interesting for you (some quoted already in the article):</p><ul><li><a href=\"https://www.datadoghq.com/blog/monitoring-101-collecting-data/\">Monitoring 101: Collecting the right data</a></li>\n<li><a href=\"https://www.datadoghq.com/blog/how-to-monitor-cassandra-performance-metrics/\">How to monitor Cassandra performance metrics</a></li>\n<li><a href=\"https://www.datadoghq.com/blog/how-to-collect-cassandra-metrics/\">How to collect Cassandra metrics</a></li>\n<li><a href=\"https://www.datadoghq.com/blog/monitoring-cassandra-with-datadog/\">Monitoring Cassandra with Datadog</a></li>\n<li><a href=\"https://medium.com/@mlowicki/alternatives-to-datastax-opscenter-8ad893efe063#.wpetbdrj9\">Alternatives to DataStax OpsCenter</a></li>\n<li><a href=\"https://medium.com/@mlowicki/cassandra-metrics-and-their-use-in-grafana-1f0dc33f9cca#.37bkpooc4\">Cassandra metrics and their use in Grafana</a></li>\n</ul>",
        "created_at": "2018-08-01T01:13:35+0000",
        "updated_at": "2018-08-01T01:13:40+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 4,
        "domain_name": "softwaremill.com",
        "preview_picture": "https://softwaremill.com/images/uploads/2016/07/cassandra-monitoring-1-nodetool.b6516be0.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11739"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11658,
        "uid": null,
        "title": "DuyHai Doan - New Cassandra 3 features that change your (developer) life",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=qihbEBBs8mI",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/qihbEBBs8mI?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-07-28T16:46:32+0000",
        "updated_at": "2018-07-28T16:46:44+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/qihbEBBs8mI/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11658"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 894,
            "label": "distributed",
            "slug": "distributed"
          },
          {
            "id": 909,
            "label": "platform",
            "slug": "platform"
          }
        ],
        "is_public": false,
        "id": 11657,
        "uid": null,
        "title": "DuyHai DOAN - Big Data 101, all the foundations you need to bootstrap a new project in 2017",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=rmAnni-jajA",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/rmAnni-jajA?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-07-28T16:45:20+0000",
        "updated_at": "2018-07-28T16:45:36+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/rmAnni-jajA/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11657"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 391,
            "label": "big.data",
            "slug": "big-data"
          }
        ],
        "is_public": false,
        "id": 11656,
        "uid": null,
        "title": "DuyHai DOAN - Big Data 101, all the foundations you need to bootstrap a new project in 2017",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=rmAnni-jajA",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/rmAnni-jajA?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-07-28T16:45:15+0000",
        "updated_at": "2018-08-07T22:27:29+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/rmAnni-jajA/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11656"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 11612,
        "uid": null,
        "title": "A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad…",
        "url": "https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016",
        "content": "A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad…\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-1-638.jpg?cb=1474068828\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-1-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-1-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-1-1024.jpg?cb=1474068828\" alt=\"JON HADDAD&#10;THE LAST PICKLE&#10;LEARN DATA MODELING BY EXAMPLE&#10;THIS IS&#10;AWESOME!!!&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-2-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-2-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-2-1024.jpg?cb=1474068828\" alt=\"WHAT’S THE LAST&#10;PICKLE DO?&#10;\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-3-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-3-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-3-1024.jpg?cb=1474068828\" alt=\"WE HELP MAKE YOU A&#10;TEAM OF EXPERTS&#10;\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-4-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-4-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-4-1024.jpg?cb=1474068828\" alt=\"&gt; 50 YEARS COMBINED&#10;EXPERIENCE&#10;\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-5-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-5-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-5-1024.jpg?cb=1474068828\" alt=\"WHO IS THIS GUY?&#10;\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-6-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-6-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-6-1024.jpg?cb=1474068828\" alt=\"15 YEARS&#10;EXPERIENCE&#10;\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-7-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-7-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-7-1024.jpg?cb=1474068828\" alt=\"4 YEARS WITH CASSANDRA&#10;\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-8-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-8-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-8-1024.jpg?cb=1474068828\" alt=\"LEARNING HOW TO&#10;CASSANDRA&#10;\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-9-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-9-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-9-1024.jpg?cb=1474068828\" alt=\"WHAT’S YOUR&#10;BACKGROUND?&#10;\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-10-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-10-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-10-1024.jpg?cb=1474068828\" alt=\"ORACLE!&#10;MYSQL!&#10;POSTGRES!&#10;\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-11-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-11-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-11-1024.jpg?cb=1474068828\" alt=\"CQL LOOKS&#10;LIKE SQL&#10;\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-12-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-12-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-12-1024.jpg?cb=1474068828\" alt=\"BAD&#10;ASSUMPTIONS&#10;\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-13-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-13-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-13-1024.jpg?cb=1474068828\" alt=\"3RD&#10;NORMAL&#10;FORM?&#10;\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-14-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-14-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-14-1024.jpg?cb=1474068828\" alt=\"WHERE’S&#10;MY&#10;JOINS?&#10;\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-15-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-15-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-15-1024.jpg?cb=1474068828\" alt=\"SECONDARY&#10;INDEX?&#10;\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-16-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-16-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-16-1024.jpg?cb=1474068828\" alt=\"DO IT WRONG&#10;TRY TO DATA MODELGET ANGRY&#10;WATCH VIDEOS &amp; READ&#10;\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-17-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-17-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-17-1024.jpg?cb=1474068828\" alt=\"EVERYTHING I&#10;KNOW IS WRONG&#10;\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-18-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-18-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-18-1024.jpg?cb=1474068828\" alt=\"LEARN BY&#10;EXAMPLE&#10;\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-19-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-19-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-19-1024.jpg?cb=1474068828\" alt=\"CASSANDRA DATASET&#10;MANAGER&#10;\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-20-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-20-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-20-1024.jpg?cb=1474068828\" alt=\"CDM&#10;\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-21-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-21-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-21-1024.jpg?cb=1474068828\" alt=\"APT FOR CASSANDRA&#10;DATA&#10;\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-22-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-22-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-22-1024.jpg?cb=1474068828\" alt=\"INSTALL DATA TO YOUR&#10;CASSANDRA CLUSTER&#10;\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-23-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-23-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-23-1024.jpg?cb=1474068828\" alt=\"cdm install &lt;dataset&gt;&#10;\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-24-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-24-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-24-1024.jpg?cb=1474068828\" alt=\"jhaddad@rustyrazorblade ~$ cdm list&#10;Starting CDM&#10;Datasets:&#10;movielens&#10;killrvideo&#10;killrweather&#10;Finished.&#10;\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-25-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-25-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-25-1024.jpg?cb=1474068828\" alt=\"jhaddad@rustyrazorblade ~$ cdm install movielens&#10;Starting CDM&#10;Installing movielens&#10;Checking for repo at /Users/jhaddad/.cd...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-26-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-26-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-26-1024.jpg?cb=1474068828\" alt=\"jhaddad@rustyrazorblade ~/dev/cassandra$ cqlsh&#10;Connected to Test Cluster at 127.0.0.1:9042.&#10;[cqlsh 5.0.1 | Cassandra 3.10-...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-27-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-27-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-27-1024.jpg?cb=1474068828\" alt=\"WHAT CAN WE DO WITH IT?&#10;▸ Learn by example&#10;▸ Blog posts / Tutorials&#10;▸ Jupyter notebooks&#10;▸ Reference applications&#10;▸ Data Mo...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-28-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-28-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-28-1024.jpg?cb=1474068828\" alt=\"MANAGING&#10;REFERENCE / TEST DATA&#10;\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-29-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-29-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-29-1024.jpg?cb=1474068828\" alt=\"DATASETS&#10;\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-30-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-30-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-30-1024.jpg?cb=1474068828\" alt=\"MOVIELENS&#10;\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-31-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-31-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-31-1024.jpg?cb=1474068828\" alt=\"DETAILS&#10;▸GroupLens Research Project&#10;▸University of Minnesota&#10;▸100K ratings&#10;▸1K users&#10;▸1700 movies&#10;\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-32-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-32-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-32-1024.jpg?cb=1474068828\" alt=\"cqlsh:movielens&gt; select id, avg_rating, genres, name&#10;... from movies limit 1;&#10;@ Row 1&#10;------------+-----------------------...\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-33-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-33-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-33-1024.jpg?cb=1474068828\" alt=\"cqlsh:movielens&gt; select * from users limit 1;&#10;@ Row 1&#10;------------+--------------------------------------&#10;id | b52fcdfc-0e...\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-34-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-34-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-34-1024.jpg?cb=1474068828\" alt=\"BLOG: WORKING&#10;RELATIONALLY WITH&#10;CASSANDRA&#10;\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-35-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-35-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-35-1024.jpg?cb=1474068828\" alt=\"CONNECTING&#10;CASSANDRA&#10;DATA WITH&#10;GRAPHFRAMES&#10;\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-36-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-36-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-36-1024.jpg?cb=1474068828\" alt=\"cdm install killrweather&#10;\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-37-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-37-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-37-1024.jpg?cb=1474068828\" alt=\"Helena Edelson&#10;Patrick McFadin&#10;\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-38-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-38-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-38-1024.jpg?cb=1474068828\" alt=\"cdm install killrvideo&#10;\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-39-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-39-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-39-1024.jpg?cb=1474068828\" alt=\"Luke Tillman&#10;Patrick McFadin&#10;\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-40-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-40-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-40-1024.jpg?cb=1474068828\" alt=\"UPCOMING&#10;DATA SETS&#10;\" /></i></section><section data-index=\"41\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-41-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-41-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-41-1024.jpg?cb=1474068828\" alt=\"openﬂights.org&#10;‣ airports&#10;‣ ﬂight data&#10;\" /></i></section><section data-index=\"42\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-42-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-42-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-42-1024.jpg?cb=1474068828\" alt=\"HEALTH CARE&#10;▸ Cancer Genome Atlas Project&#10;▸ Ebola cases&#10;▸ Healthcare ﬁnancial data&#10;▸ Dani Traphagen&#10;\" /></i></section><section data-index=\"43\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-43-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-43-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-43-1024.jpg?cb=1474068828\" alt=\"NYC TAXI DATA&#10;▸ pick up / drop off times &amp; locations&#10;▸ trip distances&#10;▸ itemized fares&#10;▸ rate types&#10;▸ payment types&#10;\" /></i></section><section data-index=\"44\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-44-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-44-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-44-1024.jpg?cb=1474068828\" alt=\"SOCIAL DATA&#10;▸Higgs Twitter Data&#10;▸Foursquare&#10;▸Enron executive emails&#10;\" /></i></section><section data-index=\"45\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-45-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-45-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-45-1024.jpg?cb=1474068828\" alt=\"HOW TO&#10;CONTRIBUTE&#10;\" /></i></section><section data-index=\"46\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-46-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-46-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-46-1024.jpg?cb=1474068828\" alt=\"https://github.com/riptano/cdm-java&#10;\" /></i></section><section data-index=\"47\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-47-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-47-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-47-1024.jpg?cb=1474068828\" alt=\"ADD FEATURES&#10;\" /></i></section><section data-index=\"48\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-48-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-48-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-48-1024.jpg?cb=1474068828\" alt=\"SUGGEST DATASETS&#10;\" /></i></section><section data-index=\"49\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-49-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-49-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-49-1024.jpg?cb=1474068828\" alt=\"CREATE A DATASET&#10;▸ create a git repo&#10;▸ datasets.yaml&#10;▸ schema.cql&#10;▸ insert data&#10;▸ “cdm dump”&#10;▸ cdm install .&#10;▸ create a PR...\" /></i></section><section data-index=\"50\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-50-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-50-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-50-1024.jpg?cb=1474068828\" alt=\"@RUSTYRAZORBLADE&#10;THANK YOU, KIND HUMANS&#10;\" /></i></section><section data-index=\"51\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-51-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-51-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-51-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"52\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-52-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-52-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-52-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"53\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-53-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-53-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-53-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"54\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-54-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-54-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-54-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"55\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-55-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-55-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-55-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"56\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-56-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-56-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-56-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"57\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-57-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-57-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-57-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><section data-index=\"58\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016\" data-small=\"https://image.slidesharecdn.com/cdm-160915175143/85/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-58-320.jpg?cb=1474068828\" data-normal=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-58-638.jpg?cb=1474068828\" data-full=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-58-1024.jpg?cb=1474068828\" alt=\"A Shortcut to Awesome: Cassandra Data Modeling By Example (Jon Haddad, The Last Pickle) | C* Summit 2016\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  1 Like\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"AnandRao21\" rel=\"nofollow\" href=\"https://www.slideshare.net/AnandRao21?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Anand Rao\n                            \n                              \n                                , \n                                Sr. SOA Architect\n                              \n                              \n                                 at \n                                Cengage Learning\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    JON HADDAD\nTHE LAST PICKLE\nLEARN DATA MODELING BY EXAMPLE\nTHIS IS\nAWESOME!!!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-2-638.jpg?cb=1474068828\" title=\"WHAT’S THE LAST&#10;PICKLE DO?&#10;\" target=\"_blank\">\n        2.\n      </a>\n    WHAT’S THE LAST\nPICKLE DO?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-3-638.jpg?cb=1474068828\" title=\"WE HELP MAKE YOU A&#10;TEAM OF EXPERTS&#10;\" target=\"_blank\">\n        3.\n      </a>\n    WE HELP MAKE YOU A\nTEAM OF EXPERTS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-4-638.jpg?cb=1474068828\" title=\"&gt; 50 YEARS COMBINED&#10;EXPERIENCE&#10;\" target=\"_blank\">\n        4.\n      </a>\n    &gt; 50 YEARS COMBINED\nEXPERIENCE\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-5-638.jpg?cb=1474068828\" title=\"WHO IS THIS GUY?&#10;\" target=\"_blank\">\n        5.\n      </a>\n    WHO IS THIS GUY?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-6-638.jpg?cb=1474068828\" title=\"15 YEARS&#10;EXPERIENCE&#10;\" target=\"_blank\">\n        6.\n      </a>\n    15 YEARS\nEXPERIENCE\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-7-638.jpg?cb=1474068828\" title=\"4 YEARS WITH CASSANDRA&#10;\" target=\"_blank\">\n        7.\n      </a>\n    4 YEARS WITH CASSANDRA\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-8-638.jpg?cb=1474068828\" title=\"LEARNING HOW TO&#10;CASSANDRA&#10;\" target=\"_blank\">\n        8.\n      </a>\n    LEARNING HOW TO\nCASSANDRA\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-9-638.jpg?cb=1474068828\" title=\"WHAT’S YOUR&#10;BACKGROUND?&#10;\" target=\"_blank\">\n        9.\n      </a>\n    WHAT’S YOUR\nBACKGROUND?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-10-638.jpg?cb=1474068828\" title=\"ORACLE!&#10;MYSQL!&#10;POSTGRES!&#10;\" target=\"_blank\">\n        10.\n      </a>\n    ORACLE!\nMYSQL!\nPOSTGRES!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-11-638.jpg?cb=1474068828\" title=\"CQL LOOKS&#10;LIKE SQL&#10;\" target=\"_blank\">\n        11.\n      </a>\n    CQL LOOKS\nLIKE SQL\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-12-638.jpg?cb=1474068828\" title=\"BAD&#10;ASSUMPTIONS&#10;\" target=\"_blank\">\n        12.\n      </a>\n    BAD\nASSUMPTIONS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-13-638.jpg?cb=1474068828\" title=\"3RD&#10;NORMAL&#10;FORM?&#10;\" target=\"_blank\">\n        13.\n      </a>\n    3RD\nNORMAL\nFORM?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-14-638.jpg?cb=1474068828\" title=\"WHERE’S&#10;MY&#10;JOINS?&#10;\" target=\"_blank\">\n        14.\n      </a>\n    WHERE’S\nMY\nJOINS?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-15-638.jpg?cb=1474068828\" title=\"SECONDARY&#10;INDEX?&#10;\" target=\"_blank\">\n        15.\n      </a>\n    SECONDARY\nINDEX?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-16-638.jpg?cb=1474068828\" title=\"DO IT WRONG&#10;TRY TO DATA MODELGET ANGRY&#10;WATCH VIDEOS &amp; READ&#10;\" target=\"_blank\">\n        16.\n      </a>\n    DO IT WRONG\nTRY TO DATA MODELGET ANGRY\nWATCH VIDEOS &amp; READ\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-17-638.jpg?cb=1474068828\" title=\"EVERYTHING I&#10;KNOW IS WRONG&#10;\" target=\"_blank\">\n        17.\n      </a>\n    EVERYTHING I\nKNOW IS WRONG\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-18-638.jpg?cb=1474068828\" title=\"LEARN BY&#10;EXAMPLE&#10;\" target=\"_blank\">\n        18.\n      </a>\n    LEARN BY\nEXAMPLE\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-19-638.jpg?cb=1474068828\" title=\"CASSANDRA DATASET&#10;MANAGER&#10;\" target=\"_blank\">\n        19.\n      </a>\n    CASSANDRA DATASET\nMANAGER\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-20-638.jpg?cb=1474068828\" title=\"CDM&#10;\" target=\"_blank\">\n        20.\n      </a>\n    CDM\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-21-638.jpg?cb=1474068828\" title=\"APT FOR CASSANDRA&#10;DATA&#10;\" target=\"_blank\">\n        21.\n      </a>\n    APT FOR CASSANDRA\nDATA\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-22-638.jpg?cb=1474068828\" title=\"INSTALL DATA TO YOUR&#10;CASSANDRA CLUSTER&#10;\" target=\"_blank\">\n        22.\n      </a>\n    INSTALL DATA TO YOUR\nCASSANDRA CLUSTER\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-23-638.jpg?cb=1474068828\" title=\"cdm install &lt;dataset&gt;&#10;\" target=\"_blank\">\n        23.\n      </a>\n    cdm install &lt;dataset&gt;\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-24-638.jpg?cb=1474068828\" title=\"jhaddad@rustyrazorblade ~$ cdm list&#10;Starting CDM&#10;Datasets:&#10;...\" target=\"_blank\">\n        24.\n      </a>\n    jhaddad@rustyrazorblade ~$ cdm list\nStarting CDM\nDatasets:\nmovielens\nkillrvideo\nkillrweather\nFinished.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-25-638.jpg?cb=1474068828\" title=\"jhaddad@rustyrazorblade ~$ cdm install movielens&#10;Starting C...\" target=\"_blank\">\n        25.\n      </a>\n    jhaddad@rustyrazorblade ~$ cdm install movielens\nStarting CDM\nInstalling movielens\nChecking for repo at /Users/jhaddad/.cdm/movielens\nPulling latest\nCDM is using dataset path: /Users/jhaddad/.cdm/movielens\ncqlsh -e \"DROP KEYSPACE IF EXISTS movielens; CREATE KEYSPACE movielens\nWITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1}\"\nSchema: /Users/jhaddad/.cdm/movielens/schema.cql\nLoading data\ncqlsh -k movielens -e \"COPY movies FROM '/Users/jhaddad/.cdm/movielens/data/\nmovies.csv'\"\ncqlsh -k movielens -e \"COPY users FROM '/Users/jhaddad/.cdm/movielens/data/\nusers.csv'\"\ncqlsh -k movielens -e \"COPY ratings_by_user FROM '/Users/jhaddad/.cdm/movielens\ndata/ratings_by_user.csv'\"\ncqlsh -k movielens -e \"COPY original_movie_map FROM '/Users/jhaddad/.cdm/\nmovielens/data/original_movie_map.csv'\"\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-26-638.jpg?cb=1474068828\" title=\"jhaddad@rustyrazorblade ~/dev/cassandra$ cqlsh&#10;Connected to...\" target=\"_blank\">\n        26.\n      </a>\n    jhaddad@rustyrazorblade ~/dev/cassandra$ cqlsh\nConnected to Test Cluster at 127.0.0.1:9042.\n[cqlsh 5.0.1 | Cassandra 3.10-SNAPSHOT | CQL spec 3.4.3 | Native protocol v4]\nUse HELP for help.\ncqlsh&gt; use movielens ;\ncqlsh:movielens&gt; desc tables;\nmovies users ratings_by_user original_movie_map ratings_by_movie\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-27-638.jpg?cb=1474068828\" title=\"WHAT CAN WE DO WITH IT?&#10;▸ Learn by example&#10;▸ Blog posts / T...\" target=\"_blank\">\n        27.\n      </a>\n    WHAT CAN WE DO WITH IT?\n▸ Learn by example\n▸ Blog posts / Tutorials\n▸ Jupyter notebooks\n▸ Reference applications\n▸ Data Models for presentations\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-28-638.jpg?cb=1474068828\" title=\"MANAGING&#10;REFERENCE / TEST DATA&#10;\" target=\"_blank\">\n        28.\n      </a>\n    MANAGING\nREFERENCE / TEST DATA\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-29-638.jpg?cb=1474068828\" title=\"DATASETS&#10;\" target=\"_blank\">\n        29.\n      </a>\n    DATASETS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-30-638.jpg?cb=1474068828\" title=\"MOVIELENS&#10;\" target=\"_blank\">\n        30.\n      </a>\n    MOVIELENS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-31-638.jpg?cb=1474068828\" title=\"DETAILS&#10;▸GroupLens Research Project&#10;▸University of Minnesot...\" target=\"_blank\">\n        31.\n      </a>\n    DETAILS\n▸GroupLens Research Project\n▸University of Minnesota\n▸100K ratings\n▸1K users\n▸1700 movies\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-32-638.jpg?cb=1474068828\" title=\"cqlsh:movielens&gt; select id, avg_rating, genres, name&#10;... fr...\" target=\"_blank\">\n        32.\n      </a>\n    cqlsh:movielens&gt; select id, avg_rating, genres, name\n... from movies limit 1;\n@ Row 1\n------------+--------------------------------------\nid | 76a38f64-94d8-4b8f-b830-a40af96f8d20\navg_rating | 3.16667\ngenres | {'Drama'}\nname | Little Lord Fauntleroy (1936)\n(1 rows)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-33-638.jpg?cb=1474068828\" title=\"cqlsh:movielens&gt; select * from users limit 1;&#10;@ Row 1&#10;-----...\" target=\"_blank\">\n        33.\n      </a>\n    cqlsh:movielens&gt; select * from users limit 1;\n@ Row 1\n------------+--------------------------------------\nid | b52fcdfc-0eaf-4432-9896-aa22db56edb2\naddress | 0322 Mattie Ramp Apt. 177\nage | 37\ncity | South Fremont\ngender | M\nname | Harrold Hills\noccupation | administrator\nzip | 06513\n(1 rows)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-34-638.jpg?cb=1474068828\" title=\"BLOG: WORKING&#10;RELATIONALLY WITH&#10;CASSANDRA&#10;\" target=\"_blank\">\n        34.\n      </a>\n    BLOG: WORKING\nRELATIONALLY WITH\nCASSANDRA\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-35-638.jpg?cb=1474068828\" title=\"CONNECTING&#10;CASSANDRA&#10;DATA WITH&#10;GRAPHFRAMES&#10;\" target=\"_blank\">\n        35.\n      </a>\n    CONNECTING\nCASSANDRA\nDATA WITH\nGRAPHFRAMES\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-36-638.jpg?cb=1474068828\" title=\"cdm install killrweather&#10;\" target=\"_blank\">\n        36.\n      </a>\n    cdm install killrweather\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-37-638.jpg?cb=1474068828\" title=\"Helena Edelson&#10;Patrick McFadin&#10;\" target=\"_blank\">\n        37.\n      </a>\n    Helena Edelson\nPatrick McFadin\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-38-638.jpg?cb=1474068828\" title=\"cdm install killrvideo&#10;\" target=\"_blank\">\n        38.\n      </a>\n    cdm install killrvideo\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-39-638.jpg?cb=1474068828\" title=\"Luke Tillman&#10;Patrick McFadin&#10;\" target=\"_blank\">\n        39.\n      </a>\n    Luke Tillman\nPatrick McFadin\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-40-638.jpg?cb=1474068828\" title=\"UPCOMING&#10;DATA SETS&#10;\" target=\"_blank\">\n        40.\n      </a>\n    UPCOMING\nDATA SETS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-41-638.jpg?cb=1474068828\" title=\"openﬂights.org&#10;‣ airports&#10;‣ ﬂight data&#10;\" target=\"_blank\">\n        41.\n      </a>\n    openﬂights.org\n‣ airports\n‣ ﬂight data\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-42-638.jpg?cb=1474068828\" title=\"HEALTH CARE&#10;▸ Cancer Genome Atlas Project&#10;▸ Ebola cases&#10;▸ H...\" target=\"_blank\">\n        42.\n      </a>\n    HEALTH CARE\n▸ Cancer Genome Atlas Project\n▸ Ebola cases\n▸ Healthcare ﬁnancial data\n▸ Dani Traphagen\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-43-638.jpg?cb=1474068828\" title=\"NYC TAXI DATA&#10;▸ pick up / drop off times &amp; locations&#10;▸ trip...\" target=\"_blank\">\n        43.\n      </a>\n    NYC TAXI DATA\n▸ pick up / drop off times &amp; locations\n▸ trip distances\n▸ itemized fares\n▸ rate types\n▸ payment types\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-44-638.jpg?cb=1474068828\" title=\"SOCIAL DATA&#10;▸Higgs Twitter Data&#10;▸Foursquare&#10;▸Enron executiv...\" target=\"_blank\">\n        44.\n      </a>\n    SOCIAL DATA\n▸Higgs Twitter Data\n▸Foursquare\n▸Enron executive emails\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-45-638.jpg?cb=1474068828\" title=\"HOW TO&#10;CONTRIBUTE&#10;\" target=\"_blank\">\n        45.\n      </a>\n    HOW TO\nCONTRIBUTE\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-46-638.jpg?cb=1474068828\" title=\"https://github.com/riptano/cdm-java&#10;\" target=\"_blank\">\n        46.\n      </a>\n    https://github.com/riptano/cdm-java\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-47-638.jpg?cb=1474068828\" title=\"ADD FEATURES&#10;\" target=\"_blank\">\n        47.\n      </a>\n    ADD FEATURES\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-48-638.jpg?cb=1474068828\" title=\"SUGGEST DATASETS&#10;\" target=\"_blank\">\n        48.\n      </a>\n    SUGGEST DATASETS\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-49-638.jpg?cb=1474068828\" title=\"CREATE A DATASET&#10;▸ create a git repo&#10;▸ datasets.yaml&#10;▸ sche...\" target=\"_blank\">\n        49.\n      </a>\n    CREATE A DATASET\n▸ create a git repo\n▸ datasets.yaml\n▸ schema.cql\n▸ insert data\n▸ “cdm dump”\n▸ cdm install .\n▸ create a PR on cdm-java\nOMG BEST\nDATASET EVER\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/cdm-160915175143/95/a-shortcut-to-awesome-cassandra-data-modeling-by-example-jon-haddad-the-last-pickle-c-summit-2016-50-638.jpg?cb=1474068828\" title=\"@RUSTYRAZORBLADE&#10;THANK YOU, KIND HUMANS&#10;\" target=\"_blank\">\n        50.\n      </a>\n    @RUSTYRAZORBLADE\nTHANK YOU, KIND HUMANS\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"66068424\" title=\"Train the Trainer\" href=\"https://www.linkedin.com/learning/train-the-trainer?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Train the Trainer\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Train the Trainer\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=AlyMjKo3rICZ%2Fa%2BXWxTdS9vSGcA%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kXCas-tafZXDqf8fcZLSiol4UeioAmQY7fOqvRDDnEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Train the Trainer</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66068424\" title=\"PowerPoint: Using Photos and Video Effectively for Great Presentations\" href=\"https://www.linkedin.com/learning/powerpoint-using-photos-and-video-effectively-for-great-presentations?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint: Using Photos and Video Effectively for Great Presentations\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint: Using Photos and Video Effectively for Great Presentations\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=3%2F2ZO%2Bb4W0FRGtSVNQkeStwUPPA%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gXyKq-NefYXPuecXdZLSioVkXcCsJlAAxeuyhRDXjE469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint: Using Photos and Video Effectively for Great Presentations</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66068424\" title=\"PowerPoint 2016: Tips and Tricks\" href=\"https://www.linkedin.com/learning/powerpoint-2016-tips-and-tricks?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint 2016: Tips and Tricks\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint 2016: Tips and Tricks\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=nIcokYvYRZ365i4sqNHaREMZ6bI%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lUyKj_tWfZH_ucMPfZLSiol8eeywDlAE0e-moQTPjFI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint 2016: Tips and Tricks</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"17917955\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra By Example: Data Modelling with CQL3\" href=\"https://www.slideshare.net/jericevans/cassandra-by-example-data-modelling-with-cql3\">\n    \n    <div class=\"related-content\"><p>Cassandra By Example: Data Modelling with CQL3</p><p>Eric Evans</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"105785146\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"How to Power Innovation with Geo-Distributed Data Management in Hybrid Cloud\" href=\"https://www.slideshare.net/DataStax/data-staxwebinar-howtopower-innovationwithgeodistributed-datainhybridcloudfor-slideshare\">\n    \n    <div class=\"related-content\"><p>How to Power Innovation with Geo-Distributed Data Management in Hybrid Cloud</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"102824145\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"How to Evaluate Cloud Databases for eCommerce\" href=\"https://www.slideshare.net/DataStax/how-to-evaluate-cloud-databases-for-ecommerce\">\n    \n    <div class=\"related-content\"><p>How to Evaluate Cloud Databases for eCommerce</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"95288307\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar: DataStax Enterprise 6: 10 Ways to Multiply the Power of Apache Cassandra™ Without the Complexity\" href=\"https://www.slideshare.net/DataStax/webinar-datastax-enterprise-6-10-ways-to-multiply-the-power-of-apache-cassandra-without-the-complexity-95288307\">\n    \n    <div class=\"related-content\"><p>Webinar: DataStax Enterprise 6: 10 Ways to Multiply the Power of Apache Cassa...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"93695860\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar: DataStax and Microsoft Azure: Empowering the Right-Now Enterprise with Real-Time Apps at Cloud Scale\" href=\"https://www.slideshare.net/DataStax/webinar-datastax-and-microsoft-azure-empowering-the-rightnow-enterprise-with-realtime-apps-at-cloud-scale-93695860\">\n    \n    <div class=\"related-content\"><p>Webinar: DataStax and Microsoft Azure: Empowering the Right-Now Enterprise wi...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"88770359\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar - Real-Time Customer Experience for the Right-Now Enterprise featuring Forrester Research\" href=\"https://www.slideshare.net/DataStax/webinar-realtime-customer-experience-for-the-rightnow-enterprise-featuring-forrester-research\">\n    \n    <div class=\"related-content\"><p>Webinar - Real-Time Customer Experience for the Right-Now Enterprise featurin...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"88713641\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Datastax - The Architect's guide to customer experience (CX)\" href=\"https://www.slideshare.net/DataStax/datastax-the-architects-guide-to-customer-experience-cx\">\n    \n    <div class=\"related-content\"><p>Datastax - The Architect's guide to customer experience (CX)</p><p>DataStax</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n    \n  \n    \n    \n  \n  \n  <noscript>\n    </noscript>",
        "created_at": "2018-07-27T03:41:36+0000",
        "updated_at": "2018-07-27T03:41:43+0000",
        "published_at": null,
        "published_by": [
          "DataStax"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/cdm-160915175143-thumbnail-4.jpg?cb=1474068828",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11612"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          }
        ],
        "is_public": false,
        "id": 11609,
        "uid": null,
        "title": "hawkular/cassalog",
        "url": "https://github.com/hawkular/cassalog",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\">\n<div id=\"user-content-preamble\">\n<div>\n<div>\n<p>Cassalog is a schema change management library and tool for\n<a href=\"http://cassandra.apache.org\" rel=\"nofollow\">Apache Cassandra</a> that can be used with\napplications running on the JVM.</p>\n</div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-why\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#why\"></a>Why?</h2>\n<div>\n<div>\n<p>Just as application code evolves and changes so do database schemas. If you are\nbuilding an application and intend to support upgrading from one version to\nanother, then managing schema changes is essential. If you are lucky, you might\nbe able to get by with running some simple upgrade scripts to bring the schema\nup to date with the new version. This likely will not work however if you\nsupport multiple upgrade paths. For example, suppose we have versions 1 and 2,\nand are introducing version 3 of an application. We want to allow upgrading to\nversion 3 from either 1 or 2 in addition to upgrading from 1 to 2.</p>\n</div>\n<div>\n<p>You could add schema upgrade logic to application code, but that is often a\nless that ideal solution as it convolutes the code base. Fortunately, there are\ntool for managing schema changes like <a href=\"http://www.liquibase.org/\" rel=\"nofollow\">Liquibase</a>,\n<a href=\"http://flywaydb.org/\" rel=\"nofollow\">Flyway</a>, and\n<a href=\"http://guides.rubyonrails.org/active_record_basics.html\" rel=\"nofollow\">Active Record</a> for Ruby\non Rails applications. These tools however, are designed specifically for\nrelational databases. I previously spent time trying to patch Liquibase to\nsupport Cassandra but found that it was not a good fit. Cassalog is designed\nsolely for use with Cassandra, not for any other database systems.</p>\n</div>\n<div>\n<p>Cassalog is written in Groovy. There are several reasons for this. First,\nGroovy offers great interoperability with Java, making it usable and accessible\nto application running on the JVM. Groovy’s dynamic and meta programming\nfeatures make it easy to write domain specific languages. Groovy has multi-line\nstrings and string interpolation out of the box, both of which can be really\nuseful for writing schema change scripts. Lastly, with Cassalog schema changes\nare not written in XML or JSON. Instead they are written as Groovy scripts\ngiving you the full power and flexibility of Groovy.</p>\n</div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-usage\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#usage\"></a>Usage</h2>\n<div>\n<div>\n<p>The Cassalog class is the primary class with which you will interact.</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-groovy\"><pre>// Groovy\ndef script = // load schema change script\ndef session = // obtain DataStax driver Session object\ndef cassalog = new Cassalog(session: session)\ncassalog.execute(script)</pre></div>\n</div>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-java\"><pre>// Java\nURI script = // load schema change script\nSession session = // obtain DataStax driver Session object\nCassalog cassalog = new Cassalog();\ncassalog.setSession(session);\ncassalog.execute(script);</pre></div>\n</div>\n</div>\n<div>\n<p>And here is what a cassalog script might look like,</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-groovy\"><pre>createKeyspace {\n  version '0.1'\n  name 'my_keyspace'\n  author 'admin'\n  description 'Set up a keyspace for unit tests'\n}\nschemaChange {\n  version '0.1.1'\n  author 'admin'\n  description 'Create table for storing time series data'\n  cql \"\"\"\nCREATE TABLE metrics (\n    id uuid,\n    time timeuuid,\n    value double,\n    PRIMARY KEY (id, time)\n)\n\"\"\"\n}</pre></div>\n</div>\n</div>\n<div>\n<table><tbody><tr><td>\n</td>\n<td>\nSchema changes are applied in the order that they are declared in the\nscript(s) regardless of the assigned versions.\n</td>\n</tr></tbody></table></div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-features\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#features\"></a>Features</h2>\n<div>\n<div>\n<ul><li>\n<p>Tagging</p>\n</li>\n<li>\n<p>Execute arbitrary Groovy / Java code in schema change scripts</p>\n</li>\n<li>\n<p>Pass variables to scripts</p>\n</li>\n<li>\n<p>Changes can stored across multiple scripts</p>\n</li>\n<li>\n<p>Schema change detection</p>\n</li>\n</ul></div>\n<div>\n<h3 id=\"user-content-tagging\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#tagging\"></a>Tagging</h3>\n<div>\n<p>You can specify tags when running Cassalog, e.g.</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-groovy\"><pre>// Groovy\ndef script = // load schema change script\ndef session = // obtain DataStax driver Session object\ndef cassalog = new Cassalog(session: session)\ncassalog.execute(script, ['dev', 'test_data'])</pre></div>\n</div>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-java\"><pre>// Java\nURI script = // load schema change script\nSession session = // obtain DataStax driver Session object\nCassalog cassalog = new Cassalog();\ncassalog.setSession(session);\ncassalog.execute(script, Collections.asList(\"dev\", \"test_data\"));</pre></div>\n</div>\n</div>\n<div>\n<p>Cassalog will apply schema changes that have not already been run and that</p>\n</div>\n<div>\n<ul><li>\n<p>Dot not specify any tags or</p>\n</li>\n<li>\n<p>Specify tags and include the <code>dev</code> and <code>test_data</code> tags</p>\n</li>\n</ul></div>\n</div>\n<div>\n<h3 id=\"user-content-execute-arbitrary-code\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#execute-arbitrary-code\"></a>Execute arbitrary code</h3>\n<div>\n<p>Cassandra is frequently used for time series data. Suppose we have a metrics\ntable, and we want to generate some sample data for tests.</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-groovy\"><pre>schemaChange {\n  version '1.0'\n  cql \"\"\"\nCREATE TABLE metrics (\n    id text PRIMARY KEY,\n    time timestamp,\n    value double\n)\n\"\"\"\n}\ntestData = []\nrandom = new Random\n10.times { i -&gt;\n  testData &lt;&lt; \"INSERT INTO metrics (id, time, value) VALUES ('$i', ${new Date().time + 100}, ${random.nextDouble()})\"\n}\nschemaChange {\n  version '1.0.1'\n  tags 'test_data'\n  cql testData\n}</pre></div>\n</div>\n</div>\n<div>\n<p>This script first calls the <code>schemaChange</code> function to create the metrics table.\nThe next few lines generate a list of INSERT statements with some test data.\nFinally, we have another call to <code>schemaChange</code>. It specifies the test_data\ntag and passes the <code>testData</code> list to the <code>cql</code> parameter.</p>\n</div>\n</div>\n<div>\n<h3 id=\"user-content-pass-variables-to-scripts\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#pass-variables-to-scripts\"></a>Pass variables to scripts</h3>\n<div>\n<p>You can pass arbitrary variables to scripts, not just strings.</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-groovy\"><pre>// Groovy\ndef vars = [\n  metricIds: ['M1', 'M2', 'M3'],\n  startDate: new Date()\n  maxValue: 100,\n  minValue: 50\n]\ncassalog.execute(script, vars)</pre></div>\n</div>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-java\"><pre>// Java\nMap&lt;String, ?&gt; vars = ImmutableMap.of(\n    \"metricIds\", asList(\"M1\", \"M2\", \"M3\"),\n    \"startDate\", new Date(),\n    \"maxValue\", 100,\n    \"minValue\", 50\n);\ncassalog.execute(script, vars);</pre></div>\n</div>\n</div>\n</div>\n<div>\n<h3 id=\"user-content-changes-can-stored-across-multiple-scripts\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#changes-can-stored-across-multiple-scripts\"></a>Changes can stored across multiple scripts</h3>\n<div>\n<p>You can use the <code>include</code> function to store changes in multiple script to\nkeep your schema changes more modular and better organized.</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-groovy\"><pre>include '/dbchanges/base_tables.groovy'\ninclude '/dbchanges/seed_data.groovy'</pre></div>\n</div>\n</div>\n<div>\n<p>The <code>include</code> function currently takes a single string argument that should\nspecify the absolute path of a script on the classpath or from the configured <code>baseScriptsPath</code>.</p>\n</div>\n<div>\n<p><code>baseScriptsPath</code> is an absolute path to where the other include scripts are located e.g. <code>/Users/john/cassalog/scripts</code>.</p>\n</div>\n</div>\n<div>\n<h3 id=\"user-content-schema-change-detection\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#schema-change-detection\"></a>Schema change detection</h3>\n<div>\n<p>Cassalog does not store the CQL code associated with each schema change. It\ncomputes a hash of the CQL and stores that instead. If the hash in the change\nlog differs from the hash of the CQL in the source script, Cassalog will throw\na ChangeSetAlteredException.</p>\n</div>\n<div>\n<p>You will need to manually resolve the issue that caused the\nChangeSetAlteredException. Cassandra does not support transactions like a\nrelational database, so there no rollback functionality to fall back on.</p>\n</div>\n</div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-change-log-table\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#change-log-table\"></a>Change Log Table</h2>\n<div>\n<div>\n<p>All schema changes are recorded in the change log table, <em>cassalog</em>. The table\nwill be created the first time Cassalog is run. Change log data looks like,</p>\n</div>\n<div>\n<div>\n<pre>bucket | revision | applied_at               | author | description | hash         | version  | tags\n--------+----------+--------------------------+--------+-----------------------------------------------------+\n     0 |        0 | 2016-01-28 11:09:54-0500 | admin | First table  | 0xe361957eeb |      1.0 | {'legacy'}\n     0 |        1 | 2016-01-28 11:09:54-0500 | admin | Second table | 0xf336e725d4 |      1.1 | {'legacy'}\n     0 |        2 | 2016-01-28 11:09:55-0500 | admin | Third table  | 0xcecef5f840 |      1.2 | {'legacy', 'dev'}\n     0 |        3 | 2016-01-28 11:09:55-0500 | admin | Fourth table | 0x4b5d24b77c |      1.3 | {'legacy'}</pre>\n</div>\n</div>\n<div>\n<p>Here is a brief overview of the schema.</p>\n</div>\n<div>\n<div>\n<pre>CREATE TABLE cassalog (\n    bucket int,\n    revision int,\n    applied_at timestamp,\n    author text,\n    description text,\n    hash blob,\n    version text,\n    tags set&lt;text&gt;,\n    PRIMARY KEY (bucket, revision)\n)</pre>\n</div>\n</div>\n<div>\n<p><strong>author</strong><br />The username, or email address, etc. of the person making the change. This is\nan optional field and can be null.</p>\n</div>\n<div>\n<p><strong>description</strong><br />A summary of the changes. This is an optional field and can be null.</p>\n</div>\n<div>\n<p><strong>hash</strong><br />Cassalog does not store the CQL statements that it executes. Instead it stores a\nhash that uniquely identifies the CQL statement(s). Cassalog generates this\nhash value.</p>\n</div>\n<div>\n<p><strong>version</strong><br />The version can be an arbitrary string. It should be a unique identifier for the\nchange; however, Cassalog does not enforce uniqueness. This is a required field.</p>\n</div>\n<div>\n<p><strong>tags</strong><br />An optional set of user-supplied tags.</p>\n</div>\n<div>\n<p><strong>revision</strong><br />Cassalog assigns a revision number to each change that it applies. It uses the\nrevision number to keep track of the order in which changes are applied. If the\norder of schema changes in a source script is changed, then a\nChangeSetAlteredException will be thrown.</p>\n</div>\n<div>\n<p><strong>bucket</strong><br />Cassalog stores multiple rows per physical partition. This is a revision offset.\nThe bucket size defaults to 100.</p>\n</div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-building-from-source\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#building-from-source\"></a>Building from Source</h2>\n<div>\n<div>\n<p>Cassandra is built with Maven and requires a JVM version 1.7 or later. Test\nexecution requires a running Cassandra cluster (which can be a single node) with\na node listening on 127.0.0.1. Cassandra 2.0 or later should be used.</p>\n</div>\n<div>\n<div>\n<div class=\"highlight highlight-source-shell\"><pre>git clone https://github.com/jsanda/cassalog.git\ncd cassalog\nmvn install</pre></div>\n</div>\n</div>\n<div>\n<table><tbody><tr><td>\n</td>\n<td>\nIf you want to build without having a running Cassandra instance, you can\nrun <code>mvn install -DskipTests</code>\n</td>\n</tr></tbody></table></div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-setting-up-cassandra-for-development-or-testing\"><a class=\"anchor\" aria-hidden=\"true\" href=\"#setting-up-cassandra-for-development-or-testing\"></a>Setting up Cassandra for development or testing</h2>\n<div>\n<div>\n<p>As Cassalog evolves and looks to support different versions of Cassandra and\nCQL, ccm is the likely tool of choice to use for testing against different\nversions.</p>\n</div>\n</div>\n</div></article>",
        "created_at": "2018-07-26T19:39:40+0000",
        "updated_at": "2018-08-07T22:58:13+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/10179627?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11609"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 883,
            "label": "java",
            "slug": "java"
          },
          {
            "id": 1287,
            "label": "spring",
            "slug": "spring"
          }
        ],
        "is_public": false,
        "id": 11607,
        "uid": null,
        "title": "Spring Data for Apache Cassandra",
        "url": "http://projects.spring.io/spring-data-cassandra/",
        "content": "<p>The learning curve for developing applications with Apache Cassandra is significantly reduced when using Spring Data for Apache Cassandra. With the power to stay at a high level with annotated POJOs, or at a low level with high performance data ingestion capabilities, the Spring Data for Apache Cassandra templates are sure to meet every application need.</p><h2 id=\"features\">Features</h2><ul><li>Build repositories based on common Spring Data interfaces</li>\n<li>Support for synchronous and asynchronous data operations</li>\n<li>Asynchronous callback support</li>\n<li>Support for XML based Keyspace creation and CQL Table creation</li>\n<li>JavaConfig and XML Support for all Cluster and Session Capabilities</li>\n<li>Exception Translation to the familiar Spring DataAccessException hierarchy</li>\n<li>Convenient QueryBuilders to eliminate the need to learn CQL</li>\n<li>Automatic implementation of Repository interfaces including support for custom query methods</li>\n<li>Based on the latest DataStax Enterprise CQL Java Driver</li>\n</ul><h2 id=\"quick-start\">Quick Start</h2>",
        "created_at": "2018-07-26T19:39:19+0000",
        "updated_at": "2018-08-07T22:35:34+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 0,
        "domain_name": "projects.spring.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11607"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 33,
            "label": "internet.architecture",
            "slug": "internet-architecture"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 253,
            "label": "analytics",
            "slug": "analytics"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 956,
            "label": "streaming",
            "slug": "streaming"
          }
        ],
        "is_public": false,
        "id": 11605,
        "uid": null,
        "title": "Streaming Analytics Architecture Delivers Real-Time Insights",
        "url": "https://www.techworld.com/data/streaming-analytics-architecture-delivers-real-time-insights-3608526/",
        "content": "<p>Accenture had the opportunity to stand-up such a streaming analytics platform that delivers insights in near-real time meaning that the time from when sensor data arrives in the system until when insight is delivered happens in the order of seconds.</p><p>Recently, we were asked to help a client manage their water distribution network that had hundreds of thousands of tagged sensors – measuring for example, water flow and pressure.  This client had already embraced analytics for reporting and capacity planning supported by a batch architecture that easily met daily turn-around times to analyze all the data.  From the time sensor readings were ingested, the existing batch architecture could deliver insights into the capacity and health of their water network within 15 minutes.</p> \n<header class=\"articleHeader\"><figure><img src=\"https://cdn2.techworld.com/graphics/fallbackimages/30/Big-data-binary-code-futuristic_thumb800.jpg\" alt=\"Big data binary code futuristic\" /></figure></header> \n \n \n<p>The client was interested in adding real-time views onto the health and events along their water network to improve operational efficiency that ultimately improves customer satisfaction.  The objective was to present dynamic views of the current state of the water network refreshed on the order of seconds.</p> \n<p><img class=\"lz\" title=\"Accenture IoT illustration\" alt=\"Accenture IoT illustration\" height=\"858\" width=\"1388\" src=\"https://cdn3.techworld.com/cmsdata/blogentries/3608526/accenture.png\" /></p><noscript>\n<img title=\"Accenture IoT illustration\" alt=\"Accenture IoT illustration\" src=\"https://cdn3.techworld.com/cmsdata/blogentries/3608526/accenture.png\" height=\"858\" width=\"1388\" /></noscript> \n<p> To begin with, we focused on the detection and prediction of leakage events.  Their existing batch architecture was too slow; even a turnaround in minutes wastes precious moments before remediation efforts can begin.  Real-time monitoring combined with predictive algorithms manages operational pressure to proactively reduce high and low pressure spikes that serve as a primary cause of leaks.  These improvements result in avoiding leaks, prolonging the longevity of assets, and reducing disruption to customers.<br /> <br />For streaming analytics, the challenge lies in handling at speed the large number of parallel computations kicked-off by arriving sensor data.  Each time a sensor reading takes part in a number of online algorithms that need to be computed on the order of seconds.  We provide a sample of these algorithms implemented to give some insight into the complexity that the solution handles at real-time.</p> \n<p>First, arriving sensor data is not periodic and is assumed to be unreliable.  Different streams of sensor data come in at different time ticks (or intervals of time) be it on the order of days, minutes, or seconds.  Before computations and visualizations occur, we first need processing to normalize streamed data across different sources according to common time ticks (e.g., every 5 seconds or 1 minute). In the event of missed sensor readings, calculations estimate the values for the missed value to allow for operations to progress, but then require additional calculations for backfilling and reconciliation upon recovery of lost data (e.g., when network outage is restored or when a sensor comes back online).   </p> \n \n<p>To do prediction, an algorithm projects readings forward into time against a function.  An online curve-fitting algorithm continuously trains the function based on the arriving streamed data.  </p> \n<p>Further analytics includes processing complex events like when a series of sensor readings that convey various high and low thresholds on critical pressure points.  These indicate detection of a leakage event, which triggers follow-up actions.  <br />In addition, we implemented more complex computations like those for estimating impact of the most recent usage and operations on reservoir supply that combine streamed data with historical data (e.g., over the past 7-days) fetched from the data store.</p> \n<p>To handle these computations, we implemented a lambda architecture pattern  based on that of many of today’s internet companies like Twitter, Facebook, and LinkedIn.  In the same way that these internet companies balance real-time analytics on live social feeds against batch analytics over a vast quantity of historical user data, we do the same with balancing streaming and batch analytics on sensor data.  To that end, we added a parallel data flow for streaming analytics to the existing batch flow.</p> \n<p>To start, we created data publishers for pushing data from the client’s sensor network into our platform.  For this client, it meant getting data from their <a rel=\"nofollow\" target=\"_blank\" href=\"http://www.osisoft.com/\">OSI Pi historian</a> that serves as a gateway for a majority of the sensor data and for REST APIs.  We ingested this data into a stream processor<a rel=\"nofollow\" target=\"_blank\" href=\"http://aws.amazon.com/kinesis/\"> Amazon Web Services’ (AWS) Kinesis</a>.  <a rel=\"nofollow\" target=\"_blank\" href=\"http://spark.apache.org/\">Apache Spark Streaming</a> runs computations for real-time detection and analytics, and <a rel=\"nofollow\" target=\"_blank\" href=\"http://aws.amazon.com/dynamodb/\">AWS Dynamo</a> and stores the time-series data and results of our processing.  A Java API tier obfuscates data store interfaces for querying the results that are then visualized in a dashboard built using D3.</p> \n<p>Over the course of six weeks, the team created the reference implementation for the water network.  For the initial build-out we leveraged as-a-Service Amazon Web Services components that were ready as-a-Service for agility sake.  The team then hardened this architecture, re-platformed onto Apache Cassandra and Kafka to allow for an on-premise option in addition to our AWS cloud-based implementation, load tested it with ingestion data at over 3000 events per second, and packaged its components for reuse.  </p> \n<p>The result is an architecture proven for leakage detection in water networks, but is now prebuilt and ready to onboard new streaming analytics applications and domains.</p> \n \n \n<p> The next opportunity can reuse the same architecture and even the same sensor health algorithms (e.g., for dealing with unreliable sensor data) accelerating the time to standup a solution by allowing the project to focus on onboarding the domain specific algorithms.<br /> </p> \n<p><strong> Posted by Teresa Tung, Ph.D.  <a rel=\"nofollow\" target=\"_blank\" href=\"http://www.accenture.com\">Accenture </a>Technology Labs' Next Generation Software Architecture R&amp;D group</strong></p>",
        "created_at": "2018-07-26T19:36:49+0000",
        "updated_at": "2018-08-07T22:36:32+0000",
        "published_at": null,
        "published_by": [
          "Accenture Technology Labs Staff"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 4,
        "domain_name": "www.techworld.com",
        "preview_picture": "https://cdn2.techworld.com/graphics/fallbackimages/30/Big-data-binary-code-futuristic_thumb800.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11605"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 25,
            "label": "react",
            "slug": "react"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 177,
            "label": "graphql",
            "slug": "graphql"
          },
          {
            "id": 883,
            "label": "java",
            "slug": "java"
          },
          {
            "id": 956,
            "label": "streaming",
            "slug": "streaming"
          }
        ],
        "is_public": false,
        "id": 11604,
        "uid": null,
        "title": "Building a Custom Spark Connector for Near Real-Time Speech-to-Text Transcription - Developer Blog",
        "url": "https://www.microsoft.com/developerblog/2017/11/01/building-a-custom-spark-connector-for-near-real-time-speech-to-text-transcription/",
        "content": "<img src=\"https://www.microsoft.com/developerblog//wp-content/uploads/fortis-radio-to-text-spark-450x300.png\" alt=\"image\" /><p>The <a href=\"http://aka.ms/fortis\">Fortis project</a> is a social data ingestion, analysis, and visualization platform. Originally developed in collaboration with the <a href=\"https://www.unocha.org/\">United Nations Office for the Coordination of Humanitarian Affairs</a> (UN OCHA), Fortis provides planners and scientists with tools to gain insight from social media, public websites and custom data sources.  UN OCHA mobilizes humanitarian aid for crises such as famines, epidemics, and war.</p><p>To understand crisis situations and formulate appropriate mitigations, UN OCHA needs access to tremendous amounts of data and intelligence, from a wide range of sources including newspapers, social media, and radio.</p><p>Fortis already has support for social media and web content, but UN OCHA wanted to be able to incorporate content from local radio broadcasts. Working together, we created a solution based on Spark Streaming to extract textual information from radio in near real-time by developing a new <a href=\"https://github.com/CatalystCode/SpeechToText-WebSockets-Java/\">Java client</a> for Azure Cognitive Services’ speech-to-text APIs. This code story delves into our Fortis solution by providing examples of Spark Streaming custom receivers needed to consume Azure Cognitive Service’s speech-to-text API and showing how to integrate with Azure Cognitive Service’s speech-to-text protocol.</p><h2>A Spark Streaming pipeline for analyzing radio</h2><p>We covered the Fortis pipeline in a <a href=\"https://www.microsoft.com/developerblog/2017/05/10/graphql-providing-context-into-global-crisiss-and-social-public-data-sources/\">previous code story</a>. Below is a brief summary of the pipeline:</p><ol><li>Real-time data sources (such as Facebook, Twitter, and news feeds) generate events</li><li>Events are filtered and analyzed by Spark Streaming</li><li>Spark stores events and aggregations in Cassandra for reporting uses</li></ol><p>A NodeJS and GraphQL services layer sends the data from Cassandra to a ReactJS dashboard.</p><p><img class=\"alignnone size-large wp-image-4891\" src=\"https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-fortis-diagram-1024x447.png\" alt=\"Overview of the Fortis pipeline\" width=\"780\" height=\"340\" srcset=\"https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-fortis-diagram-1024x447.png 1024w, https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-fortis-diagram-300x131.png 300w, https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-fortis-diagram-768x335.png 768w\" /></p><p>To add radio analysis to this pipeline, we made three simplifying assumptions:</p><ul><li>The raw radio stream is freely accessible on the web via HTTP</li><li>The content published via the radio stream can be discretized into short segments</li><li>The radio content can be meaningfully analyzed via its textual transcription</li></ul><p>Once framed in this way, we see that we need two pieces to ingest radio into a Spark Streaming pipeline:</p><ol><li>A mechanism to get transcribed radio content into our Spark cluster for processing via our text-based analyses <a href=\"https://www.microsoft.com/developerblog/2017/05/10/graphql-providing-context-into-global-crisiss-and-social-public-data-sources/\">as described in a previous code story</a></li><li>A process to receive audio from a radio feed (such as the <a href=\"http://bbcwssc.ic.llnwd.net/stream/bbcwssc_mp1_ws-einws\">BBC World Service</a> or Radio France International) and convert it to text</li></ol><p>The following sections will cover each of these pieces.</p><h2>Consuming arbitrary APIs via Spark Streaming custom receivers</h2><p>Spark’s <a href=\"https://spark.apache.org/docs/latest/streaming-custom-receivers.html\">custom receivers</a> are a powerful mechanism to turn APIs into real-time data sources for Spark Streaming. A custom receiver needs to implement just a few methods. For example, in Scala:</p><p>After we define our custom receiver, we can now turn its underlying data source into a Spark stream that is easy to consume via the usual high-level Spark APIs:</p><p>We make heavy use of custom receivers in the Fortis project as a simple way to turn APIs into streaming data-sources. Fortis has published packages to integrate with the <a href=\"https://github.com/CatalystCode/streaming-facebook\">Facebook API</a>, <a href=\"https://github.com/CatalystCode/streaming-reddit\">Reddit API</a>, <a href=\"https://github.com/CatalystCode/streaming-instagram\">Instagram API</a>, <a href=\"https://github.com/CatalystCode/streaming-bing\">Bing News API</a> and <a href=\"https://github.com/CatalystCode/streaming-rss-html\">RSS feeds</a>. Similarly, the radio-to-text functionality described below is wrapped in a <a href=\"https://github.com/CatalystCode/project-fortis-spark/tree/f2b8d2dec955c5f78eed00ff884a86a40b5b255b/src/main/scala/com/microsoft/partnercatalyst/fortis/spark/sources/streamwrappers/radio\">custom radio receiver</a> to integrate it into our Spark pipeline.</p><h2>Converting radio to text in near real-time</h2><p>After solving the problem of how to integrate custom data sources into Spark, we next require a solution to convert a stream of audio, such as from an online radio broadcast, to a stream of text.</p><p>Azure Cognitive Services has been offering speech-to-text capabilities for more than 10 languages for a long time via the <a href=\"https://azure.microsoft.com/en-ca/services/cognitive-services/speech/\">Bing Speech API</a>. However, the API is based on a request-response paradigm which is not suited to our streaming use case as it would require us to buffer large audio clips in the radio receiver, send the chunks to the speech-to-text service and wait for a transcribed response. This approach would introduce latency and HTTP request overhead.</p><p>However, Azure Cognitive Services recently published a <a href=\"https://docs.microsoft.com/en-us/azure/cognitive-services/speech/api-reference-rest/websocketprotocol\">speech-to-text protocol</a> that works via WebSockets which are well suited for a streaming scenario. We simply open a single bi-directional communication channel via a WebSocket connection and use it to continuously send audio to the speech-to-text service and receive back the transcribed results via the same channel.</p><p>A reference implementation of this speech-to-text approach via WebSockets exists for <a href=\"https://github.com/Azure-Samples/SpeechToText-WebSockets-Javascript\">Javascript</a> and an unofficial implementation exists for <a href=\"https://github.com/noopkat/ms-bing-speech-service\">NodeJS</a>. We created a new implementation of the protocol in <a href=\"https://github.com/CatalystCode/SpeechToText-WebSockets-Java/\">Java</a> so that we can leverage the WebSocket speech-to-text from our Scala-on-Spark pipeline.</p><h3>How does the WebSocket speech-to-text protocol work?</h3><p>After creating a Bing Speech API resource in Azure and noting the access key, we’re ready to start implementing the speech-to-text WebSocket protocol.</p><p><img class=\"alignnone size-full wp-image-4892\" src=\"https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-azure-dashboard.png\" alt=\"Creating the required resources in Azure\" width=\"564\" height=\"271\" srcset=\"https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-azure-dashboard.png 564w, https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-azure-dashboard-300x144.png 300w\" /></p><p>The speech-to-text WebSocket protocol specifies three phases that clients must implement:</p><ol><li>Establishing the speech-to-text WebSocket connection</li><li>Transcribing audio via the speech-to-text WebSocket</li><li>Closing the speech-to-text WebSocket connection</li></ol><p>In the first phase, the client identifies itself with the speech-to-text service and establishes the WebSocket connection. After identification, the client is free to send audio to the speech-to-text service and receive back transcriptions. Eventually, the speech-to-text service will detect the end of the audio stream. When the end is detected, the speech-to-text service will send the client an alert that the audio stream is done getting messages. The client will respond to the service’s end-of-the-audio message by closing the WebSocket speech-to-text connection.</p><p>The full end-to-end flow is illustrated in the diagram below and each phase is described in more detail in the following sections.</p><p><img class=\"alignnone size-large wp-image-4893\" src=\"https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-tts-pipeline-1024x807.png\" alt=\"WebSocket speech-to-text messages flow\" width=\"780\" height=\"615\" srcset=\"https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-tts-pipeline-1024x807.png 1024w, https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-tts-pipeline-300x236.png 300w, https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-tts-pipeline-768x605.png 768w, https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-tts-pipeline.png 1348w\" /></p><h4>Phase 1: Establishing the speech-to-text WebSocket connection</h4><p>To kick off a speech-to-text session, we need to establish a connection with the Bing speech-to-text WebSocket service and identify our client via our API token for the Bing service. Note that all authentication and configuration information must be passed in via query parameters on the WebSocket connection URL since many WebSocket clients don’t have support for headers that would typically be used for sending information like API tokens. For example, using the Java <a href=\"https://github.com/TakahikoKawasaki/nv-websocket-client\">NV WebSocket client</a>, we’d connect to the speech-to-text service as follows:</p><p>After the WebSocket connection is established, we must send additional identifying information about our client (like the operating system used, the audio source being transcribed, etc.) to the speech-to-text service via a WebSocket text message and then we’re ready to transcribe audio:</p><h4>Phase 2: Transcribing audio via the speech-to-text WebSocket</h4><p>Once the client has successfully identified itself with the speech-to-text service, it can start sending chunks of audio for transcription, via binary WebSocket messages:</p><p>The messages follow a specific schema:</p><p><img class=\"alignnone size-large wp-image-4894\" src=\"https://www.microsoft.com/developerblog//wp-content/uploads/radio-codestory-tts-audio-message-schema-1024x592.png\" alt=\"Format of speech-to-text audio transfer messages\" width=\"780\" height=\"451\" srcset=\"https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-tts-audio-message-schema-1024x592.png 1024w, https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-tts-audio-message-schema-300x173.png 300w, https://codestoryedge.azureedge.net/developerblog/wp-content/uploads/radio-codestory-tts-audio-message-schema-768x444.png 768w\" /></p><p>Note that the audio data sent to the speech-to-text service must be single-channel mono WAV audio sampled at 16kHz with 16 bits per sample. The speech-to-text service does not validate the format of the audio messages and will silently return incorrect transcriptions on receipt of messages with malformed encodings.</p><p>Most online radio is in MP3 rather than WAV format, making an audio format conversion step necessary for the radio input to be processed by the speech-to-text service. The Java ecosystem has a lack of great open source, permissively licensed streaming MP3 decoders; as a result, our client resorts to converting small batches of MP3 audio to WAV on disk using the <a href=\"https://github.com/pdudits/soundlibs/tree/master/jlayer/src/javazoom/jl/converter\">javazoom converter</a> and then <a href=\"https://github.com/CatalystCode/SpeechToText-WebSockets-Java/blob/182cb301d41427a7a8dd9221ad91aa7922f2d559/src/main/java/com/github/catalystcode/fortis/speechtotext/websocket/MessageSender.java#L58-L101\">resampling</a> the WAV using the javax.sound APIs with the <a href=\"http://www.tritonus.org\">Tritonus plugin</a>. Hopefully, in the future, the trip to disk will be made unnecessary by moving to a streaming or in-memory MP3 decoder.</p><p>After the client starts sending audio messages to the speech-to-text service, the service will transcribe the audio and send the transcription results back to the client. Via WebSocket, transcriptions will be sent with a JSON payload for the client to parse and process. Note that the service may choose to batch multiple input audio messages sent by the client into a single output transcription message returned to the client.</p><h4>Phase 3: Closing the speech-to-text WebSocket connection</h4><p>When the client wishes to close the connection with the speech-to-text service, it must execute three steps in sequence:</p><ol><li>Send an audio message as described in Phase 2, but with a zero-byte audio payload</li><li>Send telemetry as a JSON WebSocket text message about its perception of the speech-to-text service’s performance</li><li>Shut down the WebSocket connection</li></ol><p>Sending the connection-end telemetry is like the client-information telemetry described in Phase 1 above. The schema for the JSON payload of the message is <a href=\"https://docs.microsoft.com/en-us/azure/cognitive-services/speech/api-reference-rest/websocketprotocol#telemetry-schema\">well documented</a>.</p><h2>Summary</h2><p>In many parts of the world, essential information is broadcast via traditional sources such as radio. To formulate effective crisis response plans, the United Nations Office for the Coordination of Humanitarian Affairs needs to be able to access and analyze these data sources in a timely manner.</p><p>To solve this problem, we created a <a href=\"https://github.com/CatalystCode/SpeechToText-WebSockets-Java/\">Java client</a> for the Azure Cognitive Services <a href=\"https://azure.microsoft.com/en-ca/services/cognitive-services/speech/\">speech-to-text</a> WebSocket protocol. We then fed the transcribed radio into a pipeline based on Spark Streaming for further analysis, augmentation, and aggregation. This solution enabled us to ingest and analyze radio in near real-time.</p><p>Our <a href=\"https://github.com/CatalystCode/SpeechToText-WebSockets-Java/\">Java client</a> is reusable across a wide range of text-to-speech scenarios that require time-efficient speech-to-text transcription in more than 10 languages including English, French, Spanish, German and Chinese.</p><h2>Resources</h2><ul><li><a href=\"https://github.com/CatalystCode/SpeechToText-WebSockets-Java/\">Java client for speech-to-text WebSocket protocol</a></li><li><a href=\"https://github.com/Azure-Samples/SpeechToText-WebSockets-Javascript\">JavaScript client for speech-to-text WebSocket protocol</a></li><li><a href=\"https://github.com/noopkat/ms-bing-speech-service\">NodeJS client for speech-to-text WebSocket protocol</a></li><li><a href=\"https://github.com/CatalystCode/project-fortis-spark\">Fortis project using speech-to-text WebSocket and Spark custom receivers</a></li><li><a href=\"https://azure.microsoft.com/en-ca/services/cognitive-services/speech/\">Bing speech-to-text documentation</a></li></ul>",
        "created_at": "2018-07-26T19:02:21+0000",
        "updated_at": "2018-08-07T22:37:14+0000",
        "published_at": "2017-11-01T07:31:36+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 8,
        "domain_name": "www.microsoft.com",
        "preview_picture": "https://www.microsoft.com/developerblog//wp-content/uploads/fortis-radio-to-text-spark.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11604"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 190,
            "label": "sql",
            "slug": "sql"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          }
        ],
        "is_public": false,
        "id": 11593,
        "uid": null,
        "title": "Introducing FiloDB",
        "url": "http://velvia.github.io/Introducing-FiloDB/",
        "content": "<p>If you are a big data analyst, or build big data solutions for fast analytical queries, you are likely familiar with columnar storage technologies.  The open source <a href=\"http://parquet.io\">Parquet</a> file format for HDFS saves space and powers query engines from Spark to Impala and more, while cloud solutions like Amazon Redshift use columnar storage to speed up queries and minimize I/O.  Being a file format, Parquet is much more challenging to work with directly for real-time data ingest.  For applications like IoT, time-series, and event data analytics, many developers have turned to NoSQL databases such as Apache Cassandra, due to their combination of high write scalability and the ease of using an idempotent, primary key-based database API.  Most NoSQL databases are not designed for fast, bulk analytical scans, but instead for highly concurrent key-value lookups.  What is missing is a solution that combines the ease of use of a database API, the scalability of NoSQL databases, with columnar storage technology for fast analytics.</p><h2 id=\"introducing-filodb--distributed--versioned--columnar\">Introducing FiloDB.  Distributed.  Versioned.  Columnar.</h2><p>I am excited to announce FiloDB, a new open-source distributed columnar database from <a href=\"http://www.tuplejump.com\">TupleJump</a>.  FiloDB is designed to ingest streaming data of various types, including machine, event, and time-series data, and run very fast analytical queries over them.  In four-letter acronyms, it is an OLAP solution, not OLTP.</p><ul><li><strong>Distributed</strong> - FiloDB is designed from the beginning to run on best-of-breed distributed, scale-out storage platforms such as Apache Cassandra.  Queries run in parallel in Apache Spark for scale-out ad-hoc analysis.</li>\n  <li><strong>Columnar</strong> - FiloDB brings breakthrough performance levels for analytical queries by using a columnar storage layout with different space-saving techniques like dictionary compression.  The performance is comparable to Parquet, and one to two orders of magnitude faster than Spark on Cassandra 2.x for analytical queries.  For the POC performance comparison, please see the <a href=\"http://github.com/velvia/cassandra-gdelt\">cassandra-gdelt</a> repo.</li>\n  <li><strong>Versioned</strong> - At the same time, row-level, column-level operations and built in versioning gives FiloDB far more flexibility than can be achieved using file-based technologies like Parquet alone.</li>\n</ul><p><img src=\"http://velvia.github.io/images/filodb_architecture.png\" alt=\"FiloDB architecture\" /></p><h2 id=\"your-database-for-fast-streaming--big-data\">Your Database for Fast Streaming + Big Data</h2><p>FiloDB is designed for <strong>streaming</strong> applications.  Enable easy exactly-once ingestion from Apache Kafka for streaming events, time series, and IoT applications - yet enable extremely fast ad-hoc analysis using the ease of use of SQL.  Each row is keyed by a partition and sort key, and writes using the same key are idempotent.  Idempotent writes enables exactly-once storage of event data.  FiloDB does the hard work of keeping data stored in an efficient and read-optimized, sorted format.</p><h2 id=\"filodb--cassandra--spark--lightning-fast-analytics\">FiloDB + Cassandra + Spark = Lightning-fast Analytics</h2><p>FiloDB leverages <a href=\"https://cassandra.apache.org/\">Apache Cassandra</a> as its storage engine, and <a href=\"http://spark.apache.org\">Apache Spark</a> as its compute layer.  Apache Cassandra is one of the most widely deployed, rock-solid distributed databases in use today, with very well understood operational characteristics.  Many folks are combining Apache Spark with their Cassandra tables for much richer analytics on their Cassandra data than is possible with just the native Cassandra CQL interface.  However, loading massive amounts of data from Cassandra into Spark can still be very slow, especially for analytics and ad-hoc queries such as averaging or computing the correlation between two columns of data.  This is because Cassandra CQL tables stores data in a row-oriented manner.  FiloDB brings the benefits of efficient columnar storage and the flexibility and richness of Apache Spark to the rock solid storage technology of Cassandra, speeding up analytical queries by up to 100x over Cassandra 2.x.</p><h2 id=\"easy-ingestion--sql--jdbc--spark-ml\">Easy Ingestion + SQL + JDBC + Spark ML</h2><p>FiloDB uses Apache Spark SQL and DataFrames as the main query mechanism.  This lets you run familiar SQL queries over your data, and easily connect tools such as Tableau to query your data, using Spark’s JDBC connector.  At the same time, the full power of Spark is available for your data, including the machine learning MLlib library and GraphX for graph processing.</p><p>Ingesting data is also very easy through Spark DataFrames.  This means you can easily ingest data from any JDBC data source, Parquet and Avro files, Cassandra tables, and much, much, more.  This includes easily inserting data from Spark Streaming and Apache Kafka.</p><h2 id=\"simplify-your-analytics-stack--filodb--smack-for-everything\">Simplify your Analytics Stack.  FiloDB + SMACK for everything.</h2><p>Use Kafka + Spark + Cassandra + FiloDB to power your entire Lamba architecture implementation.  There is no need to implement a complex Lambda dual ingestion pipeline with both Cassandra and Hadoop!   You can use the <a href=\"http://noetl.org\">SMACK</a> stack (Spark/Scala, Mesos, Akka, Cassandra, Kafka) for a much bigger portion of your analytics stack than before, reducing infrastructure investment.</p><h2 id=\"whats-in-the-name\">What’s in the name?</h2><p><img src=\"http://velvia.github.io/images/Dantat.jpg\" alt=\"Dan tat\" /></p><p>I love desserts, and Filo dough is an essential ingredient.  One can think of columns and versions of data as layers, and FiloDB wrapping the layers in a yummy high-performance analytical database engine.</p><h2 id=\"proudly-built-with-the-typesafe-stack\">Proudly built with the Typesafe Stack</h2><p>FiloDB is built with the Typesafe reactive stack for high-performance distributed computing and asynchronous I/O - Scala, Spark, and <a href=\"http://akka.io\">Akka</a>.</p><h2 id=\"come-to-the-talk-at-cassandra-summit\">Come to the Talk at Cassandra Summit!</h2><p>If you’d like to learn more, I encourage you to come on over to <a href=\"http://cassandrasummit-datastax.com/?source=\">Cassandra Summit</a> in Santa Clara, where I’ll be speaking about FiloDB and Spark and Cassandra, on Thursday September 24th at the Santa Clara Convention Center!  Or feel free to reach out.  The repo and more details such as the roadmap will be unveiled at the talk.</p>",
        "created_at": "2018-07-26T15:14:59+0000",
        "updated_at": "2018-08-03T19:04:40+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 4,
        "domain_name": "velvia.github.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11593"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1232,
            "label": "stress",
            "slug": "stress"
          }
        ],
        "is_public": false,
        "id": 11592,
        "uid": null,
        "title": "velvia/cassandra-gdelt",
        "url": "https://github.com/velvia/cassandra-gdelt",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\"><h2><a id=\"user-content-cassandra-gdelt\" class=\"anchor\" aria-hidden=\"true\" href=\"#cassandra-gdelt\"></a>cassandra-gdelt</h2>\n<p>Just a simple test of different Cassandra layouts and their impact on the query / IO speed of the GDELT dataset.</p>\n<h3><a id=\"user-content-setup\" class=\"anchor\" aria-hidden=\"true\" href=\"#setup\"></a>Setup</h3>\n<p><a href=\"http://data.gdeltproject.org/events/index.html\" rel=\"nofollow\">GDELT dataset</a>, 1979-1984, 4,037,539 records, 57 columns* (There are actually 60 columns, but the last 3 columns are not populated for that date range)</p>\n<p>(NOTE: the current code is designed only to parse data from the link above, which is TAB-delimited, and does NOT have a header row)</p>\n<ul><li>Local MacBook Pro,  2.3GHz Core i7, 16GB RAM, SSD</li>\n<li>Cassandra 2.1.6, installed locally with one node using CCM</li>\n<li>Benchmark run using <code>sbt run</code></li>\n</ul><p>NOTE: if you run this benchmark with C* 2.0.x, <code>GdeltCaseClass2</code> ingestion may crash C*.</p>\n<p>Query and disk space benchmarks were run after a compaction cycle, with snapshots removed.</p>\n<h3><a id=\"user-content-benchmark-results---gdeltcaseclass\" class=\"anchor\" aria-hidden=\"true\" href=\"#benchmark-results---gdeltcaseclass\"></a>Benchmark Results - GdeltCaseClass</h3>\n<p>This is a simple layout with one primary key, so one physical row per record.  One would think that Cassandra could do a massive multiget_slice to save time on reading one column out of 20, but it doesn't save much time.</p>\n<p>LZ4 disk compression is enabled.</p>\n<p>Space taken up by all records: 1.9GB</p>\n<table><thead><tr><th align=\"left\">What</th>\n<th align=\"left\">Time</th>\n<th align=\"left\">Records/sec</th>\n</tr></thead><tbody><tr><td align=\"left\">Ingestion from CSV</td>\n<td align=\"left\">1927 s</td>\n<td align=\"left\">2091 rec/s</td>\n</tr><tr><td align=\"left\">Read every column</td>\n<td align=\"left\">505 s</td>\n<td align=\"left\">7998 rec/s</td>\n</tr><tr><td align=\"left\">Read 1 col (monthYear)</td>\n<td align=\"left\">504 s</td>\n<td align=\"left\">8005 rec/s</td>\n</tr></tbody></table><h3><a id=\"user-content-benchmark-results---gdeltcaseclass2\" class=\"anchor\" aria-hidden=\"true\" href=\"#benchmark-results---gdeltcaseclass2\"></a>Benchmark Results - GdeltCaseClass2</h3>\n<p>This is an improvement on GdeltCaseClass, with use of both a partition key which is a simple grouping of the primary keys, and a clustering key, to effect wide rows for faster linear reads from disk.  In addition, the names of the columns have been shortened to use up less disk space.</p>\n<p>Space taken up by all records: 2.2GB</p>\n<table><thead><tr><th align=\"left\">What</th>\n<th align=\"left\">Time</th>\n<th align=\"left\">Records/sec</th>\n</tr></thead><tbody><tr><td align=\"left\">Ingestion from CSV</td>\n<td align=\"left\">3190 s</td>\n<td align=\"left\">1300 rec/s</td>\n</tr><tr><td align=\"left\">Read every column</td>\n<td align=\"left\">941 s</td>\n<td align=\"left\">4417 rec/s</td>\n</tr><tr><td align=\"left\">Read 1 col (monthYear)</td>\n<td align=\"left\">707 s</td>\n<td align=\"left\">5880 rec/s</td>\n</tr></tbody></table><p>Oh no, what happened?  It seems it is slower to query, and it takes up more space on disk as well (logically, wide rows takes up more space uncompressed due to compound column names).  This is actually not entirely surprising, because reading only one column from such a layout, due to the way Cassandra lays out its data, essentially means having to scan all the data in a partition, and using clustering keys is inefficient because of how Cassandra prefixes clustering keys to the column names.  With the skinny layout, Cassandra is actually able to skip part of the row when reading.</p>\n<p>However, let's say you were not scanning the whole table but instead had a WHERE clause to filter on your partition key.  In this case, this layout would be a huge win over the other one -- only data from one partition needs to be read.  Thus, pick a layout based on your needs.</p>\n<h3><a id=\"user-content-compact-storage\" class=\"anchor\" aria-hidden=\"true\" href=\"#compact-storage\"></a>COMPACT STORAGE</h3>\n<p>COMPACT STORAGE is the old Cassandra 0.x - 1.x way of storing things - write your record as a blob of your choice (JSON, Protobuf, etc.) and Cassandra writes it as a single cell (physical row, column, blob).  It is not supposed to be supported going forward.  You need to parse the blob yourself, and the entire blob must be read.  If you use Protobuf or other binary format, then CQLSH won't show you the contents (in this case we are using a trick and joining the text fields together using a special \\001 character, which shows up in a different color).  Writes are fast, but reads are still much slower than columnar layout.</p>\n<p>Disk space: 260M</p>\n<table><thead><tr><th align=\"left\">What</th>\n<th align=\"left\">Time</th>\n<th align=\"left\">Records/sec</th>\n</tr></thead><tbody><tr><td align=\"left\">Ingestion from CSV</td>\n<td align=\"left\">78.6 s</td>\n<td align=\"left\">52877 rec/s</td>\n</tr><tr><td align=\"left\">Read every column</td>\n<td align=\"left\">81.8 s</td>\n<td align=\"left\">50850 rec/s</td>\n</tr><tr><td align=\"left\">Read 1 col (monthYear)</td>\n<td align=\"left\">81.8 s</td>\n<td align=\"left\">50850 rec/s</td>\n</tr></tbody></table><h3><a id=\"user-content-columnar-layout\" class=\"anchor\" aria-hidden=\"true\" href=\"#columnar-layout\"></a>Columnar Layout</h3>\n<div class=\"highlight highlight-source-sql\"><pre>CREATE TABLE data (\n  dataset text,\n  version int,\n  shard int,\n  columnname text,\n  rowid int,\n  bytes blob,\n  PRIMARY KEY ((dataset, version, shard), columnname, rowid)\n)</pre></div>\n<p>This layout places values of the same column from different rows together, and also serializes multiple row values into one cell.</p>\n<p>Dictionary encoding enabled for about 75% of column chunks\n(auto-detection with a 50% cardinality threshold for enabling dictionary encoding)</p>\n<p>Space taken up by records:  266MB .... !!!\n(LZ4 Compressed SSTable size; uncompressed actual ByteBuffers are 918MB)</p>\n<table><thead><tr><th align=\"left\">What</th>\n<th align=\"left\">Time</th>\n<th align=\"left\">Records/sec</th>\n</tr></thead><tbody><tr><td align=\"left\">Ingestion from CSV</td>\n<td align=\"left\">93.0 s</td>\n<td align=\"left\">43324 rec/s</td>\n</tr><tr><td align=\"left\">Read every column</td>\n<td align=\"left\">8.6 s</td>\n<td align=\"left\">468,980 rec/s</td>\n</tr><tr><td align=\"left\">Read 1 col (monthYear)</td>\n<td align=\"left\">0.23 s</td>\n<td align=\"left\"><strong>17.4 million rec/s</strong></td>\n</tr></tbody></table><p>The speedup and compactness is shocking.</p>\n<ul><li>On ingest - roughly 20-35x faster and 7x less disk space (of course this is from CSV with essentially no primary key, append only, probably not realistic)</li>\n<li>On reads - 42x to 59x faster for reads of all columns, and 355 - 2190x faster for read of a single column\n<ul><li>Granted, the speedup is for parsing an integer column, which is the most compact and benefits the most from efficient I/O; parsing a string column will not be quite as fast (though dictionary-encoded columns are very fast in deserialization)</li>\n</ul></li>\n<li>Dictionary encoding saves a huge amount of space, cutting the actual storage\nspace to one-third of the columnar one, both uncompressed and compressed. (sorry results without dictionary encoding are not available since a code change is needed to test that out)\n(Interesting that the compression ratio seems constant)</li>\n</ul><p>Is this for real?  Gathering stats of the data being read shows that it is:</p>\n<ul><li><code>GdeltDataTableQuery</code> compiles stats which show that every column is being read, the # of shards, chunks, and bytes seem to all make sense.  Evidently LZ4 is compressing data to roughly 1/4 of the total size of all the bytebuffers.  This debunks the theory that perhaps not all the data is being read.</li>\n<li>For the monthYear col, exactly 4037539 elements are being read back, and a top K of the monthYear values matches exactly with values derived from the original source CSV file</li>\n</ul><p>Also, FlatBuffers leaves lots of zeroes in the binary output, so there is plenty of room for improvement, plus the code for parsing the binary FlatBuffers has not been optimized at all.... plus LZ4 and different C* side compression schemes and their effects too.</p>\n<h3><a id=\"user-content-capn-proto\" class=\"anchor\" aria-hidden=\"true\" href=\"#capn-proto\"></a>Cap'n Proto</h3>\n<p>This is not yet working due to bugs in capnproto-java.  Notes for the setup:</p>\n<ol><li>\n<p>Build and install Cap'n Proto Schema Compiler</p>\n</li>\n<li>\n<p>Clone and build the java repo using <a href=\"https://dwrensha.github.io/capnproto-java/index.html\" rel=\"nofollow\">these instructions</a></p>\n</li>\n<li>\n<p>To build the schema, do something like</p>\n<pre> capnp compile -o../../capnproto-java/capnpc-java -I../../capnproto-java/compiler/src/main/schema column_storage.capnp\n</pre>\n</li>\n</ol><h3><a id=\"user-content-for-additional-investigation\" class=\"anchor\" aria-hidden=\"true\" href=\"#for-additional-investigation\"></a>For Additional Investigation</h3>\n<p>Right now the above comparison is just for C*, LZ4 C* disk compression, using the Phantom client.  Much more testing and performance evaluation would be needed to compare against, for example, Parquet, and to isolate the effects of</p>\n<ul><li>C* itself, and the disk compression scheme used</li>\n<li>Effects of the Phantom client</li>\n<li>FlatBuffers vs Capt'n Proto</li>\n</ul><p>Another good dataset to test against is NYC Taxi Trip data: <a href=\"http://www.andresmh.com/nyctaxitrips/\" rel=\"nofollow\">http://www.andresmh.com/nyctaxitrips/</a>.   There are two helper scripts: <code>split_csv.sh</code> helps break up a big CSV into smaller CSVs with header intact, and <code>socrata_pointify.sh</code> adds a point column from lat and long columns.</p>\n<h2><a id=\"user-content-spark-on-cassandra\" class=\"anchor\" aria-hidden=\"true\" href=\"#spark-on-cassandra\"></a>Spark on Cassandra</h2>\n<p>The above can also be used to compare Spark on Cassandra reads.  First start up the spark shell, like this (we ensure there is only one thread for all tests for an even comparison):</p>\n<pre>bin/spark-shell \\\n              --packages com.datastax.spark:spark-cassandra-connector_2.10:1.4.0-M3 \\\n              --conf spark.cassandra.connection.host=127.0.0.1 --conf spark.cassandra.input.split.size_in_mb=256 \\\n              --conf spark.sql.shuffle.partitions=4 \\\n              --driver-memory 5G --master \"local[1]\"\n</pre>\n<p>The split size is to ensure that the GDELT2 wide row table doesn't get too many splits when reading.  This is really tricky to configure by the way, as it is global but you will probably need different settings for different tables.</p>\n<p>To load a Cassandra table into a Spark DataFrame, do something like this:</p>\n<div class=\"highlight highlight-source-scala\"><pre>val df = sqlContext.read.format(\"org.apache.spark.sql.cassandra\").\n                    option(\"table\", \"gdelt2\").\n                    option(\"keyspace\", \"test\").load\ndf.registerTempTable(\"gdelt\")</pre></div>\n<p>Here are the queries used.  Query 1:</p>\n<pre>df.select(count(\"numarticles\")).show\n</pre>\n<p>Query 2:</p>\n<pre>sqlContext.sql(\"SELECT a1name, AVG(avgtone) AS tone FROM gdelt GROUP BY a1name ORDER BY tone DESC\").show\n</pre>\n<p>Query 3:</p>\n<pre>sqlContext.sql(\"SELECT AVG(avgtone), MIN(avgtone), MAX(avgtone) FROM gdelt WHERE MonthYear = 198012\").show\n</pre>\n<p>NOTE: the above are for querying the Cassandra CQL tables, in which the code shortens column names for performance and storage compactness reasons.  For Parquet and FiloDB, use the regular, capitalized column names (<code>Actor1Name</code> instead of <code>a1name</code>, <code>AvgTone</code> instead of <code>avgtone</code>, etc.)</p>\n<p>TODO: instructions for querying the COMPACT STORAGE table</p>\n<p>For testing Spark SQL cached tables, do the following:</p>\n<pre>sqlContext.cacheTable(\"gdelt\")\nsqlContext.sql(\"select avg(numarticles), avg(avgtone) from gdelt group by a1name\").show\n</pre>\n<p>NOTE: For any DataFrame DSL (Scala) queries, make sure to get back a new DataFrame after the <code>cacheTable</code> operation, like this: <code>val df1 = sqlContext.table(\"gdelt\")</code>.  The cached DataFrame reference is not the same as the original!</p>\n<p>For FiloDB, the table is loaded just a tiny bit differently:</p>\n<pre>df = sqlContext.read.format(\"filodb.spark\").option(\"dataset\", \"gdelt\").option(\"splits_per_node\", \"1\").load\n</pre>\n<p>For Parquet, it is even more straightforward:</p>\n<pre>df = sqlContext.load(\"/path/to/my/1979-1984.parquet\")\n</pre>\n<p>For comparisons, I generated the Parquet file from one of the above Cassandra tables (not the COMPACT STORAGE one) by saving it out, like this:</p>\n<pre>df.save(\"/path/to/my/1979-1984.parquet\")\n</pre>\n</article>",
        "created_at": "2018-07-26T15:12:50+0000",
        "updated_at": "2018-07-26T15:13:02+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/1062875?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11592"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 182,
            "label": "mongo",
            "slug": "mongo"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 11590,
        "uid": null,
        "title": "Apache Cassandra turns 10",
        "url": "https://www.zdnet.com/article/apache-cassandra-turns-10/",
        "content": "<figure class=\"image  image-medium shortcode-image\"><img src=\"https://zdnet1.cbsistatic.com/hub/i/r/2018/07/21/55ed8a96-6a44-4c71-b77c-7c645fa5ca30/resize/370xauto/a884e9c7912167555670d90cdbf72607/cassandra.gif\" class=\"\" alt=\"cassandra.gif\" height=\"auto\" width=\"370\" /></figure><p>The past couple years have seen a number of 10-year milestones being passed, like the decade anniversaries of <a href=\"https://aws.amazon.com/10year/\">Amazon Web Services</a>, <a href=\"https://www.linkedin.com/pulse/mongodb-world-2017-lonely-story-versus-john-de-goes/\">MongoDB</a>, <a href=\"https://www.cloudera.com/promos/hadoop10.html\">Hadoop</a> and many others. And so in 2018, it's <a href=\"https://cassandra.apache.org/\">Apache Cassandra's</a> turn. Today, Apache Cassandra has morphed into a modest ecosystem where there is one principle commercial platform supplier -- <a href=\"http://datastax.com/\">DataStax</a> -- supplemented by a <a href=\"https://wiki.apache.org/cassandra/ThirdPartySupport\">small collection of companies</a> delivering third-party support. It combines the versatility of a table-oriented database with the speed and efficiency of a key-value store.</p><p>But make no mistake about it -- the fact that there aren't a dozen vendors of Cassandra distros doesn't hide up the fact that Cassandra is a very popular database. It is one of a quartet of NoSQL databases that rank in <a href=\"https://db-engines.com/en/ranking\">db-Engine's</a> top ten. And in itself, Cassandra has carved out a niche for continuous online systems that can carry up to PBytes of data. Like other \"wide column\" databases that began life as key-value stores, Cassandra was first known for fast writes, but over the years, read performance has caught up.</p><p>For instance, when you get film recommendations served up on Netflix, they come from an application running on Cassandra. It has carved presence with maintaining of online user profiles, shopping carts, fraud detection, and increasingly, real-time mobile and IoT applications. For that matter, so have most of Cassandra's prime NoSQL competitors like <a href=\"https://mongodb.com/\">MongoDB</a>, <a href=\"https://aws.amazon.com/dynamodb/\">DynamoDB</a>, and <a href=\"https://azure.microsoft.com/en-us/services/cosmos-db/\">Cosmos DB</a>.</p><p>As this is 10th birthday time, it makes sense to look at Cassandra's beginnings. The story is a familiar one. An Internet giant -- Facebook -- needed a more scalable, always-on database alternative for its inbox feature and created Cassandra back in 2008 based on the <a href=\"https://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf\">Dynamo paper</a> published by Amazon. After open sourcing it, <a href=\"https://www.linkedin.com/in/jbellis/\">Jonathan Ellis</a>, an engineer at Rackspace at the time, saw its potential as a distributed database for powering cloud applications, and a year later, drew venture backing to cofound what is now DataStax with then-colleague <a href=\"https://www.linkedin.com/in/mattpfeil/\">Matt Pfeil</a>.</p><p>The biggest source of confusion early on was with Hadoop. Because of some <a href=\"https://www.theregister.co.uk/2011/03/23/cassandra_mashed_with_hadoop/\">ridiculous historical coincidences</a>, Cassandra got lumped into the Hadoop project where it still appears on the <a href=\"http://hadoop.apache.org/\">Apache project page</a>. That implies that Cassandra is an in-kind replacement for <a href=\"https://hbase.apache.org/\">HBase</a>. Well kinda and kinda not. Although both were initially designed to run as online production systems for big data, HBase requires HDFS, YARN, and Zookeeper to run, whereas Cassandra doesn't require Hadoop components and runs on its own cluster. Then there are other architectural differences, such as that HBase runs with Hadoop hierarchical topology, whereas Cassandra works in more of a peer-to-peer mode.</p><p><strong>Comparison to the usual suspects</strong></p><p>Hadoop flirtations notwithstanding, how does Cassandra differentiate from the usual NoSQL suspects? We'll start with the biggest differentiator: query language. Cassandra also has a <a href=\"https://cassandra.apache.org/doc/latest/cql/\">query language</a> that is much more like SQL compared to most rivals except <a href=\"https://www.couchbase.com/products/n1ql\">Couchbase</a>.</p>\n    <section class=\"sharethrough-top\" data-component=\"medusaContentRecommendation\" data-medusa-content-recommendation-options=\"{&quot;promo&quot;:&quot;promo_ZD_recommendation_sharethrough_top_in_article_desktop&quot;,&quot;spot&quot;:&quot;dfp-in-article&quot;}\">\n    </section><p>Compared to MongoDB, Cassandra was more write-friendly, but as both databases matured, differences in read and write performance are no longer as stark. Cassandra was initially designed as a tabular database for key-value data (compared to MongoDB's more object-like model), but in time was evolved to accommodate JSON documents. There are still basic differences in database topology: Cassandra was designed for higher availability writes with its multi-master architecture, whereas MongoDB uses a single master, but suggests <a href=\"https://www.mongodb.com/blog/post/active-active-application-architectures-with-mongodb\">managing sharding</a> for higher availability writes.</p><p>Among cloud-native counterparts, Cassandra shares lineage with Amazon DynamoDB. A detailed comparison can be found <a href=\"https://www.beyondthelines.net/databases/dynamodb-vs-cassandra/\">here</a>. But at high level, the obvious difference is where they run: DynamoDB only runs in <a href=\"http://aws.amazon.com/\">AWS</a> as a managed service (and likewise for Microsoft Azure Cosmos DB on <a href=\"http://azure.microsoft.com/\">Azure</a>); Cassandra, on the other hand, can run anywhere, but as managed service, <a href=\"https://www.datastax.com/products/datastax-managed-cloud\" target=\"_blank\">DataStax Managed Cloud Service</a> has only been introduced recently. Cassandra and DynamoDB both let you tune consistency levels -- Cassandra offers five options for consistency while DynamoDB narrows it down to two (eventual or strong). </p><p>Compared to Microsoft Azure Cosmos DB, the biggest difference is multi-model that is core to the Azure offering; by comparison, the commercial version of Cassandra -- <a href=\"https://www.datastax.com/products/datastax-enterprise\">DataStax Enterprise</a> -- is just starting on this road, as it is still integrating its graph model.</p><p><strong>Are we in a post-relational world?</strong></p><p>Given that four NoSQL databases have now made it to the mainstream (based on developer interest charted by db-Engines), one would think that the matter has been settled about the role that these platforms play. One would be wrong.</p><p>There's still healthy debate. On one side, there's the irrational exuberance of being in a <a href=\"https://www.datastax.com/2018/07/cassandras-journey-via-the-five-stages-of-grief\">post-relational world</a>. Yes, NoSQL databases have become very popular among database developers. And yes, DataStax does have its share of <a href=\"https://www.oracle.com/database/technologies/index.html\" target=\"_blank\">Oracle</a> run-ins, but these are going to be wins from outside of Oracle's core back office base. Actually, DataStax and Oracle are frenemies, as <a href=\"https://www.datastax.com/products/datastax-enterprise\" target=\"_blank\">DataStax Enterprise</a> (DSE) is one of the first third-party databases to become <a href=\"https://blogs.oracle.com/cloud-infrastructure/datastax-certified-nosql-cassandra-clusters-on-bare-metal-cloud\" target=\"_blank\">officially supported</a> in the Oracle Public Cloud's <a href=\"https://cloud.oracle.com/cloud-infrastructure\" target=\"_blank\">bare metal services</a>, but we digress.</p><p>Fortuitously, having spoken with  <a href=\"https://www.datastax.com/author/patrickdatastax-com\" target=\"_blank\">Patrick McFadin</a>, the five-stages-of-grief author, we've found his insights to be far more nuanced than his blog post would suggest. But there are many others taking more extreme views based on the notion of <a href=\"https://www.techopedia.com/2/32000/trends/big-data/why-the-world-is-moving-toward-nosql-databases\">big data becoming the mainstream</a>. On the other side, there's the constituency that still believes that <a href=\"https://read.acloud.guru/serverless-superheroes-lynn-langit-on-big-data-nosql-and-google-versus-aws-f4427dc8679c\">NoSQL is overhyped</a>.</p><p>Reality is much grayer. The fact that NoSQL databases like Cassandra allow schema to vary does not mean that they lack schema, or that developers should not bother with optimizing the database for specific types of schema. In a NoSQL database, schema still matters and so does table layout. Even if you don't design the data model exactly for the queries that you're going throw at it, you still need to consider which data the app will touch when laying out the tables.</p><p>Don't count relational out either. If your application or use case requires strict ACID guarantees and data with referential integrity, relational is going to be your choice. If the use case involves complex analytical queries, you have a couple options. You could go the NoSQL route if you denormalize the data to improve performance; design the application so you don't have to rely on complex table joins; and take advantage of the Spark connectors that are becoming checkbox items with commercial NoSQL databases like DataStax Enterprise. But if the purpose of the database is solely for analytics, NoSQL won't be the right route.</p><figure class=\"image  image-medium shortcode-image\"><img src=\"https://zdnet1.cbsistatic.com/hub/i/r/2018/07/21/c30e5d5f-c52d-409a-9076-4db9a09329a0/resize/370xauto/5bb3013fbc6e0bea9b81942248c45bea/datastaxlogo.png\" class=\"\" alt=\"datastaxlogo.png\" height=\"auto\" width=\"370\" /></figure><p><strong>DataStax and Cassandra today</strong></p><p>So what gives with Apache Cassandra and DataStax, the company that for most of its history was most closely associated with the database and open source project? It boils down to the nature of the open source project. Unlike MongoDB, which controls the underlying open source project and <a href=\"https://www.mongodb.com/community/licensing\">licenses the database</a> under <a href=\"https://www.gnu.org/licenses/agpl-3.0.en.html\">AGPL 3</a> license (which requires developers to contribute back to the community), Cassandra is an official Apache Foundation project that is governed by the Apache license.</p><p>So DataStax does not own or control Cassandra, and a couple years ago, <a href=\"https://sdtimes.com/apache/jonathan-ellis-steps-cassandra-project/\">stepped back</a> from <a href=\"https://www.datastax.com/2016/08/a-look-back-a-look-forward\">leadership</a> of the project. DataStax still contributes and maintains presence on the Cassandra project, but the bulk of its energies are in building the enterprise platform features around it. In essence, DataStax is becoming more of a classic \"open core\" software company, a strategy that is not all that different from <a href=\"http://cloudera.com/\">Cloudera's</a> on Hadoop.</p><p>With Cassandra at 10, DataStax still embraces the platform but views it as the starting point for additional features. It is reaching out to accommodate analytics and search with Spark connectivity and new search functions that have been added to its CQL query language. Then there is the addition of graph, which came from the 2015 acquisition of Aurelius that brought the leaders of the <a href=\"https://tinkerpop.apache.org/\">Apache TinkerPop</a> project to DataStax. While DataStax is still working to fully integrate graph into its implementation of Cassandra, in the <a href=\"https://www.datastax.com/products/datastax-enterprise-6\">DSE 6.0</a> release, you can load graph and Cassandra tables at the same time onto your cluster. And the company is now meeting cloud frenemies like Amazon head-on by rolling out the DataStax Managed Cloud service on AWS and Azure</p><p>There's a reason that we've been seeing all these tenth anniversaries in the big data space over the past few years. That's because in the first decade of the 2000s, a backlash formed against the post-Y2K consensus that we were at the end of times where n-tier was the de facto standard application architecture; .NET and Java were the predominant application development stacks; and relational databases were entrenched as the enterprise standard. Notably, it was the experiences of Internet companies like Amazon and Google who subsequently overthrew the enterprise IT order whose experiences with the limitations of the post-2000 technology stack gave rise to the innovations that are now hitting middle age.</p><p>A decade in, Cassandra is no longer the new kid on the block. But the database has become one of the fixtures of modern operational systems, and the company most associated with it is using it as a jumping off point to a broader platform.</p>",
        "created_at": "2018-07-25T23:47:44+0000",
        "updated_at": "2018-07-25T23:47:50+0000",
        "published_at": "2018-07-24T12:00:00+0000",
        "published_by": [
          "Tony Baer (Ovum)"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "www.zdnet.com",
        "preview_picture": "https://zdnet3.cbsistatic.com/hub/i/r/2018/07/24/8f1d3195-a797-4ac9-84ed-e5df8d436128/thumbnail/770x578/42a0211ae1b12054513feb251e7502bb/image-2018-07-24-at-11-54-18-am.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11590"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          },
          {
            "id": 1174,
            "label": "videos",
            "slug": "videos"
          }
        ],
        "is_public": true,
        "id": 11160,
        "uid": "5b57841c0897d7.47605420",
        "title": "Best Practices for Running Apache Cassandra on AWS",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=IuJldwJLyFM",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/IuJldwJLyFM?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-07-24T19:55:08+0000",
        "updated_at": "2018-07-24T19:55:08+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/IuJldwJLyFM/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11160"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          },
          {
            "id": 1157,
            "label": "blogs",
            "slug": "blogs"
          }
        ],
        "is_public": true,
        "id": 11159,
        "uid": "5b5784143a7247.03635053",
        "title": "Pythian",
        "url": "https://www.pythian.com/blog/",
        "content": null,
        "created_at": "2018-07-24T19:55:00+0000",
        "updated_at": "2018-07-24T19:55:00+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "www.pythian.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11159"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          },
          {
            "id": 1157,
            "label": "blogs",
            "slug": "blogs"
          }
        ],
        "is_public": true,
        "id": 11158,
        "uid": "5b57840749d536.45681186",
        "title": "Blog",
        "url": "https://www.datastax.com/blog",
        "content": "Blog | DataStax\n\n<noscript>\n\n\n\n<div class=\"DS17\"><div class=\"connect-us\"><a href=\"https://www.datastax.com/contactus\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Mail.svg\" alt=\"email icon\" />email</a><a href=\"https://www.datastax.com/company#offices\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Phone.svg\" alt=\"phone icon\" />call</a></div></div><header class=\"DS17\"><div class=\"container\"><div class=\"wrapper\"><div class=\"logo\"><a href=\"https://www.datastax.com/\"><img src=\"https://www.datastax.com/templates/dist/images/logo-header.png\" alt=\"DataStax logo\" /></a><a href=\"https://www.datastax.com/\"><img src=\"https://www.datastax.com/templates/dist/images/new_logo.png\" alt=\"DataStax logo\" /></a></div></div></div>\n  \n</header>\n    \n      <div class=\"DS17\"><div class=\"use-case\"><div class=\"wrapper\"><div class=\"two-col text-light-blue\"><h6>Customer Experience</h6><ul><li><a href=\"https://www.datastax.com/use-cases/customer-360\">Customer 360</a></li>\n          <li><a href=\"https://www.datastax.com/personalization\">Personalization &amp; Recommendations</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/loyalty-programs\">Loyalty Programs</a></li>\n          <li><a href=\"https://www.datastax.com/fraud-detection\">Consumer Fraud Detection</a></li>\n        </ul></div><div class=\"two-col text-light-green\"><h6><a href=\"#\">Enterprise Optimization</a></h6><ul><li><a href=\"https://www.datastax.com/use-cases/ecommerce\">eCommerce</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/identity-management\">Identity Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/security\">Security and Compliance</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/supply-chain\">Supply Chain</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/inventory-management\">Inventory Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/asset-monitoring\">Asset Monitoring</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/logistics\">Logistics</a></li>\n        </ul></div></div></div></div>\n  \n    \n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n</noscript>",
        "created_at": "2018-07-24T19:54:47+0000",
        "updated_at": "2018-07-24T19:54:47+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 0,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/themes/datastax-2014-08/images/common/DataStax_Web_Social_DefaultGenericV2_1024x351_wide.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11158"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          },
          {
            "id": 1173,
            "label": "troubleshooting and tuning",
            "slug": "troubleshooting-and-tuning"
          }
        ],
        "is_public": true,
        "id": 11157,
        "uid": "5b578400b7cc89.22172525",
        "title": "Tuning Java resources",
        "url": "https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html",
        "content": "<p class=\"shortdesc\">Tuning the Java Virtual Machine (JVM) can improve performance or reduce high memory\n    consumption.</p><p class=\"p\">Tuning the Java Virtual Machine (JVM) can improve performance or reduce high memory\n      consumption. </p><div class=\"p\">On this page:<ul class=\"ul\"><li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__about-gc\">About garbage collection</a></li>\n        <li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__choose-gc\">Choosing a Java garbage collector</a></li>\n        <li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__setting-cms-gc\">Setting CMS as the Java garbage collector</a></li>\n        <li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__tuning-the-java-heap\">Determining the heap size</a></li>\n        <li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__how-cassandra-uses-memory\">How DataStax Enterprise uses memory</a> - Read first for a better\n          understanding of the settings and recommendations on this page.</li>\n        <li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__adjusting_params_for_other_Cassandra_services\">Adjusting JVM parameters for other DataStax Enterprise services</a></li>\n        <li class=\"li\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsTuneJVM.html#opsTuneJVM__jmx-options\">Other JMX options</a></li>\n      </ul></div><section class=\"section\" id=\"opsTuneJVM__choose-gc\"><h2 class=\"title sectiontitle\">Choosing a Java garbage collector</h2><div class=\"p\">DataStax Enterprise 5.1 uses the garbage first collector (G1) by default. G1 is recommended\n        for the following reasons:<ul class=\"ul\"><li class=\"li\">Heap sizes from 16 GB to 64 GB. <p class=\"p\">G1 performs better than CMS (concurrent-mark-sweep)\n              for larger heaps because it scans the regions of the heap containing the most garbage\n              objects first, and compacts the heap on-the-go, while CMS stops the application when\n              performing garbage collection.</p></li>\n          <li class=\"li\">The workload is variable, that is, the cluster is performing the different processes\n            all the time.</li>\n          <li class=\"li\">For future proofing, as CMS will be deprecated in Java 9.</li>\n          <li class=\"li\">G1 is easier to configure.</li>\n          <li class=\"li\">G1 is self tuning.</li>\n          <li class=\"li\">You only need to set MAX_HEAP_SIZE.</li>\n        </ul>However, G1 incurs some latency due to profiling.</div><div class=\"p\">CMS is recommended only in the following circumstances:<ul class=\"ul\"><li class=\"li\">You have the time and expertise to manually tune and test garbage collection. <p class=\"p\">Be\n              aware that allocating more memory to the heap can result in diminishing performance as\n              the garbage collection facility increases the amount of database metadata in heap\n              memory.</p></li>\n          <li class=\"li\">Heap sizes are smaller than 16 GB.</li>\n          <li class=\"li\">The workload is fixed, that is, the cluster performs the same processes all the\n            time.</li>\n          <li class=\"li\">The environment requires the lowest latency possible. </li>\n        </ul><p>Note: For help configuring CMS, contact the <a class=\"xref\" href=\"https://www.datastax.com/products/services\" target=\"_blank\">DataStax Services team</a>.</p></div></section><section class=\"section\" id=\"opsTuneJVM__setting-cms-gc\"><h2 class=\"title sectiontitle\">Setting CMS as the Java garbage collector</h2><ol class=\"ol\"><li class=\"li\">Open .</li>\n        <li class=\"li\">Comment out all lines in the <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">### GI Settings</code> section.</li>\n        <li class=\"li\">Uncomment all the <code data-swiftype-name=\"codeph\" data-swiftype-type=\"text\" class=\"ph codeph\">### CMS Settings</code> section</li>\n      </ol></section><section class=\"section\" id=\"opsTuneJVM__tuning-the-java-heap\"><h2 class=\"title sectiontitle\">Determining the heap size</h2><div class=\"p\">The database automatically calculates the maximum heap size (MAX_HEAP_SIZE) based on this\n        formula:<pre>max(min(1/2 ram, 1024 megabytes), min(1/4 ram, 32765 megabytes))</pre></div><div class=\"p\">For\n        production use, use these guidelines to adjust heap size for your environment:<ul class=\"ul\"><li class=\"li\">Heap size is usually between ¼ and ½ of system memory.</li>\n          <li class=\"li\">Do not devote all memory to heap because it is also used for offheap cache and file\n            system cache.</li>\n          <li class=\"li\">Always enable GC logging when adjusting GC.</li>\n          <li class=\"li\">Adjust settings gradually and test each incremental change.</li>\n          <li class=\"li\">Enable parallel processing for GC, particularly when using DSE Search.</li>\n          <li class=\"li\">The GCInspector class logs information about any garbage collection\n            that takes longer than 200 ms. Garbage collections that occur frequently and take a\n            moderate length of time (seconds) to complete, indicate excessive garbage collection\n            pressure on the JVM. In addition to adjusting the garbage collection options, other\n            remedies include adding nodes, and lowering cache sizes.</li>\n          <li class=\"li\">For a node using G1, DataStax recommends a MAX_HEAP_SIZE as large as possible, up to\n            64 GB.</li>\n        </ul><p>Note:  For more tuning tips, see <a class=\"xref\" href=\"http://blog.ragozin.info/2012/03/secret-hotspot-option-improving-gc.html\" target=\"_blank\">Secret HotSpot option improving GC pauses on large\n            heaps</a>.</p></div><p class=\"p\"><strong class=\"ph b\">MAX_HEAP_SIZE</strong></p><div class=\"p\">The recommended maximum heap size\n        depends on which GC is used:<table class=\"table frame-all\"><caption>G1 for newer computers (8+ cores) with up to 256 GB RAM\n                16 GB to 32765 MB.See <a class=\"xref\" href=\"http://java-performance.info/over-32g-heap-java\" target=\"_blank\">Java performance tuning</a>.\n              CMS for newer computers (8+ cores) with up to 256 GB RAM\n                No more 16 GB.\n              Older computers\n                Typically 8 GB.\n              </caption></table></div><div class=\"p\">The easiest way to determine the optimum heap size for your environment is:<ol class=\"ol\"><li class=\"li\">Set the maximum heap size in the  file to\n            a high arbitrary value on a single node. For example, when using\n              G1:<pre>-Xms48G\n-Xmx48G</pre><p class=\"p\">Set the min (-Xms) and max (-Xmx) heap\n              sizes to the same value to avoid stop-the-world GC pauses during resize, and to lock\n              the heap in memory on startup which prevents any of it from being swapped\n            out.</p></li>\n          <li class=\"li\">Enable GC logging.</li>\n          <li class=\"li\">Check the logs to view the heap used by that node and use that value for setting the\n            heap size in the cluster:</li>\n        </ol><p>Note: This method decreases performance for the test node, but generally does not\n          significantly reduce cluster performance.</p></div><p class=\"p\">If you don't see improved performance, contact the <a class=\"xref\" href=\"https://www.datastax.com/products/services\" target=\"_blank\">DataStax\n          Services team</a> for additional help.</p><p class=\"p\"><strong class=\"ph b\">HEAP_NEWSIZE</strong></p><div class=\"p\">For CMS, you may also need to adjust HEAP_NEWSIZE. This setting determines the amount of\n        heap memory allocated to newer objects or<em class=\"ph i\">young generation</em>. The database calculates\n        the default value for this property in megabytes (MB) as the lesser of:<ul class=\"ul\"><li class=\"li\">100 times the number of cores</li>\n          <li class=\"li\">¼ of MAX_HEAP_SIZE</li>\n        </ul></div><div class=\"p\">As a starting point, set HEAP_NEWSIZE to 100 MB per physical CPU core. For\n        example, for a modern 8-core+ machine:<pre>-Xmn800M</pre></div><p class=\"p\">A larger\n        HEAP_NEWSIZE leads to longer GC pause times. For a smaller HEAP_NEWSIZE, GC pauses are\n        shorter but usually more expensive.</p>See<a class=\"xref\" href=\"https://docs.datastax.com/en/dse-planning/doc/planning/planningHardware.html#planningHardware__memory\" target=\"_blank\">Recommended minimum memory for dedicated hardware and virtual\n        environments</a>.</section><section class=\"section\" id=\"opsTuneJVM__how-cassandra-uses-memory\"><h2 class=\"title sectiontitle\">How DataStax Enterprise uses memory</h2><div class=\"p\">The database performs the following major operations within JVM heap:<ul class=\"ul\" id=\"opsTuneJVM__ul_enc_3dg_vw\"><li class=\"li\">To perform reads, the database maintains the following components in heap memory:<ul class=\"ul\"><li class=\"li\">Bloom filters</li>\n              <li class=\"li\">Partition summary</li>\n              <li class=\"li\">Partition key cache</li>\n              <li class=\"li\">Compression offsets</li>\n              <li class=\"li\">SSTable index summary</li>\n            </ul><p class=\"p\">This metadata resides in memory and is proportional to total data. Some of the\n              components <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-arch/datastax_enterprise/dbInternals/dbIntAboutReads.html\" target=\"_blank\">grow proportionally to the size of total\n                memory</a>.</p></li>\n          <li class=\"li\">The database gathers replicas for a read or for anti-entropy repair and compares the\n            replicas in heap memory.</li>\n          <li class=\"li\">Data written to the database is first stored in memtables in heap memory. Memtables\n            are flushed to SSTables on disk.</li>\n        </ul></div><div class=\"p\">To improve performance, the database also uses off-heap memory as follows:<ul class=\"ul\" id=\"opsTuneJVM__ul_uxp_c2g_vw\"><li class=\"li\">Page cache. The database uses additional memory as page cache when reading files on\n            disk.</li>\n          <li class=\"li\">The Bloom filter and compression offset maps reside off-heap.</li>\n          <li class=\"li\">The database can store cached rows in native memory, outside the Java heap. This\n            reduces JVM heap requirements, which helps keep the heap size in the sweet spot for JVM\n            garbage collection performance. </li>\n        </ul></div></section><section class=\"section\" id=\"opsTuneJVM__adjusting_params_for_other_Cassandra_services\"><h2 class=\"title sectiontitle\">Adjusting JVM parameters for other DataStax Enterprise services</h2><ul class=\"ul\"><li class=\"li\"><strong class=\"ph b\">DSE Search</strong>: Some users have reported that increasing the stack size improves\n          performance under Tomcat. <div class=\"p\">To increase the stack size, uncomment and modify the default\n            setting in thefile.<pre># Per-thread stack size.\nJVM_OPTS=\"$JVM_OPTS -Xss256k\"</pre>Also,\n            decreasing the memtable space to make room for search caches can improve performance.\n            Modify the memtable space by changing the<a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/config/configCassandra_yaml.html#configCassandra_yaml__memtable_heap_space_in_mb\">memtable_heap_space_in_mb</a>and<a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/config/configCassandra_yaml.html#configCassandra_yaml__memtable_offheap_space_in_mb\">memtable_offheap_space_in_mb</a>properties in thefile.</div></li>\n        <li class=\"li\">\n          <p class=\"p\"><strong class=\"ph b\">MapReduce</strong>: Because MapReduce runs outside the JVM, changes to the JVM do not\n            affect Analytics/Hadoop operations directly.</p>\n        </li>\n      </ul></section><section class=\"section\" id=\"opsTuneJVM__jmx-options\"><h2 class=\"title sectiontitle\">Other JMX options</h2><p class=\"p\">DataStax Enterprise exposes other statistics and management operations via Java Management\n        Extensions (JMX). <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsMonitoring.html#opsMonitoringJconsole\">JConsole</a>, the <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/opsMonitoring.html#opsMonitorNodetool\" title=\"Get statistics using nodetool commands.\">nodetool</a> are JMX-compliant management tools.</p><p class=\"p\">Configure the database for JMX management by editing these properties in\n        .</p><ul class=\"ul\"><li class=\"li\">com.sun.management.jmxremote.port: sets the port on which the\n          database listens from JMX connections. </li>\n        <li class=\"li\">com.sun.management.jmxremote.ssl: enables or disables SSL for JMX. </li>\n        <li class=\"li\">com.sun.management.jmxremote.authenticate: enables or disables remote\n          authentication for JMX. </li>\n        <li class=\"li\">-Djava.rmi.server.hostname: sets the interface hostname or IP that\n          JMX should use to connect. Uncomment and set if you are having trouble connecting. </li>\n      </ul><p>Note: By default, you can interact with DataStax Enterprise using JMX on port 7199 without\n        authentication.</p></section><section class=\"section\"></section>",
        "created_at": "2018-07-24T19:54:40+0000",
        "updated_at": "2018-07-24T19:54:40+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 5,
        "domain_name": "docs.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11157"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          },
          {
            "id": 1173,
            "label": "troubleshooting and tuning",
            "slug": "troubleshooting-and-tuning"
          }
        ],
        "is_public": true,
        "id": 11156,
        "uid": "5b5783fcbf4fe3.58982921",
        "title": "Secret HotSpot option improving GC pauses on large heaps",
        "url": "http://blog.ragozin.info/2012/03/secret-hotspot-option-improving-gc.html",
        "content": "<div><p>\n<a href=\"http://www.blogger.com/2011/07/openjdk-patch-cutting-down-gc-pause.html\">my\nPatch mentioned in this post</a> (<i><a href=\"http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7068625\">RFE-7068625</a>)</i>\nfor JVM garbage collector was accepted into HotSpot JDK code base and available starting from 7u40 version of HotSport JVM from Oracle.</p></div>\n<div class=\"MsoNormal\">\n<br />\nThis was a reason for me to redo some of my GC benchmarking experiments. I have already mentioned ParGCCardsPerStrideChunk\nin article related to patch. This time, I decided study effect of this option more closely.</div>\n<div class=\"MsoNormal\">\n<br />\nParallel copy collector (ParNew), responsible for young\ncollection in CMS, use ParGCCardsPerStrideChunk\n value to control granularity of tasks\ndistributed between worker threads. Old space is broken into strides of equal\nsize and each worker responsible for processing (find dirty pages, find old to\nyoung references, copy young objects etc) a subset of strides. Time to process each\nstride may vary greatly, so workers may steal work from each other. For that\nreason number of strides should be greater than number of workers. </div>\n<div class=\"MsoNormal\">\n<br />\nBy default ParGCCardsPerStrideChunk\n=256 (card is 512 bytes, so it would be 128KiB of heap space per stride) which means that 28GiB heap\nwould be broken into 224 thousands of strides. Provided that number of parallel\nGC threads is usually 4 orders of magnitude less, this is probably too many.</div>\n<h4>\nSynthetic benchmark</h4>\n<div class=\"MsoNormal\">\nFirst, I have run GC benchmark from <a href=\"http://www.blogger.com/2011/07/openjdk-patch-cutting-down-gc-pause.html\">previous\narticle</a> using 2k, 4k and 8K for this option. HotSpot JVM 7u3 was used in\nexperiment. </div>\n<div class=\"MsoNormal\">\n<div class=\"separator\">\n<a href=\"http://1.bp.blogspot.com/-JqGI0UUFw_8/T3NivPutmSI/AAAAAAAAKTs/X5H4eqqOdOs/s1600/blog-25.png\"><img border=\"0\" height=\"242\" src=\"http://1.bp.blogspot.com/-JqGI0UUFw_8/T3NivPutmSI/AAAAAAAAKTs/X5H4eqqOdOs/s400/blog-25.png\" width=\"400\" alt=\"image\" /></a></div>\n<br /></div>\n<div class=\"MsoNormal\">\nIt seems that default value (256 cards per strides) is too\nsmall even for moderate size heaps. I decided to continue my experiments with\nstride size 4k as it shows most consistent improvement across whole range of\nheap sizes.</div>\n<div class=\"MsoNormal\">\n<br />\nBenchmark above is synthetic and very simple. Next step is\nto choose more realistic use case. I usual, my choice is to use <a href=\"http://www.oracle.com/technetwork/middleware/coherence/overview/index.html\">Oracle\nCoherence</a> storage node as my guinea pig.</div>\n<h4>\nBenchmarking Coherence storage node</h4>\n<div class=\"MsoNormal\">\nIn this experiment I’m filling cache node with objects (object\n70% of old space filled with live objects), then put it under mixed read/write\nload and measuring young GC pauses of JVM. Experiment was conducted with two\ndifferent heap sizes (28 GiB and 14 GiB), young space for both cases was\nlimited by 128MiB, compressed pointers were enabled.</div>\n<h5>\nCoherence node with 28GiB of heap</h5>\n<table border=\"1\" cellpadding=\"0\" class=\"MsoNormalTable\" style=\"border-spacing: 0px;\"><tbody><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n<b>JVM</b></div>\n</td>\n  <td valign=\"top\" style=\"width: 90px;\"><div class=\"MsoNormal\">\n<b>Avg. pause</b></div>\n</td>\n  <td valign=\"top\" style=\"width: 108px;\"><div class=\"MsoNormal\">\n<b>Improvement</b></div>\n</td>\n </tr><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n7u3</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 90px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0.0697</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 108px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0</div>\n</td>\n </tr><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n7u3, stride=4k</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 90px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0.045</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 108px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n35.4%</div>\n</td>\n </tr><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n<a href=\"http://blog.ragozin.info/2011/07/openjdk-patch-cutting-down-gc-pause.html\">Patched OpenJDK 7</a></div>\n</td>\n  <td valign=\"bottom\" style=\"width: 90px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0.0546</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 108px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n21.7%</div>\n</td>\n </tr><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n<a href=\"http://blog.ragozin.info/2011/07/openjdk-patch-cutting-down-gc-pause.html\">Patched OpenJDK 7</a>,\n  stride=4k</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 90px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0.0284</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 108px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n59.3%</div>\n</td>\n </tr></tbody></table><h5>\nCoherence node with 14GiB of heap</h5>\n<table border=\"1\" cellpadding=\"0\" class=\"MsoNormalTable\" style=\"border-spacing: 0px;\"><tbody><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n<b>JVM</b></div>\n</td>\n  <td valign=\"top\" style=\"width: 90px;\"><div class=\"MsoNormal\">\n<b>Avg. pause</b></div>\n</td>\n  <td valign=\"top\" style=\"width: 108px;\"><div class=\"MsoNormal\">\n<b>Improvement</b></div>\n</td>\n </tr><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n7u3</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 90px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0.05</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 108px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0</div>\n</td>\n </tr><tr><td valign=\"top\" style=\"width: 193px;\"><div class=\"MsoNormal\">\n7u3, stride=4k</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 90px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n0.0322</div>\n</td>\n  <td valign=\"bottom\" style=\"width: 108px;\"><div class=\"MsoNormal\" style=\"text-align: right;\">\n35.6%</div>\n</td>\n </tr></tbody></table><div class=\"MsoNormal\">\nThis test is close enough to real live Coherence work\nprofile and such improvement of GC pause time has practical importance. I have\nalso included JVM built from OpenJDK trunk with enabled <i><a href=\"http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=7068625\">RFE-7068625</a>\n</i>patch for 28 GiB test, as expected effect of patch is cumulative with\nstride size tuning.</div>\n<div class=\"MsoNormal\">\n<h4>\nStock JVMs from Oracle are supported</h4>\nGood news is that you do not have to wait for next version\nof JVM, ParGCCardsPerStrideChunk\noption is available in all Java 7 HotSpot JVMs and most recent Java 6 JVMs. But\nthis option is classified as diagnostic so you should enable diagnostic options\nto use it.</div>\n<div>\n<div class=\"MsoNormal\">\n-XX:+UnlockDiagnosticVMOptions </div>\n<div class=\"MsoNormal\">\n-XX:ParGCCardsPerStrideChunk=4096</div>\n</div>\n<div>\n<br /></div>\n<div></div>",
        "created_at": "2018-07-24T19:54:36+0000",
        "updated_at": "2018-07-24T19:54:36+0000",
        "published_at": "2012-03-28T00:00:00+0000",
        "published_by": [
          "Alexey Ragozin"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 2,
        "domain_name": "blog.ragozin.info",
        "preview_picture": "http://1.bp.blogspot.com/-JqGI0UUFw_8/T3NivPutmSI/AAAAAAAAKTs/X5H4eqqOdOs/w1200-h630-p-k-no-nu/blog-25.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11156"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1128,
            "label": "tutorials",
            "slug": "tutorials"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          }
        ],
        "is_public": true,
        "id": 11153,
        "uid": "5b5783f2584c52.03160098",
        "title": "Basic Rules of Cassandra Data Modeling",
        "url": "https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling",
        "content": "<div class=\"DXDvBlgCtaAD_wrp1\"><p>Learn more about Apache Cassandra and data modeling</p></div><p>Picking the right <a href=\"http://planetcassandra.org/getting-started-with-time-series-data-modeling/\">data model</a> is the hardest part of using Cassandra.  If you have a relational background, CQL will look familiar, but the way you use it can be very different.  The goal of this post is to explain the basic rules you should keep in mind when designing your schema for Cassandra.  If you follow these rules, you'll get pretty good performance out of the box.  Better yet, your performance should scale linearly as you add nodes to the cluster.</p><h2>Non-Goals</h2><p>Developers coming from a relational background usually carry over rules about relational modeling and try to apply them to Cassandra.  To avoid wasting time on rules that don't really matter with Cassandra, I want to point out some <em>non</em>-goals:</p><h3>Minimize the Number of Writes</h3><p>Writes in Cassandra aren't free, but they're awfully cheap.  Cassandra is optimized for high write throughput, and almost all writes are equally efficient <a href=\"#footnote\"><sup>[1]</sup></a>.  If you can perform extra writes to improve the efficiency of your read queries, it's almost always a good tradeoff.  Reads tend to be more expensive and are much more difficult to tune.</p><h3>Minimize Data Duplication</h3><p>Denormalization and duplication of data is a fact of life with Cassandra.  Don't be afraid of it.  Disk space is generally the cheapest resource (compared to CPU, memory, disk IOPs, or network), and Cassandra is architected around that fact.  In order to get the most efficient reads, you often need to duplicate data.</p><p>Besides, Cassandra doesn't have <code>JOIN</code>s, and you don't really want to use those in a distributed fashion.</p><h2>Basic Goals</h2><p>These are the two high-level goals for your data model:</p><ol><li>Spread data evenly around the cluster</li>\n<li>Minimize the number of partitions read</li>\n</ol><p>There are other, lesser goals to keep in mind, but these are the most important. For the most part, I will focus on the basics of achieving these two goals.  There are other fancy tricks you can use, but you should know how to evaluate them, first.</p><h3>Rule 1: Spread Data Evenly Around the Cluster</h3><p>You want every node in the cluster to have roughly the same amount of data. Cassandra makes this easy, but it's not a given.  Rows are spread around the cluster based on a hash of the <a href=\"https://www.datastax.com/documentation/cassandra/2.1/cassandra/architecture/architectureDataDistributeHashing_c.html\"><em>partition key</em></a>, which is the first element of the <a href=\"https://www.datastax.com/documentation/cql/3.0/cql/ddl/ddl_compound_keys_c.html\"><code>PRIMARY KEY</code></a>.  So, the key to spreading data evenly is this: <b>pick a good primary key</b>.  I'll explain how to do this in a bit.</p><h3>Rule 2: Minimize the Number of Partitions Read</h3><p>Partitions are groups of rows that share the same partition key.  When you issue a read query, you want to read rows from as few partitions as possible.</p><p>Why is this important?  Each partition may reside on a different node. The coordinator will generally need to issue separate commands to separate nodes for each partition you request.  This adds a lot of overhead and increases the variation in latency.  Furthermore, even on a single node, it's more expensive to read from multiple partitions than from a single one due to the way rows are stored.</p><h3>Conflicting Rules?</h3><p>If it's good to minimize the number of partitions that you read from, why not put everything in a single big partition?  You would end up violating Rule #1, which is to spread data evenly around the cluster.</p><p>The point is, these two goals often conflict, so you'll need to try to balance them.</p><h2>Model Around Your Queries</h2><p>The way to minimize partition reads is to model your data to fit your queries. Don't model around relations.  Don't model around objects.  Model around your queries.  Here's how you do that:</p><h3>Step 1: Determine What Queries to Support</h3><p>Try to determine <em>exactly</em> what queries you need to support.  This can include a lot of considerations that you may not think of at first.  For example, you may need to think about:</p><ul><li>Grouping by an attribute</li>\n<li>Ordering by an attribute</li>\n<li>Filtering based on some set of conditions</li>\n<li>Enforcing uniqueness in the result set</li>\n<li>etc ...</li>\n</ul><p>Changes to just one of these query requirements will frequently warrant a data model change for maximum efficiency.</p><h3>Step 2: Try to create a table where you can satisfy your query by reading (roughly) one partition</h3><p>In practice, this generally means you will use roughly one table per query pattern. If you need to support multiple query patterns, you usually need more than one table.</p><p>To put this another way, each table should pre-build the \"answer\" to a high-level query that you need to support.  If you need different types of answers, you usually need different tables.  This is how you optimize for reads.</p><p>Remember, data duplication is okay.  Many of your tables may repeat the same data.</p><h2>Applying the Rules: Examples</h2><p>To show some examples of a good throught process, I will walk you through the design of a data model for some simple problems.</p><h3>Example 1: User Lookup</h3><p>The high-level requirement is \"we have users and want to look them up\".  Let's go through the steps:</p><p><b>Step 1</b>: <em>Determine what specific queries to support</em><br />Let's say we want to either be able to look up a user by their username or their email.  With either lookup method, we should get the full set of user details.</p><p><b>Step 2</b>: <em>Try to create a table where you can satisfy your query by reading (roughly) one partition</em><br />Since we want to get the full details for the user with either lookup method, it's best to use two tables:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE users_by_username (&#13;\n    username text PRIMARY KEY,&#13;\n    email text,&#13;\n    age int&#13;\n)&#13;\n&#13;\nCREATE TABLE users_by_email (&#13;\n    email text PRIMARY KEY,&#13;\n    username text,&#13;\n    age int&#13;\n)&#13;\n</pre><p>Now, let's check the two rules for this model:</p><p><b>Spreads data evenly?</b> Each user gets their own partition, so yes.<br /><b>Minimal partitions read?</b> We only have to read one partition, so yes.</p><p>Now, let's suppose we tried to optimize for the <em>non</em>-goals, and came up with this data model instead:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE users (&#13;\n    id uuid PRIMARY KEY,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int&#13;\n)&#13;\n&#13;\nCREATE TABLE users_by_username (&#13;\n    username text PRIMARY KEY,&#13;\n    id uuid&#13;\n)&#13;\n&#13;\nCREATE TABLE users_by_email (&#13;\n    email text PRIMARY KEY,&#13;\n    id uuid&#13;\n)&#13;\n</pre><p>This data model also spreads data evenly, but there's a downside: we now have to read two partitions, one from <code>users_by_username</code> (or <code>users_by_email</code>) and then one from <code>users</code>.  So reads are roughly twice as expensive.</p><h3>Example 2: User Groups</h3><p>Now the high-level requirement has changed.  Users are in groups, and we want to get all users in a group.</p><p><b>Step 1</b>: <em>Determine what specific queries to support</em><br />We want to get the full user info for every user in a particular group.  Order of users does not matter.</p><p><b>Step 2</b>: <em>Try to create a table where you can satisfy your query by reading (roughly) one partition</em><br />How do we fit a group into a partition?  We can use a compound <code>PRIMARY KEY</code> for this:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE groups (&#13;\n    groupname text,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    PRIMARY KEY (groupname, username)&#13;\n)&#13;\n</pre><p>Note that the <code>PRIMARY KEY</code> has two components: <code>groupname</code>, which is the partitioning key, and <code>username</code>, which is called the clustering key.  This will give us one partition per <code>groupname</code>.  Within a particular partition (group), rows will be ordered by <code>username</code>.  Fetching a group is as simple as doing the following:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nSELECT * FROM groups WHERE groupname = ?&#13;\n</pre><p>This satisfies the goal of minimizing the number of partitions that are read, because we only need to read one partition.  However, it doesn't do so well with the first goal of evenly spreading data around the cluster.  If we have thousands or millions of small groups with hundreds of users each, we'll get a pretty even spread.  But if there's one group with millions of users in it, the entire burden will be shouldered by one node (or one set of replicas).</p><p>If we want to spread the load more evenly, there are a few strategies we can use. The basic technique is to add another column to the PRIMARY KEY to form a compound partition key.  Here's one example:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE groups (&#13;\n    groupname text,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    hash_prefix int,&#13;\n    PRIMARY KEY ((groupname, hash_prefix), username)&#13;\n)&#13;\n</pre><p>The new column, <code>hash_prefix</code>, holds a prefix of a hash of the username.  For example, it could be the first byte of the hash modulo four. Together with <code>groupname</code>, these two columns form the compound partition key.  Instead of a group residing on one partition, it's now spread across four partitions.  Our data is more evenly spread out, but we now have to read four times as many partitions.  This is an example of the two goals conflicting.  You need to find a good balance for your particular use case.  If you do a lot of reads and groups don't get too large, maybe changing the modulo value from four to two would be a good choice.  On the other hand, if you do very few reads, but any given group can grow very large, changing from four to ten would be a better choice.</p><p>There are other ways to split up a partition, which I will cover in the next example.</p><p>Before we move on, let me point out something else about this data model: we're duplicating user info potentially many times, once for each group. You might be tempted to try a data model like this to reduce duplication:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE users (&#13;\n    id uuid PRIMARY KEY,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int&#13;\n)&#13;\n&#13;\nCREATE TABLE groups (&#13;\n    groupname text,&#13;\n    user_id uuid,&#13;\n    PRIMARY KEY (groupname, user_id)&#13;\n)&#13;\n</pre><p>Obviously, this minimizes duplication.  But how many partitions do we need to read? If a group has 1000 users, we need to read 1001 partitions.  This is probably 100x more expensive to read than our first data model.  If reads need to be efficient at all, this isn't a good model.  On the other hand, if reads are extremely infrequent, but updates to user info (say, the username) are extremely common, this data model might actually make sense.  Make sure to take your read/update ratio into account when designing your schema.</p><h3>Example 3: User Groups by Join Date</h3><p>Suppose we continue with the previous example of groups, but need to add support for getting the X newest users in a group.</p><p>We can use a similar table to the last one:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE group_join_dates (&#13;\n    groupname text,&#13;\n    joined timeuuid,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    PRIMARY KEY (groupname, joined)&#13;\n)&#13;\n</pre><p>Here we're using a <code>timeuuid</code> (which is like a timestamp, but avoids collisions) as the clustering column.  Within a group (partition), rows will be ordered by the time the user joined the group.  This allows us to get the newest users in a group like so:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nSELECT * FROM group_join_dates&#13;\n    WHERE groupname = ?&#13;\n    ORDER BY joined DESC&#13;\n    LIMIT ?&#13;\n</pre><p>This is reasonably efficient, as we're reading a slice of rows from a single partition.  However, instead of always using <code>ORDER BY joined DESC</code>, which makes the query less efficient, we can simply reverse the clustering order:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE group_join_dates (&#13;\n    groupname text,&#13;\n    joined timeuuid,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    PRIMARY KEY (groupname, joined)&#13;\n) WITH CLUSTERING ORDER BY (joined DESC)&#13;\n</pre><p>Now we can use the slightly more efficient query:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nSELECT * FROM group_join_dates&#13;\n    WHERE groupname = ?&#13;\n    LIMIT ?&#13;\n</pre><p>As with the previous example, we could have problems with data being spread evenly around the cluster if any groups get too large.  In that example, we split partitions somewhat randomly, but in this case, we can utilize our knowledge about the query patterns to split partitions a different way: by a time range.</p><p>For example, we might split partitions by date:</p><pre class=\"brush: sql; gutter: true; title: ; notranslate\" title=\"\">&#13;\nCREATE TABLE group_join_dates (&#13;\n    groupname text,&#13;\n    joined timeuuid,&#13;\n    join_date text,&#13;\n    username text,&#13;\n    email text,&#13;\n    age int,&#13;\n    PRIMARY KEY ((groupname, join_date), joined)&#13;\n) WITH CLUSTERING ORDER BY (joined DESC)&#13;\n</pre><p>We're using a compound partition key again, but this time we're using the join date.  Each day, a new partition will start.  When querying the X newest users, we will first query today's partition, then yesterday's, and so on, until we have X users.  We may have to read multiple partitions before the limit is met.</p><p>To minimize the number of partitions you need to query, try to select a time range for splitting partitions that will typically let you query only one or two partitions.  For example, if we usually need the ten newest users, and groups usually acquire three users per day, we should split by four-day ranges instead of a single day <a href=\"#footnote\"><sup>[2]</sup></a>.</p><h2>Summary</h2><p>The basic rules of data modeling covered here apply to all (currently) existing versions of Cassandra, and are very likely to apply to all future versions.  Other lesser data modeling problems, such as <a href=\"https://medium.com/@foundev/domain-modeling-around-deletes-1cc9b6da0d24\">dealing with tombstones</a>, may also need to be considered, but these problems are more likely to change (or be mitigated) by future versions of Cassandra.</p><p>Besides the basic strategies covered here, some of Cassandra's fancier features, like <a href=\"https://www.datastax.com/dev/blog/cql3_collections\">collections</a>, <a href=\"https://www.datastax.com/dev/blog/cql-in-2-1\">user-defined types</a>, and <a href=\"https://www.datastax.com/documentation/cql/3.1/cql/cql_reference/refStaticCol.html\">static columns</a>, can also be used to reduce the number of partitions that you need to read to satisfy a query.  Don't forget to consider these options when designing your schema.</p><p>Hopefully I've given you some useful fundamental tools for evaluating different schema designs.  If you want to go further, I suggest taking <a href=\"https://academy.datastax.com/courses/ds220-data-modeling?dxt=blogposting\">Datastax's free, self-paced online data modeling course (DS220)</a>.  Good luck!</p><p>[1]: Notable exceptions: <a href=\"https://www.datastax.com/dev/blog/whats-new-in-cassandra-2-1-a-better-implementation-of-counters\">counters</a>, <a href=\"https://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0\">lightweight transactions</a>, and <a href=\"http://cassandra.apache.org/doc/cql3/CQL.html#collections\">inserting into the middle of a list collection</a>.</p><p>[2]: I suggest using a timestamp truncated by some number of seconds.  For example, to handle four-day ranges, you might use something like this:</p><pre class=\"brush: python; gutter: true; title: ; notranslate\" title=\"\">&#13;\nnow = time()&#13;\nfour_days = 4 * 24 * 60 * 60&#13;\nshard_id = now - (now % four_days)&#13;\n</pre><hr /><p><a href=\"https://www.datastax.com/\">DataStax</a> has many ways for you to advance in your career and knowledge. \n</p><p>You can take <a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" title=\"academy.datastax.com\">free classes</a>, <a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" title=\"academy.datastax.com/certifications\">get certified</a>, or read <a href=\"https://www.datastax.com/dbas-guide-to-nosql\" target=\"_self\" title=\"dbas-guide-to-nosql\">one of our many white papers</a>.\n</p><p><a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com\">register for classes</a>\n</p><p><a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com/certifications\">get certified</a>\n</p><p><a href=\"http://www.datastax.com/dbas-guide-to-nosql?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_dbasguidetonosql\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"dbas-guide-to-nosql\">DBA's Guide to NoSQL</a>\n</p><br class=\"clear\" /><div id=\"mto_newsletter_121316_Css\"><p>Subscribe for newsletter:</p><br /></div>",
        "created_at": "2018-07-24T19:54:26+0000",
        "updated_at": "2018-07-24T19:54:26+0000",
        "published_at": "2015-02-02T16:50:05+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 12,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/themes/datastax-2014-08/images/blog/DataModelingPostBannerAd_cover_brght.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11153"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1128,
            "label": "tutorials",
            "slug": "tutorials"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          }
        ],
        "is_public": true,
        "id": 11152,
        "uid": "5b5783ec170b22.98669396",
        "title": "Installing the Cassandra / Spark OSS Stack",
        "url": "http://tobert.github.io/post/2014-07-15-installing-cassandra-spark-stack.html",
        "content": "<h2>Init</h2><p>As mentioned in my <a href=\"http://tobert.github.io/post/2014-07-14-portacluster-system-imaging.html\">portacluster system imaging</a> post,\nI am performing this install on 1 admin node (node0) and 6 worker nodes (node[1-6]) running 64-bit Arch Linux.\nMost of what I describe in this post should work on other Linux variants with minor adjustments.</p><h2>Overview</h2><p>When assembling an analytics stack, there are usually myriad choices to make. For this build, I decided to\nbuild the smallest stack possible that lets me run Spark queries on Cassandra data. As configured it is\nnot highly available since the Spark master is standalone. (note: Datastax Enterprise Spark's master has\nHA based on Cassandra). It's a decent tradeoff for portacluster, since\nI can run the master on the admin node which doesn't get rebooted/reimaged constantly. I'm also going to\nskip HDFS or some kind of HDFS replacement for now. Options I plan to look at later are GlusterFS's HDFS\nadapter and <a href=\"http://pithos.io\">Pithos</a> as an S3 adapter. In the end, the stack is simply Cassandra and\nSpark with the <a href=\"https://github.com/datastax/spark-cassandra-connector\">spark-cassandra-connector</a>.</p><h2>Responsible Configuration</h2><p>For this post I've used my <a href=\"https://github.com/tobert/perl-ssh-tools\">perl-ssh-tools</a> suite. The intent\nis to show what needs to be done and one way to do it. For production deployments, I recommend using\nyour favorite configuration management tool.</p><p>perl-ssh-tools uses a configuration similar to dsh, which uses simple files with one\nhost per line. I use two lists below. Most commands run on the fleet of workers. Because\ncl-run.pl provides more than ssh commands, it's also used to run commands on node0 using\nits --incl flag e.g. <code>cl-run.pl --list all --incl node0</code>.</p><pre>cat .dsh/machines.workers\nnode1\nnode2\nnode3\nnode4\nnode5\nnode6\n</pre><p><code>machines.all</code> is the same with node0 added.</p><h2>Install Cassandra</h2><p>My first pass at this section involved setting up a package repo, but since I don't have time to package\nSpark properly right now, I'm going to use the tarball distros of Cassandra and Spark to keep it simple.\n<a href=\"https://github.com/joschi\">joschi</a> maintains a package on the <a href=\"https://aur.archlinux.org/packages/cassandra/\">AUR</a>\nbut I have chosen not to use it for this install.\nI'm also using the Arch packages of OpenJDK, which isn't supported by Datastax, but works fine for hacking.\nThe JDK is pre-installed on my Arch image, it's as simple as <code>sudo pacman -S extra/jdk7-openjdk</code>.</p><p>First, I downloaded the Cassandra tarball from <a href=\"http://cassandra.apache.org/download/\">apache.org</a> to\nnode0 in /srv/public/tgz. Then on the worker nodes, it gets downloaded and expanded in /opt.</p><pre>pkg=\"apache-cassandra-2.0.9-bin.tar.gz\"\nsudo curl -o /srv/public/tgz/$pkg \\\n  http://mirrors.gigenet.com/apache/cassandra/2.0.9/apache-cassandra-2.0.9-bin.tar.gz\ncl-run.pl --list workers -c \"curl http://node0/tgz/$pkg |sudo tar -C /opt -xzf -\"\ncl-run.pl --list workers -c \"sudo ln -s /opt/apache-cassandra-2.0.9 /opt/cassandra\"\n</pre><p>To make it easier to do upgrades without regenerating the configuration, I\nrelocate the conf dir to /etc/cassandra to match what packages do. This assumes there\nis no existing /etc/cassandra.</p><pre>cl-run.pl --list workers -c \"sudo mv /opt/cassandra/conf /etc/cassandra\"\ncl-run.pl --list workers -c \"sudo ln -s /etc/cassandra /opt/cassandra/conf\"\n</pre><p>I will start Cassandra with a systemd unit, so I push that out as well. This unit\nfile runs Cassandra out of the tarball as the cassandra user with the stdout/stderr going\nto the systemd journal (view with <code>journalctl -f</code>). I also included some\nulimit settings and bump the OOM score downwards to make it less likely that the kernel\nwill kill Cassandra when out of memory. Since we're going to be running two large JVM apps\non each worker node, this unit also enables cgroups so Cassandra can be given priority\nover Spark. Finally, since the target machines have 16GB of RAM, the heap needs to be\nset to 8GB (cassandra-env.sh calculates 3995M which is way too low).</p><pre>cat &gt; cassandra.service &lt;&lt;EOF\n[Unit]\nDescription=Cassandra Tarball\nAfter=network.target\n[Service]\nUser=cassandra\nGroup=cassandra\nRuntimeDirectory=cassandra\nPIDFile=/run/cassandra/cassandra.pid\nExecStart=/opt/cassandra/bin/cassandra -f -p /run/cassandra/cassandra.pid\nStandardOutput=journal\nStandardError=journal\nOOMScoreAdjust=-500\nLimitNOFILE=infinity\nLimitMEMLOCK=infinity\nLimitNPROC=infinity\nLimitAS=infinity\nEnvironment=MAX_HEAP_SIZE=8G HEAP_NEWSIZE=1G CASSANDRA_HEAPDUMP_DIR=/srv/cassandra/log\nCPUAccounting=true\nCPUShares=1000\n[Install]\nWantedBy=multi-user.target\nEOF\ncl-sendfile.pl --list workers -x -l cassandra.service -r /etc/systemd/system/multi-user.target.wants/cassandra.service\ncl-run.pl --list workers -c \"sudo systemctl daemon-reload\"\n</pre><p>Since all Cassandra data is being redirected to /srv/cassandra and it's going to run as the\ncassandra user, those need to be created.</p><pre>cat &gt; cassandra-user.sh &lt;&lt;EOF\nmkdir -p /srv/cassandra/{log,data,commitlogs,saved_caches}\n(grep -q '^cassandra:' /etc/group)  || groupadd -g 1234 cassandra\n(grep -q '^cassandra:' /etc/passwd) || useradd -u 1234 -c \"Apache Cassandra\" -g cassandra -s /bin/bash -d /srv/cassandra cassandra\nchown -R cassandra:cassandra /srv/cassandra\nEOF\ncl-run.pl --list workers -x -s cassandra-user.sh\n</pre><h2>Configure Cassandra</h2><p>Before starting Cassandra I want to make a few changes to the standard configurations. I'm not a big\nfan of LSB so I redirect all of the /var files to /srv/cassandra so they're all in one place. There's\nonly one SSD in the target systems so the commit log goes on the same drive.</p><p>I configured portacluster nodes to have a bridge in front of the default interface, making br0 the default interface.</p><pre>cat cassandra-config.sh\nip=$(ip addr show br0 |perl -ne 'if ($_ =~ /inet (\\d+\\.\\d+\\.\\d+\\.\\d+)/) { print $1 }')\nperl -i.bak -pe \"\n  s/^(cluster_name:).*/\\$1 'Portable Cluster'/;\n  s/^(listen|rpc)_address:.*/\\${1}_address: $ip/;\n  s|/var/lib|/srv|;\n  s/(\\s+-\\s+seeds:).*/\\$1 '192.168.10.11,192.168.10.12,192.168.10.13,192.168.10.14,192.168.10.15,192.168.10.16'/\n\" /opt/cassandra/conf/cassandra.yaml\n# EOF\ncl-run.pl --list workers -x -s cassandra-config.sh\n</pre><p>The default log4-server.propterties has log4j printing to stdout. This is not desirable in a background\nservice configuration, so I remove it. The logs are also now written to /srv/cassandra/log.</p><pre>cat &gt; log4j-server.properties &lt;&lt;EOF\nlog4j.rootLogger=INFO,R\nlog4j.appender.R=org.apache.log4j.RollingFileAppender\nlog4j.appender.R.maxFileSize=20MB\nlog4j.appender.R.maxBackupIndex=20\nlog4j.appender.R.layout=org.apache.log4j.PatternLayout\nlog4j.appender.R.layout.ConversionPattern=%5p [%t] %d{ISO8601} %F (line %L) %m%n\nlog4j.appender.R.File=/srv/cassandra/log/system.log\nlog4j.logger.org.apache.thrift.server.TNonblockingServer=ERROR\nEOF\ncl-sendfile.pl --list workers -x -l log4j-server.properties -r /opt/cassandra/conf/log4j-server.properties\n</pre><p>And with that, Cassandra is ready to start.</p><pre>cl-run.pl --list workers -c \"sudo systemctl start cassandra.service\"\nssh node3 tail -f /srv/cassandra/log/system.log\n</pre><h2>Installing Spark</h2><p>The process for Spark is quite similar, except that unlike Cassandra, it has a master.</p><p>Since I'm not using any Hadoop components, any of the builds should be fine so I used the\nhadoop2 build.</p><pre>pkg=\"spark-1.0.1-bin-hadoop2.tgz\"\nsudo curl -o /srv/public/tgz/$pkg http://d3kbcqa49mib13.cloudfront.net/spark-1.0.1-bin-hadoop2.tgz\ncl-run.pl --list all -c \"curl http://node0/tgz/$pkg |sudo tar -C /opt -xzf -\"\ncl-run.pl --list all -c \"sudo ln -s /opt/spark-1.0.1-bin-hadoop2 /opt/spark\"\ncl-run.pl --list all -c \"sudo mv /opt/spark/conf /etc/spark\"\ncl-run.pl --list all -c \"sudo ln -s /etc/spark /opt/spark/conf\"\n</pre><p>Create /srv/spark and the spark user.</p><pre>cat &gt; spark-user.sh &lt;&lt;EOF\nmkdir -p /srv/spark/{logs,work,tmp,pids}\n(grep -q '^spark:' /etc/group)  || groupadd -g 4321 spark\n(grep -q '^spark:' /etc/passwd) || useradd -u 4321 -c \"Apache Spark\" -g spark -s /bin/bash -d /srv/spark spark\nchown -R spark:spark /srv/spark\n# make spark tmp world writable and sticky\nchmod 4755 /srv/spark/tmp\nEOF\ncl-run.pl --list all -x -s spark-user.sh\n</pre><h2>Configuring Spark</h2><p>Many of Spark's settings are controlled by environment variables. Since I want all volatile data\nin /srv, many of these need to be changed. Spark will pick up spark-env.sh automatically.</p><p>The Intel NUC systems I'm running this stack on have 4 cores and 16G of RAM, so I'll give\nSpark 2 cores and 4G of memory for now.</p><p>One line worth calling out is the <code>SPARK_WORKER_PORT=9000</code>. It can be any port. If you don't set\nit, every time a work is restarted the master will have a stale entry for a while. It's not\na big deal but I like it better this way.</p><pre>cat &gt; spark-env.sh &lt;&lt;EOF\nexport SPARK_WORKER_CORES=\"2\"\nexport SPARK_WORKER_MEMORY=\"4g\"\nexport SPARK_DRIVER_MEMORY=\"2g\"\nexport SPARK_REPL_MEM=\"4g\"\nexport SPARK_WORKER_PORT=9000\nexport SPARK_CONF_DIR=\"/etc/spark\"\nexport SPARK_TMP_DIR=\"/srv/spark/tmp\"\nexport SPARK_PID_DIR=\"/srv/spark/pids\"\nexport SPARK_LOG_DIR=\"/srv/spark/logs\"\nexport SPARK_WORKER_DIR=\"/srv/spark/work\"\nexport SPARK_LOCAL_DIRS=\"/srv/spark/tmp\"\nexport SPARK_COMMON_OPTS=\"$SPARK_COMMON_OPTS -Dspark.kryoserializer.buffer.mb=32 \"\nLOG4J=\"-Dlog4j.configuration=file://$SPARK_CONF_DIR/log4j.properties\"\nexport SPARK_MASTER_OPTS=\" $LOG4J -Dspark.log.file=/srv/spark/logs/master.log \"\nexport SPARK_WORKER_OPTS=\" $LOG4J -Dspark.log.file=/srv/spark/logs/worker.log \"\nexport SPARK_EXECUTOR_OPTS=\" $LOG4J -Djava.io.tmpdir=/srv/spark/tmp/executor \"\nexport SPARK_REPL_OPTS=\" -Djava.io.tmpdir=/srv/spark/tmp/repl/\\$USER \"\nexport SPARK_APP_OPTS=\" -Djava.io.tmpdir=/srv/spark/tmp/app/\\$USER \"\nexport PYSPARK_PYTHON=\"/bin/python2\"\nEOF\n</pre><p>spark-submit and other tools may use spark-defaults.conf to find the master and other configuration items.</p><pre>cat &gt; spark-defaults.conf &lt;&lt;EOF\nspark.master            spark://node0.pc.datastax.com:7077\nspark.executor.memory   512m\nspark.eventLog.enabled  true\nspark.serializer        org.apache.spark.serializer.KryoSerializer\nEOF\n</pre><p>The systemd units are a little less complex than Cassandra's. The spark-master.service unit\nshould only exist on node0, while every other node runs spark-worker. Spark workers are given\na weight of 100 compared to Cassandra's weight of 1000 so that Cassandra is given priority over\nSpark without starving it entirely.</p><pre>cat &gt; spark-worker.service &lt;&lt;EOF\n[Unit]\nDescription=Spark Worker\nAfter=network.target\n[Service]\nType=forking\nUser=spark\nGroup=spark\nExecStart=/opt/spark/sbin/start-slave.sh 1 spark://node0.pc.datastax.com:7077\nStandardOutput=journal\nStandardError=journal\nLimitNOFILE=infinity\nLimitMEMLOCK=infinity\nLimitNPROC=infinity\nLimitAS=infinity\nCPUAccounting=true\nCPUShares=100\n[Install]\nWantedBy=multi-user.target\nEOF\n</pre><p>The master unit is similar and only gets installed on node0. Since it is not competing\nfor resources, there's no need to turn on cgroups for now.</p><pre>cat &gt; spark-master.service &lt;&lt;EOF\n[Unit]\nDescription=Spark Master\nAfter=network.target\n[Service]\nType=forking\nUser=spark\nGroup=spark\nExecStart=/opt/spark/sbin/start-master.sh 1\nStandardOutput=journal\nStandardError=journal\nLimitNOFILE=infinity\nLimitMEMLOCK=infinity\nLimitNPROC=infinity\nLimitAS=infinity\n[Install]\nWantedBy=multi-user.target\nEOF\n</pre><p>Now deploy all of these configs. Relocate the spark config into /etc/spark and copy\na couple templates, then write all the files there. spark-env.sh goes on all nodes.\nThe unit files are described above. Finally,\na command is run to instruct systemd to read the new unit files.</p><pre>cl-run.pl --list all -c \"sudo cp /opt/spark/conf/log4j.properties.template /opt/spark/conf/log4j.properties\"\ncl-run.pl --list all -c \"sudo cp /opt/spark/conf/fairscheduler.xml.template /opt/spark/conf/fairscheduler.xml\"\ncl-sendfile.pl --list all -x -l spark-env.sh -r /etc/spark/spark-env.sh\ncl-sendfile.pl --list all -x -l spark-defaults.conf -r /etc/spark/spark-defaults.conf\ncl-sendfile.pl --list workers -x -l spark-worker.service -r /etc/systemd/system/multi-user.target.wants/spark-worker.service\ncl-sendfile.pl --list all --incl node0 -x -l spark-master.service -r /etc/systemd/system/multi-user.target.wants/spark-master.service\ncl-run.pl --list all -c \"sudo systemctl daemon-reload\"\n</pre><p>With all of that done, it's time to turn on Spark to see if it works.</p><pre>cl-run.pl --list all --incl node0 -c \"sudo systemctl start spark-master.service\"\ncl-run.pl --list workers -c \"sudo systemctl start spark-worker.service\"\n</pre><p>Now I can browse to the Spark master webui.</p><p><img src=\"http://tobert.github.io/images/spark-master-screenshot-2014-07-15.jpg\" alt=\"screenshot\" /></p><h2>Installing spark-cassandra-connector</h2><p>The connector is now published in Maven and can be installed easiest using ivy on the\ncommand line. Ivy can pull all dependencies as well as the connector jar, saving a lot of\nfiddling around. In addition, while ivy can download the connector directly, it will\nend up pulling down all of Cassandra and Spark. The script fragment below pulls down only what\nis necessary to run the connector against a pre-built Spark.</p><p>This is only really needed for the spark-shell so it can access Cassandra. Most projects\nshould include the necessary jars in a fat jar rather than pushing these packages\nto every node.</p><p>I run these commands on node0 since that's where I usually work with spark-shell. To run it on\nanother machine, Spark will have to be present and match the version of the cluster, then this\nsame process will get everything needed to use the connector.</p><pre>cat &gt; download-connector.sh &lt;&lt;EOF\nmkdir /opt/connector\ncd /opt/connector\nrm *.jar\ncurl -o ivy-2.3.0.jar \\\n  'http://search.maven.org/remotecontent?filepath=org/apache/ivy/ivy/2.3.0/ivy-2.3.0.jar'\ncurl -o spark-cassandra-connector_2.10-1.0.0-beta1.jar \\\n  'http://search.maven.org/remotecontent?filepath=com/datastax/spark/spark-cassandra-connector_2.10/1.0.0-beta1/spark-cassandra-connector_2.10-1.0.0-beta1.jar'\nivy () { java -jar ivy-2.3.0.jar -dependency \\$* -retrieve \"[artifact]-[revision](-[classifier]).[ext]\"; }\nivy org.apache.cassandra cassandra-thrift 2.0.9\nivy com.datastax.cassandra cassandra-driver-core 2.0.3\nivy joda-time joda-time 2.3\nivy org.joda joda-convert 1.6\nrm -f *-{sources,javadoc}.jar\nEOF\nsudo bash download-connector.sh\n</pre><h2>Using spark-cassandra-connector With spark-shell</h2><p>All that's left to get started with the connector now is to get spark-shell to pick it up. The easiest\nway I've found is to set the classpath with --driver-class-path then restart the context in the REPL\nwith the necessary classes imported to make sc.cassandraTable() visible.</p><p>The newly loaded methods will not show up in tab completion. I don't know why.</p><pre>/opt/spark/bin/spark-shell --driver-class-path $(echo /opt/connector/*.jar |sed 's/ /:/g')\n</pre><p>It will print a bunch of log information then present scala&gt; prompt.</p><pre>scala&gt; sc.stop\n</pre><p>Now that the context is stopped, it's time to import the connector.</p><pre>scala&gt; import com.datastax.spark.connector._\nscala&gt; val conf = new SparkConf()\nscala&gt; conf.set(\"cassandra.connection.host\", \"node1.pc.datastax.com\")\nscala&gt; val sc = new SparkContext(\"local[2]\", \"Cassandra Connector Test\", conf)\nscala&gt; val table = sc.cassandraTable(\"keyspace\", \"table\")\nscala&gt; table.count\n</pre><p>To make sure everything is working, I ran some code I'm working on for my 2048 game analytics\nproject. Each context gets an application webui that displays job status.</p><p><img src=\"http://tobert.github.io/images/spark-stages-screenshot-2014-07-15.jpg\" alt=\"screenshot\" /></p><h2>Conclusion</h2><p>It was a lot of work getting here, but what we have at the end is a Spark shell that can\naccess tables in Cassandra as RDDs with types pre-mapped and ready to go.</p><p>There are some things that can be improved upon. I will likely package all of this into\na Docker image at some point. For now, I need it up and running for some demos that will\nbe running on portacluster at <a href=\"http://www.oscon.com/oscon2014\">OSCON 2014</a>.</p>",
        "created_at": "2018-07-24T19:54:20+0000",
        "updated_at": "2018-07-24T19:54:20+0000",
        "published_at": "2014-07-15T00:00:00+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 13,
        "domain_name": "tobert.github.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11152"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1128,
            "label": "tutorials",
            "slug": "tutorials"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          }
        ],
        "is_public": true,
        "id": 11151,
        "uid": "5b5783e5c0e858.98924635",
        "title": "Tuning DSE Search – Indexing latency and query latency",
        "url": "https://www.datastax.com/dev/blog/tuning-dse-search",
        "content": "<p>DataStax Enterprise offers out of the box search indexing for your Apache Cassandra™ data. The days of double writes or ETL's between separate DBMS and Search clusters are gone. I have my CQL table, I execute the following API call, and (boom) my Cassandra data is available for:</p><p>1) full text/fuzzy search</p><p>2) ad hoc Lucene secondary index powered filtering, and</p><p>3) geospatial searchHere is my API call:</p><pre>$ bin/dsetool create_core &lt;keyspace&gt;.&lt;table&gt; generateResources=true reindex=true&#13;\n</pre><p>or if you prefer curl (or are using basic auth) use the following:</p><pre>$ curl \"http://localhost:8983/solr/admin/cores?action=CREATE&amp;name=&lt;keyspace&gt;.&lt;table&gt;&amp;generateResources=true\"&#13;\n</pre><p>Rejoice! we are in inverted index, single cluster, operational simplicity bliss!</p><p>The remainder of this post will be focused on <strong>advanced tuning</strong> for DSE Search both for <strong>a)</strong> search indexing latency (the time it takes for data to be searchable after it has been inserted through cql), and <strong>b)</strong> search query latency (timings for your search requests).</p><h2 id=\"indexinglatency\">Indexing latency</h2><p>In this section I'll talk about the kinds of things we can do in order to</p><p>1) instrument and monitor DSE Search indexing and<br />2) tune indexing for lower latencies and increased performance</p><p><strong>Note</strong>: DSE Search ships with Real Time (RT) indexing which will give you faster indexing with 4.7.3, especially when it comes to the tails of your latency distribution. Here's one of our performance tests. It shows you real time vs near-real time indexing as of 4.7.0:</p><p><img class=\"\" src=\"https://s3.amazonaws.com/uploads.hipchat.com/6528/1116934/P10Ckn4e4cijTf0/upload.png\" alt=\"indexing chart\" width=\"488\" height=\"302\" /></p><p>Perhaps more importantly, as you get machines with more cores, you can continue to increase your indexing performance linearly:<br /><img class=\"\" src=\"https://s3.amazonaws.com/uploads.hipchat.com/6528/1116934/5OFvz6SgZsl68b1/Screen%20Shot%202016-03-15%20at%2011.03.22%20PM.png\" alt=\"rt vs nrt\" width=\"480\" height=\"302\" /></p><p>Be aware, however, that you should only run one RT search core per cluster since it is significantly more resource hungry than near-real time (NRT).</p><p><strong>Side note on GC</strong>: Because solr and Cassandra run on the same JVM in DSE Search and the indexing process generates a lot of java objects, running Search requires a larger JVM Heap. When running traditional <strong>CMS</strong>, we recommend a 14gb heap with about 2gb new gen. Consider the Stump's <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-8150\">CASSANDRA-8150</a> settings when running search with CMS. <strong>G1GC</strong> has been found to perform quite well with search workloads, I personally run with a 25gb heap (do not set new gen with G1, the whole point of G1 is that it sets it itself based on your workload!) and <code>gc_pause_ms</code> at about 1000 (go higher for higher throughput or lower to minimize latencies / p99's; don't go below 500). Update (thanks mc) you configure this setting in cassandra-env.sh.</p><h3 id=\"1instrumentation\">1) Instrumentation</h3><p><strong>Index Pool Stats:</strong></p><p>DSE Search parallelizes the indexing process and allocates work to a thread pool for indexing of your data.</p><p>Using JMX, you can see statistics on your indexing threadpool depth, completion, timings, and whether backpressure is active.</p><p>This is important because if your indexing queues get too deep, we risk having too much heap pressure =&gt; OOM's. Backpressure will throttle commits and eventually load shed if search can't keep up with an indexing workload. Backpressure gets triggered when the queues get too large.</p><p>The mbean is called:</p><pre>com.datastax.bdp.search.&lt;keyspace&gt;.&lt;table&gt;.IndexPool&#13;\n</pre><p><img class=\"\" src=\"https://s3.amazonaws.com/uploads.hipchat.com/6528/1116934/ObHJEFKOEuQnLm8/upload.png\" alt=\"Indexing queues\" width=\"523\" height=\"287\" /></p><p><strong>Commit/Update Stats:</strong></p><p>You can also see statistics on indexing performance (in microseconds) based on the particular stage of the indexing process for both <code>commit</code>s and <code>update</code>s.</p><p><strong>Commit:</strong></p><p>The stages are:</p><ul><li><code>FLUSH</code> - Comprising the time spent by flushing the async indexing queue.</li>\n<li><code>EXECUTE</code> - Comprising the time spent by actually executing the commit on the index.</li>\n</ul><p>The mbean is called:</p><ul><li><code>com.datastax.bdp.search.&lt;keyspace&gt;.&lt;table&gt;.CommitMetrics</code></li>\n</ul><p><strong>Update:</strong></p><p>The stages are:</p><ul><li><code>WRITE</code> - Comprising the time spent to convert the Solr document and write it into Cassandra (only available when indexing via the Solrj HTTP APIs). If you're using cql this will be 0.</li>\n<li><code>QUEUE</code> - Comprising the time spent by the index update task into the index pool.</li>\n<li><code>PREPARE</code>- Comprising the time spent preparing the actual index update.</li>\n<li><code>EXECUTE</code> - Comprising the time spent to actually executing the index update on Lucene.</li>\n</ul><p>The mbean is:</p><ul><li><code>`com.datastax.bdp.search.&lt;keyspace&gt;.&lt;table&gt;.UpdateMetrics` </code></li>\n</ul><p><img class=\"\" src=\"https://s3.amazonaws.com/uploads.hipchat.com/6528/1116934/CLrZrQNbRatrmck/upload.png\" alt=\"indexing stats\" width=\"524\" height=\"308\" /></p><p>Here, the average latency for the QUEUE stage of the <code>update</code> is 767 micros. See our docs for more details on the <a href=\"https://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/srch/metricsMBeans.html\">metrics mbeans</a> and their stages.</p><h3 id=\"2tuning\">2) Tuning</h3><p>Almost everything in c* and DSE is configurable. Here's the key levers to get you better search indexing performance. Based on what you see in your instrumentation you can tune accordingly.</p><p>The main lever is <code>soft autocommit</code>, that's the minimum amount of time that will go by before queries are available for search. With RT we can set it to 250 ms or even as low as 100ms--given the right hardware. Tune this based on your SLA's.</p><p>The next most important lever is concurrency per core (or <code>max_solr_concurrency_per_core</code>). You can usually set this to number of CPU cores available to maximize indexing throughput.</p><p>Backpressure threshold will become more important as your load increases. Larger boxes can handle higher bp thresholds.</p><p>Don't forget to set up the ramBuffer to 2gb per the docs when you turn on RT indexing.</p><h2 id=\"querylatency\">Query Latency</h2><p>Now, I'll go over how we can monitor query performance in DSE Search, identify issues, and some of the tips / tricks we can use to improve search query performance. I will cover how to:</p><p>1) instrument and monitor DSE Search indexing and<br />2) tune indexing for lower latencies and increased performance.</p><p>Simliar to how search indexing performance scales with CPU's, search query performance scales with RAM. Keeping your search indexes in OS page cache is the biggest thing you can do to minimize latencies; so scale deliberately!</p><h3>1) Instrumentation</h3><p>There are multiple tools available for monitoring search performance.</p><h4 id=\"opscenter\">OpsCenter:</h4><p>OpsCenter supports a few search metrics that can be configured per node, datacenter, and solr core:</p><p>1) search latencies<br />2) search requests<br />3) index size<br />4) search timeouts<br />5) search errors</p><p><img class=\"\" src=\"https://s3.amazonaws.com/uploads.hipchat.com/6528/1116934/uoq1hLRQ58AZBhn/Screen%20Shot%202016-03-15%20at%2011.47.12%20PM.png\" alt=\"opscenter\" width=\"368\" height=\"226\" /></p><h4 id=\"metricsmbeans\">Metrics mbeans:</h4><p>In the same way that indexing has performance metrics, DSE Search <a href=\"https://docs.datastax.com/en/datastax_enterprise/4.0/datastax_enterprise/srch/srchQryMbean.html\">query performance metrics</a> are available through JMX and can be useful for troubleshooting perofrmance issues. We can use the <code>query.name</code> parameter in your DSE Search queries to capture metrics for specifically tagged queries.</p><p><strong>Query:</strong></p><p>The stages of <code>query</code> are:</p><ul><li><code>COORDINATE</code> - Comprises the total amount of time spent by the coordinator node to distribute the query and gather/process results from shards. This value is computed only on query coordinator nodes.</li>\n<li><code>EXECUTE</code> - Comprises the time spent by a single shard to execute the actual index query. This value is computed on the local node executing the shard query.</li>\n<li><code>RETRIEVE</code> - Comprises the time spent by a single shard to retrieve the actual data from Cassandra. This value will be computed on the local node hosting the requested data.</li>\n</ul><p>The mbean is:</p><ul><li><code>com.datastax.bdp.search.&lt;keyspace&gt;.&lt;table&gt;.QueryMetrics </code></li>\n</ul><h4 id=\"querytracing\">Query Tracing:</h4><p>When using <code>solr_query</code> via cql, query tracing can provide useful information as to where a particular query spent time in the cluster.</p><p>Query tracing is available in cqlsh <code>tracing on</code>, in DevCenter (in the tab at the bottom of the screen), and via probabilistic tracing which is configurable via <a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsSetTraceProbability.html\">nodetool</a>.</p><p>When users complain about a slow query and you need to find out what it is, the DSE Search slow query log is a good starting point.</p><pre>dsetool perf solrslowlog enable&#13;\n</pre><p>Stores to a table in cassandra in the <code>dse_perf.solr_slow_sub_query_log</code> table</p><h3>2) Tuning</h3><p>Now let's focus on some tips for how you can improve search query performance.</p><h3 id=\"indexsize\">Index size</h3><p>Index size is so important that, I wrote <a href=\"http://www.sestevez.com/solr-space-saving-profile/\">a separate post</a> just on that subject.</p><h4 id=\"qvsfq\">Q vs. FQ</h4><p>In order to take advantage of the solr filter cache, build your queries using fq not q. The filter cache is the only solr cache that persists across commits so don't spend time or valuable RAM trying to leverage the other caches.</p><h4 id=\"solrqueryrouting\">Solr query routing</h4><p>Partition routing is a great multi-tennancy feature in DSE Search that lets you limit the amount of fan out that a search query will take under the hood. Essentially, you're able to specify a Cassandra partition that you are interested in limiting your search to. This will limit the number of nodes that DSE Search requires to fullfil your request.</p><h4 id=\"usedocvaluesforfacetingandsorting\">Use docvalues for Faceting and Sorting.</h4><p>To get improved performance and to avoid OOMs from the field cache, always remember to turn on docvalues on fields that you will be sorting and faceting over. This may become mandatory in DSE at some point so plan ahead.</p><h3 id=\"otherdsedifferentiators\">Other DSE Differentiators</h3><p>If you're comparing DSE Search against other search offerings / technologies, the following two differentiators are unique to DSE Search.</p><h4 id=\"faulttolerantdistributedqueries\">Fault tolerant distributed queries</h4><p>If a node dies during a query, we retry the query on another node.</p><h4 id=\"nodehealth\">Node health</h4><p>Node health and shard router behavior.<br />DSE Search monitors node health and makes distributed query routing decisions based on the following:</p><p>1) Uptime: a node that just started may well be lacking the most up-to-date data (to be repaired via HH or AE).<br />2) Number of dropped mutations.<br />3) Number of hints the node is a target for.<br />4) \"failed reindex\" status.</p><p>All you need to take advantage of this is be on a modern DSE version.</p><hr /><p><a href=\"https://www.datastax.com/\">DataStax</a> has many ways for you to advance in your career and knowledge. \n</p><p>You can take <a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" title=\"academy.datastax.com\">free classes</a>, <a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" title=\"academy.datastax.com/certifications\">get certified</a>, or read <a href=\"https://www.datastax.com/dbas-guide-to-nosql\" target=\"_self\" title=\"dbas-guide-to-nosql\">one of our many white papers</a>.\n</p><p><a href=\"https://academy.datastax.com/user/register?destination=home&amp;utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_register\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com\">register for classes</a>\n</p><p><a href=\"https://academy.datastax.com/certifications?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_DSA_certifications\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"academy.datastax.com/certifications\">get certified</a>\n</p><p><a href=\"http://www.datastax.com/dbas-guide-to-nosql?utm_campaign=DevBlog&amp;utm_medium=blog&amp;utm_source=devblog&amp;utm_term=DevBlogPosts_CTA1_dbasguidetonosql\" target=\"_self\" class=\"dxAllButtons_v3Rad2_whiteNGrayOL\" title=\"dbas-guide-to-nosql\">DBA's Guide to NoSQL</a>\n</p><br class=\"clear\" /><div id=\"mto_newsletter_121316_Css\"><p>Subscribe for newsletter:</p><br /></div>",
        "created_at": "2018-07-24T19:54:13+0000",
        "updated_at": "2018-07-24T19:54:13+0000",
        "published_at": "2016-03-17T22:04:52+0000",
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 7,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://s3.amazonaws.com/uploads.hipchat.com/6528/1116934/P10Ckn4e4cijTf0/upload.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11151"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 748,
            "label": "documentation",
            "slug": "documentation"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          }
        ],
        "is_public": true,
        "id": 11149,
        "uid": "5b5783dab93f29.86582613",
        "title": "DataStax Docs",
        "url": "https://docs.datastax.com/en/landing_page/doc/landing_page/current.html",
        "content": "<p class=\"shortdesc\">Documentation for configuring, upgrading, and deploying DataStax Enterprise, DataStax\n    OpsCenter, CQL, DataStax Drivers, DataStax Studio, and DataStax DevCenter.</p><p></p><table class=\"table frame-all\"><caption>Simba ODBC Driver for Spark <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/spark/simbaOdbcDriverLinux.html\" target=\"_blank\">(Linux)</a> /\n                                                  <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/spark/simbaOdbcDriverWindows.html\" target=\"_blank\">(Windows)</a>\n                                                  Simba ODBC Driver for Spark <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/spark/simbaOdbcDriverLinux.html\" target=\"_blank\">(Linux)</a> /\n                                                  <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/spark/simbaOdbcDriverWindows.html\" target=\"_blank\">(Windows)</a>\n                                                  <a class=\"xref\" href=\"https://downloads.datastax.com/odbc-cql/2.5.6.1011/Simba%20Cassandra%20ODBC%20Install%20and%20Configuration%20Guide.pdf\" target=\"_blank\">DataStax ODBC\n                                                  Driver for Apache Cassandra and DataStax\n                                                  Enterprise with CQL connector</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/spark/simbaJdbcDriver.html\" target=\"_blank\">Simba JDBC Driver\n                                                  for Spark</a>\n                                                  <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/spark/simbaJdbcDriver.html\" target=\"_blank\">Simba JDBC Driver\n                                                  for Spark</a>\n                                                  \n                                                </caption></table><p></p><table class=\"table frame-all\"><caption><a class=\"xref\" href=\"https://docs.datastax.com/en/developer/cpp-driver/latest\" target=\"_blank\">C/C++ OSS\n                                                  driver</a>\n                                                \n                                                  <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/csharp-driver/latest\" target=\"_blank\">C# OSS\n                                                  driver</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/java-driver/latest\" target=\"_blank\">Java OSS\n                                                  driver</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/nodejs-driver/latest\" target=\"_blank\">Node.js OSS\n                                                  driver</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/php-driver/latest\" target=\"_blank\">PHP OSS\n                                                  driver</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/python-driver/latest\" target=\"_blank\">Python OSS\n                                                  driver</a>\n                                                  \n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/ruby-driver/latest\" target=\"_blank\">Ruby OSS\n                                                  driver</a>\n                                                </caption></table><p></p><table class=\"table frame-all\"><caption><a class=\"xref\" href=\"https://docs.datastax.com/en/developer/cpp-driver-dse/latest\" target=\"_blank\">C/C++ DSE\n                                                  driver</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/csharp-driver-dse/latest\" target=\"_blank\">C# DSE\n                                                  driver</a> / <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/csharp-dse-graph/latest\" target=\"_blank\">DSE Graph\n                                                  Extension</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/java-driver-dse/latest\" target=\"_blank\">Java DSE driver\n                                                  </a>(DSE Graph Extension included)\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/nodejs-driver-dse/latest\" target=\"_blank\">Node.js DSE driver\n                                                  </a>/ <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/nodejs-dse-graph/latest\" target=\"_blank\">DSE Graph\n                                                  Extension</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/php-driver-dse/latest\" target=\"_blank\">PHP DSE\n                                                  driver</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/python-dse-driver/latest\" target=\"_blank\">Python DSE\n                                                  driver</a> / <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/python-dse-graph/latest\" target=\"_blank\">DSE Graph\n                                                  Extension</a>\n                                                <a class=\"xref\" href=\"https://docs.datastax.com/en/developer/ruby-driver-dse/latest\" target=\"_blank\">Ruby DSE\n                                                  driver</a>\n                                                </caption></table><section class=\"section\"><table class=\"table z_lp-table frame-none\"><caption><a class=\"xref\" href=\"https://docs.datastax.com/ja/landing_page-jajp/doc/landing_page/current.html\" target=\"_blank\">Japanese docs: 日本語ドキュメント</a>\n            \n            Getting startedPlanning and installing\n                  softwareDSE Installation Guide <a class=\"xref\" href=\"https://docs.datastax.com/en/install/doc/\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/install/installTOC.html\" target=\"_blank\">5.1</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/datastax_enterprise/5.0/datastax_enterprise/install/installTOC.html\" target=\"_blank\">5.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/datastax_enterprise/4.8/datastax_enterprise/install/installTOC.html\" target=\"_blank\">4.8</a><br />                  OpsCenter Installation Guide <a class=\"xref\" href=\"https://docs.datastax.com/en/install/doc/install60/opscInstallOpsc.html\" target=\"_blank\">6.5</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.1/opsc/install/opscInstallOpsc_g.html\" target=\"_blank\">6.1</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.0/opsc/install/opscInstallOpsc_g.html\" target=\"_blank\">6.0</a>Architecture Guide <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-arch/\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-arch/\" target=\"_blank\">5.1</a><a class=\"xref\" href=\"https://docs.datastax.com/en/landing_page/doc/landing_page/docker.html\" title=\"Use DataStax-provided Docker images to learn DataStax Enterprise (DSE), DataStax OpsCenter, and DataStax Studio, try new ideas, and test and demonstrate an application.\">Docker</a> <a class=\"xref\" href=\"https://docs.datastax.com/en/dse-planning/doc/\" target=\"_blank\">Planning Guide</a>Upgrading\n                  software<a class=\"xref\" href=\"https://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgrdDSE.html\" target=\"_blank\">Upgrading DSE</a><a class=\"xref\" href=\"https://docs.datastax.com/en/upgrade/doc/upgrade/datastax_enterprise/upgrdCstarToDSE.html\" target=\"_blank\">Upgrading from Apache Cassandra</a>™<a class=\"xref\" href=\"https://docs.datastax.com/en/upgrade/doc/upgrade/opscenter/upgdOpsc.html\" target=\"_blank\">Upgrading OpsCenter</a>LearningTutorials<a class=\"xref\" href=\"https://docs.datastax.com/en/playlist/doc/\" target=\"_blank\">Playlist tutorial</a><a class=\"xref\" href=\"https://docs.datastax.com/en/tutorials/kerberos/\" target=\"_blank\">Kerberos tutorial</a><a class=\"xref\" href=\"https://www.datastax.com/what-we-offer/products-services/sandbox\" target=\"_blank\">DataStax Sandbox</a>Additional\n                  resources<a class=\"xref\" href=\"https://academy.datastax.com/\" target=\"_blank\">DataStax Academy</a><a class=\"xref\" href=\"https://academy.datastax.com/developer-blog\" target=\"_blank\">DataStax Developer Blog</a><a class=\"xref\" href=\"https://support.datastax.com/\" target=\"_blank\">DataStax Support</a>DSE Security <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/security/securityTOC.html\" target=\"_blank\"> 6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/search/searchTOC.html\" target=\"_blank\">5.1</a>DSE utilitiesAdditional\n                  tools<a class=\"xref\" href=\"https://docs.datastax.com/en/dsbulk/doc/index.html\" target=\"_blank\">DataStax Bulk Loader</a> for DSE 4.8-6.0DSE Graph Loader <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/graph/dgl/dglOverview.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-dev/datastax_enterprise/graph/dgl/dglOverview.html\" target=\"_blank\">5.1</a>DSE stress tools <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/tools/stressToolsTOC.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/tools/stressToolsTOC.html\" target=\"_blank\">5.1</a>dsetool <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/tools/dsetool/dsetool.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/tools/dsetool/dsetool.html\" target=\"_blank\">5.1</a>nodetool <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/tools/nodetool/toolsNodetool.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/tools/nodetool/toolsNodetool.html\" target=\"_blank\">5.1</a>SSTable utilities <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/tools/toolsSStables/toolsSSTableUtilitiesTOC.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/tools/toolsSStables/toolsSSTableUtilitiesTOC.html\" target=\"_blank\">5.1</a>\n              Developing and administering\n                  applicationsDSE\n                    <sup class=\"ph sup\">Administrator Guide <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/\" target=\"_blank\">5.1</a>Developer Guide <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-dev/\" target=\"_blank\">5.1</a><a class=\"xref\" href=\"https://docs.datastax.com/en/dse-trblshoot/doc/\" target=\"_blank\">Troubleshooting Guide</a><a class=\"xref\" href=\"https://docs.datastax.com/en/datastax_enterprise/5.0/\" target=\"_blank\">DSE 5.0</a> (<a class=\"xref\" href=\"https://docs.datastax.com/en/cassandra/3.0/\" target=\"_blank\">C* 3.0</a>) | <a class=\"xref\" href=\"https://docs.datastax.com/en/datastax_enterprise/4.8/\" target=\"_blank\">4.8</a> (<a class=\"xref\" href=\"https://docs.datastax.com/en/cassandra/2.1/\" target=\"_blank\">C* 2.1</a>) <sup class=\"ph sup\"><br />Tools and\n                  extensionsCQL <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/cql/\" target=\"_blank\"> 6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/cql/\" target=\"_blank\">5.1</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/cql/3.3/cql/cqlIntro.html\" target=\"_blank\">3.3</a> <sup class=\"ph sup\"><a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-dev/datastax_enterprise/studio/studioToc.html\" target=\"_blank\">Studio 6.0</a> for DSE 6.0<a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-dev/datastax_enterprise/studio/stdToc.html\" target=\"_blank\">Studio 2.0</a> for DSE 5.0, 5.1<a class=\"xref\" href=\"https://docs.datastax.com/en/developer/devcenter/doc/\" target=\"_blank\">DevCenter</a> for DSE 4.8, 5.0<a class=\"xref\" href=\"https://docs.datastax.com/en/landing_page/doc/landing_page/apiDocs.html\" title=\"The following links provide access to DataStax API documentation.\">DataStax API</a>DSE\n                  OpsCenterOpsCenter <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.5/\" target=\"_blank\">6.5</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.1/\" target=\"_blank\">6.1</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.0/\" target=\"_blank\">6.0</a>Lifecycle Manager <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.5/opsc/LCM/opscLCM.html\" target=\"_blank\">6.5</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.1/opsc/LCM/opscLCM.html\" target=\"_blank\">6.1</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/opscenter/6.0/opsc/LCM/opscLCM.html\" target=\"_blank\">6.0</a>DataStax\n                  driversDSE advanced functionalityAdvanced\n                featuresDSE Analytics <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/analytics/analyticsOverview.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/analytics/analyticsTOC.html\" target=\"_blank\">5.1</a>DSE Graph <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/graph/graphTOC.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/graph/graphTOC.html\" target=\"_blank\">5.1</a>DSE Search <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/6.0/dse-admin/datastax_enterprise/search/searchTOC.html\" target=\"_blank\">6.0</a> | <a class=\"xref\" href=\"https://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/search/searchTOC.html\" target=\"_blank\">5.1</a>\n            </sup></sup></sup></caption></table></section>",
        "created_at": "2018-07-24T19:54:02+0000",
        "updated_at": "2018-07-24T19:54:02+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "docs.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11149"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 748,
            "label": "documentation",
            "slug": "documentation"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          }
        ],
        "is_public": true,
        "id": 11148,
        "uid": "5b5783d6cac8b5.74611097",
        "title": "Documentation",
        "url": "http://cassandra.apache.org/doc/latest/",
        "content": "<p>This documentation is currently a work-in-progress and contains a number of TODO sections.\n    <a href=\"http://cassandra.apache.org/doc/latest/bugs.html\">Contributions</a> are welcome.</p><h3>Main documentation</h3><table class=\"contentstable doc-landing-table\" style=\"margin: auto;\"><tr><td class=\"left-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/getting_started/index.html\">Getting started</a><br />Newbie friendly starting point</p>\n    </td>\n    <td class=\"right-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/operating/index.html\">Operating Cassandra</a><br />The operator's corner</p>\n    </td>\n  </tr><tr><td class=\"left-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/architecture/index.html\">Cassandra Architecture</a><br />Cassandra's big picture</p>\n    </td>\n    <td class=\"right-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/tools/index.html\">Cassandra's Tools</a><br />cqlsh, nodetool, ...</p>\n    </td>\n  </tr><tr><td class=\"left-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/data_modeling/index.html\">Data Modeling</a><br />Or how to make square pegs fit round holes</p>\n    </td>\n    <td class=\"right-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/troubleshooting/index.html\">Troubleshooting</a><br />What to look for when you have a problem</p>\n    </td>\n  </tr><tr><td class=\"left-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/cql/index.html\">Cassandra Query Language</a><br />CQL reference documentation</p>\n    </td>\n    <td class=\"right-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/development/index.html\">Cassandra Development</a><br />Learn how to improve Cassandra and contribute patches</p>\n    </td>\n  </tr><tr><td class=\"left-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/faq/index.html\">FAQs</a><br />Frequently Asked Questions (with answers!)</p>\n    </td>\n    <td class=\"right-column\">\n      <p class=\"biglink\"><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/configuration/index.html\">Configuration</a><br />Cassandra's handles and knobs</p>\n    </td>\n  </tr></table><h3>Meta informations</h3><ul><li><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/bugs.html\">Reporting bugs</a></li>\n  <li><a class=\"biglink\" href=\"http://cassandra.apache.org/doc/latest/contactus.html\">Contact us</a></li>\n</ul><h3>Documentation for older releases</h3><p>The Cassandra Query Language (CQL) documentation for older releases are:\n</p><ul><li><a href=\"http://cassandra.apache.org/doc/old/CQL-3.0.html\">CQL for the 3.0 series</a></li>\n  <li><a href=\"http://cassandra.apache.org/doc/old/CQL-2.2.html\">CQL for the 2.2 series</a></li>\n  <li><a href=\"http://cassandra.apache.org/doc/old/CQL-2.1.html\">CQL for the 2.1 series</a></li>\n</ul>",
        "created_at": "2018-07-24T19:53:58+0000",
        "updated_at": "2018-07-24T19:53:58+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "cassandra.apache.org",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11148"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          },
          {
            "id": 1166,
            "label": "admin-monitor",
            "slug": "admin-monitor"
          }
        ],
        "is_public": true,
        "id": 11147,
        "uid": "5b5783a1b90d00.22334937",
        "title": "DataStax Enterprise OpsCenter",
        "url": "https://www.datastax.com/products/datastax-opscenter",
        "content": "DataStax Enterprise OpsCenter | DataStax\n\n<noscript>\n\n\n\n<div class=\"DS17\"><div class=\"connect-us\"><a href=\"https://www.datastax.com/contactus\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Mail.svg\" alt=\"email icon\" />email</a><a href=\"https://www.datastax.com/company#offices\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Phone.svg\" alt=\"phone icon\" />call</a></div></div><header class=\"DS17\"><div class=\"container\"><div class=\"wrapper\"><div class=\"logo\"><a href=\"https://www.datastax.com/\"><img src=\"https://www.datastax.com/templates/dist/images/logo-header.png\" alt=\"DataStax logo\" /></a><a href=\"https://www.datastax.com/\"><img src=\"https://www.datastax.com/templates/dist/images/new_logo.png\" alt=\"DataStax logo\" /></a></div></div></div>\n  \n</header><section class=\"cards wow fadeInUp dse-cards no-padding-bottom\" data-wow-delay=\"0.5s\"><div class=\"container\"><p>&#13;\n        </p><h3> DSE OpsCenter Features</h3>&#13;<div class=\"column-wrapper text-center\"><div class=\"three-col wow fadeInUp col card-blue bg-gray\"><p>&#13;\n              </p><h4>Operational Simplicity</h4>&#13;<div class=\"icon-box\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Operational_Simplicity.svg\" alt=\"operational simplicity icon\" /></div><p>Eliminate operational headaches with automatic backups, reduced manual operations, and rapid performance issue detection.</p></div><div class=\"three-col wow fadeInUp col card-blue bg-gray\"><p>&#13;\n              </p><h4>Visual Monitoring</h4>&#13;<div class=\"icon-box\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Visual_Monitoring.svg\" alt=\"visual monitoring icon\" /></div><p>Easily spot outliers and performance bottlenecks via customized dashboards with real-time and historical system metrics.</p></div><div class=\"three-col wow fadeInUp col card-blue bg-gray\"><p>&#13;\n              </p><h4>Enterprise-Ready</h4>&#13;<div class=\"icon-box\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Enterprise-Ready.svg\" alt=\"Enterprise ready icon\" /></div><p>Automatic failover and fine-grained roles and permissions ensure always-on, integrated, and secure management and monitoring.</p></div></div></div></section><section class=\"content-with-media wow fadeInUp dse-content-with-media bg-white\" data-wow-delay=\"0.5s\"><section class=\"intro-with-content  dse-intro bg-gray\"><div class=\"container\"><div class=\"wrapper text-center\"><div class=\"logo wow fadeInUp\"><img src=\"https://www.datastax.com/templates/dist/images/Lock.png\" alt=\"circular icon with lock and key hole\" /></div><div class=\"content wow fadeInUp\"><h3>Large-Scale Operations on Autopilot</h3><p>Make your database management and monitoring as simple as the push of a button to increase operational simplicity and productivity and reduce operational overhead.</p></div><div class=\"col-wrapper\"><div class=\"three-col wow fadeInUp\"><h6>Automated Data Synchronization</h6><p>DSE NodeSync significantly reduces manual repair operations and eliminates manual repair-related cluster outages, resulting in lower operational cost and shorter support cycles. OpsCenter provides a user-friendly view of NodeSync operations for easy monitoring and troubleshooting.</p></div><div class=\"three-col wow fadeInUp\"><h6>Simplified Upgrades</h6><p>Upgrade Service in OpsCenter LifeCycle Manager enables you to perform patch upgrades of DSE clusters at the data center, rack, or node level with up to 60% less manual involvement. You can easily clone your existing config profile to ensure compatibility with DSE upgrades, which is key for running business-critical applications.</p></div><div class=\"three-col wow fadeInUp\"><h6>Full and Continuous Backups</h6><p>Our backup service delivers full backup and disaster recovery protection for DSE clusters, including the ability to visually schedule the backup and restore of hundreds of nodes at a point in time, visually monitor backup and restore tasks, and clone database clusters.</p></div><div class=\"three-col wow fadeInUp\"><h6> End-to-End Performance Visibility </h6><p>Performance Service collects key metrics to assess the health of DSE nodes, stores them, and makes them accessible visually or via command-line tools, allowing you to capture granular statistics at the system, user, or statement level and provide context-specific recommendations to resolve performance bottlenecks.</p></div><div class=\"three-col wow fadeInUp\"><h6>Seamless Enterprise Integration</h6><p>A comprehensive set of OpsCenter RESTful APIs allows you to easily provision, monitor, and execute maintenance tasks using your favorite scripting language and integrate powerful functionality into your existing tools and workflows.</p></div><div class=\"three-col wow fadeInUp\"><h6>Comprehensive Cluster Health Management</h6><p>Our Best Practice Service periodically scans database clusters and automatically detects and reports issues that threaten the cluster’s security, availability, or performance. Our Capacity Service accumulates and analyzes cluster health and resource utilization metrics to help you understand cluster performance over time as well as predict future usage and growth.</p></div></div></div></div></section><section class=\"full-width-cta no-padding dse-full-cta\"><div class=\"bg-img\"><img src=\"https://www.datastax.com/templates/dist/images/opscenter/Enterprise_OpsCenter_Image3.jpg\" alt=\"two women software developers learning about DataStax Graph Academy\" /></div><div class=\"v-middle-wrapper wow fadeInUp\"><div class=\"v-middle-inner\"><div class=\"v-middle\"><div class=\"text-center content-650 content-wrapper\"><h3>Monitoring Your Cluster with DSE OpsCenter</h3><p>Get started on DSE OpsCenter using this free course on DataStax Academy.</p><a href=\"https://academy.datastax.com/units/monitoring-your-cluster-opscenter\" class=\"btn-default\">Sign Up</a></div></div></div></div></section><section class=\"latest-block\"><section class=\"cta round-left-bottom no-padding\"><div class=\"DS17\"><div class=\"use-case\"><div class=\"wrapper\"><div class=\"two-col text-light-blue\"><h6>Customer Experience</h6><ul><li><a href=\"https://www.datastax.com/use-cases/customer-360\">Customer 360</a></li>\n          <li><a href=\"https://www.datastax.com/personalization\">Personalization &amp; Recommendations</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/loyalty-programs\">Loyalty Programs</a></li>\n          <li><a href=\"https://www.datastax.com/fraud-detection\">Consumer Fraud Detection</a></li>\n        </ul></div><div class=\"two-col text-light-green\"><h6><a href=\"#\">Enterprise Optimization</a></h6><ul><li><a href=\"https://www.datastax.com/use-cases/ecommerce\">eCommerce</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/identity-management\">Identity Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/security\">Security and Compliance</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/supply-chain\">Supply Chain</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/inventory-management\">Inventory Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/asset-monitoring\">Asset Monitoring</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/logistics\">Logistics</a></li>\n        </ul></div></div></div></div>\n\t\n\t\n</section></section></section></noscript>",
        "created_at": "2018-07-24T19:53:05+0000",
        "updated_at": "2018-07-24T19:53:05+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 2,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/themes/datastax-2014-08/images/common/DataStax_Web_Social_DefaultGenericV2_1024x351_wide.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11147"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          }
        ],
        "is_public": true,
        "id": 11146,
        "uid": "5b57835573c0e3.06974438",
        "title": "Stratio/deep-spark",
        "url": "https://github.com/Stratio/deep-spark",
        "content": "<article class=\"markdown-body entry-content\" itemprop=\"text\"><p>*Disclaimer: As of 01/06/2015 this project has been deprecated. Thank you for your understanding and continued help throughout the project's life.</p>\n<p>Deep is a thin integration layer between Apache Spark and several NoSQL datastores.\nWe actually support Apache Cassandra, MongoDB, Elastic Search, Aerospike, HDFS, S3 and any database accessible through JDBC, but in the near future we will add support for sever other datastores.</p>\n<ul><li>JIRA: <a href=\"https://deep-spark.atlassian.net\" rel=\"nofollow\">https://deep-spark.atlassian.net</a></li>\n</ul>\n<p>In order to compile the deep-jdbc module is necessary to add the Oracle ojdbc driver into your local repository. You can download it from the URL: <a href=\"http://www.oracle.com/technetwork/database/features/jdbc/default-2280470.html\" rel=\"nofollow\">http://www.oracle.com/technetwork/database/features/jdbc/default-2280470.html</a>. When you are on the web you must click in \"Accept License Agreement\" and later downlad ojdbc7.jar library. You need a free oracle account to download the official driver.</p>\n<p>To install the ojdbc driver in your local repository you must execute the command below:</p>\n<blockquote>\n<p>mvn install:install-file -Dfile= -DgroupId=com.oracle -DartifactId=ojdbc7  -Dversion=12.1.0.2 -Dpackaging=jar</p>\n</blockquote>\n<p>After that you can compile Deep executing the following steps:</p>\n<blockquote>\n<p>cd deep-parent</p>\n</blockquote>\n<blockquote>\n<p>mvn clean install</p>\n</blockquote>\n<p>If you want to create a Deep distribution you must execute the following steps:</p>\n<blockquote>\n<p>cd deep-scripts</p>\n</blockquote>\n<blockquote>\n<p>make-distribution-deep.sh</p>\n</blockquote>\n<p>During the creation you'll see the following question:</p>\n<blockquote>\n<p>What tag want to use for Aerospike native repository?</p>\n</blockquote>\n<p>You must type 0.7.0 and press enter.</p>\n<p>The integration is <em>not</em> based on the Cassandra's Hadoop interface.</p>\n<p>Deep comes with an user friendly API that lets developers create Spark RDDs mapped to Cassandra column families.\nWe provide two different interfaces:</p>\n<ul><li>\n<p>The first one will let developers map Cassandra tables to plain old java objects (POJOs), just like if you were using any other ORM. We call this API the 'entity objects' API.\nThis abstraction is quite handy, it will let you work on RDD (under the hood Deep will transparently map Cassandra's columns to entity properties).\nYour domain entities must be correctly annotated using Deep annotations (take a look at deep-core example entities in package com.stratio.deep.core.entity).</p>\n</li>\n<li>\n<p>The second one is a more generic 'cell' API, that will let developerss work on RDD&lt;com.stratio.deep.entity.Cells&gt; where a 'Cells' object is a collection of com.stratio.deep.entity.Cell objects.\nColumn metadata is automatically fetched from the data store. This interface is a little bit more cumbersome to work with (see the example below),\nbut has the advantage that it doesn't require the definition of additional entity classes.\nExample: you have a table called 'users' and you decide to use the 'Cells' interface. Once you get an instance 'c' of the Cells object,\nto get the value of column 'address' you can issue a c.getCellByName(\"address\").getCellValue().\nPlease, refer to the Deep API documentation to know more about the Cells and Cell objects.</p>\n</li>\n</ul><p>We encourage you to read the more comprehensive documentation hosted on the <a href=\"http://www.openstratio.org/\" rel=\"nofollow\">Openstratio website</a>.</p>\n<p>Deep comes with an example sub project called 'deep-examples' containing a set of working examples, both in Java and Scala.\nPlease, refer to the deep-example project README for further information on how to setup a working environment.</p>\n<p>Spark-MongoDB connector is based on Hadoop-mongoDB.</p>\n<p>Support for MongoDB has been added in version 0.3.0.</p>\n<p>We provide two different interfaces:</p>\n<ul><li>\n<p>ORM API, you just have to annotate your POJOs with Deep annotations and magic will begin, you will be able to connect MongoDB with Spark using your own model entities.</p>\n</li>\n<li>\n<p>Generic cell API, you do not need to specify the collection's schema or add anything to your POJOs, each document will be transform to an object \"Cells\".</p>\n</li>\n</ul><p>We added a few working examples for MongoDB in deep-examples subproject, take a look at:</p>\n<p>Entities:</p>\n<ul><li>com.stratio.deep.examples.java.ReadingEntityFromMongoDB</li>\n<li>com.stratio.deep.examples.java.WritingEntityToMongoDB</li>\n<li>com.stratio.deep.examples.java.GroupingEntityWithMongoDB</li>\n</ul><p>Cells:</p>\n<ul><li>com.stratio.deep.examples.java.ReadingCellFromMongoDB</li>\n<li>com.stratio.deep.examples.java.WritingCellToMongoDB</li>\n<li>com.stratio.deep.examples.java.GroupingCellWithMongoDB</li>\n</ul><p>You can check out our first steps guide here:</p>\n<p><a href=\"https://github.com/Stratio/deep-spark/blob/master/doc/src/site/sphinx/t20-first-steps-deep-mongodb.rst\">First steps with Deep-MongoDB</a></p>\n<p>We are working on further improvements!</p>\n<p>Support for ElasticSearch has been added in version 0.5.0.</p>\n<p>Support for Aerospike has been added in version 0.6.0.</p>\n<p>Examples:</p>\n<p>Entities:</p>\n<ul><li>com.stratio.deep.examples.java.ReadingEntityFromAerospike</li>\n<li>com.stratio.deep.examples.java.WritingEntityToAerospike</li>\n<li>com.stratio.deep.examples.java.GroupingEntityWithAerospike</li>\n</ul><p>Cells:</p>\n<ul><li>com.stratio.deep.examples.java.ReadingCellFromAerospike</li>\n<li>com.stratio.deep.examples.java.WritingCellToAerospike</li>\n<li>com.stratio.deep.examples.java.GroupingCellWithAerospike</li>\n</ul>\n<p>Support for JDBC has been added in version 0.7.0.</p>\n<p>Examples:</p>\n<p>Entities:</p>\n<ul><li>package com.stratio.deep.examples.java.ReadingEntityWithJdbc</li>\n<li>package com.stratio.deep.examples.java.WritingEntityWithJdbc</li>\n</ul><p>Cells:</p>\n<ul><li>package com.stratio.deep.examples.java.ReadingCellWithJdbc</li>\n<li>package com.stratio.deep.examples.java.WritingCellWithJdbc</li>\n</ul>\n<ul><li>Cassandra, we tested versions from 1.2.8 up to 2.0.11 (for Spark &lt;=&gt; Cassandra integration).</li>\n<li>MongoDB, we tested the integration with MongoDB versions 2.2, 2.4 y 2.6 using Standalone, Replica Set and Sharded Cluster (for Spark &lt;=&gt; MongoDB integration).</li>\n<li>ElasticSearch, 1.3.0+</li>\n<li>Aerospike, 3.3.0+</li>\n<li>Spark 1.1.1</li>\n<li>Apache Maven &gt;= 3.0.4</li>\n<li>Java 1.7</li>\n<li>Scala 2.10.3</li>\n</ul>\n<ul><li>\n<p>Clone the project</p>\n</li>\n<li>\n<p>To configure a development environment in Eclipse: import as Maven project. In IntelliJ: open the project by selecting the deep-parent POM file</p>\n</li>\n<li>\n<p>Install the project in you local maven repository. Enter deep-parent subproject and perform: mvn clean install (add -DskipTests to skip tests)</p>\n</li>\n<li>\n<p>Put Deep to work on a working cassandra + spark cluster. You have several options:</p>\n<ul><li>\n<p>Download a pre-configured Stratio platform VM <a href=\"http://www.stratio.com/\" rel=\"nofollow\">Stratio's BigData platform (SDS)</a>.\nThis VM will work on both Virtualbox and VMWare, and comes with a fully configured distribution that also includes Stratio Deep. We also distribute the VM with several preloaded datasets in Cassandra. This distribution will include Stratio's customized Cassandra distribution containing our powerful <a href=\"https://github.com/Stratio/stratio-cassandra\">open-source lucene-based secondary indexes</a>, see Stratio documentation for further information.\nOnce your VM is up and running you can test Deep using the shell. Enter /opt/sds and run bin/stratio-deep-shell.</p>\n</li>\n<li>\n<p>Install a new cluster using the Stratio installer. Please refer to Stratio's website to download the installer and its documentation.</p>\n</li>\n<li>\n<p>You already have a working Cassandra server on your development machine: you need a spark+deep bundle, we suggest to create one by running:</p>\n<p><code>cd deep-scripts</code></p>\n<p><code>./make-distribution-deep.sh</code></p>\n</li>\n</ul><p>this will build a Spark distribution package with StratioDeep and Cassandra's jars included (depending on your machine this script could take a while, since it will compile Spark from sources).\nThe package will be called <code>spark-deep-distribution-X.Y.Z.tgz</code>, untar it to a folder of your choice, enter that folder and issue a <code>./stratio-deep-shell</code>, this will start an interactive shell where you can test StratioDeep (you may have noticed this is will start a development cluster started with MASTER=\"local\").</p>\n<ul><li>\n<p>You already have a working installation os Cassandra and Spark on your development machine: this is the most difficult way to start testing Deep, but you know what you're doing you will have to</p>\n<ol><li>copy the Stratio Deep jars to Spark's 'jars' folder (<code>$SPARK_HOME/jars</code>).</li>\n<li>copy Cassandra's jars to Spark's 'jar' folder.</li>\n<li>copy Datastax Java Driver jar (v 2.0.x) to Spark's 'jar' folder.</li>\n<li>start spark shell and import the following:</li>\n</ol><p><code>import com.stratio.deep.commons.annotations._</code></p>\n<p><code>import com.stratio.deep.commons.config._</code></p>\n<p><code>import com.stratio.deep.commons.entity._</code></p>\n<p><code>import com.stratio.deep.core.context._</code></p>\n<p><code>import com.stratio.deep.cassandra.config._</code></p>\n<p><code>import com.stratio.deep.cassandra.extractor._</code></p>\n<p><code>import com.stratio.deep.mongodb.config._</code></p>\n<p><code>import com.stratio.deep.mongodb.extractor._</code></p>\n<p><code>import com.stratio.deep.es.config._</code></p>\n<p><code>import com.stratio.deep.es.extractor._</code></p>\n<p><code>import com.stratio.deep.aerospike.config._</code></p>\n<p><code>import com.stratio.deep.aerospike.extractor._</code></p>\n<p><code>import org.apache.spark.rdd._</code></p>\n<p><code>import org.apache.spark.SparkContext._</code></p>\n<p><code>import org.apache.spark.sql.api.java.JavaSQLContext</code></p>\n<p><code>import org.apache.spark.sql.api.java.JavaSchemaRDD</code></p>\n<p><code>import org.apache.spark.sql.api.java.Row</code></p>\n<p><code>import scala.collection.JavaConversions._</code></p>\n</li>\n</ul></li>\n</ul><p>Once you have a working development environment you can finally start testing Deep. This are the basic steps you will always have to perform in order to use Deep:</p>\n<ul><li><strong>Build an instance of a configuration object</strong>: this will let you tell Deep the Cassandra endpoint, the keyspace, the table you want to access and much more.\nIt will also let you specify which interface to use (the domain entity or the generic interface).\nWe have a factory that will help you create a configuration object using a fluent API. Creating a configuration object is an expensive operation.\nPlease take the time to read the java and scala examples provided in 'deep-examples' subproject and to read the comprehensive documentation at <a href=\"https://github.com/Stratio/deep-spark/blob/release/0.6/doc/t10-first-steps-deep-cassandra.md\">OpenStratio website</a>.</li>\n<li><strong>Create an RDD</strong>: using the DeepSparkContext helper methods and providing the configuration object you've just instantiated.</li>\n<li><strong>Perform some computation over this RDD(s)</strong>: this is up to you, we only help you fetching the data efficiently from Cassandra, you can use the powerful <a href=\"https://spark.apache.org/docs/1.1.1/api/java/index.html\" rel=\"nofollow\">Spark API</a>.</li>\n<li><strong>(optional) write the computation results out to Cassandra</strong>: we provide a way to efficiently save the result of your computation to Cassandra.\nIn order to do that you must have another configuration object where you specify the output keyspace/column family. We can create the output column family for you if needed.\nPlease, refer to the comprehensive Stratio Deep documentation at <a href=\"https://github.com/Stratio/deep-spark/blob/release/0.6/doc/about.md\">Stratio website</a>.</li>\n</ul>\n<ul><li><strong>Build an instance of a configuration object</strong>: this will let you tell Stratio Deep the MongoDB endpoint, the MongoDB database and collection you want to access and much more.\nIt will also let you specify which interface to use (the domain entity).\nWe have a factory that will help you create a configuration object using a fluent API. Creating a configuration object is an expensive operation.\nPlease take the time to read the java and scala examples provided in 'deep-examples' subproject and to read the comprehensive Deep documentation at <a href=\"https://github.com/Stratio/deep-spark/blob/release/0.6/doc/t20-first-steps-deep-mongodb.md\">OpenStratio website</a>.</li>\n<li><strong>Create an RDD</strong>: using the DeepSparkContext helper methods and providing the configuration object you've just instantiated.</li>\n<li><strong>Perform some computation over this RDD(s)</strong>: this is up to you, we only help you fetching the data efficiently from MongoDB, you can use the powerful <a href=\"https://spark.apache.org/docs/1.1.1/api/java/index.html\" rel=\"nofollow\">Spark API</a>.</li>\n<li><strong>(optional) write the computation results out to MongoDB</strong>: we provide a way to efficiently save the result of your computation to MongoDB.</li>\n</ul>\n<p>From version 0.4.x, Deep supports multiple datastores, in order to correctly implement this new feature Deep has undergone an huge refactor between versions 0.2.9 and 0.4.x. To port your code to the new version you should take into account a few changes we made.</p>\n<h2><a id=\"user-content-new-project-structure\" class=\"anchor\" aria-hidden=\"true\" href=\"#new-project-structure\"></a>New Project Structure</h2>\n<p>From version 0.4.x, Deep supports multiple datastores, in your project you should import only the maven dependency you will use: deep-cassandra, deep-mongodb, deep-elasticsearch or deep-aerospike.</p>\n<h2><a id=\"user-content-changes-to-comstratiodeepentitycells\" class=\"anchor\" aria-hidden=\"true\" href=\"#changes-to-comstratiodeepentitycells\"></a>Changes to 'com.stratio.deep.entity.Cells'</h2>\n<ul><li>Until version 0.4.x the 'Cells' was implicitly associated to a record coming from a specific table. When performing a join in Spark, 'Cell' objects coming from different tables are mixed into an single 'Cells' object.\nDeep now keeps track of the original table a Cell object comes from, changing the internal structure of 'Cells', where each 'Cell' is associated to its 'table'.\n<ol><li>If you are a user of 'Cells' objects returned from Deep, nothing changes for you. The 'Cells' API keeps working as usual.</li>\n<li>If you manually create 'Cells' objects you can keep using the original API, in this case each Cell you add to your Cells object is automatically associated to a default table name.</li>\n<li>You can specify the default table name, or let Deep chose an internal default table name for you.</li>\n<li>We added a new constructor to 'Cells' accepting the default table name. This way the 'old' API will always manipulate 'Cell' objects associated to the specified default table.</li>\n<li>For each method manipulating the content of a 'Cells' object, we added a new method that also accepts the table name: if you call the method\t whose signature does <em>not</em> have the table name, the table action is performed over the Cell associated to the default table, otherwise the action is performed over the 'Cell'(s) associated to the specified table.</li>\n<li>size() y isEmpty() will compute their results taking into account all the 'Cell' objects contained.</li>\n<li>size(String tableName) and isEmpty(tableName) compute their result taking into account only the 'Cell' objects associated to the specified table.</li>\n<li>Obviously, when dealing with Cells objects, Deep always associates a Cell to the correct table name.</li>\n</ol></li>\n</ul><p>Examples:</p>\n<pre>Cells cells1 = new Cells(); // instantiate a Cells object whose default table name is generated internally.\nCells cells2 = new Cells(\"my_default_table\"); // creates a new Cells object whose default table name is specified by the user\ncells2.add(new Cell(...)); // adds to the 'cells2' object a new Cell object associated to the default table\ncells2.add(\"my_other_table\", new Cell(...)); // adds to the 'cells2' object a new Cell associated to \"my_other_table\"  \n</pre>\n<h2><a id=\"user-content-changes-to-objects-hierarchy\" class=\"anchor\" aria-hidden=\"true\" href=\"#changes-to-objects-hierarchy\"></a>Changes to objects hierarchy</h2>\n<ul><li>IDeepJobConfig interface has been splitted into ICassandraDeepJobConfig and IMongoDeepJobConfig sub-interfaces. Each sub-interface exposes only the configuration properties that make sense for each data base.\ncom.stratio.deep.config.DeepJobConfigFactory's factory methods now return the proper subinterface.</li>\n<li><strong>DeepSparkContext</strong> has been splitted into <strong>CassandraDeepSparkContext</strong> and <strong>MongoDeepSparkContext</strong>.</li>\n<li><strong>DeepJobConfigFactory</strong> has been renamed to <strong>ConfigFactory</strong> (to reduce verbosity).</li>\n</ul><h2><a id=\"user-content-rdd-creation\" class=\"anchor\" aria-hidden=\"true\" href=\"#rdd-creation\"></a>RDD creation</h2>\n<p>Methods used to create Cell and Entity RDD has been merged into one single method:</p>\n<ul><li><strong>DeepSparkContext</strong>: createRDD(...)</li>\n</ul></article>",
        "created_at": "2018-07-24T19:51:49+0000",
        "updated_at": "2018-07-24T19:51:49+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 11,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/5228027?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11146"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1052,
            "label": "tools",
            "slug": "tools"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          }
        ],
        "is_public": true,
        "id": 11145,
        "uid": "5b57834a677951.14844004",
        "title": "Cassandra Parameters for Dummies",
        "url": "https://www.ecyrd.com/cassandracalculator/",
        "content": "<p>This simple form allows you to try out different values for your <a href=\"http://cassandra.apache.org\">Apache Cassandra</a> cluster\nand see what the impact is for your application.</p><div><p><label for=\"N\">Cluster size</label>\n  </p>\n<p><label for=\"RF\">Replication Factor</label>\n  </p>\n<p><label for=\"W\">Write Level</label>\n  </p>\n<p><label for=\"R\">Read Level</label>\n  </p>\n<hr /><div class=\"calculated\">Your reads are<p>\"Consistent\" means that for this particular Read/Write level combo, all nodes will \"see\" the same data.  \"Eventually consistent\" means\n        that you might get old data from some nodes and new data for others until the data has been replicated across all devices.  The idea is that this way you can\n        increase read/write speeds and improve tolerance against dead nodes.</p></div>\n<div class=\"calculated\">You can survive the loss of  without impacting the application.<p>How many nodes can go down without application noticing? This is a lower bound - in large clusters, you could lose more nodes and if they happen to be handling different parts of the keyspace, then you wouldn't notice either.</p></div>\n<div class=\"calculated\">You can survive the loss of  without data loss.<p>How many nodes can go down without physically losing data? This is a lower bound - in large clusters, you could lose more nodes and if they happen to be handling different parts of the keyspace, then you wouldn't notice either.</p></div>\n<div class=\"calculated\">You are really reading from  every time.<p>The more nodes you read from, more network traffic ensues, and the bigger the latencies involved.  Cassandra read operation won't return until at least this many nodes have responded with some data value.</p></div>\n<div class=\"calculated\">You are really writing to  every time.<p>The more nodes you write to, more network traffic ensues, and the bigger the latencies involved. Cassandra write operation won't return until at least this many nodes have acknowledged receiving the data.</p></div>\n<div class=\"calculated\">Each node holds  of your data.<p>The bigger your cluster is, the more the data gets distributed across your nodes.  If you are using the RandomPartitioner, or are very\n   good at distributing your keys when you use OrderedPartitioner, this is how much data each of your nodes has to handle.  This is also how much\n   of your keyspace becomes inaccessible for each node that you lose beyond the safe limit, above.</p></div></div>",
        "created_at": "2018-07-24T19:51:38+0000",
        "updated_at": "2018-07-24T19:51:38+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 1,
        "domain_name": "www.ecyrd.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11145"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1052,
            "label": "tools",
            "slug": "tools"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          }
        ],
        "is_public": true,
        "id": 11144,
        "uid": "5b57833d0756b2.13851326",
        "title": "Reaper: Easy Repair Management for Apache Cassandra",
        "url": "http://cassandra-reaper.io/",
        "content": "Reaper: Easy Repair Management for Apache Cassandra\n    <div id=\"all\"><header>\n        </header><section><div class=\"home-carousel\"><div class=\"container\"><div class=\"homepage owl-carousel\"><div class=\"item\"><div class=\"row\"><div class=\"col-sm-5 right\"><ul class=\"list-style-none\"><li>Simple web based UI</li>\n  <li>Full and incremental support</li>\n  <li>Supports all Cassandra versions</li>\n</ul></div><div class=\"col-sm-7\"><img class=\"img-responsive\" src=\"http://cassandra-reaper.io/img/carousel/repairs.png\" alt=\"\" /></div></div></div><div class=\"item\"><div class=\"row\"><div class=\"col-sm-5 right\"><ul class=\"list-style-none\"><li>Set and forget</li>\n  <li>No crontab required</li>\n  <li>Automatically scales with your cluster</li>\n</ul></div><div class=\"col-sm-7\"><img class=\"img-responsive\" src=\"http://cassandra-reaper.io/img/carousel/schedules.png\" alt=\"\" /></div></div></div><div class=\"item\"><div class=\"row\"><div class=\"col-sm-5 right\"><ul class=\"list-style-none\"><li>Healthy nodes are green</li>\n  <li>Downed nodes are red</li>\n</ul></div><div class=\"col-sm-7\"><img class=\"img-responsive\" src=\"http://cassandra-reaper.io/img/carousel/cluster-view.jpg\" alt=\"\" /></div></div></div></div></div></div></section><section class=\"bar background-white\"><div class=\"container\"><div class=\"col-md-12\"><div class=\"row\"><div class=\"col-md-4\"><div class=\"box-simple\"><h3>Easy to use web interface</h3><p>Point and click repair administration.  Set up a weekly repair schedule in minutes.</p></div></div><div class=\"col-md-4\"><div class=\"box-simple\"><h3>Manage multiple clusters</h3><p>Centralized repair for hundreds of clusters.</p></div></div><div class=\"col-md-4\"><div class=\"box-simple\"><h3>Open source, always free</h3><p>Apache Licensed.  Based on the original Spotify reaper codebase.  Adopted with love by <a href=\"http://thelastpickle.com/\">The Last Pickle.</a></p></div></div></div></div></div></section><div id=\"copyright\"><div class=\"container\"><div class=\"col-md-12\"><p class=\"pull-right\">\n              Template by <a href=\"http://bootstrapious.com/free-templates\">Bootstrapious</a>.\n              \n              Ported to Hugo by <a href=\"https://github.com/devcows/hugo-universal-theme\">DevCows</a>\n            </p></div></div></div></div>",
        "created_at": "2018-07-24T19:51:25+0000",
        "updated_at": "2018-07-24T19:51:25+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "cassandra-reaper.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11144"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1135,
            "label": "packages",
            "slug": "packages"
          },
          {
            "id": 1163,
            "label": "libraries",
            "slug": "libraries"
          }
        ],
        "is_public": true,
        "id": 11143,
        "uid": "5b578335734169.53530064",
        "title": "Caffinitas",
        "url": "http://caffinitas.org/mapper/",
        "content": null,
        "created_at": "2018-07-24T19:51:17+0000",
        "updated_at": "2018-07-24T19:51:17+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "caffinitas.org",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11143"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1161,
            "label": "general",
            "slug": "general"
          },
          {
            "id": 1162,
            "label": "relational -> cassandra",
            "slug": "relational-cassandra"
          }
        ],
        "is_public": true,
        "id": 11142,
        "uid": "5b5783188dc203.28625366",
        "title": "MySQL to Cassandra Migrations",
        "url": "https://academy.datastax.com/planet-cassandra/mysql-to-cassandra-migration",
        "content": "<p>For 15+ years, Oracle’s MySQL has been a de facto infrastructure piece in web applications, enjoying wide adoption. This is for good reason: MySQL provides a solid relational database that enables companies to build systems that perform well in many use cases. Yet, even its strongest supporters admit that it is not architected to tackle the new wave of big data applications. Modern businesses that need to manage big data use cases are turning to Apache Cassandra to replace MySQL.</p><p>Migrating from MySQL to Cassandra: General Info</p><h3>Is Cassandra Right for Your Application?</h3><p>A new class of databases (sometimes referred to as “NoSQL”) have been developed and designed with 18+ years worth of lessons learned from traditional relational databases such as MySQL. Cassandra (and other distributed or “NoSQL” databases) aim to make the “right” tradeoffs to<br />ultimately deliver a database that provides the scalability, redundancy, and performance needed in todays applications. Although MySQL may have performed well for you in the past, new business requirements and/or the need to both scale and improve the reliability of your application might mean that MySQL is no longer the correct fit.</p><p>Before committing any further time towards a MySQL to Cassandra migration, ask yourself:<br />“Is MySQL currently preventing development of new features or providing acceptable uptime, reliability, and scalability for my users?”</p><p>“No”: Not only should you not migrate to Cassandra, but also you most likely should not be considering a migration to any alternative database. Migrating an application to a new database is a very difficult, time consuming, and error-prone process.</p><p>“Yes”: Then hopefully you’ve found a helpful resource to help guide and plan your migration from MySQL to Cassandra. There are many databases<br />available, all with their various advantages, disadvantages and tradeoffs. This article is not an attempt to portray Cassandra as a perfect solution; in fact, Cassandra’s tradeoffs, advantages, and disadvantages will be highlighted. Hopefully this will help you make a decision that is both informed and educated; not one motivated by marketing hype or change for the sake of change.</p><p>Don’t try to shove a square peg in a round hole!</p><ul><li>Cassandra is not a relational database.</li>\n<li>Cassandra is not a 100%/“drop-in” replacement for MySQL.</li>\n<li>Simply migrating existing code to Cassandra without modifying and rethinking your existing data model will not result in perfect uptime or fix performance bottlenecks for your application. In fact, it might make things worse.</li>\n</ul><h3>Key Terminology</h3><p>The following overview of Cassandra terminology provides descriptions and their MySQL equivalent. The goal is to introduce the most basic terms and concepts required to get a basic understanding of Cassandra. To read more on the key terms and architecture of Cassandra you can find more detail in the <a href=\"http://www.datastax.com/documentation/cassandra/2.0/cassandra/architecture/architectureIntro_c.html\">Cassandra architecture documentation</a> or for a higher level overview visit the “<a href=\"http://planetcassandra.org/what-is-apache-cassandra/\">What is Cassandra</a>” page on Planet Cassandra.</p><p><img alt=\"\" src=\"https://academy.datastax.com/sites/default/files/keyterms-1.png\" /></p><p><img alt=\"\" src=\"https://academy.datastax.com/sites/default/files/keyterms-2.png\" /></p><p><img alt=\"\" src=\"https://academy.datastax.com/sites/default/files/keyterms-3.png\" /></p><h3>How is Data Handled?</h3><p>At a very high level, Cassandra operates by dividing all data evenly around a cluster of nodes, which can be visualized as aring. Nodes generally run on commodity hardware. Each Cassandra node in the cluster is responsible for and assigned a token range (which is essentially a range of hashes defined by a partitioner, which defaults to Murmur3Partitioner in Cassandra v1.2+). By default this hash range is defined with a maximum number of possible hash values ranging from 0 to 2^127-1.</p><p>Each update or addition of data contains a unique row key (also known as a primary key). The primary key is hashed to determine a replica (or node) responsible for a token range inclusive of a given row key. The data is then stored in the cluster<strong>n</strong> times (where <strong>n</strong> is defined by the keyspace’s replication factor), or once on each replica responsible a given query’s row key. All nodes in Cassandra are peers and a client’s read or write request can be sent to any node in the cluster, regardless of whether or not that node actually contains and is responsible for the requested data. There is no concept of a master or slave, and nodes dynamically learn about each other and the state and health of other nodes thru the gossip protocol. A node that receives a client query is referred to as the coordinator for the client operation; it facilitates communication between all replica nodes responsible for the query (contacting at least n replica nodes to satisfy the query’s consistency level) and prepares and returns a result to the client.</p><h3>Reads and Writes</h3><p>Clients may interface with Cassandra for reads and writes via either the native binary protocol or Thrift. CQL queries can be made over both transports. As a general recommendation, if you are just getting started with Cassandra you should stick to the native binary protocol and CQL and ignore Thrift.</p><p>When a client performs a read or write request, the coordinator node contacts the number of required replicas to satisfy the consistency level included with each request. For example, if a read request is processed using QUORUM consistency, and the keyspace was created with a “replication factor” of 3, 2 of the 3 replicas for the requested data would be contacted, their results merged, and a single result returned to the client. With write requests, the coordinator node will send a write requests with all mutated columns to all replica nodes for a given row key.</p><h3>Processing a Local Update</h3><p>When an update is processed – also known as a mutation — an entry is first added to the commit log, which ensures durability of the transaction. Next, it is also added to the memtable. A memtable is a bounded in memory write-back cache that contains recent writes which have not yet been flushed to an SSTable (a permanent, immutable, and serialized on disk copy of the tables data).</p><p>When updates cause a memtable to reach it’s configured maximum in-memory size, the memtable is flushed to an immutable SSTable, persisting the data from the memtable permanently on disk while making room for future updates. In the event of a crash or node failure, events are replayed from the commit log, which prevents the loss of any data from memtables that had not been flushed to disk prior to an unexpected event such as a power outage or crash.</p><h3>Distributed Computing</h3><p>Distributed logic and designs will inevitably cause an increase in complexity in application logic. When done right however, the rewards are obvious and easy to appreciate. Operationally, while it might be possible to get away with a single non-sharded MySQL instance installed via apt-get/emerge/yum/etc., operations with Cassandra need to be taken seriously to achieve desired performance and uptime of the cluster. Or, if you currently shard data across multiple MySQL instances, knowing that Cassandra deals with sharding and replication for you might be a huge benefit and upsell for Cassandra. But, unfortunately there is no such thing as a free lunch. For example, although Cassandra will remove all of your homegrown database abstraction and sharding code, you ultimately ended simply moving that logic from your code to Cassandra. Luckily, given the number of people and corporations of all sizes using Cassandra in production combined with an engaged and involved community, it’s fair to assume and argue that Cassandra’s equivalent of your MySQL sharding code will be better than your old homegrown solution.</p><h2>Development Considerations</h2><h3>Be Thoughtful About Your Data Model</h3><p>Creating a thoughtful and conscious data model in Cassandra from the very beginning is very important. A bad data model can easily ruin and erase any of the benefits you want by migrating to Cassandra in the first place. With MySQL, the lack of a thoughtful or poor data model can frequently be worked around and accommodated thanks to the various relational database features (for example, the use of complex JOINS).<br />While these MySQL queries might be slow and expensive, given enough time and resources it’s possible to get the exact desired result from the dataset. With Cassandra, it is much harder to retroactively “fix” a poor data model. First, the lack of JOINS in Cassandra removes complex reads as a hacked solution to a bad data model. Additionally, thanks to the power and architecture of Cassandra, it becomes very easy to store more rows and data than imaginable with MySQL. With increased amounts of data stored, comes an increased complexity in successfully getting the exact data needed within the given performance boundaries required by your application. A SELECT query containing only 30 rows will return quickly and predictably. Performing a query over 5 million rows requires processing significantly more IO. Just as more data in MySQL made complex JOINS more difficult, accommodating for a Cassandra data model that requires the iteration over multiple nodes and rows will be slow, inefficient, and most likely not work at all. Obviously, faster database responses are always better in any application; so don’t let your data model be the cause of slow database latency in your application!</p><h3>Denormalization</h3><p>Denormalization is the concept that a data model should be designed so that a given query can be served from the results from one row and query. Instead of doing multiple reads from multiple tables and rows to gather all the required data for a response, instead modify your application logic to insert the required data multiple times into every row that might need it in the future. This way, all required data can be available in just one read which prevents multiple lookups.</p><h2>Operational Considerations</h2><h3>Optimization and Tuning Cassandra</h3><p>There are lots of options to tweak in Cassandra. Much like turning the treble, bass, and volume nobs of your car’s sound system all to 11 won’t sound very good to your ears, it’s easy to do more harm than good when “optimizing” Cassandra and it’s many nobs and dials.</p><p>Options such as key cache and row cache are two great examples. In a MySQL world, much of the configuration tuning is spent on optimizing the various amounts of cache allocated. In the Cassandra world, these settings actually tend to decrease node and cluster stability. Cassandra is written in Java, and thus it must operate within the limitations of Java. One of the biggest considerations is Garbage Collection and the maximum size of the heap possible without running into large garbage collection related issues, which will crater the performance of Cassandra. As of JDK7 with CMS (the default in Cassandra 1.2.x and 2.0.x) the maximum recommended size of the heap is 8GB. This 8GB must be shared between all of the various Cassandra components. 2GB allocated to the key cache will (obviously) put another 2GB of pressure on the heap. Caches are an optimization not a requirement, so allocating more memory to caches should be considered as part of the big picture. If you can allocate the full 8GB to Cassandra, a suggestion would be to start with allocating no more than 768MB to the key cache (key_cache_size_in_mb) and 0MB to the row cache (row_cache_size_in_mb).</p><p>Another example is multithreaded_compaction. While this might seem like an obvious option to enable, in most cases leaving this option disabled can actually improve overall cluster stability and performance. In many cases, less is more.</p><h2>Migration Plan Considerations</h2><h3>Maintaining Data Integrity</h3><p>Sometimes the most difficult component of a migration is not in writing a set of reliable scripts to read from MySQL and insert into Cassandra, but trivial coding mistakes that can cause significant data discrepancies between the MySQL and Cassandra versions of the data.</p><p>Because migrating from MySQL to Cassandra will most likely require a change in your data model, the logic required to “convert” your relational MySQL data to it’s de-normalized form is the hardest part of the migration and certainly has the biggest risk.</p><p>Treat your migration scripts and logic not as one-off instances, but production quality code that can be run in any order, at any time. Mistakes in migration logic that result in an inconsistent version of the migrated data in Cassandra most likely will have a much greater impact than other dataset migration related bugs.</p><h3>Get to Know Bulk Loading</h3><p>Regardless of your migration strategy, in almost all cases you will have to perform an initial bulk import of your existing MySQL data into Cassandra. While it might be tempting to simply iterate over every MySQL result and then insert that result one mutation at a time into Cassandra, a more efficient way is to use the Cassandra Bulk Loader. At a high level, the Bulk Loader requires you to create a CSV file containing all of the rows and columns that need to be loaded into Cassandra. Using the Java class SSTableSimpleUnsortedWriter, you can create an SSTable from your CSV file, which can then be loaded directly into Cassandra using SSTableloader.</p><p>For more details and code samples reference the article at <a href=\"http://www.datastax.com/dev/blog/bulk-loading\">http://www.datastax.com/dev/blog/bulk-loading</a></p><h3>Migration Methods</h3><p>Sync Data Method:<br />When migrating to Cassandra and choosing a new data model might significantly increase your database workload. Alternatively, you might still need a live dataset in MySQL after the initial migration for legacy scripts that have not yet been migrated to use Cassandra.</p><p>Syncing from MySQL to Cassandra<br />In some cases it might not be practicable to add Cassandra to a legacy application. In this case it might be necessary to have an external process sync data from MySQL to Cassandra while running both new and old logic in parallel.</p><p>Suggestion:<br />Add a timestamp column to the MySQL table to be synced. With each update to MySQL also update the timestamp with the last updated time. At a scheduled interval then do a SELECT query from all MySQL shards where the last updated timestamp is greater than or equal to the time your last sync started.</p><p>Syncing from Cassandra back to MySQL<br />Some data models will be hard to sync from Cassandra back to MySQL (for example time series data). However, rows containing more de-normalized<br />“metadata”-like information can be synced.</p><p>What won’t work: Creating a sync script that executes via cron every n minutes and attempts to do a SELECT * FROM TABLE from Cassandra (and<br />then update and insert all of those records into MySQL) is a recipe for failure. Inherent to Cassandra’s design is that data is sharded across multiple nodes by a hash of it’s key. Performing a SELECT * query is a Cassandra anti-pattern and should be avoided. Iterating through every key across all nodes and returning a single paged dataset is both inefficient and impractical.</p><p>1st Suggestion:<br />Implement a queue that your application additionally writes to when it modifies a row in Cassandra. Have a script consume from this queue and de-duplicate the modified keys on a time interval and then bulk insert updates into MySQL.</p><p>2nd Suggestion:<br />If the data can be updated less frequently into MySQL, you could write a Hadoop Map/Reduce job that iterates over the column families that you need to sync. This solution gives a practicable and reproducible way to iterate through all keys in a column family. Using this approach as an additional sanity option to resolve missed updates from other incremental sync options.</p><p>3rd Suggestion:<br />Another option if you can afford a greater delay in the delta between updates from Cassandra back to MySQL is to use a tool such as SSTable2JSON to dump a column families SSTables into a JSON format, which can then be parsed and then used to update MySQL. This is a pretty heavy-handed method. Additionally, you’ll have to write logic to ensure you dump the SSTables from all nodes to get the entire column family.</p><p>Write Twice and Forget Method:<br />If you are able to modify your existing application to also interface with Cassandra, you can initially start your migration by writing database updates twice, once to MySQL and an additional time to Cassandra. Once you have all new updates being written to both MySQL and Cassandra, you can run a migration script that pages through all your existing MySQL data and inserts those records into Cassandra.</p><p>Initially, you might want to implement this second write to Cassandra as a completely non-blocking, write and forget, operation. If you experience initial issues during your Cassandra deployment, make sure not to impact your existing application when Cassandra is down.</p><p>Once you are satisfied with the fire-and-forget writes, you can slowly modify your application logic to start performing reads from Cassandra instead of MySQL. Thanks to the dual writes, if you run into issues, simply revert back to doing reads from MySQL.</p><h2>Use Cases and Migration Resources</h2><h3>Use Cases</h3><p><a href=\"http://planetcassandra.org/blog/post/youve-got-scale-aol-migrates-from-mysql-to-apache-cassandra-for-8x-improvement/\">AOL</a><br />AOL migrated their article index, in use for several AOL technologies form MySQL. The result was an 8X increase in writes, and considering the move to Cassandra as a “big win”.</p><p><a href=\"http://planetcassandra.org/blog/interview/coursera-migrates-to-the-top-of-the-class-moves-to-cassandra-for-an-always-on-on-demand-classroom/\">Coursera</a></p><p>Coursera was experiencing unexpected downtime, due to the RDBMS’ single point of failure.  In addition, Cassandra has enabled Coursera to become more dynamic; introducing their over 9 million users to an always available, on-demand course system.</p><p><a href=\"http://www.datastax.com/wp-content/uploads/2011/06/DataStax-CaseStudy-Mahalo.pdf\">Mahalo</a><br />Mahalo’s search technology was forced to move off of MySQL to Cassandra as their primary data store in order to realize lower costs and higher performance and scalability.</p><p><a href=\"http://planetcassandra.org/blog/post/scaling-in-the-cloud-with-cassandra-at-pantheon\">Pantheon Systems</a><br />Pantheon Systems, offering a platform for Drupal websites in the cloud, migrated to Cassandra primarily for greater scalability and ease of use.</p><p><a href=\"http://planetcassandra.org/blog/post/scoopit-turns-to-apache-cassandra-the-latest-and-best-technology-when-mysql-fails-to-keep-up\">Scoop.it</a><br />Scoop.it’s content curation publishing platform experienced the limitations of MySQL for handling their data growth and moved to Apache Cassandra for scalability and requirement of no downtime.</p><p><a href=\"http://planetcassandra.org/blog/post/ampushs-migration-from-mysql-to-cassandra-for-data-volume-high-availability-and-performance\">Ampush</a><br />Ampush’s migration from MySQL to Cassandra due to their increase in data volume, high availability and performance requirements which only Cassandra could satisfy.</p><p><a href=\"http://planetcassandra.org/blog/post/barracuda-networks-and-cassandra---battling-the-zombies\">Barracuda Networks</a><br />Barracudna Networks were not able to monitor customer threats in real-time with MySQL and went to Cassandra for the scalability and availability benefits.</p><p><a href=\"http://planetcassandra.org/blog/post/cassandra-summit-2013-cabs-cassandra-and-hailo-mysql-to-cassandra-by-dave-gardner\">Hailo</a><br />Hailo has leveraged Cassandra to build one of the most successful startups in European history. This presentation looks at how Hailo grew from a simple MySQL-backed infrastructure to a resilient Cassandra-backed system running in three data centers globally.</p><p><a href=\"http://www.datastax.com/wp-content/uploads/2011/04/DataStax-CS-Ooyala.pdf\">Ooyala</a><br />Ooyala chose Apache Cassandra for its elastic scalability and high performance – especially when their MySQL environment was not meeting customer service levels – to help their customers take a more strategic approach when delivering a digital video experience.</p><p><a href=\"http://planetcassandra.org/blog/post/appssavvy-fixes-mysql-scalability-by-switching-to-apache-cassandra\">AppsSavvy</a><br />AppsSavvy’s targeted advertising delivery solution moved from MySQL to Cassandra for increased scalability and performance under load.</p><p><a href=\"http://planetcassandra.org/blog/post/dating-site-zoosk-breaks-up-with-mysql-migrates-to-apache-cassandra-for-persistent-notifications\">Zoosk</a><br />Zoosk’s persistent notification system was moved off of MySQL and onto Apache Cassandra because it is a superior database for their high volume of writes of time series data.</p><p><a href=\"http://planetcassandra.org/blog/post/agentis-energy-stores-over-15-billion-records-of-time-series-usage-data-in-apache-cassandra\">Agentis</a><br />Agentis Energy had to move to Cassandra once the scale of their data became unmanageable on MySQL as they now store over 15 billion records of time series usage energy usage data.</p><h3>Migration Resources</h3><p>Whitepaper: <a href=\"http://www.datastax.com/wp-content/uploads/2012/08/WP-DataStax-MySQLtoCassandra.pdf\">Why Migrate From MySQL to Cassandra?</a> By Robin Schumacher<br />This whitepaper discusses the ‘why’ and ‘how’ to migrate from MySQL to Cassandra as well as what a good migration candidate looks like.</p><p>Hindsight is 20/20: <a href=\"http://www.youtube.com/watch?v=gW4jEOKRB04\" target=\"_blank\">MySQL to Cassandra</a>. This webinar offers a brief intro to how Barracuda Networks uses Cassandra and the ways in which they are replacing their MySQL infrastructure, with Cassandra including lessons learned. A slideshare from this presentation is available as well: <a href=\"http://www.slideshare.net/planetcassandra/c-summit-2013-hindsight-is-2020-mysql-to-cassandra-by-michael-kjellman\">Hindsight is 20/20: MySQL to Cassandra</a></p><p>5 lessons learned by Zoosk for <a href=\"https://about.zoosk.com/en/engineering-blog/moving-persistent-notifications-from-mysql-to-cassandra/\" target=\"_blank\">moving persistent notifications from MySQL to Apache Cassandra</a> in order to support very high volumes of write while minimizing write latency.</p><h3>About the Author</h3><p>Michael Kjellman is a San Francisco based Software Engineer. Michael works across multiple products, technologies, and languages. He primarily works on Barracuda’s spam infrastructure and web filter classification data. Follow him on Twitter at<a href=\"https://twitter.com/mkjellman\">@mkjellman</a>.</p>",
        "created_at": "2018-07-24T19:50:48+0000",
        "updated_at": "2018-07-24T19:50:48+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 16,
        "domain_name": "academy.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11142"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1161,
            "label": "general",
            "slug": "general"
          },
          {
            "id": 1162,
            "label": "relational -> cassandra",
            "slug": "relational-cassandra"
          }
        ],
        "is_public": true,
        "id": 11141,
        "uid": "5b578315187192.66551135",
        "title": "Relational Databases vs. NoSQL",
        "url": "https://www.datastax.com/relational-database-to-nosql",
        "content": "<noscript>\n\n\n\n<div class=\"DS17\"><div class=\"connect-us\"><a href=\"https://www.datastax.com/contactus\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Mail.svg\" alt=\"email icon\" />email</a><a href=\"https://www.datastax.com/company#offices\"><img src=\"https://www.datastax.com/templates/dist/images/svg/Datastax_Icons_Phone.svg\" alt=\"phone icon\" />call</a></div></div><header class=\"DS17\"><div class=\"container\"><div class=\"wrapper\"><div class=\"logo\"><a href=\"https://www.datastax.com/\"><img src=\"https://www.datastax.com/templates/dist/images/logo-header.png\" alt=\"DataStax logo\" /></a><a href=\"https://www.datastax.com/\"><img src=\"https://www.datastax.com/templates/dist/images/new_logo.png\" alt=\"DataStax logo\" /></a></div></div></div>\n  \n</header><div id=\"DXDIV0\"><div id=\"MainBody\"><div id=\"MainChannel\" class=\"width-100 clearfix\"><div id=\"Content\" class=\"\"><div id=\"ContentChannel\"><div class=\"dx_stlfntmobmod_pge\" id=\"dx_thispage_div1\"><div class=\"thismb2content1width1\"><div class=\"dx_hnltstd_lt_div dx_marginvertical_approx68pxless\"><div class=\"dxfnts_ds_f20l30\">Technology that can scale, perform and deliver continuous availability is the difference between today’s successful online applications and those that fail. Relational databases (RDBMS) have struggled to keep up with the wave of modernization, leading to the rise of NoSQL as the most viable database option for online Web and mobile applications.<p>The path to understanding whether a NoSQL technology like DataStax Enterprise is right for your business as either a complementary technology to an RDBMS or as a complete replacement is a three step approach.&#13;\n            </p></div></div></div></div></div></div><div class=\"DXpgA2AclassV1\"><p>SHARE THIS PAGE</p></div></div></div></div> \n\t \n              \n  <div class=\"DS17\"><div class=\"use-case\"><div class=\"wrapper\"><div class=\"two-col text-light-blue\"><h6>Customer Experience</h6><ul><li><a href=\"https://www.datastax.com/use-cases/customer-360\">Customer 360</a></li>\n          <li><a href=\"https://www.datastax.com/personalization\">Personalization &amp; Recommendations</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/loyalty-programs\">Loyalty Programs</a></li>\n          <li><a href=\"https://www.datastax.com/fraud-detection\">Consumer Fraud Detection</a></li>\n        </ul></div><div class=\"two-col text-light-green\"><h6><a href=\"#\">Enterprise Optimization</a></h6><ul><li><a href=\"https://www.datastax.com/use-cases/ecommerce\">eCommerce</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/identity-management\">Identity Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/security\">Security and Compliance</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/supply-chain\">Supply Chain</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/inventory-management\">Inventory Management</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/asset-monitoring\">Asset Monitoring</a></li>\n          <li><a href=\"https://www.datastax.com/use-cases/logistics\">Logistics</a></li>\n        </ul></div></div></div></div>\n  \n    \n\t\n\t\n\t\n\t\n\t\n\t\n</noscript>",
        "created_at": "2018-07-24T19:50:45+0000",
        "updated_at": "2018-07-24T19:50:45+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 0,
        "domain_name": "www.datastax.com",
        "preview_picture": "https://www.datastax.com/wp-content/themes/datastax-2014-08/images/common/DataStax_Web_Social_DefaultGenericV2_1024x351_wide.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11141"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 11070,
        "uid": null,
        "title": "Data Modelling Recommended Practices - Instaclustr",
        "url": "https://www.instaclustr.com/support/documentation/cluster-management/data-modelling-recommended-practices/",
        "content": "<strong>Item</strong> <strong>Rational</strong> <strong><em>Schema</em></strong>  Is it properly denormalised? Does it require multiple queries to fetch information, or could the table just include info from the other table? Is there potential to consolidate data from multiple tables? Developers from relational background may tend to normalised models resulting in inefficient use of Cassandra. Partition key cardinality allows high number of partitions (minimum 100,000 possible preferred) A low number of partitions will lead to inefficient read and writes and increase risk of unevenly sized partitions Partition key prevents substantial skewing of partitions? If it is possible for a small number of partitions to have vastly higher numbers of rows than average (say 100x) then this can cause significantly uneven performance and disk usage. Using collections (maps,list,set)? Number of elements is 64k, keep the total size of the collect small (&lt;1MB) as the map is not paged. Very large collections can negatively  impact read/write performance. Is  gc_grace_seconds changed from default (864000)? If so, is that appropriate and impact considered? Lowering gc_crace_seconds results in space being reclaimed more quickly after deletes but runs small risk of “resurrected deletes” given we only run repairs weekly. Is caching set to KEYS_ONLY or NONE? Row caching for Cassandra 2.0 is often not effective. 2.1 row caching features may be effective if tuned correctly (see <a href=\"http://www.datastax.com/dev/blog/row-caching-in-cassandra-2-1\">row-caching-in-cassandra-2-1</a> and <a href=\"http://docs.datastax.com/en/cassandra/2.1/cassandra/operations/ops_monitoring_cache_c.html\">Cassandra Docs</a>) Is chosen compaction strategy appropriate?  <ul><li>SizeTieredCompactionStrategy: default and suitable as a starting point for most uses cases with balance of reads and writes</li> <li>LevelledCompactionStrategy: does more compaction work to improve read performance. Generally used if high ratio of reads to writes.</li> <li>DateTieredCompactionStrategy: useful for data where data is “hot” when first written but sees less access over time.</li> <li>Check that the compaction strategy is appropriately tuned (see <a href=\"http://docs.datastax.com/en/cassandra/2.1/cassandra/operations/ops_configure_compaction_t.html\">Cassandra Docs</a>) defaults are usually ok, but DTCS requires specific compaction options set to be effective.</li> </ul> Are counters used? Instaclustr only supports the use of counters with Cassandra 2.1 as Cassandra 2.0 counters are unreliable in many circumstances.   <strong><em>Secondary Indexes</em></strong>  Is cardinality of secondary index low? Cardinality of index should be at least an order of magnitude lower and preferable at least 100x lower than indexed table. <p>Also secondary indexes on boolean columns are not effective.</p> <p>See <a href=\"http://docs.datastax.com/en/cql/3.1/cql/ddl/ddl_when_use_index_c.html\" target=\"_blank\" rel=\"noopener noreferrer\">Cassandra Docs</a> and</p> <p><a href=\"http://www.wentnet.com/blog/?p=77\">http://www.wentnet.com/blog/?p=77</a></p> Is the indexed column frequently updated/deleted? Overhead of maintaining index will be incurred on each update/delete and may also result in excessive tombstones in the index table.   <strong><em>Queries</em></strong>  Are there logged batches used? If so, are they relatively small (&lt;100) Logged batches require coordinate node to control all operations and can result in very high load on coordinator node for large batches. Logged batches are only required for atomic operations across multiple rows/tables (not performance). Are there unlogged batches? If so, are they small (&lt;100) or on the same partition key? Unlogged batches can improve performance but need to either be small or on a single partition key otherwise they can negatively impact performance. Not that unlogged batches do not provide atomic operations. For large range queries, is the client paging through results? Paging is necessary to read large results sets without memory constraints. Most drivers have inbuilt paging support but needs to be explicitly turned on in query code.   <p>Does the query on the index lookup a row in a large partition?</p>   <p>Whole partition will be scanned to find matching rows – potentially expensive reads.</p>",
        "created_at": "2018-07-24T12:44:51+0000",
        "updated_at": "2018-09-13T14:48:52+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11070"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1233,
            "label": "data.modeling",
            "slug": "data-modeling"
          }
        ],
        "is_public": false,
        "id": 11069,
        "uid": null,
        "title": "6-step-guide-to-apache-cassandra-data-modelling-white-paper",
        "url": "https://www.instaclustr.com/resource/6-step-guide-to-apache-cassandra-data-modelling-white-paper/",
        "content": null,
        "created_at": "2018-07-24T12:44:20+0000",
        "updated_at": "2018-09-13T14:48:24+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": null,
        "language": null,
        "reading_time": 0,
        "domain_name": "www.instaclustr.com",
        "preview_picture": null,
        "http_status": null,
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11069"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11068,
        "uid": null,
        "title": "Cassandra NoSQL Data Model Design  - High Scalability -",
        "url": "http://highscalability.com/blog/2017/11/13/cassandra-nosql-data-model-design.html",
        "content": "<div class=\"journal-entry-tag journal-entry-tag-post-title\">   <img title=\"date\" alt=\"date\" class=\"inline-icon date-icon\" src=\"http://highscalability.com/universal/images/transparent.png\" />Monday, November 13, 2017 at 8:56AM </div><div class=\"body\">\n  \n        \n        \n        \n          <p><img src=\"https://c1.staticflickr.com/5/4535/24521720278_d4be4b74b5_q.jpg\" alt=\"\" style=\"float: right;\" /></p>\n<p>We at <a href=\"https://www.instaclustr.com/\">Instaclustr</a> recently published a blog post on the most common data modelling mistakes that we see with Cassandra. This post was very popular and led me to think about what advice we could provide on how to approach designing your Cassandra data model so as to come up with a quality design that avoids the traps.</p>\n<p>There are a number of good articles around that with rules and patterns to fit your data model into: <a rel=\"noopener\" href=\"https://www.instaclustr.com/resource/6-step-guide-to-apache-cassandra-data-modelling-white-paper/\" target=\"_blank\">6 Step Guide to Apache Cassandra Data Modelling</a> and <a rel=\"noopener\" href=\"https://support.instaclustr.com/hc/en-us/articles/207071957-Data-Modelling-Recommended-Practices\" target=\"_blank\">Data Modelling Recommended Practices</a>.</p>\n<p>However, we haven’t found a step by step guide to analysing your data to determine how to fit in these rules and patterns. This white paper is a quick attempt at filling that gap.</p>\n<h2>Phase 1: Understand the data</h2>\n<p>This phase has two distinct steps that are both designed to gain a good understanding of the data that you are modelling and the access patterns required.</p>\n<h3>Define the data domain</h3>\n<p>The first step is to get a good understanding of your data domain. As someone very familiar with relation data modelling, I tend to sketch (or at least think) ER diagrams to understand the entities, their keys and relationships. However, if you’re familiar with another notation then it would likely work just as well. The key things you need to understand at a logical level are:</p>\n<p>• What are the entities (or objects) in your data model?<br />• What are the primary key attributes of the entities?<br />• What are the relationships between the entities (i.e. references from one to the other)?<br />• What is the relative cardinality of the relationships (i.e. if you have a one to many is it one to 10 or one to 10,000 on average)?</p>\n<p>Basically, these are the same things you’d expect in from logical ER model (although we probably don’t need a complete picture of all the attributes) along with a complete understanding of the cardinality of relationships that you’d normally need for a relational model. An understanding of the demographics of key attributes (cardinality, distribution) will also be useful in finalising your Cassandra model. Also, understand which key attributes are fixed and which change over the life of a record.</p>\n<h2>Define the required access patterns</h2>\n<p>The next step, or quite likely a step carried out in conjunction with step 1, is to understand how you will need to access your data:</p>\n<ul><li>List out the paths you will follow to access the data, such as:           \n<ul><li>Start with a customer id, search for transactions in a date range and then look up all the details about a particular transaction from the search resultsStart with a particular server and metric, retrieve x metrics values in ascending age</li>\n<li>Start with a particular server and metric, retrieve x metrics values in ascending age starting at a particular point in time.</li>\n<li>For a given sensor, retrieve all readings of multiple metrics for a given day.</li>\n<li>For a given sensor, retrieve the current value.</li>\n</ul></li>\n</ul><ul><li>Remember that any updates of a record are an access path that needs to be considered</li>\n<li>Determine which accesses are the most crucial from a performance point of view – are there some which need to be as quick as possible while performance requirements for others allow time for multiple reads or range scans?</li>\n<li>Remember that you need a pretty complete understanding of how you will access your data at this stage – part of the trade-off for Cassandra’s performance, reliability and scalability is a fairly restricted set of methods for accessing data in a particular table.</li>\n</ul><h2>Phase 2: Understand the entities</h2>\n<p>This phase has two specific steps designed to gain an understanding of both the primary and secondary entities associated with the data.</p>\n<h3>Identify primary access entities</h3>\n<p>Now we’re moving from analysing your data domain and application requirements to starting to design your data model. You really want to be pretty solid on steps 1 and 2 before moving on to this stage.</p>\n<p>The idea here is to denormalize your data into the smallest number of tables possible based on your access patterns. For each lookup by key that your access patterns require, you will need a table to satisfy that lookup. I’ve coined the term primary access entity to describe the entity your using for the lookup (for example, a lookup by client id is using client as the primary access entity, a lookup by server and metric name is using a server-metric entity as the primary access entity).</p>\n<p>The primary access entity defines the partition level (or grain if you’re familiar with dimensional modelling) of the resulting denormalized table (i.e. there will be one partition in the table for each instance of the primary access entity).</p>\n<p>You may choose to satisfy some access patterns using secondary indexes rather than complete replicas of the data with a different primary access entity. Keep in mind that columns in include in a secondary index should have a significantly lower cardinality than the table being indexed and be aware of the frequency of updates of the indexed value.</p>\n<p>For the example access patterns above, we would define the following primary access entities:</p>\n<ul><li>customer and transaction (get a list of transactions from the customer entity and then use that to look up transaction details from the transaction entity)</li>\n<li>server-metric</li>\n<li>sensor</li>\n<li>sensor</li>\n</ul><h3>Allocate secondary entities</h3>\n<p>The next step is to find a place to store the data that belongs to entities that have not been chosen as primary access entities (I’ll call these entities secondary entities). You can choose to:</p>\n<ul><li>Push down by taking data from a parent secondary entity (one side) of a one to many relationship and storing multiple copies of it at the primary access entity level (for example, storing customer phone number in each customer order record); or</li>\n<li>Push up by taking data from the child secondary entity (many side) of a one to many relationship and storing it at the primary access entity level either by use of cluster keys or by use of multi-value types (list and maps) (for example adding a list of line items to a transaction level table).</li>\n</ul><p>For some secondary entities, there will only be one related primary access entity and so there is no need to choose where and which direction to push. For other entities, you will need to choose will need to choose which primary access entities to push the data into.</p>\n<p>For optimal read performance, you should push a copy of the data to every primary access entity that is used as an access path for the data in the secondary entity.</p>\n<p>However, this comes at an insert/update performance and application complexity cost of maintaining multiple copies the data. This trade-off between read performance and data maintenance cost needs to be judged in the context of the specific performance requirements of your application.</p>\n<p>The other decision to be made at this stage is between using a cluster key or a multi-value type for pushing up. In general:</p>\n<ul><li>Use a clustering key where there is only one child secondary entity to push up and particularly where the child secondary entity itself has children to roll-up.</li>\n<li>Use multi-value types where there are multiple child entities to push up into the primary entity</li>\n</ul><p>Note that these rules are probably oversimplified but serve as a starting point for more detailed consideration.</p>\n<h2>Phase 3: Review &amp; Tune</h2>\n<p>The last phase provides an opportunity to review the data model, test and to tune as necessary.</p>\n<h3>Review partition &amp; cluster keys</h3>\n<p>Entering this stage, you have all the data you need to store allocated to a table or tables and your tables support accessing that data according to your required access patterns. The next step is to check that the resulting data model makes efficient use of Cassandra and, if not, to adjust. The items to check and adjust at this stage are:</p>\n<ul><li>Do your partition keys have sufficient cardinality? If not, it may be necessary to move columns from the clustering key to the partition key (e.g. changing primary key (client_id, timestamp) to primary key ((client_id, timestamp))) or introduce new columns which group multiple cluster keys into partitions (e.g. changing primary key (client_id, timestamp) to primary key ((client_id, day), timestamp).</li>\n<li>Will the values in your partition keys be updated frequently?Updates of a primary key value will result in deletion and re-insertion of the record which can result in issues with tombstones. For example, trying to maintain a table with all clients of a particular status, you might have primary key (status, client ID). However, this will result in a delete and re-insert every time a client’s status changes. This would be a good candidate to use a set or list data type rather than including client ID as the cluster key.</li>\n<li>Is the number of records in each partition bounded? Extremely large partitions and/or very unevenly sized partitions can cause issues. For example, if you have a client_updates table with primary key (client_id, update_timestamp) there is potentially no limit to how many times a particular client record can be update and you may have significant unevenness if you have a small number of clients that have been around for 10 years and most clients only having a day or two’s history. This is another example where it’s useful to introduce new columns which group multiple cluster keys into partitions partitions (e.g. changing primary key (client_ id, update_timestamp) to primary key ((client_id, month), update_timestamp).</li>\n</ul><h3>Test and tune</h3>\n<p>The final step is perhaps the most important – test your data model and tune it as required. Keep in mind that issues like partitions or rows growing too large or tombstones building up in a table may only become visible after days (or longer) of use under real-world load. It’s therefore important to test as closely as possible to real-world load and to monitor closely for any warning signs (the nodetool cfstats and cfhistograms commands are very useful for this).</p>\n<p>At this stage you may also consider tuning some of the settings that effect the physical storage of your data. For example:</p>\n<ul><li>changing compaction strategy;</li>\n<li>reducing gc_grace_seconds if you are only deleting data using TTL; or</li>\n<li>setting caching options.</li>\n</ul><h2>A Worked Example</h2>\n<p>To illustrate this, I’ll walk through a basic example based on building a database to store and retrieve log messages from multiple servers. Note this is quite simplified compared to most real-world requirements.</p>\n<h3>Step 1: Define the data domain</h3>\n<p><a class=\"fbx-link fbx-instance\" href=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design.png\"><img class=\"aligncenter wp-image-6928 size-large\" src=\"https://www.instaclustr.com/wp-content/uploads/2017/10/Defining-the-data-domain-Instaclustr-Data-model-Design-1024x616.png\" alt=\"Defining the data model domain Instaclustr Data Model design\" width=\"1024\" height=\"616\" /></a></p>\n<p>The previous ER diagram illustrated the data domain. We have:</p>\n<ul><li>Lots (millions) of log messages which have a timestamp and a body. Although message ID is shown as the primary key in the ER diagram, message time plus message type is an alternate primary key.</li>\n<li>Each log message has a message type and types are further grouped into a message category (for example, a message type might be “out of memory error” and category might be “error”). There a couple of hundred message types and around 20 categories.</li>\n<li>Each log message comes from a message source. The message source is the server that generated the message. There are 1000s of servers in our system. Each message source has a source type to categorise the source (e.g. red hat server, ubuntu server, windows server, router, etc.). There are around 20 source types. There are ~10,000 messages per source per day.</li>\n<li>The message body can be parsed and stored as multiple message parts (basically key, value pairs). There is typically less than 20 parts per message.</li>\n</ul><h3>Step 2: Define the required access patterns</h3>\n<p>We need to be able to:</p>\n<ul><li>Retrieve all available information about the most recent 10 messages for a given source (and be able to work back in time from there).</li>\n<li>Retrieve all available information about the most recent 10 message for a given source type.</li>\n</ul><h3>Step 3: Identify primary access entities</h3>\n<p>There are two primary access entities here – source and source type. The cardinality (~20) of source type makes it a good candidate for a secondary index so we will use source as the primary access entity and add a secondary index for source type.</p>\n<h3>Step 4: Allocate secondary entities</h3>\n<p>In this example, this step is relatively simple as all data needs to roll into the log source primary access entity. So we:</p>\n<ul><li>Push down source type name</li>\n<li>Push down message category and message type to log message</li>\n<li>Push up log message as the clustering key for the new entity</li>\n<li>Push up message part as a map type with.</li>\n</ul><p>The end result is that would be a single table with a partition key of source ID and a clustering key of (message time, message type).</p>\n<h3>Step 5: Review partition and cluster keys</h3>\n<p>Checking these partition and cluster keys against the checklist:</p>\n<ul><li>Do your partition keys have sufficient cardinality? Yes, there are 1000s of sources.</li>\n<li>Will the values in your partition keys being updated frequently? No, all the data is write-once.</li>\n<li>Is the number of records in each partition bounded? No – messages could build up indefinitely over time.</li>\n</ul><p>So, we need to address the unbound partition size. A typical pattern to address that in time series data such as this is to introduce a grouping of time periods into the cluster key. In this case 10,000 messages per day is a reasonable number to include in one partition so we’ll use day as part of our partition key.</p>\n<p>The resulting Cassandra table will look some like:</p>\n<div id=\"crayon-5a0099a71ecf5394143671\" class=\"crayon-syntax crayon-font-monaco crayon-os-pc print-yes notranslate crayon-theme-sublime-text\">\n<div class=\"crayon-main\">\n<table class=\"crayon-table\"><tbody><tr class=\"crayon-row\"><td class=\"crayon-nums\">\n</td>\n<td class=\"crayon-code\">\n<div class=\"crayon-pre\">\n<div id=\"crayon-5a0099a71ecf5394143671-1\" class=\"crayon-line\">CREATE TABLE example.log_messages (</div>\n<div id=\"crayon-5a0099a71ecf5394143671-2\" class=\"crayon-line crayon-striped-line\">message_id uuid,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-3\" class=\"crayon-line\">source_name text,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-4\" class=\"crayon-line crayon-striped-line\">source_type text,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-5\" class=\"crayon-line\">message_type text,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-6\" class=\"crayon-line crayon-striped-line\">message_urgency int,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-7\" class=\"crayon-line\">message_category text,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-8\" class=\"crayon-line crayon-striped-line\">message_time timestamp,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-9\" class=\"crayon-line\">message_time_day text,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-10\" class=\"crayon-line crayon-striped-line\">message_body text,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-11\" class=\"crayon-line\">message_parts map&amp;lt;text, frozen &amp;gt;</div>\n<div id=\"crayon-5a0099a71ecf5394143671-13\" class=\"crayon-line\">PRIMARY KEY ((source_name, message_time_day,</div>\n<div id=\"crayon-5a0099a71ecf5394143671-14\" class=\"crayon-line crayon-striped-line\">message_time, message_type)</div>\n<div id=\"crayon-5a0099a71ecf5394143671-15\" class=\"crayon-line\">) WITH CLUSTERING ORDER BY (message_time DESC);</div>\n<div id=\"crayon-5a0099a71ecf5394143671-16\" class=\"crayon-line crayon-striped-line\">CREATE INDEX log_messages_sourcetype_idx ON</div>\n<div id=\"crayon-5a0099a71ecf5394143671-17\" class=\"crayon-line\">example.log_messages (source_type);</div>\n</div>\n</td>\n</tr></tbody></table></div>\n</div>\n\n<h2>Conclusion</h2>\n<p>Hopefully, this process and basic example will help you start to get familiar with Cassandra data modelling. We’ve only covered a basic implementation that fits well with Cassandra, however there are many other examples on the web which can help you work through more complex requirements. Instaclustr also provides our customers with data modelling review and assistance, so get in touch with us if you need some hands-on assistance.</p>\n        \n  \n          \n  \n         \n  \n      </div>",
        "created_at": "2018-07-24T12:44:03+0000",
        "updated_at": "2018-07-24T12:44:10+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 12,
        "domain_name": "highscalability.com",
        "preview_picture": "https://c1.staticflickr.com/5/4535/24521720278_d4be4b74b5_q.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11068"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11067,
        "uid": null,
        "title": "Cassandra Time Series Data Modeling For Massive Scale",
        "url": "http://thelastpickle.com/blog/2017/08/02/time-series-data-modeling-massive-scale.html",
        "content": "<p>One of the big challenges people face when starting out working with Cassandra and time series data is understanding the impact of how your write workload will affect your cluster.  Writing too quickly to a single partition can create hot spots that limit your ability to scale out. Partitions that get too large can lead to issues with repair, streaming, and read performance.  Reading from the middle of a large partition carries a lot of overhead, and results in increased GC pressure.  Cassandra 4.0 should <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-9754\">improve the performance of large partitions</a>, but it won’t fully solve the other issues I’ve already mentioned.  For the foreseeable future, we will need to consider their performance impact and plan for them accordingly.</p><p>In this post, I’ll discuss a common Cassandra data modeling technique called <em>bucketing</em>.  Bucketing is a strategy that lets us control how much data is stored in each partition as well as spread writes out to the entire cluster.  This post will discuss two forms of bucketing. These techniques can be combined when a data model requires further scaling.  Readers should already be familiar with the anatomy of a partition and basic CQL commands.</p>\n<p>When we first learn about data modeling with Cassandra, we might see something like the following:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>CREATE TABLE raw_data (\n    sensor text,\n    ts timeuuid,\n    readint int,\n    primary key(sensor, ts)\n) WITH CLUSTERING ORDER BY (ts DESC) \n  AND compaction = {'class': 'TimeWindowCompactionStrategy', \n                    'compaction_window_size': 1, \n                    'compaction_window_unit': 'DAYS'};\n</pre></div></div>\n<p>This is a great first data model for storing some very simple sensor data.  Normally the data we collect is more complex than an integer, but in this post we’re going to focus on the keys.  We’re leveraging <a href=\"http://thelastpickle.com/blog/2016/12/08/TWCS-part1.html\">TWCS</a> as our compaction strategy.  TWCS will help us deal with the overhead of compacting large partitions, which should keep our CPU and I/O under control.  Unfortunately it still has some significant limitations.  If we aren’t using a TTL, as we take in more data, our partition size will grow constantly, unbounded.  As mentioned above, large partitions carry significant overhead when repairing, streaming, or reading from arbitrary time slices.</p>\n<p>To break up this big partition, we’ll leverage our first form of bucketing.  We’ll break our partitions into smaller ones based on time window.  The ideal size is going to keep partitions under 100MB.  For example, one partition per sensor per day would be a good choice if we’re storing 50-75MB of data per day.  We could just as easily use week (starting from some epoch), or month and year as long as the partitions stay under 100MB.  Whatever the choice, leaving a little headroom for growth is a good idea.</p>\n<p>To accomplish this, we’ll add another component to our partition key.  Modifying our earlier data model, we’ll add a <code class=\"highlighter-rouge\">day</code> field:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>CREATE TABLE raw_data_by_day (\nsensor text,\nday text,\nts timeuuid,\nreading int,\nprimary key((sensor, day), ts)\n) WITH CLUSTERING ORDER BY (ts DESC) \n       AND COMPACTION = {'class': 'TimeWindowCompactionStrategy', \n                     'compaction_window_unit': 'DAYS', \n                     'compaction_window_size': 1};\n</pre></div></div>\n<p>Inserting into the table requires using the date as well as the <code class=\"highlighter-rouge\">now()</code> value (you could also generate a TimeUUID in your application code):</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>INSERT INTO raw_data_by_day (sensor, day, ts, reading) \nVALUES ('mysensor', '2017-01-01', now(), 10);\n</pre></div></div>\n<p>This is one way of limiting the amount of data per partition.  For fetching large amounts of data across multiple days, you’ll need to issue one query per day.  The nice part about querying like this is we can spread the work over the entire cluster rather than asking a single node to perform a lot of work.  We can also issue these queries in parallel by relying on the async calls in the driver.  The Python driver even has a convenient helper function for this sort of use case:</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre>from itertools import product\nfrom cassandra.concurrent import execute_concurrent_with_args\ndays = [\"2017-07-01\", \"2017-07-12\", \"2017-07-03\"]  # collecting three days worth of data\nsession  = Cluster([\"127.0.0.1\"]).connect(\"blog\")\nprepared = session.prepare(\"SELECT day, ts, reading FROM raw_data_by_day WHERE sensor = ? and day = ?\")\nargs = product([\"mysensor\"], days) \n# args: ('test', '2017-07-01'), ('test', '2017-07-12'), ('test', '2017-07-03')\n# driver handles concurrency for you\nresults = execute_concurrent_with_args(session, prepared, args)\n# Results:\n#[ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d36750&gt;),\n# ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d36a90&gt;),\n# ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d36550&gt;)]\n</pre></div></div>\n<p>A variation on this technique is to use a different table per time window.  For instance, using a table per month means you’d have twelve tables per year:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>CREATE TABLE raw_data_may_2017 (\n    sensor text,\n    ts timeuuid,\n    reading int,\n    primary key(sensor, ts)\n) WITH COMPACTION = {'class': 'TimeWindowCompactionStrategy', \n                     'compaction_window_unit': 'DAYS', \n                     'compaction_window_size': 1};\n</pre></div></div>\n<p>This strategy has a primary benefit of being useful for archiving and quickly dropping old data.  For instance, at the beginning of each month, we could archive last month’s data to HDFS or S3 in parquet format, taking advantage of cheap storage for analytics purposes.  When we don’t need the data in Cassandra anymore, we can simply drop the table.  You can probably see there’s a bit of extra maintenance around creating and removing tables, so this method is really only useful if archiving is a requirement.  There are other methods to archive data as well, so this style of bucketing may be unnecessary.</p>\n<p>The above strategies focuses on keeping partitions from getting too big over a long period of time.  This is fine if we have a predictable workload and partition sizes that have very little variance.  It’s possible to be ingesting so much information that we can overwhelm a single node’s ability to write data out, or the ingest rate is significantly higher for a small percentage of objects.  Twitter is a great example, where certain people have tens of millions of followers but it’s not the common case.  It’s common to have a separate code path for these types of accounts where we need massive scale</p>\n<p>The second technique uses multiple partitions at any given time to fan out inserts to the entire cluster.  The nice part about this strategy is we can use a single partition for low volume, and many partitions for high volume.</p>\n<p>The tradeoff we make with this design is on reads we need to use a scatter gather, which has significantly higher overhead.  This can make pagination more difficult, amongst other things.  We need to be able to track how much data we’re ingesting for each gizmo we have.  This is to ensure we can pick the right number of partitions to use.  If we use too many buckets, we end up doing a lot of really small reads across a lot of partitions.  Too few buckets, we end up with really large partitions that don’t compact, repair, stream well, and have poor read performance.</p>\n<p>For this example, we’ll look at a theoretical model for someone who’s following a lot of users on a social network like Twitter.  Most accounts would be fine to have a single partition for incoming messages, but some people / bots might follow millions of accounts.</p>\n<p><em>Disclaimer: I have no knowledge of how Twitter is actually storing their data, it’s just an easy example to discuss.</em></p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>CREATE TABLE tweet_stream (\n    account text,\n    day text,\n    bucket int,\n    ts timeuuid,\n    message text,\n    primary key((account, day, bucket), ts)\n) WITH CLUSTERING ORDER BY (ts DESC) \n         AND COMPACTION = {'class': 'TimeWindowCompactionStrategy', \n                       'compaction_window_unit': 'DAYS', \n                       'compaction_window_size': 1};\n</pre></div></div>\n<p>This data model extends our previous data model by adding <code class=\"highlighter-rouge\">bucket</code> into the partition key.  Each day can now have multiple buckets to fetch from.  When it’s time to read, we need to fetch from all the partitions, and take the results we need.  To demonstrate, we’ll insert some data into our partitions:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>cqlsh:blog&gt; insert into tweet_stream (account, day, bucket, ts, message) VALUES ('jon_haddad', '2017-07-01', 0, now(), 'hi');\ncqlsh:blog&gt; insert into tweet_stream (account, day, bucket, ts, message) VALUES ('jon_haddad', '2017-07-01', 1, now(), 'hi2');\ncqlsh:blog&gt; insert into tweet_stream (account, day, bucket, ts, message) VALUES ('jon_haddad', '2017-07-01', 2, now(), 'hi3');\ncqlsh:blog&gt; insert into tweet_stream (account, day, bucket, ts, message) VALUES ('jon_haddad', '2017-07-01', 3, now(), 'hi4');\n</pre></div></div>\n<p>If we want the ten most recent messages, we can do something like this:</p>\n<div class=\"language-python highlighter-rouge\"><div class=\"highlight\"><pre>from itertools import chain\nfrom cassandra.util import unix_time_from_uuid1\nprepared = session.prepare(\"SELECT ts, message FROM tweet_stream WHERE account = ? and day = ? and bucket = ? LIMIT 10\")\n# let's get 10 buckets \npartitions = range(10)\n# [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\nargs = product([\"jon_haddad\"], [\"2017-07-01\"], partitions)\nresult = execute_concurrent_with_args(session, prepared, args)\n# [ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1e6d0&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1d710&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1d4d0&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1d950&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1db10&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1dfd0&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1dd90&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1d290&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1e250&gt;),\n#  ExecutionResult(success=True, result_or_exc=&lt;cassandra.cluster.ResultSet object at 0x106d1e490&gt;)]\nresults = [x.result_or_exc for x in result]\n# append all the results together\ndata = chain(*results)\n            \nsorted_results = sorted(data, key=lambda x: unix_time_from_uuid1(x.ts), reverse=True)            \n# newest stuff first\n# [Row(ts=UUID('e1c59e60-7406-11e7-9458-897782c5d96c'), message=u'hi4'),\n#  Row(ts=UUID('dd6ddd00-7406-11e7-9458-897782c5d96c'), message=u'hi3'),\n#  Row(ts=UUID('d4422560-7406-11e7-9458-897782c5d96c'), message=u'hi2'),\n#  Row(ts=UUID('d17dae30-7406-11e7-9458-897782c5d96c'), message=u'hi')]\n</pre></div></div>\n<p>This example is only using a LIMIT of 10 items, so we can be lazy programmers, merge the lists, and then sort them.  If we wanted to grab a lot more elements we’d want to use a k-way merge algorithm.  We’ll come back to that in a future blog post when we expand on this topic.</p>\n<p>At this point you should have a better understanding of how you can distribute your data and requests around the cluster, allowing it to scale much further than if a single partition were used.  Keep in mind each problem is different, and there’s no one size fits all solution.</p>",
        "created_at": "2018-07-24T12:43:50+0000",
        "updated_at": "2018-07-24T12:43:59+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 9,
        "domain_name": "thelastpickle.com",
        "preview_picture": "http://thelastpickle.com/android-chrome-192x192.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11067"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11066,
        "uid": null,
        "title": "Designing a Cassandra Data Model",
        "url": "https://shermandigital.com/blog/designing-a-cassandra-data-model/",
        "content": "<p>Cassandra is an open source, distributed database. It’s useful for managing large quantities of data across multiple data centers as well as the cloud.</p><h3 id=\"cassandra-data-model\">Cassandra data model</h3><p>Cassandra’s data model consists of keyspaces, column families, keys, and columns. The table below compares each part of the Cassandra data model to its analogue in a relational data model.</p><table><thead><tr><th><strong>Cassandra Data Model</strong></th>\n<th><strong>Relational Data Model</strong></th>\n</tr></thead><tbody><tr><td>Keyspace</td>\n<td>Database</td>\n</tr><tr><td>Column Family</td>\n<td>Table</td>\n</tr><tr><td>Partition Key</td>\n<td>Primary Key</td>\n</tr><tr><td>Column Name/Key</td>\n<td>Column Name</td>\n</tr><tr><td>Column Value</td>\n<td>Column Value</td>\n</tr></tbody></table><h3 id=\"how-cassandra-organizes-data\">How Cassandra organizes data</h3><p>Cassandra organizes data into partitions. Each partition consists of multiple columns. Partitions are stored on a node. Nodes are generally part of a cluster where each node is responsible for a fraction of the partitions.</p><p>When inserting records, Cassandra will hash the value of the inserted data’s partition key; Cassandra uses this hash value to determine which node is responsible for storing the data.</p><h3 id=\"where-are-the-rows\">Where are the rows?</h3><p>Cassandra is a column data store, meaning that each partition key has a set of one or more columns. Let’s say we have a list of fruits:</p><ul><li>[Apple, Banana, Orange, Pear]</li>\n</ul><p>We create a column family of fruits, which is essentially the same as a table in the relational model. Inside our column family, Cassandra will hash the name of each fruit to give us the partition key, which is essentially the primary key of the fruit in the relational model.</p><p>Now things start to diverge from the relational model. Cassandra will store each fruit on its own partition, since the hash of each fruit’s name will be different. Because each fruit has its own partition, it doesn’t map well to the concept of a row, as Cassandra has to issue commands to potentially four separate nodes to retrieve all data from the fruit column family.</p><p>We’ll get into more details later, but for now it’s enough to know that for Cassandra to look up a set of data (or a set of rows in the relational model), we have to store all of the data under the same partition key. To summarize, rows in Cassandra are essentially data embedded within a partition due to the fact that the data share the same partition key.</p><h3 id=\"data-model-goals\">Data model goals</h3><ol><li>Spread data evenly around the cluster. Paritions are distributed around the cluster based on a hash of the partition key. To distribute work across nodes, it’s desirable for every node in the cluster to have roughly the same amount of data.</li>\n<li>Minimize the number of partitions read. Partitions are groups of columns that share the same partition key. Since each partition may reside on a different node, the query coordinator will generally need to issue separate commands to separate nodes for each partition we query.</li>\n<li>Satisfy a query by reading a single partition. This means we will use roughly one table per query. Supporting multiple query patterns usually means we need more than one table. Data duplication is encouraged.</li>\n</ol><h3 id=\"components-of-the-cassandra-data-model\">Components of the Cassandra data model</h3><p><strong>Column family</strong></p><p>Column families are established with the CREATE TABLE command. Column families are represented in Cassandra as a map of sorted maps. The partition key acts as the lookup value; the sorted map consists of column keys and their associated values.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">Map&lt;ParitionKey, SortedMap&lt;ColumnKey, ColumnValue&gt;&gt;</pre></div><p><strong>Partition key</strong></p><p>The partition key is responsible for distributing data among nodes. A partition key is the same as the primary key when the primary key consists of a single column.</p><p><img src=\"https://shermandigital.com/img/blog/cassandra-partition.png\" alt=\"Cassandra partition\" /></p><p>Partition keys belong to a node. Cassandra is organized into a cluster of nodes, with each node having an equal part of the partition key hashes.</p><p>Imagine we have a four node Cassandra cluster. In the example cluster below, Node 1 is responsible for partition key hash values 0-24; Node 2 is responsible for partition key hash values 25-49; and so on.</p><p><img src=\"https://shermandigital.com/img/blog/cassandra-cluster.png\" alt=\"Cassandra cluster\" /></p><p><strong>Replication factor</strong></p><p>Depending on the replication factor configured, data written to Node 1 will be replicated in a clockwise fashion to its sibling nodes. So in our example above, assume we have a four-node cluster with a replication factor of three. When we insert data with a partition key of 23, the data will get written to Node 1 and replicated to Node 2 and Node 3. When we insert data with a partition key of 88, the data will get written to Node 4 and replicated to Node 1 and Node 2.</p><p><strong>Compound key</strong></p><p>Compound keys include multiple columns in the primary key, but these additional columns do not necessarily affect the partition key. A partition key with multiple columns is known as a composite key and will be discussed later.</p><p>Let’s borrow an example from <a href=\"https://twitter.com/AdamHutson\">Adam Hutson’s</a> excellent <a href=\"http://datascale.io/cassandra-data-model-basics/\">blog on Cassandra data modeling</a>. Consider a Cassandra database that stores information on CrossFit gyms. One property of CrossFit gyms is that each gym must have a unique name i.e. no two gyms are allowed to share the same name.</p><p>The table below is useful for looking up a gym when we know the name of the gym we’re looking for.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE crossfit_gyms (  \n   gym_name text,  \n   city text,  \n   state_province text,  \n   country_code text,  \n   PRIMARY KEY (gym_name)  \n);</pre></div><p>Now suppose we want to look up gyms by location. If we use the crossfit_gyms table, we’ll need to iterate over the entire result set. Instead, we’ll create a new table that will allow us to query gyms by country.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE crossfit_gyms_by_location (  \n   country_code text,  \n   state_province text,  \n   city text,  \n   gym_name text,  \n   PRIMARY KEY (country_code, state_province, city, gym_name)  \n);</pre></div><p>Note that only the first column of the primary key above is considered the partition key; the rest of columns are clustering keys. This means that while the primary key represents a unique gym record/row, all gyms within a country reside on the same partition. So when we query the crossfit_gyms_by_location table, we receive a result set consisting of every gym sharing a given country_code. While useful for searching gyms by country, using this table to identify gyms within a particular state or city requires iterating over all gyms within the country in which the state or city is located.</p><p><strong>Clustering key</strong></p><p>Clustering keys are responsible for sorting data within a partition. Each primary key column after the partition key is considered a clustering key. In the crossfit_gyms_by_location example, country_code is the partition key; state_province, city, and gym_name are the clustering keys. Clustering keys are sorted in ascending order by default. So when we query for all gyms in the United States, the result set will be ordered first by state_province in ascending order, followed by city in ascending order, and finally gym_name in ascending order.</p><p><strong>Order by</strong></p><p>To sort in descending order, add a WITH clause to the end of the CREATE TABLE statement.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE crossfit_gyms_by_location (  \n   country_code text,  \n   state_province text,  \n   city text,  \n   gym_name text,  \n   PRIMARY KEY (country_code, state_province, city, gym_name)  \n) WITH CLUSTERING ORDER BY (state_province DESC, city ASC, gym_name ASC);</pre></div><p> The result set will now contain gyms ordered first by state_province in descending order, followed by city in ascending order, and finally gym_name in ascending order. You must specify the sort order for each of the clustering keys in the ORDER BY statement. The partition key is not part of the ORDER BY statement because its values are hashed and therefore won’t be close to each other in the cluster.</p><p><strong>Composite key</strong></p><p>Composite keys are partition keys that consist of multiple columns. The crossfit_gyms_by_location example only used country_code for partitioning. The result is that all gyms in the same country reside within a single partition. This can lead to wide rows. In the case of our example, there are over 7,000 CrossFit gyms in the United States, so using the single column partition key results in a row with over 7,000 combinations.</p><p>To avoid wide rows, we can move to a composite key consisting of additional columns. If we change the partition key to include the state_province and city columns, the partition hash value will no longer be calculated off only country_code. Now, each combination of country_code, state_province, and city will have its own hash value and be stored in a separate partition within the cluster. We accomplish this by nesting parenthesis around the columns we want included in the composite key. </p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE crossfit_gyms_by_city (  \n country_code text,  \n state_province text,  \n city text,  \n gym_name text,  \n opening_date timestamp,  \n PRIMARY KEY ((country_code, state_province, city), opening_date, gym_name)  \n) WITH CLUSTERING ORDER BY ( opening_data ASC, gym_name ASC );</pre></div><p>Notice that we are no longer sorting on the partition key columns. Each combination of the partition keys is stored in a separate partition within the cluster.</p><p>When issuing a CQL query, you must include all partition key columns, at a minimum. You can then apply an additional filter by adding each clustering key in the order in which the clustering keys appear. Below you can see valid queries and invalid queries from our crossfit_gyms_by_city example.</p><p><strong>Valid queries:</strong></p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">SELECT * FROM crossfit_gyms_by_city WHERE country_code = 'USA' and state_province = 'VA' and city = 'Arlington'</pre></div><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">SELECT * FROM crossfit_gyms_by_city WHERE country_code = 'USA' and state_province = 'VA' and city = 'Arlington' and opening_date &lt;  '2015-01-01 00:00:00+0200'</pre></div><p><strong>Invalid queries:</strong></p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">SELECT * FROM crossfit_gyms_by_city WHERE country_code = 'USA' and state_province = 'VA'</pre></div><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">SELECT * FROM crossfit_gyms_by_city WHERE country_code = 'USA' and state_province = 'VA' and city = 'Arlington' and gym_name = 'CrossFit Route 7'</pre></div><p>The first invalid query is missing the city partition key column. The second invalid query uses the clustering key gym_name without including the preceding clustering key opening_date.</p><p>The reason the order of clustering keys matters is because the clustering keys provide the sort order of the result set. Because of the clustering key’s responsibility for sorting, we know all data matching the first clustering key will be adjacent to all other data matching that clustering key.</p><p>In our example, this means all gyms with the same opening date will be grouped together in alphabetical order. Gyms with different opening dates will appear in temporal order.</p><p>Because we know the order, CQL can easily truncate sections of the partition that don’t match our query to satisfy the WHERE conditions pertaining to columns that are not part of the partition key. However, because the clustering key gym_name is secondary to clustering key opening_date, gyms will appear in alphabetical order only for gyms opened on the same day (within a particular city, in this case). Therefore, we can’t specify the gym name in our CQL query without first specifying an opening date.</p><h3 id=\"internal-data-structure\">Internal data structure</h3><p>If we create a column family (table) with CQL:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE crossfit_gyms (  \n gym_name text PRIMARY KEY,  \n country_code text,  \n state_province text,  \n city text  \n);</pre></div><p> And insert a row:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">INSERT INTO crossfit_gyms (country_code, state_province, city, gym_name) VALUES ('USA', 'CA', 'San Francisco', 'San Francisco CrossFit');</pre></div><p>Assuming we don’t encode the data, it is stored internally as:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">Partition Key: San Francisco CrossFit  \n=&gt;(column=, value=, timestamp=1374683971220000)  \n=&gt;(column=city, value='San Francisco', timestamp=1374683971220000)  \n=&gt;(column=country_code, value='USA', timestamp=1374683971220000)  \n=&gt;(column=state_province, value='CA', timestamp=1374683971220000)</pre></div><p>You can see that the partition key is used for lookup. In this case the first column is also the partition key, so Cassandra does not repeat the value. The next three columns hold the associated column values.</p><p>If we use a composite key, the internal structure changes a bit. Let’s start with a general example borrowed from <a href=\"https://www.gitbook.com/@teddyma\">Teddy Ma’s</a> <a href=\"https://teddyma.gitbooks.io/learncassandra/content/index.html\">step-by-step guide to learning Cassandra</a>.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE example (\n    partitionKey1 text,\n    partitionKey2 text,\n    clusterKey1 text,\n    clusterKey2 text,\n    normalField1 text,\n    normalField2 text,\n    PRIMARY KEY (\n        (partitionKey1, partitionKey2),\n        clusterKey1, clusterKey2\n        )\n    );</pre></div><p>And insert a row:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">INSERT INTO example (\n    partitionKey1,\n    partitionKey2,\n    clusterKey1,\n    clusterKey2,\n    normalField1,\n    normalField2\n    )  VALUES (\n    'partitionVal1',\n    'partitionVal2',\n    clusterVal1',\n    'clusterVal2',\n    'normalVal1',\n    'normalVal2');</pre></div><p>This data is stored internally as:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">RowKey: partitionVal1:partitionVal2\n=&gt; (column=clusterVal1:clusterVal2:, value=, timestamp=1374630892473000)\n=&gt; (column=clusterVal1:clusterVal2:normalfield1, value=6e6f726d616c56616c31, timestamp=1374630892473000)\n=&gt; (column=clusterVal1:clusterVal2:normalfield2, value=6e6f726d616c56616c32, timestamp=1374630892473000)</pre></div><p>The composite key columns are concatenated to form the partition key (RowKey). The clustering keys are concatenated to form the first column and then used in the names of each of the following columns that are not part of the primary key. The actual values we inserted into normalField1 and normalField2 have been encoded, but decoding them results in normalValue1 and normalValue2, respectively.</p><p>Now we can adapt this to our CrossFit example.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE crossfit_gyms_by_state (  \n   country_code text,  \n   state_province text,  \n   city text,  \n   gym_name text,  \n   opening_date timestamp,  \n   street text,  \n   PRIMARY KEY (  \n      (country_code, state_province),  \n      city, gym_name  \n   )  \n);</pre></div><p>And insert a row:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">INSERT INTO crossfit_gyms_by_state (  \n   country_code,   \n   state_province,   \n   city,   \n   gym_name,   \n   opening_date,  \n   street  \n)   \nVALUES (  \n   'USA',   \n   'CA',  \n   'San Francisco',   \n   'San Francisco CrossFit',  \n   '2015-01-01 00:00:00+0200',  \n   '1162A Gorgas Ave');\n   </pre></div><p>For the sake of readability, I won’t encode the values of the columns. The internal structure is approximately:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">Partition Key: USA:CA  \n=&gt;(column='San Francisco':'San Francisco CrossFit', value=, timestamp=1374683971220000)  \n=&gt;(column='San Francisco':'San Francisco CrossFit':'​opening_date', value='2015-01-01 00:00:00+0200', timestamp=1374683971220000)  \n=&gt;(column='San Francisco':'San Francisco CrossFit':'​street', value='1162A Gorgas Ave', timestamp=1374683971220000)</pre></div><p>Finally, we’ll show how Cassandra represents sets, lists, and maps internally. Once again, we’ll use an example from <a href=\"https://www.gitbook.com/@teddyma\">Teddy Ma’s</a> <a href=\"https://teddyma.gitbooks.io/learncassandra/content/index.html\">step-by-step guide to learning Cassandra</a>.</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">CREATE TABLE example (  \n   key1 text PRIMARY KEY,  \n   map1 map&lt;text,text&gt;,  \n   list1 list&lt;text&gt;,  \n   set1 set&lt;text&gt;  \n);</pre></div><p>Insert a row:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">INSERT INTO example (  \n   key1,  \n   map1,  \n   list1,  \n   set1  \n) VALUES (  \n   'john',  \n   {'patricia':'555-4326','doug':'555-1579'},  \n   ['doug','scott'],  \n   {'patricia','scott'}  \n);</pre></div><p>Internally:</p><div class=\"highlight\"><pre class=\"language-bash\" data-lang=\"bash\">RowKey: john  \n=&gt; (column=, value=, timestamp=1374683971220000)  \n=&gt; (column=map1:doug, value='555-1579', timestamp=1374683971220000)  \n=&gt; (column=map1:patricia, value='555-4326', timestamp=1374683971220000)  \n=&gt; (column=list1:26017c10f48711e2801fdf9895e5d0f8, value='doug', timestamp=1374683971220000)  \n=&gt; (column=list1:26017c12f48711e2801fdf9895e5d0f8, value='scott', timestamp=1374683971220000)  \n=&gt; (column=set1:'patricia', value=, timestamp=1374683971220000)  \n=&gt; (column=set1:'scott', value=, timestamp=1374683971220000)</pre></div><p>To store maps, Cassandra adds a column for each item in the map. The column name is a concatenation of the the column name and the map key. The value is the key’s value.</p><p>To store lists, Cassandra adds a column for each entry in the list. The column name is a concatenation of the the column name and a UUID generated by Cassandra. The value is the value of the list item.</p><p>To store sets, Cassandra adds a column for each entry. The column name is a concatenation of the the column name and the entry value. Cassandra does not repeat the entry value in the value, leaving it empty.</p><h3 id=\"conclusion\">Conclusion</h3><p>You now have enough information to begin designing a Cassandra data model. Remember to work with the unstructured data features of Cassandra rather than against them. Designing a data model for Cassandra can be an adjustment coming from a relational database background, but the ability to store and query large quantities of data at scale make Cassandra a valuable tool.</p><h3 id=\"appendix\">Appendix</h3><p><strong>Cassandra Features</strong></p><ul><li>Continuous availability. The peer-to-peer replication of data to nodes within a cluster results in no single point of failure. This is true even across data centers.</li>\n<li>Linear performance when scaling nodes in a cluster. If three nodes are achieving 3,000 writes per second, adding three more nodes will result in a cluster of six nodes achieving 6,000 writes per second.</li>\n<li>Tunable consistency. If we want to replicate data across three nodes, we can have a replication factor of three, yet not necessarily wait for all three nodes to acknowledge the write. Data will eventually be written to all three nodes, but we can acknowledge the write after writing the data to one or more nodes without waiting for the full replication to finish.</li>\n<li>Flexible data model. Every row can have a different number of columns with support for many types of data.</li>\n<li>Query language (CQL) with a SQL-like syntax.</li>\n<li>Support for Java Monitoring Extensions (JMX). Metrics about performance, latency, system usage, etc. are available for consumption by other applications.</li>\n</ul><p><strong>Cassandra Limitations</strong></p><ul><li>No join or subquery support for aggregation. According to Cassandra’s documentation, this is by design, encouraging denormalization of data into partitions that can be queried efficiently from a single node, rather than gathering data from across the entire cluster.</li>\n<li>Ordering is set at table creation time on a per-partition basis. This avoids clients attempting to sort billions of rows at run time.</li>\n<li>All data for a single partition must fit on disk in a single node in the cluster.</li>\n<li>It’s recommended to keep the number of rows within a partition below 100,000 items and the disk size under 100 MB.</li>\n<li>A single column value is limited to 2 GB (1 MB is recommended).</li>\n</ul><p>A less obvious limitation of Cassandra is its lack of row-level consistency. Modifications to a column family (table) that affect the same row and are processed with the same timestamp will result in a tie.</p><p>In the event of a tie Cassandra follows two rules:</p><ol><li>Deletes take precedence over inserts/updates.</li>\n<li>If there are two updates, the one with the lexically larger value wins.</li>\n</ol><p>This means for inserts/updates, Cassandra resolves row-level ties by comparing values at the column (cell) level, writing the greater value. This can result in one update modifying one column while another update modifies another column, resulting in rows with combinations of values that never existed.</p><p class=\"margin-top-25\">\n<i class=\"fa fa-tag color-medium-gray\">\n<a href=\"https://shermandigital.com/tags/cassandra\" class=\"margin-right-10\">\nCassandra\n</a>\n</i></p><noscript><p>Please enable JavaScript to view the <a href=\"https://disqus.com/?ref_noscript\">comments powered by Disqus.</a></p></noscript>",
        "created_at": "2018-07-24T12:43:26+0000",
        "updated_at": "2019-01-18T21:03:05+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 14,
        "domain_name": "shermandigital.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11066"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11065,
        "uid": null,
        "title": "Tech Talk: Cassandra Data Modeling",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=tg6eIht-00M",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/tg6eIht-00M?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-07-24T12:42:29+0000",
        "updated_at": "2018-07-24T12:42:42+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/tg6eIht-00M/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11065"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 603,
            "label": "book",
            "slug": "book"
          }
        ],
        "is_public": false,
        "id": 11064,
        "uid": null,
        "title": "Apache Cassandra - Dzone Refcardz",
        "url": "https://dzone.com/refcardz/apache-cassandra?chapter=1",
        "content": "<div class=\"col-md-12 content-html\"><p>Apache Cassandra is a high-performance, extremely scalable, fault tolerant (i.e., no single point of failure), distributed non-relational database solution. Cassandra combines all the benefits of Google Bigtable and Amazon Dynamo to handle the types of database management needs that traditional RDBMS vendors cannot support. DataStax is the leading worldwide commercial provider of Cassandra products, services, support, and training.</p></div><div class=\"col-md-12 content-html\"><p>Cassandra is in use at <a href=\"http://www.slideshare.net/adrianco/migrating-netflix-from-oracle-to-global-cassandra\">Netflix</a>, <a href=\"http://www.slideshare.net/kevinweil/rainbird-realtime-analytics-at-twitter-strata-2011\">Twitter</a>, <a href=\"http://www.slideshare.net/eonnen/from-100s-to-100s-of-millions/\">Urban Airship</a>, <a href=\"http://www.slideshare.net/daveconnors/cassandra-puppet-scaling-data-at-15-per-month\">Constant Contact</a>, <a href=\"http://blog.reddit.com/2010/03/she-who-entangles-men.html\">Reddit</a>, Cisco, OpenX, Rackspace, Ooyala, and <a href=\"http://www.datastax.com/cassandrausers\">more companies</a> that have large active data sets. The largest known Cassandra cluster has over 300 TB of data in over 400 machines.</p><p>(From: <a href=\"http://cassandra.apache.org/\">http://cassandra.apache.org/</a>)</p></div><div class=\"col-md-12 content-html\"><table border=\"1\" style=\"width: 697px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 117px;\"> </td>\n   <td class=\"right_th_colored\" style=\"width: 261px;\"><h3>Cassandra</h3></td>\n   <td class=\"right_th_colored\" style=\"width: 297px;\"><h3>RDBMS</h3></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">Atomicity</td>\n   <td class=\"right_td_colored\">Success or failure on a row-by-row basis.</td>\n   <td class=\"right_td_colored\">Enforced at every scope, at the cost of performance and scalability.</td>\n  </tr><tr><td class=\"left_td_colored\">Sharding</td>\n   <td class=\"right_td_colored\">Native share-nothing architecture, inherently partitioned by a configurable strategy.</td>\n   <td class=\"right_td_colored\">Often forced when scaling, partitioned by key or function</td>\n  </tr><tr><td class=\"left_td_colored\">Consistency</td>\n   <td class=\"right_td_colored\">No consistency in the ACID sense. Can be tuned to provide consistency in the CAP sense--data is consistent across all the nodes in a distributed database cluster ,guaranteeing read-after-write or eventual readability.</td>\n   <td class=\"right_td_colored\">Favors consistency over availability tunable via isolation levels.</td>\n  </tr><tr><td class=\"left_td_colored\">Durability</td>\n   <td class=\"right_td_colored\">Writes are durable to a replica node, being recorded in memory and the commit log before acknowledged. In the event of a crash, the commit log replays on restart to recover any lost writes before data is flushed to disk.</td>\n   <td class=\"right_td_colored\">Typically, data is written to a single master node, sometimes configured with synchronous replication at the cost of performance and cumbersome data restoration.</td>\n  </tr><tr><td class=\"left_td_colored\">Multi-Datacenter Replication</td>\n   <td class=\"right_td_colored\">Native capabilities for data replication over lower bandwidth, higher latency, less reliable connections.</td>\n   <td class=\"right_td_colored\">Typically only limited long-distance replication to read-only slaves receiving asynchronous updates.</td>\n  </tr><tr><td class=\"left_td_colored\">Security</td>\n   <td class=\"right_td_colored\">Coarse-grained and primitive.</td>\n   <td class=\"right_td_colored\">Fine-grained access control to objects.</td>\n  </tr></tbody></table></div><div class=\"col-md-12 content-html\"><p>Cassandra has a simple schema comprising keyspaces, column families, rows, and columns.</p><table border=\"1\" style=\"width: 662px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 114px;\"> </td>\n   <td class=\"right_th_colored\" style=\"width: 122px;\">Definition</td>\n   <td class=\"right_th_colored\" style=\"width: 124px;\">RDBMS Analogy</td>\n   <td class=\"right_th_colored\" style=\"width: 274px;\">Object Equivalent</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">Schema/ Keyspace</td>\n   <td class=\"right_td_colored\">A collection of column families. </td>\n   <td class=\"right_td_colored\">Schema/Database</td>\n   <td class=\"right_td_colored\">Set </td>\n  </tr><tr><td class=\"left_td_colored\">Table/ Column Family</td>\n   <td class=\"right_td_colored\">A set of rows.</td>\n   <td class=\"right_td_colored\">Table</td>\n   <td class=\"right_td_colored\">Map </td>\n  </tr><tr><td class=\"left_td_colored\">Row</td>\n   <td class=\"right_td_colored\">An ordered set of columns.</td>\n   <td class=\"right_td_colored\">Row</td>\n   <td class=\"right_td_colored\">OrderedMap </td>\n  </tr><tr><td class=\"left_td_colored\">Column</td>\n   <td class=\"right_td_colored\">A key/value pair and timestamp.</td>\n   <td class=\"right_td_colored\">Column (Name, Value)</td>\n   <td class=\"right_td_colored\">(key, value, timestamp)</td>\n  </tr></tbody></table></div><div class=\"col-md-12 content-html\"><p>Also known as a keyspace, the schema is akin to a database or schema in RDBMS and contains a set of tables. A schema is also the unit for Cassandra's access control mechanism. When enabled, users must authenticate to access and manipulate data in a schema or table.</p></div><div class=\"col-md-12 content-html\"><p>A table, also known as a column family, is a map of rows. A table defines the column names and data types. The client application provides rows that conform to the schema. Each row has the same fixed set of columns.</p><p>As Values for these properties, Cassandra provides the following CQL data types for columns.</p><table border=\"1\" style=\"width: 673px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 149px;\"><strong>Type</strong></td>\n   <td class=\"right_th_colored\" style=\"width: 260px;\"><strong>Purpose</strong></td>\n   <td class=\"right_th_colored\" style=\"width: 242px;\"><strong>Storage </strong></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">ascii</td>\n   <td class=\"right_td_colored\">Efficient storage for simple ASCII strings.</td>\n   <td class=\"right_td_colored\">Arbitrary number of ASCII bytes (i.e., values are 0-127).</td>\n  </tr><tr><td class=\"left_td_colored\">boolean</td>\n   <td class=\"right_td_colored\">True or False.</td>\n   <td class=\"right_td_colored\">Single byte.</td>\n  </tr><tr><td class=\"left_td_colored\">blob</td>\n   <td class=\"right_td_colored\">Arbitrary byte content.</td>\n   <td class=\"right_td_colored\">Arbitrary number of byes.</td>\n  </tr><tr><td class=\"left_td_colored\">CompositeType</td>\n   <td class=\"right_td_colored\">A single type comprising sub-components each with their own types.</td>\n   <td class=\"right_td_colored\">An arbitrary number of bytes comprising concatenated values of the subtypes..</td>\n  </tr><tr><td class=\"left_td_colored\">counter</td>\n   <td class=\"right_td_colored\">Used for counters, which are cluster-wide incrementing values.</td>\n   <td class=\"right_td_colored\">8 bytes.</td>\n  </tr><tr><td class=\"left_td_colored\">timestamp</td>\n   <td class=\"right_td_colored\">Stores time in milliseconds.</td>\n   <td class=\"right_td_colored\">8 bytes.</td>\n  </tr><tr><td class=\"left_td_colored\">decimal</td>\n   <td class=\"right_td_colored\">Stores BigDecimals.</td>\n   <td class=\"right_td_colored\">4 bytes to store the scale, plus an arbitrary number of bytes to store the value.</td>\n  </tr><tr><td class=\"left_td_colored\">double</td>\n   <td class=\"right_td_colored\">Stores Doubles.</td>\n   <td class=\"right_td_colored\">8 bytes.</td>\n  </tr><tr><td class=\"left_td_colored\">float</td>\n   <td class=\"right_td_colored\">Stores Floats.</td>\n   <td class=\"right_td_colored\">4 bytes.</td>\n  </tr><tr><td class=\"left_td_colored\">int</td>\n   <td class=\"right_td_colored\">Stores 4-byte integer.</td>\n   <td class=\"right_td_colored\">4 bytes.</td>\n  </tr><tr><td class=\"left_td_colored\">varint</td>\n   <td class=\"right_td_colored\">Stores variable precision integer.</td>\n   <td class=\"right_td_colored\">An arbitraty number of bytes used to store the value.</td>\n  </tr><tr><td class=\"left_td_colored\">bigint</td>\n   <td class=\"right_td_colored\">Stores Longs.</td>\n   <td class=\"right_td_colored\">8 bytes.</td>\n  </tr><tr><td class=\"left_td_colored\">text, varchar</td>\n   <td class=\"right_td_colored\">Stores text as UTF8.</td>\n   <td class=\"right_td_colored\">UTF8.</td>\n  </tr><tr><td class=\"left_td_colored\">uuid</td>\n   <td class=\"right_td_colored\">Suitable for UUID storage.</td>\n   <td class=\"right_td_colored\">16 bytes.</td>\n  </tr></tbody></table></div><div class=\"col-md-12 content-html\"><p>Cassandra 1.1 supports tables defined with composite primary keys. The first column in a composite key definition is used as the partition key. Remaining columns are automatically clustered. Rows that share a partition key are sorted by the remaining components of the primary key.</p></div><div class=\"col-md-12 content-html\"><p>A column is a triplet: key, value, and timestamp. The validation and comparator on the column family define how Cassandra sorts and stores the bytes in column keys.<br />The timestamp portion of the column is used to sequence mutations. The timestamp is defined and specified by the client and can be anything the client wishes to use.  By convention, the timestamp is typically microseconds since epoch.  If time-based, clients must be careful to synchronize clocks.</p><p>Columns may optionally have a time-to-live (TTL), after which Cassandra asynchronously deletes them. </p><div class=\"../images/hot_tip.gif\">\n <p><img alt=\"Hot Tip\" class=\"../images/hot_tip.gif_icon fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14343-thumb.png\" /></p> Originally SuperColumns were one of Cassandra’s data model primitives.  Although they are still supported in the API, we recommend you use CompositeTypes instead. \n</div></div><div class=\"col-md-12 content-html\"><p>Cassandra uses a ring architecture. The ring represents a cyclic range of token values (i.e., the token space).  Each node is assigned a position on the ring based on its token.   A node is responsible for all tokens between its initial token and the initial token of the closest previous node along the ring.</p></div><div class=\"col-md-12 content-html\"><p>Keys are mapped into the token space by a partitioner. The important distinction between the partitioners is order preservation (OP). Users can define their own partitioners by implementing IPartitioner, or they can use one of the native partitioners:</p><table border=\"1\" style=\"width: 467px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 196px;\"> </td>\n   <td class=\"right_th_colored\" style=\"width: 73px;\">Map Function</td>\n   <td class=\"right_th_colored\" style=\"width: 80px;\">Token Space</td>\n   <td class=\"right_th_colored\" style=\"width: 90px;\">OP</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">RandomPartitioner</td>\n   <td class=\"right_td_colored\">MD5</td>\n   <td class=\"right_td_colored\">BigInteger</td>\n   <td class=\"right_td_colored\">No</td>\n  </tr><tr><td class=\"left_td_colored\">BytesOrderPartitioner</td>\n   <td class=\"right_td_colored\">Identity</td>\n   <td class=\"right_td_colored\">Bytes</td>\n   <td class=\"right_td_colored\">Yes</td>\n  </tr></tbody></table><p>The following examples illustrate this point.</p><h3>Random Partitioner</h3><p>Since the Random Partitioner uses an MD5 hash function to map keys into tokens, on average those keys will evenly distribute across the cluster. For this reason, RandomPartitioner is the default partitioner.</p><p>The row key determines the node placement:</p><table border=\"1\" style=\"width: 537px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 84px;\">Row Key</td>\n   <td class=\"right_th_colored\" style=\"width: 116px;\"> </td>\n   <td class=\"right_th_colored\" style=\"width: 172px;\"> </td>\n   <td class=\"right_th_colored\" style=\"width: 137px;\"> </td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">Lisa</td>\n   <td class=\"right_td_colored\">state: CA</td>\n   <td class=\"right_td_colored\">graduated: 2008</td>\n   <td class=\"right_td_colored\">gender: F</td>\n  </tr><tr><td class=\"left_td_colored\">Owen</td>\n   <td class=\"right_td_colored\">state: TX</td>\n   <td class=\"right_td_colored\">gender: M</td>\n   <td class=\"right_td_colored\"> </td>\n  </tr><tr><td class=\"left_td_colored\">Collin</td>\n   <td class=\"right_td_colored\">state: UT</td>\n   <td class=\"right_td_colored\">gender: M</td>\n   <td class=\"right_td_colored\"> </td>\n  </tr></tbody></table><p>This may result in following ring formation, where \"collin\", \"owen\", and \"lisa\" are rowkeys.</p><p><img alt=\"“Random\" class=\"fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14344-thumb.png\" /></p><p>With Cassandra’s storage model, where each node owns the preceding token space, this results in the following storage allocation based on the tokens.</p><table border=\"1\" style=\"width: 258px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 41px;\">Row Key</td>\n   <td class=\"right_th_colored\" style=\"width: 142px;\">MD5 Hash</td>\n   <td class=\"right_th_colored\" style=\"width: 53px;\">Node</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">collin</td>\n   <td class=\"right_td_colored\">CC982736AD62AB</td>\n   <td class=\"right_td_colored\">3</td>\n  </tr><tr><td class=\"left_td_colored\">owen</td>\n   <td class=\"right_td_colored\">9567238FF72635</td>\n   <td class=\"right_td_colored\">2</td>\n  </tr><tr><td>lisa</td>\n   <td>001AB62DE123FF</td>\n   <td>1</td>\n  </tr></tbody></table><p>Notice that the keys are not in order. With RandomPartitioner, the keys are evenly distributed across the ring using hashes, but you sacrifice order, which means any range query needs to query all nodes in the ring.</p><h3>Order Preserving Partitioners (OPP)</h3><p>The Order Preserving Partitioners preserve the order of the row keys as they are mapped into the token space.  </p><p>In our example, since:</p><p><em> \"collin\" &lt; \"lisa\" &lt; \"owen\"</em></p><p>then,</p><p><em>token(\"collin\") &lt; token(\"lisa\") &lt; token(\"owen\")</em></p><p>With OPP, range queries are simplified and a query may not need to consult each node in the ring.  This seems like an advantage, but it comes at a price.  Since the partitioner is preserving order, the ring may become unbalance unless the rowkeys are naturally distributed across the token space.  </p><p>This is illustrated below. </p><p><img alt=\"“OPP”\" class=\"fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14345-thumb.png\" /></p><p>To manually balance the cluster, you can set the initial token for each node in the Cassandra configuration.</p><div class=\"../images/hot_tip.gif\">\n <p><img alt=\"Hot Tip\" class=\"../images/hot_tip.gif_icon fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14346-thumb.png\" /></p> If possible, it is best to design your data model to use RandomPartitioner to take advantage of the automatic load balancing and decreased administrative overhead of manually managing token assignment. \n</div></div><div class=\"col-md-12 content-html\"><p>Cassandra provides high availability and fault tolerance through data replication.  The replication uses the ring to determine nodes used for replication.   Each keyspace has an independent replication factor, <em>n</em>.  When writing information, the data is written to the target node as determined by the partitioner and <em>n-1 </em>subsequent nodes along the ring.</p><p>There are two replication strategies: SimpleStrategy and NetworkTopologyStrategy.</p><h3>SimpleStrategy</h3><p>The SimpleStrategy is the default strategy and blindly writes the data to subsequent nodes along the ring.  In the previous example with a replication factor of <em>2</em>, this would result in the following storage allocation.</p><table border=\"1\" style=\"width: 469px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 59px;\">Row Key</td>\n   <td class=\"right_th_colored\" style=\"width: 179px;\">Replica 1 <br />(as determined by partitioner)</td>\n   <td class=\"right_th_colored\" style=\"width: 209px;\">Replica 2<br />(found by traversing the ring)</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">collin</td>\n   <td class=\"right_td_colored\">3</td>\n   <td class=\"right_td_colored\">1</td>\n  </tr><tr><td class=\"left_td_colored\">owen</td>\n   <td class=\"right_td_colored\">2</td>\n   <td class=\"right_td_colored\">3</td>\n  </tr><tr><td class=\"left_td_colored\">lisa</td>\n   <td class=\"right_td_colored\">1</td>\n   <td class=\"right_td_colored\">2</td>\n  </tr></tbody></table><h3>NetworkTopologyStrategy</h3><p>The NetworkTopologyStrategy is useful when deploying to multiple data centers. It ensures data is replicated across data centers.</p><p>Effectively, the NetworkTopologyStrategy executes the SimpleStrategy independently for each data center, spreading replicas across distant racks. Cassandra writes a copy in each data center as determined by the partitioner. Data is written simultaneously along the ring to subsequent nodes within that data center with preference for nodes in different racks to offer resilience to hardware failure. All nodes are peers and data files can be loaded through any node in the cluster, eliminating the single point of failure inherent in master-slave architecture and making Cassandra fully fault-tolerant and highly available.</p><p>Given the following ring and deployment topology:</p><p><img alt=\"“Topology&amp;quot;\" class=\"fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14347-thumb.png\" /></p><p>With blue nodes (N1-N3) deployed to one data center (DC1), red nodes (N4-N6) deployed to another data center (DC2), and a replication factor of 4, Cassandra would write a row with key “lisa” as follows.</p><p>NOTE: Cassandra attempts to write data simultaneously to all target nodes then waits for confirmation from the relevant number of nodes needed to satisfy the specified consistency level. </p><h3>Consistency Levels</h3><p>One of the unique characteristics of Cassandra that sets it apart from other databases is its approach to consistency.  Clients can specify the consistency level on both read and write operations trading off between high availability, consistency, and performance.</p><h3>Write</h3><table border=\"1\" style=\"width: 545px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 130px;\"><strong>Level</strong></td>\n   <th class=\"right_th_colored\" style=\"width: 399px;\"><strong>Expectation</strong></th>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">ANY</td>\n   <td class=\"right_td_colored\">The write was logged, but the data may not be available for reads immediately. This is useful where you need high availability for writes but only eventual consistency on reads.</td>\n  </tr><tr><td class=\"left_td_colored\">ONE</td>\n   <td class=\"right_td_colored\">Data is committed to at least one replica and is available for reads.</td>\n  </tr><tr><td class=\"left_td_colored\">TWO</td>\n   <td class=\"right_td_colored\">Data is committed to at least two replicas and is available for reads.</td>\n  </tr><tr><td class=\"left_td_colored\">THREE</td>\n   <td class=\"right_td_colored\">Data is committed to at least three replicas and is available for reads.</td>\n  </tr><tr><td class=\"left_td_colored\">QUORUM</td>\n   <td class=\"right_td_colored\">Data is committed to at least n/2+1 replicas and is available for reads, where n is the replication factor.</td>\n  </tr><tr><td class=\"left_td_colored\">LOCAL_QUORUM</td>\n   <td class=\"right_td_colored\">Data is committed to at least n/2+1 replicas within the local data center.</td>\n  </tr><tr><td class=\"left_td_colored\">EACH_QUORUM</td>\n   <td class=\"right_td_colored\">Data is committed to at least n/2+1 replicas within each data center.</td>\n  </tr><tr><td class=\"left_td_colored\">ALL</td>\n   <td class=\"right_td_colored\">Data is committed to and available from all n replicas. This is useful when absolute read consistency and/or fault tolerance are necessary (e.g., online disaster recovery).</td>\n  </tr></tbody></table><h3>Read</h3><table border=\"1\" style=\"width: 548px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 130px;\"><strong>Level</strong></td>\n   <td class=\"right_th_colored\" style=\"width: 380px;\"><strong>Expectation</strong></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">ONE</td>\n   <td class=\"right_td_colored\">The client receives data from the first replica to respond.</td>\n  </tr><tr><td class=\"left_td_colored\">TWO</td>\n   <td class=\"right_td_colored\">The client receives the most current data between two replicas based on the timestamps.</td>\n  </tr><tr><td class=\"left_td_colored\">THREE</td>\n   <td class=\"right_td_colored\">The client receives the most current data between three replicas based on the timestamps.</td>\n  </tr><tr><td class=\"left_td_colored\">QUORUM</td>\n   <td class=\"right_td_colored\">The client receives the most current data once n/2+1 replicas have responded.</td>\n  </tr><tr><td class=\"left_td_colored\">LOCAL_QUORUM</td>\n   <td class=\"right_td_colored\">The client receives the most current data once n/2+1 replicas have responded within the local data center.</td>\n  </tr><tr><td class=\"left_td_colored\">EACH_QUORUM</td>\n   <td class=\"right_td_colored\">The client receives the most current data once n/2+1 replicas have responded within each data center.</td>\n  </tr><tr><td class=\"left_td_colored\">ALL</td>\n   <td class=\"right_td_colored\">The client receives the most current data once all replicas have responded</td>\n  </tr></tbody></table></div><div class=\"col-md-12 content-html\"><p>As input into the replication strategy and to efficiently route communication, Cassandra uses a <em>snitch</em> to determine the data center and rack of the nodes in the cluster.  A snitch is a component that detects and informs Cassandra about the network topology of the deployment.  </p><p>The snitch dictates what is used in the strategy options to identify replication groups when configuring replication for a keyspace.</p><table border=\"1\" style=\"width: 395px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 41px;\">Nodes</td>\n   <td class=\"right_th_colored\" style=\"width: 32px;\">Rack</td>\n   <td class=\"right_th_colored\" style=\"width: 22px;\">DC</td>\n   <td class=\"right_th_colored\" style=\"width: 272px;\">Reason</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">N4</td>\n   <td class=\"right_td_colored\">3</td>\n   <td class=\"right_td_colored\">2</td>\n   <td class=\"right_td_colored\">As determined by partitioner in DC1.</td>\n  </tr><tr><td class=\"left_td_colored\">N2</td>\n   <td class=\"right_td_colored\">1</td>\n   <td class=\"right_td_colored\">1</td>\n   <td class=\"right_td_colored\">As determined by partitioner in DC2.</td>\n  </tr><tr><td class=\"left_td_colored\">N6</td>\n   <td class=\"right_td_colored\">4</td>\n   <td class=\"right_td_colored\">2</td>\n   <td class=\"right_td_colored\">Preference shown for Rack 4 (over Rack 3).</td>\n  </tr><tr><td class=\"left_td_colored\">N3</td>\n   <td class=\"right_td_colored\">1</td>\n   <td class=\"right_td_colored\">1</td>\n   <td>Written to same rack hosting N2 since no other rack was available.</td>\n  </tr></tbody></table><p>. </p><p>The following table shows the four snitches provided by Cassandra and what you should use in your keyspace configuration for each snitch.</p><table border=\"1\" style=\"width: 443px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 119px;\">Snitch</td>\n   <td class=\"right_th_colored\" style=\"width: 324px;\">Specify</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">SimpleSnitch</td>\n   <td class=\"right_td_colored\">Specify only the replication factor in your strategy options.</td>\n  </tr><tr><td class=\"left_td_colored\">PropertyFileSnitch</td>\n   <td class=\"right_td_colored\">Specify the data center names from your properties file in the keyspace strategy options.</td>\n  </tr><tr><td class=\"left_td_colored\">RackInferringSnitch</td>\n   <td class=\"right_td_colored\">Specify the second octet of the IPv4 address in your keyspace strategy options.</td>\n  </tr><tr><td class=\"left_td_colored\">EC2Snitch</td>\n   <td class=\"right_td_colored\">Specify the region name in the keyspace strategy options.</td>\n  </tr></tbody></table><h3>SimpleSnitch</h3><p>The SimpleSnitch provides Cassandra no information regarding racks or data centers.  It is the default setting and is useful for simple deployments where all servers are collocated.</p><h3>PropertyFileSnitch</h3><p>The PropertyFileSnitch allows users to be explicit about their network topology.  The user specifies the topology in a properties file, <em>cassandra-topology.properties. </em> The file specifies which nodes belong to which racks and data centers.  Below is an example property file for our sample cluster.</p><p><em># DC1<br />192.168.0.1=DC1:RAC1<br />192.168.0.2=DC1:RAC1<br />192.168.0.3=DC1:RAC2</em></p><p># DC2<br />192.168.1.4=DC2:RAC3<br />192.168.1.5=DC2:RAC3<br />192.168.1.6=DC2:RAC4</p><p># Default for nodes<br />default=DC3:RAC5 </p><h3>RackInferringSnitch</h3><p>The RackInferringSnitch infers network topology by convention.  From the IPv4 address (e.g., 9.100.47.75), the snitch uses the following convention to identify the data center and rack:</p><table border=\"1\" style=\"width: 221px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 46px;\">Octet</td>\n   <td class=\"right_th_colored\" style=\"width: 62px;\">Example</td>\n   <td class=\"right_th_colored\" style=\"width: 91px;\">Indicates</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">1</td>\n   <td class=\"right_td_colored\">9</td>\n   <td class=\"right_td_colored\">Nothing</td>\n  </tr><tr><td class=\"left_td_colored\">2</td>\n   <td class=\"right_td_colored\">100</td>\n   <td class=\"right_td_colored\">Data Center</td>\n  </tr><tr><td class=\"left_td_colored\">3</td>\n   <td class=\"right_td_colored\">47</td>\n   <td class=\"right_td_colored\">Rack</td>\n  </tr><tr><td class=\"left_td_colored\">4</td>\n   <td class=\"right_td_colored\">75</td>\n   <td class=\"right_td_colored\">Node</td>\n  </tr></tbody></table><h3>EC2Snitch</h3><p>The EC2Snitch is useful for deployments to Amazon's EC2. It uses Amazon's API to examine the regions to which nodes are deployed. It then treats each region as a separate data center.</p><p>EC2MultiRegionSnitch</p><p>Use this snitch for deployments on Amazon EC2 where the cluster spans multiple regions. This snitch treats data centers and availability zones as racks within a data center and uses public IPs as broadcast_address to allow cross-region connectivity. Cassandra nodes in one EC2 region can bind to nodes in another region, thus enabling multi-data center support.</p></div><div class=\"col-md-12 content-html\"><p>Cassandra provides simple primitives. Its simplicity allows it to scale linearly with high availability and very little performance degradation.   That simplicity allows for extremely fast read and write operations for specific keys, but servicing more sophisticated queries that span keys requires pre-planning.</p><p>Using the primitives that Cassandra provides, you can construct indexes that support exactly the query patterns of your application.  Note, however, that queries may not perform well without properly designing your schema. </p><h3>Secondary Indexes</h3><p>To satisfy simple query patterns, Cassandra provides a native indexing capability called Secondary Indexes. A column family may have multiple secondary indexes. A secondary index is hash-based and uses specific columns to provide a reverse lookup mechanism from a specific column value to the relevant row keys. Under the hood, Cassandra maintains hidden column families that store the index. The strength of Secondary Indexes is allowing queries by value. Secondary indexes are built in the background automatically without blocking reads or writes. To create a Secondary Index using CQL is straight-forward. For example, define a table of data about movie fans, and then create a secondary index of states where they live:</p><p>CREATE TABLE fans ( watcherID uuid, favorite_actor text, address text, zip int, state text PRIMARY KEY (watcherID) );</p><p>CREATE INDEX watcher_state ON fans (state);</p><h3>Range Queries</h3><p>It is important to consider partitioning when designing your schema to support range queries.</p><h4>Range Queries with Order Preservation</h4><p>Since order is preserved, order preserving partitioners better supports range queries across a range of rows.  Cassandra only needs to retrieve data from the subset of nodes responsible for that range.  For example, if we are querying against a column family keyed by phone number and we want to find all phone numbers between that begin with <em>215-555</em>, we could create a range query with start key <em>215-555-0000 </em>and end key <em>215-555-9999.</em></p><p>To service this request with OrderPreservingPartitioning, it’s possible for Cassandra to compute the two relevant tokens: <em>token(215-555-0000) </em>and<em> token(215-555-9999)</em>.</p><p>Then satisfying that querying simply means consulting nodes responsible for that token range and retrieving the rows/tokens in that range.</p><h4>Range Queries with Random Partitioning</h4><p>The RandomPartitioner provides no guarantees of any kind between keys and tokens.  In fact, ideally row keys are distributed around the token ring evenly.  Thus, the corresponding tokens for a start key and end key are not useful when trying to retrieve the relevant rows from tokens in the ring with the RandomPartitioner.  Consequently, Cassandra must consult all nodes to retrieve the result.  Fortunately, there are well known design patterns to accommodate range queries.  These are described below.</p><h3>Index Patterns</h3><p>There are a few design patterns to implement indexes.  Each services different query patterns.  The patterns leverage the fact that Cassandra columns are always stored in sorted order and all columns for a single row reside on a single host.</p><h3>Inverted Indexes</h3><p>First, let’s consider the <em>inverted index</em> pattern.  In an inverted index, columns in one row become row keys in another.  Consider the following data set, where users IDs are row keys.</p><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 458px;\"><thead><tr><td class=\"left_th_colored\" colspan=\"4\" valign=\"top\"><p><strong>Column Family: Users</strong></p></td>\n  </tr></thead><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 76px;\"><p><strong>RowKey</strong></p></td>\n   <th class=\"right_th_colored\" colspan=\"3\" valign=\"top\"><p><strong>Columns</strong></p></th>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 76px;\"><p>BONE42</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 128px;\"><p>{ name : “Brian”}</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 105px;\"><p>{ zip: 15283}</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 139px;\"><p>{dob : 09/19/1982}</p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 76px;\"><p>LKEL76</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 128px;\"><p>{ name : “Lisa”}</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 105px;\"><p>{ zip: 98612}</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 139px;\"><p>{dob : 07/23/1993}</p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 76px;\"><p>COW89</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 128px;\"><p>{ name : “Dennis”}</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 105px;\"><p>{ zip: 98612}</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 139px;\"><p>{dob : 12/25/2004}</p></td>\n  </tr></tbody></table><p>Without indexes, searching for users in a specific Zip Code would mean scanning our Users column family row-by-row to find the users in the relevant Zip Code.  Obviously, this does not perform well. <br />To remedy the situation, we can create a column family that represents the query we want to perform, inverting rows and columns.  This would result in the following column family.</p><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 458px;\"><thead><tr><td class=\"left_th_colored\" colspan=\"4\" valign=\"top\"><p><strong>Column Family: Users_by_ZipCode</strong></p></td>\n  </tr></thead><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 77px;\"><p><strong>RowKey</strong></p></td>\n   <td class=\"right_th_colored\" colspan=\"3\" valign=\"top\"><p><strong>Columns</strong></p></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 77px;\"><p>98612</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 137px;\"><p>{ user_id : LKEL76 }</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 139px;\"><p>{ user_id : COW89 }</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 95px;\"></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 77px;\"><p>15283</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 137px;\"><p>{ user_id : BONE42 }</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 139px;\"></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 95px;\"></td>\n  </tr></tbody></table><p>Since each row is stored on a single machine, Cassandra can quickly return all user IDs within a single Zip Code by returning all columns within a single row.  Cassandra simply goes to a single host based on <em>token(zipcode)</em> and returns the contents of that single row.</p><h3>Wide-Row Indexes</h3><p>When working with time series data, consider storing the complete set of data for each event in the timeline itself by serializing the entire event into a single column value or by using composite column names of the form &lt; timestamp &gt; : &lt; event_field &gt;. Unless the data for each event is very large, this approach scales well with large data sets and provides efficient reads. Fetch a time slice of events by reading a contiguous portion of a row on one set of replicas. When you track the same event in multiple timelines, denormalizing and storing all of the event data in each of the timelines works well.</p><table border=\"1\" style=\"width: 455px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 445px;\"><strong>Materialized View Table</strong></td>\n  </tr></thead></table><table border=\"1\" style=\"width: 456px;\"><tbody><tr><td class=\"left_td_colored\" style=\"width: 97px;\">lsmith: 1332960000</td>\n   <td class=\"right_td_colored\" style=\"width: 220px;\">C4e1ee6f-e053-41f5-9890-<br />674636d51095:<br />{\"user\": \"lsmith\", \"body\": \"There<br />are . . . \"}</td>\n   <td class=\"left_td_colored\" style=\"width: 117px;\">39f71a85-7af0 . . .<br />{\"user\": \"lsmith\",<br />\"body\": \"Yes, . . .</td>\n  </tr><tr><td class=\"left_td_colored\">cbrown:<br />1332960000</td>\n   <td class=\"right_td_colored\">e572bad1-f98d-4346-80a0-<br />13e7d37d38d0:<br />{\"user\":\"cbrown\", \"body\": \"My dog<br />is . . .\"}</td>\n   <td class=\"right_td_colored\">aa33bgbfd-8f16 . . .<br />{\"user\":\"cbrown\",<br />\"body\":\"No, . . .</td>\n  </tr></tbody></table><p>When you use composite keys in CQL, Cassandra supports wide Cassandra rows using composite column names. In CQL 3, a primary key can have any number (1 or more) of component columns, but there must be at least one column in the column family that is not part of the primary key. The new wide row technique consumes more storage because for every piece of data stored, the column name is stored along with it.</p><p><em>CREATE TABLE History.events (<br />event uuid PRIMARY KEY,<br />author varchar,<br />body varchar);</em></p><p>CREATE TABLE timeline (<br />user varchar,<br />event uuid,<br />author varchar,<br />body varchar,</p><div class=\"../images/hot_tip.gif\">\n <p><img alt=\"Hot Tip\" class=\"../images/hot_tip.gif_icon fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14348-thumb.png\" /></p> Wide-Row indexes can cause hotspots in the cluster.  Since the index is a single row, it is stored on a single node (plus replicas).  If that is a heavily used index, those nodes may be overwhelmed. \n</div><h3>Composite-Types in Indexes</h3><p>The previous examples were one-dimensional and used a simple concatenation to illustrate the point.  Instead, you may prefer to use composite keys and/or values in your data model.  </p><p>Using composite keys in indexes, we can create queries along multiple dimensions.  If we combine the previous examples, we could create a <br />single wide-row capable of serving a compound query such as, “How many users within the 18964 Zip Code are older than 21?”</p><p>Simply create a composite type containing the Zip Code and the date of birth and use that as the column name in the index.</p><h3>Denormalization</h3><p>Finally, it is worth noting that each of the indexing strategies as presented would require two steps to service a query if the request requires the actual column data (e.g., user name). The first step would retrieve the keys out of the index. The second step would fetch each relevant column by row key.</p><p>We can skip the second step if we denormalize the data. In Cassandra, denormalization is the norm. If we duplicate the data, the index becomes a true materialized view that is custom tailored to the exact query we need to support.</p></div><div class=\"col-md-12 content-html\"><p>Everything in Cassandra is a write, typically referred to as a mutation. Since Cassandra is effectively a key-value store, operations are simply mutations of a key/value pairs. The column is atomic, but the fundamental unit is a row in the ACID sense. If you have multiple updates to the same key, group the changes into a single update.</p><div class=\"../images/hot_tip.gif\">\n <p><img alt=\"Hot Tip\" class=\"../images/hot_tip.gif_icon fr-dii fr-fil\" src=\"https://dzone.com/storage/rc-covers/14349-thumb.png\" /></p> When performing multiple operations on the same key in sequence, be sure to increment the timestamp! Do not simply specify System.currentTimeMillis().  If the code executes too quickly, the mutations will have the same timestamp and Cassandra will not be able to determine the proper sequencing of events. \n</div><h3>Hinted Handoff</h3><p>Similar to ReadRepair, Hinted Handoff is a background process that ensures data integrity and eventual consistency.  If a replica is down in the cluster and the client requests a consistency level of ANY, a write may still succeed by writing a “hint” to a coordinator node, which will disseminate that data to replicas when they become available.</p></div><div class=\"col-md-12 content-html\"><p>Cassandra provides tools for operations and maintenance.  Some of the maintenance is mandatory because of Cassandra’s eventually consistent architecture.  Other facilities are useful to support alerting and statistics gathering.  Use <em>nodetool</em> to manage Cassandra.  Datastax provides a reference card on nodetool available here:</p><p><a href=\"http://www.datastax.com/wp-content/uploads/2012/01/DS_nodetool_web.pdf\">http://www.datastax.com/wp-content/uploads/2012/01/DS_nodetool_web.pdf</a></p><h3>Nodetool Repair</h3><p>Cassandra keeps record of deleted values for some time to support the eventual consistency of distributed deletes.  These values are called tombstones.  Tombstones are purged after some time (GCGraceSeconds, which defaults to 10 days).  Since tombstones prevent improper data propagation in the cluster, you will want to ensure that you have consistency before they get purged.</p><p>To ensure consistency, run:</p><p><em>&gt;$CASSANDRA_HOME/bin/nodetool repair</em></p><p>The repair command replicates any updates missed due to downtime or loss of connectivity. This command ensures consistency across the cluster and obviates the tombstones. You will want to do this periodically on each node in the cluster (within the window before tombstone purge).</p><h3>Monitoring</h3><p>Cassandra has support for monitoring via JMX, but the simplest way to monitor the Cassandra node is by using OpsCenter, which is designed to manage and monitor Cassandra database clusters. There is a free community edition as well as an enterprise edition that provides management of Apache SOLR and Hadoop.</p><p>Simply download mx4j and execute the following:</p><p><em>cp $MX4J_HOME/lib/mx4j-tools.jar $CASSANDRA_HOME/lib</em></p><p>The following are key attributes to track per column family.</p><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 422px;\"><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 97px;\"><p><strong>Attribute</strong></p></td>\n   <td class=\"right_th_colored\" valign=\"top\" style=\"width: 319px;\"><p><strong>Provides</strong></p></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 97px;\"><p>Read Count</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 319px;\"><p>Frequency of reads against the column family.</p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 97px;\"><p>Read Latency</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 319px;\"><p>Latency of reads against the column family.</p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 97px;\"><p>Write Count</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 319px;\"><p>Frequency of writes against the column family.</p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 97px;\"><p>Write Latency</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 319px;\"><p>Latency of writes against the column family.</p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 97px;\"><p>Pending Tasks</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 319px;\"><p>Queue of pending tasks, informative to know if tasks are queuing.</p></td>\n  </tr></tbody></table><h3>Backup</h3><p>OpsCenter facilitates backing up data by providing snapshots of the data. A snapshot creates a new hardlink to every live SSTable. Cassandra also provides online backup facilities using nodetool. To take a snapshot of the data on the cluster, invoke:</p><p><em>$CASSANDRA_HOME/bin/nodetool snapshot</em></p><p>This will create a snapshot directory in each keyspace data directory.  Restoring the snapshot is then a matter of shutting down the node, deleting the commitlogs and the data files in the keyspace, and copying the snapshot files back into the keyspace directory.</p></div><div class=\"col-md-12 content-html\"><p>Cassandra has a very active community developing libraries in different languages.  </p><h3>Java</h3><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px;\"><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 76px;\"><p><strong>Client</strong></p></td>\n   <td class=\"right_th_colored\" valign=\"top\" style=\"width: 520px;\"><p><strong>Description</strong></p></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 76px;\"><p>Astyanax</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 520px;\"><p>Inspired by Hector, Astyanax is a client library developed by the folks at Netflix.   <br /><a href=\"https://github.com/Netflix/astyanax\">https://github.com/Netflix/astyanax</a></p></td>\n  </tr><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 76px;\"><p>Hector</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 520px;\"><p>Hector is one of the first APIs to wrap the underlying Thrift API. Hector is one of the most commonly used client libraries.<br /><a href=\"https://github.com/rantav/hector\">https://github.com/rantav/hector</a></p></td>\n  </tr></tbody></table><h3>CQL</h3><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 629px;\"><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 82px;\"><p><strong>Client</strong></p></td>\n   <th class=\"right_th_colored\" valign=\"top\" style=\"width: 541px;\"><p><strong>Description</strong></p></th>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 82px;\"><p>CQL</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 541px;\"><p>Cassandra provides an SQL-like query language called the Cassandra Query Language (CQL).  The CQL shell allows you to interact with Cassandra as if it were a SQL database.  Start the shell with:<br />&gt;$CASSANDRA_HOME/bin/cqlsh<br />Datastax provides a reference card for CQL available here:<br /><a href=\"http://www.datastax.com/wp-content/uploads/2012/01/DS_CQL_web.pdf\">http://www.datastax.com/wp-content/uploads/2012/01/DS_CQL_web.pdf</a></p></td>\n  </tr></tbody></table><h3>Python</h3><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 472px;\"><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 49px;\"><p><strong>Client</strong></p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 417px;\"><p><strong>Description</strong></p></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 49px;\"><p>Pycassa</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 417px;\"><p>Pycassa is the most well known Python library for Cassandra.<br /><a href=\"https://github.com/pycassa/pycassa\">https://github.com/pycassa/pycassa</a></p></td>\n  </tr></tbody></table><h3>PHP CQL</h3><table border=\"1\" style=\"width: 473px;\"><thead><tr><td class=\"left_th_colored\" style=\"width: 70px;\">Client</td>\n   <td class=\"right_th_colored\" style=\"width: 387px;\">Description</td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\">Cassandra-<br />PDO</td>\n   <td class=\"right_td_colored\">A CQL (Cassandra Query Language) driver for PHP.<br /><a href=\"http://code.google.com/a/apache-extras.org/p/cassandra-pdo/\">http://code.google.com/a/apache-extras.org/p/cassandra-pdo/</a></td>\n  </tr></tbody></table><h3>Ruby</h3><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 473px;\"><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 56px;\"><p><strong>Client</strong></p></td>\n   <td class=\"right_th_colored\" valign=\"top\" style=\"width: 411px;\"><p><strong>Description</strong></p></td>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 56px;\"><p>Ruby Gem</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 411px;\"><p>Ruby has support for Cassandra via a gem.<br /><a href=\"http://rubygems.org/gems/cassandra\">http://rubygems.org/gems/cassandra</a></p></td>\n  </tr></tbody></table><h3>REST</h3><table border=\"1\" cellpadding=\"0\" style=\"border-spacing: 0px; width: 468px;\"><thead><tr><td class=\"left_th_colored\" valign=\"top\" style=\"width: 49px;\"><p><strong>Client</strong></p></td>\n   <th class=\"right_th_colored\" valign=\"top\" style=\"width: 413px;\"><p><strong>Description</strong></p></th>\n  </tr></thead><tbody><tr><td class=\"left_td_colored\" valign=\"top\" style=\"width: 49px;\"><p>Virgil</p></td>\n   <td class=\"right_td_colored\" valign=\"top\" style=\"width: 413px;\"><p>Virgil is a java-based REST client for Cassandra.<br /><a href=\"https://github.com/hmsonline/virgil\">https://github.com/hmsonline/virgil</a></p></td>\n  </tr></tbody></table><h3>Command Line Interface (CLI)</h3><p>Cassandra also provides a Command Line Interface (CLI) through which you can perform all schema related changes. It also allows you to manipulate data. Datastax provides a reference card on the CLI available here:</p><p><a href=\"http://www.datastax.com/wp-content/uploads/2012/01/DS_CLI_web.pdf\">http://www.datastax.com/wp-content/uploads/2012/01/DS_CLI_web.pdf</a></p><h3>Hadoop Support</h3><p>DataStax Enterprise provides Cassandra with an enhanced Hadoop distribution that is compatible with existing HDFS, Hadoop, and Hive tools and utilities. Cassandra also provides out-of-the-box support for Hadoop. To see the canonical word count example, take a look at:</p><p><a href=\"https://github.com/apache/cassandra/tree/trunk/examples/hadoop_word_count\">https://github.com/apache/cassandra/tree/trunk/examples/hadoop_word_count</a></p><h3>DataStax Community Edition</h3><p>DataStax Community Edition provides the latest release from the Apache Cassandra community.</p><ul><li>Binary tarballs for Linux and Mac installation</li>\n <li>Packaged installations for Red Hat Enterprise Linux, CentOS, Debian, and Ubuntu</li>\n <li>A GUI installer for Windows</li>\n</ul><p>RHEL and Debian packages are supported through yum and apt package management tools.he DataStax Community Edition also includes the DataStax OpsCenter.</p></div>",
        "created_at": "2018-07-24T11:19:17+0000",
        "updated_at": "2018-07-24T11:19:22+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 21,
        "domain_name": "dzone.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11064"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 11060,
        "uid": null,
        "title": "Distributed Storage Systems - Cassandra",
        "url": "http://www.eurecom.fr/~michiard/teaching/slides/clouds/cassandra.pdf",
        "content": "ć<br />\n<br />\n \t<br />\n <br />\n <br />\n\t<br />\n \t<br />\n<br />\n<br />\n \t<br />\n <br />\n \t<br />\n  \t<br />\n<br />\n \t<br />\n \t<br />\n \t<br />\n \t<br />\n \t<br />\n<br />\n<br />\n<br />\n\t<br />\n    <br />\n \t<br />\n   \t<br />\n <br />\n  \t<br />\n \t<br />\n <br />\n    \t<br />\n<br />\n<br />\n \t<br />\n  \t<br />\n  \t<br />\n <br />\n<br />\n  \t<br />\n  <br />\n \t<br />\n<br />\n<br />\n <br />\n  \t<br />\n\t<br />\n<br />\n<br />\n<br />\n \t<br />\n <br />\n  <br />\n\t<br />\n \t<br />\n <br />\n<br />\n<br />\n\t\t\t\t<br />\n <br />\n   <br />\n  <br />\n<br />\n<br />\n<br />\n <br />\n  \t<br />\n <br />\n  <br />\n\t<br />\n   \t<br />\n<br />\n<br />\n<br />\n \t<br />\n <br />\n \t<br />\n<br />\n<br />\n<br />\n<br />\n <br />\n <br />\n \t<br />\n<br />\n<br />\n \t<br />\n  \t<br />\n<br />\n<br />\n<br />\n <br />\n  \t<br />\n<br />\n \t<br />\n\t<br />\n  \t<br />\n  <br />\n  <br />\n \t<br />\n<br />\n<br />\n  \t<br />\n <br />\n\t<br />\n  \t<br />\n <br />\n  <br />\n\t<br />\n   <br />\n \t<br />\n<br />\n<br />\n \t<br />\n\t  \t<br />\n <br />\n <br />\n  <br />\n <br />\n <br />\n  \t<br />\n<br />\n<br />\n <br />\n\t<br />\n\t\t\t<br />\n   <br />\n  <br />\n<br />\n<br />\n \t<br />\n <br />\n \t<br />\n\t<br />\n <br />\n  \t<br />\n \t<br />\n <br />\n<br />\n<br />\n <br />\n<br />\n <br />\n<br />\n   <br />\n <br />\n <br />\n<br />\n<br />\n<br />\n<br />\n<br />\n <br />\n<br />\n <br />\n  \t<br />\n <br />\n<br />\n<br />\n <br />\n <br />\n <br />\n\t<br />\n\t\t <br />\n \t<br />\n\t<br />\n <br />\n <br />\n<br />\n<br />\n<br />\n<br />\n \t<br />\n<br />\n<br />\n <br />\n\t<br />\n  \t<br />\n <br />\n<br />\n<br />\n<br />\n<br />\n   \t<br />\n  <br />\n<br />\n<br />\n<br />\n \t<br />\n  <br />\n <br />\n <br />\n \t<br />\n \t<br />\n<br />\n<br />\n \t<br />\n<br />\n<br />\n <br />\n   <br />\n<br />\n <br />\n\t<br />\n   <br />\n \t<br />\n<br />\n<br />\n<br />\n <br />\n <br />\n<br />\n  \t<br />\n <br />\n <br />\n<br />\n<br />\n  \t<br />\n  <br />\n \t<br />\n\t \t\t <br />\n \t<br />\n\t  <br />\n  \t<br />\n  \t<br />\n  <br />\n <br />\n <br />\n<br />\n<br />\n \t<br />\n   <br />\n \t<br />\n\t<br />\n<br />\n<br />\n  \t<br />\n <br />\n <br />\n<br />\n<br />\n \t<br />\n <br />\n <br />\n  <br />\n<br />\n <br />\n     <br />\n \t<br />\n  \t<br />\n<br />\n<br />\n <br />\n  <br />\n\t<br />\n \t\t\t<br />\n  <br />\n<br />\n<br />\n <br />\n  <br />\n  <br />\n<br />\n<br />\n<br />\n <br />\n  \t<br />\n <br />\n <br />\n <br />\n <br />\n<br />\n<br />\n \t<br />\n \t<br />\n \t<br />\n <br />\n \t<br />\n   \t<br />\n <br />\n \t<br />\n<br />\n \t<br />\n<br />\n<br />\n <br />\n  <br />\n   <br />\n\t<br />\n  \t\t\t<br />\n<br />\n<br />\n <br />\n    <br />\n \t<br />\n <br />\n <br />\n <br />\n\t<br />\n   <br />\n \t<br />\n <br />\n<br />\n<br />\n  \t<br />\n<br />\n<br />\n<br />\n  <br />\n   <br />\n   <br />\n <br />\n <br />\n<br />\n<br />\n \t<br />\n  <br />\n    <br />\n \t<br />\n  \t<br />\n <br />\n    <br />\n   <br />\n  <br />\n <br />\n<br />\n<br />\n \t<br />\n <br />\n <br />\n  <br />\n  \t<br />\n<br />\n\t<br />\n<br />\n<br />\n<br />\n <br />\n<br />\n<br />\n<br />\n<br />\n \t<br />\n  <br />\n  <br />\n  <br />\n \t<br />\n  \t<br />\n <br />\n <br />\n<br />\n<br />\n\t<br />\n  <br />\n <br />\n    \t<br />\n <br />\n  \t<br />\n  <br />\n<br />\n<br />\n<br />\n \t<br />\n  <br />\n  \t<br />\n    <br />\n\t<br />\n  \t<br />\n <br />\n <br />\n <br />\n<br />\n<br />\n <br />\n  <br />\n   <br />\n   <br />\n\t<br />\n\t<br />\n<br />\n\t<br />\n \t<br />\n<br />\n  \t<br />\n \t<br />\n \t<br />\n \t<br />\n \t<br />\n<br />\n <br />\n<br />\n<br />\n \t<br />\n \t<br />\n \t<br />\n \t<br />\n<br />\n <br />\n  <br />\n \t<br />\n<br />\n<br />\n \t<br />\n\t<br />\n<br />\n  \t<br />\n \t<br />\n  <br />\n  <br />\n \t<br />\n  \t<br />\n \t<br />\n \t<br />\n<br />\n<br />\n \t<br />\n \t<br />\n \t<br />\n\t<br />\n <br />\n\t<br />\n  \t<br />\n <br />\n <br />\n\t<br />\n<br />\n<br />\n<br />\n<br />\n \t<br />\n <br />\n  <br />\n\t<br />\n  <br />\n <br />\n<br />\n <br />\n<br />\n<br />\n<br />\n <br />\n  <br />\n   <br />\n   <br />\n  <br />\n \t<br />\n  \t<br />\n\t<br />\n \t<br />\n<br />\n<br />\n  \t<br />\n<br />\n <br />\n <br />\n\t<br />\n  <br />\n \t<br />\n",
        "created_at": "2018-07-24T11:15:33+0000",
        "updated_at": "2018-07-24T11:15:59+0000",
        "published_at": "2013-05-17T12:56:23+0000",
        "published_by": [
          "Marko Vukolic"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "application/pdf",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.eurecom.fr",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11060"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 228,
            "label": "dotnet",
            "slug": "net"
          },
          {
            "id": 236,
            "label": "csharp",
            "slug": "csharp"
          }
        ],
        "is_public": false,
        "id": 11055,
        "uid": null,
        "title": "Access Cassandra Data with Entity Framework 6",
        "url": "https://www.cdata.com/kb/tech/cassandra-ado-codefirst.rst",
        "content": "<p>\n     Entity Framework is an object-relational mapping framework that can be used to work with data as objects. While you can run the ADO.NET Entity Data Model wizard in Visual Studio to handle generating the Entity Model, this approach, the model-first approach, can put you at a disadvantage if there are changes in your data source or if you want more control over how the entities operate. In this article you will complete the code-first approach to accessing Cassandra data using the CData ADO.NET Provider.\n\t</p><ol><li>Open Visual Studio and create a new Windows Form Application. This article uses a C# project with .NET 4.5.</li>\n    <li>Run the command 'Install-Package EntityFramework' in the Package Manger Console in Visual Studio to install the latest release of Entity Framework.</li>\n    <li><p>Modify the App.config file in the project to add a reference to the Cassandra Entity Framework 6 assembly and the connection string.</p>\n\t   &#13;\n<p>&#13;\nSet the Server, Port, and Database connection properties to connect to Cassandra. Additionally, to use internal authentication set the User and Password connection properties.&#13;\n</p> \n<code lang=\"xml\" xml:lang=\"xml\">\n&lt;configuration&gt;\n   ... \n  &lt;connectionStrings&gt;\n    &lt;add name=\"CassandraContext\" connectionString=\"Offline=False;Database=MyCassandraDB;Port=7000;Server=127.0.0.1;\" providerName=\"System.Data.CData.Cassandra\" /&gt;\n  &lt;/connectionStrings&gt;\n  &lt;entityFramework&gt;\n    &lt;providers&gt;\n       ... \n      &lt;provider invariantName=\"System.Data.CData.Cassandra\" type=\"System.Data.CData.Cassandra.CassandraProviderServices, System.Data.CData.Cassandra.Entities.EF6\" /&gt;\n    &lt;/providers&gt;\n  &lt;entityFramework&gt;\n&lt;/configuration&gt;\n&lt;/code&gt; \n</code>\n    </li>\n    <li>Add a reference to System.Data.CData.Cassandra.Entities.EF6.dll, located in the lib -&gt; 4.0 subfolder in the installation directory.\n    </li><li>\nBuild the project at this point to ensure everything is working correctly.  Once that's done, you can start coding using Entity Framework.\n  </li>\n\t\t<li>Add a new .cs file to the project and add a class to it. This will be your database context, and it will extend the DbContext class. In the example, this class is named CassandraContext. The following code example overrides the OnModelCreating method to make the following changes:\n<ul><li>Remove PluralizingTableNameConvention from the ModelBuilder Conventions.\n</li><li>Remove requests to the MigrationHistory table.\n</li></ul><code lang=\"csharp\" xml:lang=\"csharp\">\nusing System.Data.Entity;\nusing System.Data.Entity.Infrastructure;\nusing System.Data.Entity.ModelConfiguration.Conventions;\nclass CassandraContext : DbContext {\n  public CassandraContext() { }\n  protected override void OnModelCreating(DbModelBuilder modelBuilder)\n  {\n    // To remove the requests to the Migration History table\n    Database.SetInitializer&lt;CassandraContext&gt;(null);  \n    // To remove the plural names    \n    modelBuilder.Conventions.Remove&lt;PluralizingTableNameConvention&gt;();\n  }  \n}\n</code>\n  </li>\n    <li>Create another .cs file and name it after the Cassandra entity you are retrieving, for example, Customer. In this file, define both the Entity and the Entity Configuration, which will resemble the example below:\n    <code lang=\"csharp\" xml:lang=\"csharp\">\nusing System.Data.Entity.ModelConfiguration;\nusing System.ComponentModel.DataAnnotations.Schema;\n[System.ComponentModel.DataAnnotations.Schema.Table(\"Customer\")]\npublic class Customer {\n  [System.ComponentModel.DataAnnotations.Key] \n  public System.String Id { get; set; }\n  public System.String City { get; set; }\n}\n    \n</code>\n  </li>\n    <li>Now that you have created an entity, add the entity to your context class:\n  \n<code lang=\"csharp\" xml:lang=\"csharp\">\npublic DbSet&lt;Customer&gt; Customer { set; get; }\n</code>\n  </li>\n    <li>With the context and entity finished, you are now ready to query the data in a separate class. For example: \n\t\n<code lang=\"csharp\" xml:lang=\"csharp\">\nCassandraContext context = new CassandraContext();\ncontext.Configuration.UseDatabaseNullSemantics = true;\nvar query = from line in context.Customer select line;\n</code>\n    </li>\n  </ol>",
        "created_at": "2018-07-24T11:06:06+0000",
        "updated_at": "2018-07-24T11:06:15+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 2,
        "domain_name": "www.cdata.com",
        "preview_picture": "https://www.cdata.com///www.cdata.com/kb/img/default.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11055"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          }
        ],
        "is_public": false,
        "id": 11048,
        "uid": null,
        "title": "Data processing platforms architectures with Spark, Mesos, Akka, Cass…",
        "url": "https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka",
        "content": "Data processing platforms architectures with Spark, Mesos, Akka, Cass…\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Data processing platforms architectures with Spark, Mesos, Akka, Cassandra and Kafka<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-1-638.jpg?cb=1441958613\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-1-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-1-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-1-1024.jpg?cb=1441958613\" alt=\"SMACK Architectures&#10;Building data processing platforms with&#10;Spark, Mesos, Akka, Cassandra and Kafka&#10;Anton Kirillov Big Dat...\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-2-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-2-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-2-1024.jpg?cb=1441958613\" alt=\"Who is this guy?&#10;● Scala programmer&#10;● Focused on distributed systems&#10;● Building data platforms with SMACK/Hadoop&#10;● Ph.D. i...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-3-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-3-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-3-1024.jpg?cb=1441958613\" alt=\"Roadmap&#10;● SMACK stack overview&#10;● Storage layer layout&#10;● Fixing NoSQL limitations&#10;● Cluster resource management&#10;● Reliable ...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-4-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-4-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-4-1024.jpg?cb=1441958613\" alt=\"SMACK Stack&#10;● Spark - fast and general engine for distributed, large-scale data&#10;processing&#10;● Mesos - cluster resource mana...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-5-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-5-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-5-1024.jpg?cb=1441958613\" alt=\"Storage Layer: Cassandra&#10;● optimized for heavy write&#10;loads&#10;● configurable CA (CAP)&#10;● linearly scalable&#10;● XDCR support&#10;● ea...\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-6-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-6-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-6-1024.jpg?cb=1441958613\" alt=\"Cassandra Data Model&#10;● nested sorted map&#10;● should be optimized for&#10;read queries&#10;● data is distributed across&#10;nodes by part...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-7-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-7-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-7-1024.jpg?cb=1441958613\" alt=\"Spark/Cassandra Example&#10;● calculate total views per&#10;campaign for given month&#10;for all campaigns&#10;CREATE TABLE event(&#10;id uuid...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-8-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-8-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-8-1024.jpg?cb=1441958613\" alt=\"Naive Lambda example with Spark SQL&#10;case class CampaignReport(id: String, views: Long, clicks: Long)&#10;sql(&quot;&quot;&quot;SELECT campaig...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-9-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-9-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-9-1024.jpg?cb=1441958613\" alt=\"Let’s take a step back: Spark Basics&#10;● RDD operations(transformations and actions) form DAG&#10;● DAG is split into stages of ...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-10-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-10-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-10-1024.jpg?cb=1441958613\" alt=\"Architecture of Spark/Cassandra Clusters&#10;Separate Write &amp; Analytics:&#10;● clusters can be scaled&#10;independently&#10;● data is repl...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-11-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-11-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-11-1024.jpg?cb=1441958613\" alt=\"Spark Applications Deployment Revisited&#10;Cluster Manager:&#10;● Spark Standalone&#10;● YARN&#10;● Mesos&#10;11&#10;\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-12-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-12-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-12-1024.jpg?cb=1441958613\" alt=\"Managing Cluster Resources: Mesos&#10;● heterogenous workloads&#10;● full cluster utilization&#10;● static vs. dynamic resource&#10;alloca...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-13-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-13-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-13-1024.jpg?cb=1441958613\" alt=\"Mesos Architecture Overview&#10;● leader election and&#10;service discovery via&#10;ZooKeeper&#10;● slaves publish available&#10;resources to ...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-14-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-14-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-14-1024.jpg?cb=1441958613\" alt=\"Bringing Spark, Mesos and Cassandra Together&#10;Deployment example&#10;● Mesos Masters and&#10;ZooKeepers collocated&#10;● Mesos Slaves a...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-15-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-15-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-15-1024.jpg?cb=1441958613\" alt=\"Marathon&#10;● long running tasks&#10;execution&#10;● HA mode with ZooKeeper&#10;● Docker executor&#10;● REST API&#10;15&#10;\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-16-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-16-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-16-1024.jpg?cb=1441958613\" alt=\"Chronos&#10;● distributed cron&#10;● HA mode with ZooKeeper&#10;● supports graphs of jobs&#10;● sensitive to network failures&#10;16&#10;\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-17-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-17-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-17-1024.jpg?cb=1441958613\" alt=\"More Mesos frameworks&#10;● Hadoop&#10;● Cassandra&#10;● Kafka&#10;● Myriad: YARN on Mesos&#10;● Storm&#10;● Samza&#10;17&#10;\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-18-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-18-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-18-1024.jpg?cb=1441958613\" alt=\"Data ingestion: endpoints to consume the data&#10;Endpoint requirements:&#10;● high throughput&#10;● resiliency&#10;● easy scalability&#10;● b...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-19-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-19-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-19-1024.jpg?cb=1441958613\" alt=\"Akka features&#10;class JsonParserActor extends Actor {&#10;def receive = {&#10;case s: String =&gt; Try(Json.parse(s).as[Event]) match {...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-20-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-20-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-20-1024.jpg?cb=1441958613\" alt=\"Writing to Cassandra with Akka&#10;class CassandraWriterActor extends Actor with ActorLogging {&#10;//for demo purposes, session i...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-21-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-21-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-21-1024.jpg?cb=1441958613\" alt=\"Cassandra meets Batch Processing&#10;● writing raw data (events) to Cassandra with Akka is easy&#10;● but computation time of aggr...\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-22-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-22-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-22-1024.jpg?cb=1441958613\" alt=\"Kafka: distributed commit log&#10;● pre-aggregation of incoming data&#10;● consumers read data in batches&#10;● available as Kinesis o...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-23-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-23-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-23-1024.jpg?cb=1441958613\" alt=\"Publishing to Kafka with Akka Http&#10;val config = new ProducerConfig(KafkaConfig())&#10;lazy val producer = new KafkaProducer[A,...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-24-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-24-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-24-1024.jpg?cb=1441958613\" alt=\"Spark Streaming&#10;24&#10;● variety of data sources&#10;● at-least-once semantics&#10;● exactly-once semantics&#10;available with Kafka Direc...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-25-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-25-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-25-1024.jpg?cb=1441958613\" alt=\"Spark Streaming: Kinesis example&#10;val ssc = new StreamingContext(conf, Seconds(10))&#10;val kinesisStream = KinesisUtils.create...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-26-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-26-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-26-1024.jpg?cb=1441958613\" alt=\"Designing for Failure: Backups and Patching&#10;● be prepared for failures and broken data&#10;● design backup and patching strate...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-27-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-27-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-27-1024.jpg?cb=1441958613\" alt=\"Restoring backup from S3&#10;val sc = new SparkContext(conf)&#10;sc.textFile(s&quot;s3n://bucket/2015/*/*.gz&quot;)&#10;.map(s =&gt; Try(JsonUtils....\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-28-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-28-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-28-1024.jpg?cb=1441958613\" alt=\"The big picture&#10;28&#10;\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-29-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-29-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-29-1024.jpg?cb=1441958613\" alt=\"So what SMACK is&#10;● concise toolbox for wide variety of data processing scenarios&#10;● battle-tested and widely used software ...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/akirillov/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka\" data-small=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/85/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-30-320.jpg?cb=1441958613\" data-normal=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-30-638.jpg?cb=1441958613\" data-full=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-30-1024.jpg?cb=1441958613\" alt=\"Questions&#10;@antonkirillov datastrophic.io&#10;30&#10;\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    4 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  132 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"IlyaAshikhmin1\" rel=\"nofollow\" href=\"https://www.slideshare.net/IlyaAshikhmin1?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            IlyaAshikhmin1\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"rajarp9\" rel=\"nofollow\" href=\"https://www.slideshare.net/rajarp9?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Raja Rp\n                            \n                              \n                                \n                                \n                              \n                              \n                                 at \n                                Infosys\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"WalterDiCarlo\" rel=\"nofollow\" href=\"https://www.slideshare.net/WalterDiCarlo?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Walter Di Carlo\n                            \n                              \n                                , \n                                Software Analyst/Developer Consultant presso Nokia S.p.A.\n                              \n                              \n                                 at \n                                Nokia\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"MathieuGoeminne\" rel=\"nofollow\" href=\"https://www.slideshare.net/MathieuGoeminne?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Mathieu Goeminne\n                            \n                              \n                                , \n                                Senior R&amp;D Engineer chez CETIC\n                              \n                              \n                                 at \n                                CETIC\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"KevinNguyen100\" rel=\"nofollow\" href=\"https://www.slideshare.net/KevinNguyen100?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Kevin Nguyen\n                            \n                              \n                                , \n                                Vice Manager at FPT Telecom NOC\n                              \n                              \n                                 at \n                                FPT Telecom\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul><div class=\"more-container text-center\"><a href=\"#\" class=\"j-more-favs\">\n                    Show More\n                    \n                  </a></div></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    SMACK Architectures\nBuilding data processing platforms with\nSpark, Mesos, Akka, Cassandra and Kafka\nAnton Kirillov Big Data AW Meetup\nSep 2015\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-2-638.jpg?cb=1441958613\" title=\"Who is this guy?&#10;● Scala programmer&#10;● Focused on distribute...\" target=\"_blank\">\n        2.\n      </a>\n    Who is this guy?\n● Scala programmer\n● Focused on distributed systems\n● Building data platforms with SMACK/Hadoop\n● Ph.D. in Computer Science\n● Big Data engineer/consultant at Big Data AB\n● Currently at Ooyala Stockholm (Videoplaza AB)\n● Working with startups\n2\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-3-638.jpg?cb=1441958613\" title=\"Roadmap&#10;● SMACK stack overview&#10;● Storage layer layout&#10;● Fix...\" target=\"_blank\">\n        3.\n      </a>\n    Roadmap\n● SMACK stack overview\n● Storage layer layout\n● Fixing NoSQL limitations\n● Cluster resource management\n● Reliable scheduling and execution\n● Data ingestion options\n● Preparing for failures\n3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-4-638.jpg?cb=1441958613\" title=\"SMACK Stack&#10;● Spark - fast and general engine for distribut...\" target=\"_blank\">\n        4.\n      </a>\n    SMACK Stack\n● Spark - fast and general engine for distributed, large-scale data\nprocessing\n● Mesos - cluster resource management system that provides efficient\nresource isolation and sharing across distributed applications\n● Akka - a toolkit and runtime for building highly concurrent, distributed,\nand resilient message-driven applications on the JVM\n● Cassandra - distributed, highly available database designed to handle\nlarge amounts of data across multiple datacenters\n● Kafka - a high-throughput, low-latency distributed messaging system\ndesigned for handling real-time data feeds\n4\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-5-638.jpg?cb=1441958613\" title=\"Storage Layer: Cassandra&#10;● optimized for heavy write&#10;loads&#10;...\" target=\"_blank\">\n        5.\n      </a>\n    Storage Layer: Cassandra\n● optimized for heavy write\nloads\n● configurable CA (CAP)\n● linearly scalable\n● XDCR support\n● easy cluster resizing and\ninter-DC data migration\n5\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-6-638.jpg?cb=1441958613\" title=\"Cassandra Data Model&#10;● nested sorted map&#10;● should be optimi...\" target=\"_blank\">\n        6.\n      </a>\n    Cassandra Data Model\n● nested sorted map\n● should be optimized for\nread queries\n● data is distributed across\nnodes by partition key\nCREATE TABLE campaign(\nid uuid,\nyear int,\nmonth int,\nday int,\nviews bigint,\nclicks bigint,\nPRIMARY KEY (id, year, month, day)\n);\nINSERT INTO campaign(id, year, month, day, views, clicks)\nVALUES(40b08953-a…,2015, 9, 10, 1000, 42);\nSELECT views, clicks FROM campaign\nWHERE id=40b08953-a… and year=2015 and month&gt;8; 6\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-7-638.jpg?cb=1441958613\" title=\"Spark/Cassandra Example&#10;● calculate total views per&#10;campaig...\" target=\"_blank\">\n        7.\n      </a>\n    Spark/Cassandra Example\n● calculate total views per\ncampaign for given month\nfor all campaigns\nCREATE TABLE event(\nid uuid,\nad_id uuid,\ncampaign uuid,\nts bigint,\ntype text,\nPRIMARY KEY(id)\n);\nval sc = new SparkContext(conf)\ncase class Event(id: UUID, ad_id: UUID, campaign: UUID, ts: Long, `type`: String)\nsc.cassandraTable[Event](\"keyspace\", \"event\")\n.filter(e =&gt; e.`type` == \"view\" &amp;&amp; checkMonth(e.ts))\n.map(e =&gt; (e.campaign, 1))\n.reduceByKey(_ + _)\n.collect() 7\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-8-638.jpg?cb=1441958613\" title=\"Naive Lambda example with Spark SQL&#10;case class CampaignRepo...\" target=\"_blank\">\n        8.\n      </a>\n    Naive Lambda example with Spark SQL\ncase class CampaignReport(id: String, views: Long, clicks: Long)\nsql(\"\"\"SELECT campaign.id as id, campaign.views as views,\ncampaign.clicks as clicks, event.type as type\nFROM campaign\nJOIN event ON campaign.id = event.campaign\n\"\"\").rdd\n.groupBy(row =&gt; row.getAs[String](\"id\"))\n.map{ case (id, rows) =&gt;\nval views = rows.head.getAs[Long](\"views\")\nval clicks = rows.head.getAs[Long](\"clicks\")\nval res = rows.groupBy(row =&gt; row.getAs[String](\"type\")).mapValues(_.size)\nCampaignReport(id, views = views + res(\"view\"), clicks = clicks + res(\"click\"))\n}.saveToCassandra(“keyspace”, “campaign_report”)\n8\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-9-638.jpg?cb=1441958613\" title=\"Let’s take a step back: Spark Basics&#10;● RDD operations(trans...\" target=\"_blank\">\n        9.\n      </a>\n    Let’s take a step back: Spark Basics\n● RDD operations(transformations and actions) form DAG\n● DAG is split into stages of tasks which are then submitted to cluster manager\n● stages combine tasks which don’t require shuffling/repartitioning\n● tasks run on workers and results then return to client 9\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-10-638.jpg?cb=1441958613\" title=\"Architecture of Spark/Cassandra Clusters&#10;Separate Write &amp; A...\" target=\"_blank\">\n        10.\n      </a>\n    Architecture of Spark/Cassandra Clusters\nSeparate Write &amp; Analytics:\n● clusters can be scaled\nindependently\n● data is replicated by\nCassandra asynchronously\n● Analytics has different\nRead/Write load patterns\n● Analytics contains additional\ndata and processing results\n● Spark resource impact\nlimited to only one DC\nTo fully facilitate Spark-C* connector data locality awareness,\nSpark workers should be collocated with Cassandra nodes 10\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-11-638.jpg?cb=1441958613\" title=\"Spark Applications Deployment Revisited&#10;Cluster Manager:&#10;● ...\" target=\"_blank\">\n        11.\n      </a>\n    Spark Applications Deployment Revisited\nCluster Manager:\n● Spark Standalone\n● YARN\n● Mesos\n11\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-12-638.jpg?cb=1441958613\" title=\"Managing Cluster Resources: Mesos&#10;● heterogenous workloads&#10;...\" target=\"_blank\">\n        12.\n      </a>\n    Managing Cluster Resources: Mesos\n● heterogenous workloads\n● full cluster utilization\n● static vs. dynamic resource\nallocation\n● fault tolerance and disaster\nrecovery\n● single resource view at\ndatacenter levelimage source: http://www.slideshare.net/caniszczyk/apache-mesos-at-twitter-texas-linuxfest-2014\n12\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-13-638.jpg?cb=1441958613\" title=\"Mesos Architecture Overview&#10;● leader election and&#10;service d...\" target=\"_blank\">\n        13.\n      </a>\n    Mesos Architecture Overview\n● leader election and\nservice discovery via\nZooKeeper\n● slaves publish available\nresources to master\n● master sends resource\noffers to frameworks\n● scheduler replies with\ntasks and resources\nneeded per task\n● master sends tasks to\nslaves\n13\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-14-638.jpg?cb=1441958613\" title=\"Bringing Spark, Mesos and Cassandra Together&#10;Deployment exa...\" target=\"_blank\">\n        14.\n      </a>\n    Bringing Spark, Mesos and Cassandra Together\nDeployment example\n● Mesos Masters and\nZooKeepers collocated\n● Mesos Slaves and Cassandra\nnodes collocated to enforce\nbetter data locality for Spark\n● Spark binaries deployed to all\nworker nodes and spark-env is\nconfigured\n● Spark Executor JAR uploaded\nto S3\nInvocation example\nspark-submit --class io.datastrophic.SparkJob /etc/jobs/spark-jobs.jar\n14\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-15-638.jpg?cb=1441958613\" title=\"Marathon&#10;● long running tasks&#10;execution&#10;● HA mode with ZooK...\" target=\"_blank\">\n        15.\n      </a>\n    Marathon\n● long running tasks\nexecution\n● HA mode with ZooKeeper\n● Docker executor\n● REST API\n15\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-16-638.jpg?cb=1441958613\" title=\"Chronos&#10;● distributed cron&#10;● HA mode with ZooKeeper&#10;● suppo...\" target=\"_blank\">\n        16.\n      </a>\n    Chronos\n● distributed cron\n● HA mode with ZooKeeper\n● supports graphs of jobs\n● sensitive to network failures\n16\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-17-638.jpg?cb=1441958613\" title=\"More Mesos frameworks&#10;● Hadoop&#10;● Cassandra&#10;● Kafka&#10;● Myriad...\" target=\"_blank\">\n        17.\n      </a>\n    More Mesos frameworks\n● Hadoop\n● Cassandra\n● Kafka\n● Myriad: YARN on Mesos\n● Storm\n● Samza\n17\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-18-638.jpg?cb=1441958613\" title=\"Data ingestion: endpoints to consume the data&#10;Endpoint requ...\" target=\"_blank\">\n        18.\n      </a>\n    Data ingestion: endpoints to consume the data\nEndpoint requirements:\n● high throughput\n● resiliency\n● easy scalability\n● back pressure 18\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-19-638.jpg?cb=1441958613\" title=\"Akka features&#10;class JsonParserActor extends Actor {&#10;def rec...\" target=\"_blank\">\n        19.\n      </a>\n    Akka features\nclass JsonParserActor extends Actor {\ndef receive = {\ncase s: String =&gt; Try(Json.parse(s).as[Event]) match {\ncase Failure(ex) =&gt; log.error(ex)\ncase Success(event) =&gt; sender ! event\n}\n}\n}\nclass HttpActor extends Actor {\ndef receive = {\ncase req: HttpRequest =&gt;\nsystem.actorOf(Props[JsonParserActor]) ! req.body\ncase e: Event =&gt;\nsystem.actorOf(Props[CassandraWriterActor]) ! e\n}\n}\n● actor model\nimplementation for JVM\n● message-based and\nasynchronous\n● no shared mutable state\n● easy scalability from one\nprocess to cluster of\nmachines\n● actor hierarchies with\nparental supervision\n● not only concurrency\nframework:\n○ akka-http\n○ akka-streams\n○ akka-persistence\n19\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-20-638.jpg?cb=1441958613\" title=\"Writing to Cassandra with Akka&#10;class CassandraWriterActor e...\" target=\"_blank\">\n        20.\n      </a>\n    Writing to Cassandra with Akka\nclass CassandraWriterActor extends Actor with ActorLogging {\n//for demo purposes, session initialized here\nval session = Cluster.builder()\n.addContactPoint(\"cassandra.host\")\n.build()\n.connect()\noverride def receive: Receive = {\ncase event: Event =&gt;\nval statement = new SimpleStatement(event.createQuery)\n.setConsistencyLevel(ConsistencyLevel.QUORUM)\nTry(session.execute(statement)) match {\ncase Failure(ex) =&gt; //error handling code\ncase Success =&gt; sender ! WriteSuccessfull\n}\n}\n} 20\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-21-638.jpg?cb=1441958613\" title=\"Cassandra meets Batch Processing&#10;● writing raw data (events...\" target=\"_blank\">\n        21.\n      </a>\n    Cassandra meets Batch Processing\n● writing raw data (events) to Cassandra with Akka is easy\n● but computation time of aggregations/rollups will grow with\namount of data\n● Cassandra is still designed for fast serving but not batch\nprocessing, so pre-aggregation of incoming data is needed\n● actors are not suitable for performing aggregation due to\nstateless design model\n● micro-batches partially solve the problem\n● reliable storage for raw data is still needed\n21\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-22-638.jpg?cb=1441958613\" title=\"Kafka: distributed commit log&#10;● pre-aggregation of incoming...\" target=\"_blank\">\n        22.\n      </a>\n    Kafka: distributed commit log\n● pre-aggregation of incoming data\n● consumers read data in batches\n● available as Kinesis on AWS\n22\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-23-638.jpg?cb=1441958613\" title=\"Publishing to Kafka with Akka Http&#10;val config = new Produce...\" target=\"_blank\">\n        23.\n      </a>\n    Publishing to Kafka with Akka Http\nval config = new ProducerConfig(KafkaConfig())\nlazy val producer = new KafkaProducer[A, A](config)\nval topic = “raw_events”\nval routes: Route = {\npost{\ndecodeRequest{\nentity(as[String]){ str =&gt;\nJsonParser.parse(str).validate[Event] match {\ncase s: JsSuccess[String] =&gt; producer.send(new KeyedMessage(topic, str))\ncase e: JsError =&gt; BadRequest -&gt; JsError.toFlatJson(e).toString()\n}\n}\n}\n}\n}\nobject AkkaHttpMicroservice extends App with Service {\nHttp().bindAndHandle(routes, config.getString(\"http.interface\"), config.getInt(\"http.\nport\"))\n}\n23\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-24-638.jpg?cb=1441958613\" title=\"Spark Streaming&#10;24&#10;● variety of data sources&#10;● at-least-onc...\" target=\"_blank\">\n        24.\n      </a>\n    Spark Streaming\n24\n● variety of data sources\n● at-least-once semantics\n● exactly-once semantics\navailable with Kafka Direct\nand idempotent storage\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-25-638.jpg?cb=1441958613\" title=\"Spark Streaming: Kinesis example&#10;val ssc = new StreamingCon...\" target=\"_blank\">\n        25.\n      </a>\n    Spark Streaming: Kinesis example\nval ssc = new StreamingContext(conf, Seconds(10))\nval kinesisStream = KinesisUtils.createStream(ssc,appName,streamName,\nendpointURL,regionName, InitialPositionInStream.LATEST,\nDuration(checkpointInterval), StorageLevel.MEMORY_ONLY)\n}\n//transforming given stream to Event and saving to C*\nkinesisStream.map(JsonUtils.byteArrayToEvent)\n.saveToCassandra(keyspace, table)\nssc.start()\nssc.awaitTermination()\n25\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-26-638.jpg?cb=1441958613\" title=\"Designing for Failure: Backups and Patching&#10;● be prepared f...\" target=\"_blank\">\n        26.\n      </a>\n    Designing for Failure: Backups and Patching\n● be prepared for failures and broken data\n● design backup and patching strategies upfront\n● idempotece should be enforced 26\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-27-638.jpg?cb=1441958613\" title=\"Restoring backup from S3&#10;val sc = new SparkContext(conf)&#10;sc...\" target=\"_blank\">\n        27.\n      </a>\n    Restoring backup from S3\nval sc = new SparkContext(conf)\nsc.textFile(s\"s3n://bucket/2015/*/*.gz\")\n.map(s =&gt; Try(JsonUtils.stringToEvent(s)))\n.filter(_.isSuccess).map(_.get)\n.saveToCassandra(config.keyspace, config.table)\n27\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-28-638.jpg?cb=1441958613\" title=\"The big picture&#10;28&#10;\" target=\"_blank\">\n        28.\n      </a>\n    The big picture\n28\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-29-638.jpg?cb=1441958613\" title=\"So what SMACK is&#10;● concise toolbox for wide variety of data...\" target=\"_blank\">\n        29.\n      </a>\n    So what SMACK is\n● concise toolbox for wide variety of data processing scenarios\n● battle-tested and widely used software with large communities\n● easy scalability and replication of data while preserving low latencies\n● unified cluster management for heterogeneous loads\n● single platform for any kind of applications\n● implementation platform for different architecture designs\n● really short time-to-market (e.g. for MVP verification)\n29\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/smackstackarchitectures1-150911080248-lva1-app6891/95/data-processing-platforms-architectures-with-spark-mesos-akka-cassandra-and-kafka-30-638.jpg?cb=1441958613\" title=\"Questions&#10;@antonkirillov datastrophic.io&#10;30&#10;\" target=\"_blank\">\n        30.\n      </a>\n    Questions\n@antonkirillov datastrophic.io\n30\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\">\n<dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"52661852\" title=\"100 Courses and Counting: David Rivers on Elearning\" href=\"https://www.linkedin.com/learning/100-courses-and-counting-david-rivers-on-elearning?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_100 Courses and Counting: David Rivers on Elearning\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"100 Courses and Counting: David Rivers on Elearning\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=R7g5ycQkdBoUBm%2BKSKfr%2BU9%2BcPo%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gUyWo-9efYX_pe8bdZLSiol4TeyQHmQMyeuauQDDgEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>100 Courses and Counting: David Rivers on Elearning</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"52661852\" title=\"SMART Board Essential Training\" href=\"https://www.linkedin.com/learning/smart-board-essential-training?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_SMART Board Essential Training\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"SMART Board Essential Training\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=L8W%2BAILfUItMtiIFfmNzSmnQWEA%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gXyGq_d2fYXPtecDXZLSioV8QfiwHlQIzfO6sQzToF469LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>SMART Board Essential Training</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"52661852\" title=\"Teaching Techniques: Writing Effective Learning Objectives\" href=\"https://www.linkedin.com/learning/teaching-techniques-writing-effective-learning-objectives?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Teaching Techniques: Writing Effective Learning Objectives\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Teaching Techniques: Writing Effective Learning Objectives\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=8g9LchVQPqjokhBzWyb8MRYUQls%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXySs-tCfZHPof8faZLSiol8QcS4DkAQ7feitRzXjEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Teaching Techniques: Writing Effective Learning Objectives</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53162817\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"How to deploy Apache Spark  to Mesos/DCOS\" href=\"https://www.slideshare.net/Typesafe_Inc/how-to-deploy-apache-spark-to-mesosdcos\">\n    \n    <div class=\"related-content\"><p>How to deploy Apache Spark  to Mesos/DCOS</p><p>Legacy Typesafe (now Lightbend)</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"55353060\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Rethinking Streaming Analytics For Scale\" href=\"https://www.slideshare.net/helenaedelson/rethinking-streaming-analytics-for-scale\">\n    \n    <div class=\"related-content\"><p>Rethinking Streaming Analytics For Scale</p><p>Helena Edelson</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"54999973\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Intro to Apache Spark\" href=\"https://www.slideshare.net/MammothData/intro-to-apache-spark-54999973\">\n    \n    <div class=\"related-content\"><p>Intro to Apache Spark</p><p>Mammoth Data</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53371129\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Reactive app using actor model &amp; apache spark\" href=\"https://www.slideshare.net/RahulKumar405/reactive-app-using-actor-model-apache-spark\">\n    \n    <div class=\"related-content\"><p>Reactive app using actor model &amp; apache spark</p><p>Rahul Kumar</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"54590394\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Streaming Analytics with Spark, Kafka, Cassandra and Akka\" href=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\">\n    \n    <div class=\"related-content\"><p>Streaming Analytics with Spark, Kafka, Cassandra and Akka</p><p>Helena Edelson</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"43475359\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Streaming Big Data with Spark, Kafka, Cassandra, Akka &amp; Scala (from webinar)\" href=\"https://www.slideshare.net/helenaedelson/streaming-bigdata-helenawebinarv3\">\n    \n    <div class=\"related-content\"><p>Streaming Big Data with Spark, Kafka, Cassandra, Akka &amp; Scala (from webinar)</p><p>Helena Edelson</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"55646316\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Sa introduction to big data pipelining with cassandra &amp;amp; spark   west minster meetup - black-2015 0.11-2\" href=\"https://www.slideshare.net/langworth/sa-introduction-to-big-data-pipelining-with-cassandra-amp-spark-west-minster-meetup-black2015-0112\">\n    \n    <div class=\"related-content\"><p>Sa introduction to big data pipelining with cassandra &amp;amp; spark   west mins...</p><p>Simon Ambridge</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-07-23T16:07:44+0000",
        "updated_at": "2018-07-23T16:07:54+0000",
        "published_at": null,
        "published_by": [
          "Anton Kirillov"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/smackstackarchitectures1-150911080248-lva1-app6891-thumbnail-4.jpg?cb=1441958613",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/11048"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1117,
            "label": "elassandra",
            "slug": "elassandra"
          }
        ],
        "is_public": false,
        "id": 10888,
        "uid": null,
        "title": "Elassandra = Elasticsearch + Cassandra",
        "url": "https://www.elassandra.io/",
        "content": "<p class=\"card-text\">Cassandra is optimised for write-intensive workloads, therefore, Elassandra is suitable for applications where a large amount of data needs to be inserted (such as infrastructure logging, IOT, or events).\n                  Then, Elasticsearch indices can be rebuilt at any time from Cassandra tables without data duplication.</p>",
        "created_at": "2018-07-18T18:53:50+0000",
        "updated_at": "2018-07-18T18:54:01+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "www.elassandra.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10888"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1117,
            "label": "elassandra",
            "slug": "elassandra"
          }
        ],
        "is_public": false,
        "id": 10887,
        "uid": null,
        "title": "strapdata/elassandra",
        "url": "https://github.com/strapdata/elassandra",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p><a target=\"_blank\" href=\"https://github.com/strapdata/elassandra/blob/v6.2.3-strapdata/elassandra-logo.png\"><img src=\"https://github.com/strapdata/elassandra/raw/v6.2.3-strapdata/elassandra-logo.png\" alt=\"Elassandra Logo\" /></a></p>\n<p>Elassandra is a fork of <a href=\"https://github.com/elastic/elasticsearch\">Elasticsearch</a> modified to run as a plugin for <a href=\"http://cassandra.apache.org\" rel=\"nofollow\">Apache Cassandra</a> in a scalable and resilient peer-to-peer architecture. Elasticsearch code is embedded in Cassanda nodes providing advanced search features on Cassandra tables and Cassandra serve as an Elasticsearch data and configuration store.</p>\n<p><a target=\"_blank\" href=\"https://github.com/strapdata/elassandra/blob/v6.2.3-strapdata/docs/elassandra/source/images/elassandra1.jpg\"><img src=\"https://github.com/strapdata/elassandra/raw/v6.2.3-strapdata/docs/elassandra/source/images/elassandra1.jpg\" alt=\"Elassandra architecture\" /></a></p>\n<p>Elassandra supports Cassandra vnodes and scales horizontally by adding more nodes.</p>\n<p>Project documentation is available at <a href=\"http://doc.elassandra.io\" rel=\"nofollow\">doc.elassandra.io</a>.</p>\n<h2><a id=\"user-content-benefits-of-elassandra\" class=\"anchor\" aria-hidden=\"true\" href=\"#benefits-of-elassandra\"></a>Benefits of Elassandra</h2>\n<p>For Cassandra users, elassandra provides Elasticsearch features :</p>\n<ul><li>Cassandra update are indexed in Elasticsearch.</li>\n<li>Full-text and spatial search on your Cassandra data.</li>\n<li>Real-time aggregation (does not require Spark or Hadoop to GROUP BY)</li>\n<li>Provide search on multiple keyspaces and tables in one query.</li>\n<li>Provide automatic schema creation and support nested document using <a href=\"https://docs.datastax.com/en/cql/3.1/cql/cql_using/cqlUseUDT.html\" rel=\"nofollow\">User Defined Types</a>.</li>\n<li>Provide a read/write JSON REST access to Cassandra data.</li>\n<li>Numerous Elasticsearch plugins and products like <a href=\"https://www.elastic.co/guide/en/kibana/current/introduction.html\" rel=\"nofollow\">Kibana</a>.</li>\n</ul><p>For Elasticsearch users, elassandra provides useful features :</p>\n<ul><li>Elassandra is masterless, cluster state is managed through a <a href=\"http://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0\" rel=\"nofollow\">cassandra lightweight transactions</a>.</li>\n<li>Elassandra is a sharded multi-master database, where Elasticsearch is sharded master-slave, Thus, Elassandra has no Single Point Of Write, helping to achieve high availability.</li>\n<li>Elassandra inherits Cassandra data repair mechanisms (hinted handoff, read repair and nodetool repair) allowing to support <strong>cross datacenter replication</strong>.</li>\n<li>When adding a node to an Elassandra cluster, only data pulled from existing nodes are re-indexed in Elasticsearch.</li>\n<li>Cassandra could be your unique datastore for indexed and non-indexed data, it's easier to manage and secure. Source documents are now stored in Cassandra, reducing disk space if you need a NoSQL database and Elasticsearch.</li>\n<li>Write operations are not more restricted to one primary shards, but distributed on all Cassandra nodes in a virtual datacenter. Number of shards does not limit your write throughput, just add some elassandra nodes to increase both read and write throughput.</li>\n<li>Elasticsearch indices can be replicated between many Cassandra datacenters, allowing to write to the closest datacenter and search globally.</li>\n<li>The <a href=\"http://www.planetcassandra.org/client-drivers-tools/\" rel=\"nofollow\">cassandra driver</a> is Datacenter and Token aware, providing automatic load-balancing and failover.</li>\n</ul><h2><a id=\"user-content-quick-start\" class=\"anchor\" aria-hidden=\"true\" href=\"#quick-start\"></a>Quick start</h2>\n<h4><a id=\"user-content-elasticsearch-6x-changes\" class=\"anchor\" aria-hidden=\"true\" href=\"#elasticsearch-6x-changes\"></a>Elasticsearch 6.x changes</h4>\n<ul><li>Elasticsearch now supports only one document type per index backed by one Cassandra table. Unless you specify an elasticsearch type name in your mapping, data are stored in a cassandra table named <strong>\"_doc\"</strong>. If you want to search in many cassandra tables, you now need to create and search in many indices.</li>\n<li>Elasticsearch 6.x manages shards consistency through several metadata fields (_primary_term, _seq_no, _version) that are not more used in elassandra because replication is fully managed by cassandra.</li>\n</ul><h4><a id=\"user-content-requirements\" class=\"anchor\" aria-hidden=\"true\" href=\"#requirements\"></a>Requirements</h4>\n<p>Ensure Java 8 is installed and <code>JAVA_HOME</code> points to the correct location.</p>\n<h4><a id=\"user-content-installation\" class=\"anchor\" aria-hidden=\"true\" href=\"#installation\"></a>Installation</h4>\n<ul><li><a href=\"https://github.com/strapdata/elassandra/releases\">Download</a> and extract the distribution tarball</li>\n<li>Define the CASSANDRA_HOME environment variable : <code>export CASSANDRA_HOME=&lt;extracted_directory&gt;</code></li>\n<li>Run <code>bin/cassandra -e</code></li>\n<li>Run <code>bin/nodetool status</code></li>\n<li>Run <code>curl -XGET localhost:9200/_cluster/state</code></li>\n</ul><h4><a id=\"user-content-example\" class=\"anchor\" aria-hidden=\"true\" href=\"#example\"></a>Example</h4>\n<p>Try indexing a document on a non-existing index:</p>\n<div class=\"highlight highlight-source-shell\"><pre>curl -XPUT 'http://localhost:9200/twitter/_doc/1?pretty' -H 'Content-Type: application/json' -d '\n{\n    \"user\": \"Poulpy\",\n    \"post_date\": \"2017/10/4 13:12:00\",\n    \"message\": \"Elassandra adds dynamic mapping to Cassandra\"\n}'</pre></div>\n<p>Then look-up in Cassandra:</p>\n<div class=\"highlight highlight-source-shell\"><pre>bin/cqlsh -c \"SELECT * from twitter.\\\"_doc\\\"\"</pre></div>\n<p>Behind the scene, Elassandra has created a new Keyspace <code>twitter</code> and table <code>_doc</code>.</p>\n<p>Now, insert a row with CQL :</p>\n<div class=\"highlight highlight-source-sql\"><pre>INSERT INTO twitter.doc (\"_id\", user, post_date, message)\nVALUES ( '2', ['Jimmy'], [dateof(now())], ['New data is indexed automatically']);</pre></div>\n<p>Then search for it with the Elasticsearch API:</p>\n<div class=\"highlight highlight-source-shell\"><pre>curl \"localhost:9200/twitter/_search?q=user:Jimmy&amp;pretty\"</pre></div>\n<p>And here is a sample response :</p>\n<div class=\"highlight highlight-source-json\"><pre>{\n  \"took\" : 1,\n  \"timed_out\" : false,\n  \"_shards\" : {\n    \"total\" : 1,\n    \"successful\" : 1,\n    \"failed\" : 0\n  },\n  \"hits\" : {\n    \"total\" : 1,\n    \"max_score\" : 0.9808292,\n    \"hits\" : [\n      {\n        \"_index\" : \"twitter\",\n        \"_type\" : \"doc\",\n        \"_id\" : \"2\",\n        \"_score\" : 0.9808292,\n        \"_source\" : {\n          \"post_date\" : \"2017/10/04 13:20:00\",\n          \"message\" : \"New data is indexed automatically\",\n          \"user\" : \"Jimmy\"\n        }\n      }\n    ]\n  }\n}\n</pre></div>\n<h2><a id=\"user-content-support\" class=\"anchor\" aria-hidden=\"true\" href=\"#support\"></a>Support</h2>\n<ul><li>Commercial support is available through <a href=\"http://www.strapdata.com/\" rel=\"nofollow\">Strapdata</a>.</li>\n<li>Community support available via <a href=\"https://groups.google.com/forum/#!forum/elassandra\" rel=\"nofollow\">elassandra google groups</a>.</li>\n<li>Post feature requests and bugs on <a href=\"https://github.com/strapdata/elassandra/issues\">https://github.com/strapdata/elassandra/issues</a></li>\n</ul><h2><a id=\"user-content-license\" class=\"anchor\" aria-hidden=\"true\" href=\"#license\"></a>License</h2>\n<pre>This software is licensed under the Apache License, version 2 (\"ALv2\"), quoted below.\nCopyright 2015-2018, Strapdata (contact@strapdata.com).\nLicensed under the Apache License, Version 2.0 (the \"License\"); you may not\nuse this file except in compliance with the License. You may obtain a copy of\nthe License at\n    http://www.apache.org/licenses/LICENSE-2.0\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\nWARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\nLicense for the specific language governing permissions and limitations under\nthe License.\n</pre>\n<h2><a id=\"user-content-acknowledgments\" class=\"anchor\" aria-hidden=\"true\" href=\"#acknowledgments\"></a>Acknowledgments</h2>\n<ul><li>Elasticsearch and Kibana are trademarks of Elasticsearch BV, registered in the U.S. and in other countries.</li>\n<li>Apache Cassandra, Apache Lucene, Apache, Lucene and Cassandra are trademarks of the Apache Software Foundation.</li>\n<li>Elassandra is a trademark of Strapdata SAS.</li>\n</ul></article>",
        "created_at": "2018-07-18T18:53:39+0000",
        "updated_at": "2018-07-18T18:53:47+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/19846919?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10887"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10874,
        "uid": null,
        "title": "Data Modeling in Cassandra | Baeldung",
        "url": "http://www.baeldung.com/cassandra-data-modeling",
        "content": "<div class=\"short_box short_start\">\n<h3><b>I just announced the new <em>Spring 5</em> modules in REST With Spring: </b></h3>\n<p><strong><a href=\"http://www.baeldung.com/rest-with-spring-course#new-modules\" id=\"announcement_CTA\">&gt;&gt; CHECK OUT THE COURSE</a></strong></p>\n</div><h2><strong>1. Overview</strong></h2>\n<p>Cassandra is a NoSQL database that provides high availability and horizontal scalability without compromising performance.</p>\n<p>To get the best performance out of Cassandra, we need to carefully design the schema around query patterns specific to the business problem at hand.</p>\n<p>In this article, we will review some of the key concepts around <strong>how to approach data modeling in Cassandra</strong>.</p>\n<p>Before proceeding, you can go through our <a href=\"http://www.baeldung.com/cassandra-with-java\">Cassandra with Java</a> article to understand the basics and how to connect to Cassandra using Java.</p>\n<h2><strong>2. Partition Key<br /></strong></h2>\n<p>Cassandra is a distributed database in which data is partitioned and stored across multiple nodes within a cluster.</p>\n<p>The partition key is made up of one or more data fields and is <strong>used by the partitioner to generate a token via hashing to distribute the data uniformly across a cluster</strong>.</p>\n<h2><strong>3. Clustering Key<br /></strong></h2>\n<p>A clustering key is made up of one or more fields and helps in clustering or grouping together rows with same partition key and storing them in sorted order.</p>\n<p>Let’s say that we are storing time-series data in Cassandra and we want to retrieve the data in chronological order. A clustering key that includes time-series data fields will be very helpful for efficient retrieval of data for this use case.</p>\n<p><strong>Note: The combination of partition key and clustering key makes up the primary key and uniquely identifies any record in the Cassandra cluster.</strong></p>\n<h2><strong>4. Guidelines Around Query Patterns<br /></strong></h2>\n<p>Before starting with data modeling in Cassandra, we should identify the query patterns and ensure that they adhere to the following guidelines:</p>\n<ol><li>Each query should fetch data from a single partition</li>\n<li>We should keep track of how much data is getting stored in a partition, as Cassandra has limits around the number of columns that can be stored in a single partition</li>\n<li>It is OK to denormalize and duplicate the data to support different kinds of query patterns over the same data</li>\n</ol><p>Based on the above guidelines, let’s look at some real-world use cases and how we would model the Cassandra data models for them.</p>\n<h2><strong>5. Real World Data Modeling Examples<br /></strong></h2>\n<h3><strong>5.1. Facebook Posts</strong></h3>\n<p>Suppose that we are storing Facebook posts of different users in Cassandra. One of the common query patterns will be fetching the top ‘<em>N</em>‘ posts made by a given user.</p>\n<p>Thus, <strong>we need to</strong> <strong>store all data for a particular user on a single partition</strong> as per the above guidelines.</p>\n<p>Also, using the post timestamp as the clustering key will be helpful for retrieving the top ‘<em>N</em>‘ posts more efficiently.</p>\n<p>Let’s define the Cassandra table schema for this use case:</p>\n<pre class=\"brush: sql; gutter: true\">CREATE TABLE posts_facebook (&#13;\n  user_id uuid,&#13;\n  post_id timeuuid, &#13;\n  content text,&#13;\n  PRIMARY KEY (user_id, post_id) )&#13;\nWITH CLUSTERING ORDER BY (post_id DESC);</pre>\n<p>Now, let’s write a query to find the top 20 posts for the user <em>Anna</em>:</p>\n<pre class=\"brush: sql; gutter: true\">SELECT content FROM posts_facebook WHERE user_id = \"Anna_id\" LIMIT 20</pre>\n<h3><strong>5.2. Gyms Across the Country</strong></h3>\n<p>Suppose that we are storing the details of different partner gyms across the different cities and states of many countries and we would like to fetch the gyms for a given city.</p>\n<p>Also, let’s say we need to return the results having gyms sorted by their opening date.</p>\n<p>Based on the above guidelines, we should store the gyms located in a given city of a specific state and country on a single partition and use the opening date and gym name as a clustering key.</p>\n<p>Let’s define the Cassandra table schema for this example:</p>\n<pre class=\"brush: sql; gutter: true\">CREATE TABLE gyms_by_city (&#13;\n country_code text,&#13;\n state text,&#13;\n city text,&#13;\n gym_name text,&#13;\n opening_date timestamp,&#13;\n PRIMARY KEY (&#13;\n   (country_code, state_province, city), &#13;\n   (opening_date, gym_name)) &#13;\n WITH CLUSTERING ORDER BY (opening_date ASC, gym_name ASC);</pre>\n<p>Now, let’s look at a query that fetches the first ten gyms by their opening date for the city of Phoenix within the U.S. state of Arizona:</p>\n<pre class=\"brush: sql; gutter: true\">SELECT * FROM gyms_by_city&#13;\n  WHERE country_code = \"us\" AND state = \"Arizona\" AND city = \"Phoenix\"&#13;\n  LIMIT 10</pre>\n<p class=\"brush: sql; gutter: true\">Next, let’s see a query that fetches the ten most recently-opened gyms in the city of Phoenix within the U.S. state of Arizona:</p>\n<pre class=\"brush: sql; gutter: true\">SELECT * FROM gyms_by_city&#13;\n  WHERE country_code = \"us\" and state = \"Arizona\" and city = \"Phoenix\"&#13;\n  ORDER BY opening_date DESC &#13;\n  LIMIT 10</pre>\n<p>Note: As the last query’s sort order is opposite of the sort order defined during the table creation, the query will run slower as Cassandra will first fetch the data and then sort it in memory.</p>\n<h3><strong>5.3. E-commerce Customers and Products</strong></h3>\n<p>Let’s say we are running an e-commerce store and that we are storing the <em>Customer</em> and <em>Product</em> information within Cassandra. Let’s look at some of the common query patterns around this use case:</p>\n<ol><li>Get <em>Customer</em> info</li>\n<li>Get <em>Product</em> info</li>\n<li>Get all <em>Customers</em> who like a given <em>Product</em></li>\n<li>Get all <em>Products</em> a given <em>Customer</em> likes</li>\n</ol><p>We will start by using separate tables for storing the <em>Customer</em> and <em>Product</em> information. However, we need to introduce a fair amount of denormalization to support the 3rd and 4th queries shown above.</p>\n<p>We will create two more tables to achieve this – “<em>Customer_by_Product</em>” and “<em>Product_by_Customer</em>“.</p>\n<p>Let’s look at the Cassandra table schema for this example:</p>\n<pre class=\"brush: sql; gutter: true\">CREATE TABLE Customer (&#13;\n  cust_id text,&#13;\n  first_name text, &#13;\n  last_name text,&#13;\n  registered_on timestamp, &#13;\n  PRIMARY KEY (cust_id));&#13;\n&#13;\nCREATE TABLE Product (&#13;\n  prdt_id text,&#13;\n  title text,&#13;\n  PRIMARY KEY (prdt_id));&#13;\n&#13;\nCREATE TABLE Customer_By_Liked_Product (&#13;\n  liked_prdt_id text,&#13;\n  liked_on timestamp,&#13;\n  title text,&#13;\n  cust_id text,&#13;\n  first_name text, &#13;\n  last_name text, &#13;\n  PRIMARY KEY (prdt_id, liked_on));&#13;\n&#13;\nCREATE TABLE Product_Liked_By_Customer (&#13;\n  cust_id text, &#13;\n  first_name text,&#13;\n  last_name text,&#13;\n  liked_prdt_id text, &#13;\n  liked_on timestamp,&#13;\n  title text,&#13;\n  PRIMARY KEY (cust_id, liked_on));</pre>\n<p>Note: To support both the queries, recently-liked products by a given customer and customers who recently liked a given product, we have used the “<em>liked_on</em>” column as a clustering key.</p>\n<p>Let’s look at the query to find the ten Customers who most recently liked the product “<em>Pepsi</em>“:</p>\n<pre class=\"brush: sql; gutter: true\">SELECT * FROM Customer_By_Liked_Product WHERE title = \"Pepsi\" LIMIT 10</pre>\n<p>And let’s see the query that finds the recently-liked products (up to ten) by a customer named “<em>Anna</em>“:</p>\n<pre class=\"brush: sql; gutter: true\">SELECT * FROM Product_Liked_By_Customer &#13;\n  WHERE first_name = \"Anna\" LIMIT 10</pre>\n<h2><strong>6. Inefficient Query Patterns</strong></h2>\n<p>Due to the way that Cassandra stores data, some query patterns are not at all efficient, including the following:</p>\n<ul><li><strong>Fetching data from multiple partitions</strong> – this will require a coordinator to fetch the data from multiple nodes, store it temporarily in heap, and then aggregate the data before returning results to the user</li>\n<li><strong>Join-based queries</strong> – due to its distributed nature, Cassandra does not support table joins in queries the same way a relational database does, and as a result, <strong>queries with</strong> <strong>joins will be slower and can also lead to inconsistency and availability issues</strong></li>\n</ul><h2><strong>7. Conclusion</strong></h2>\n<p>In this tutorial, we have covered several best practices around how to approach data modeling in Cassandra.</p>\n<p>Understanding the core concepts and identifying the query patterns in advance is necessary for designing a correct data model that gets the best performance from a Cassandra cluster.</p>\n<div class=\"short_box short_end\">\n<h3><b>I just announced the new Spring 5 modules in REST With Spring: </b></h3>\n<p><strong><a href=\"http://www.baeldung.com/rest-with-spring-course#new-modules\">&gt;&gt; CHECK OUT THE LESSONS</a></strong></p>\n</div>",
        "created_at": "2018-07-17T23:23:57+0000",
        "updated_at": "2018-07-17T23:24:06+0000",
        "published_at": "2017-07-22T16:36:05+0000",
        "published_by": [
          "by \n\nbaeldung"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 6,
        "domain_name": "www.baeldung.com",
        "preview_picture": "http://www.baeldung.com/wp-content/uploads/2016/10/social-Persistence-On-Baeldung-1.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10874"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 603,
            "label": "book",
            "slug": "book"
          },
          {
            "id": 854,
            "label": "gitbook",
            "slug": "gitbook"
          }
        ],
        "is_public": false,
        "id": 10873,
        "uid": null,
        "title": "Introduction | Introduction To Cassandra",
        "url": "https://pandaforme.gitbooks.io/introduction-to-cassandra/content/",
        "content": "<p>This book is a basic introduction to Cassandra. The major information and knowledge came from <a href=\"http://datastax.com/\" target=\"_blank\">http://datastax.com/</a>.</p><p>I will also comment some concepts based on realistic examples and can understand those concepts clearly.</p>",
        "created_at": "2018-07-17T23:21:11+0000",
        "updated_at": "2018-07-17T23:21:19+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "pandaforme.gitbooks.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10873"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10871,
        "uid": null,
        "title": "Understand the Cassandra data model",
        "url": "https://pandaforme.gitbooks.io/introduction-to-cassandra/content/understand_the_cassandra_data_model.html",
        "content": "<p>The Cassandra data model defines  </p><ul><li>Column family as a way to store and organize data </li>\n<li>Table as a two-dimensional view of a multi-dimensional column family </li>\n<li>Operations on tables using the Cassandra Query Language (CQL) </li>\n</ul><p>Cassandra1.2+reliesonCQLschema,concepts,andterminology, though the older Thrift API remains available</p><table><thead><tr><th>Table (CQL API terms)</th>\n<th>Column Family (Thrift API terms)</th>\n</tr></thead><tbody><tr><td>Table is a set of partitions</td>\n<td>Column family is a set of rows</td>\n</tr><tr><td>Partition may be single or multiple row</td>\n<td>Row may be skinny or wide</td>\n</tr><tr><td>Partition key uniquely identifies a partition, and may be simple or composite</td>\n<td>Row key uniquely identifies a row, and may be simple or composite</td>\n</tr><tr><td>Column uniquely identifies a cell in a partition, and may be regular or clustering</td>\n<td>Column key uniquely identies a cell in a row, and may be simple or composite</td>\n</tr><tr><td>Primary key is comprised of a partition key plus clustering columns, if any, and uniquely identifies a row in both its partition and table</td>\n<td>\n</td></tr></tbody></table><h2 id=\"row-partition\">Row (Partition)</h2><p>Row is the smallest unit that stores related data in Cassandra  </p><ul><li>Rows: individual rows constitute a column family </li>\n<li>Row key: uniquely identifies a row in a column family </li>\n<li>Row: stores pairs of column keys and column values </li>\n<li>Column key: uniquely identifies a column value in a row  </li>\n<li>Column value: stores one value or a collection of values </li>\n</ul><p><img src=\"https://pandaforme.gitbooks.io/introduction-to-cassandra/content/Screen%20Shot%202016-02-24%20at%2011.46.09.png\" alt=\"\" /></p><p>Rows may be described as <code>skinny</code> or <code>wide</code> </p><ul><li>Skinny row: has a fixed, relatively small number of column keys </li>\n<li>Wide row: has a relatively large number of column keys (hundreds or\nthousands); this number may increase as new data values are inserted</li>\n</ul><h2 id=\"key-partition-key\">Key (Partition Key)</h2><h3 id=\"composite-row-key\">Composite row key</h3><p>multiple components separated by colon\n<img src=\"https://pandaforme.gitbooks.io/introduction-to-cassandra/content/Screen%20Shot%202016-02-24%20at%2011.53.17.png\" alt=\"\" /></p><h3 id=\"composite-column-key\">Composite column key</h3><p>multiple components separated by colon \n<img src=\"https://pandaforme.gitbooks.io/introduction-to-cassandra/content/Screen%20Shot%202016-02-24%20at%2011.54.01.png\" alt=\"\" /></p><h2 id=\"column-family-table\">Column family (Table)</h2><p>set of rows with a similar structure<br /><img src=\"https://pandaforme.gitbooks.io/introduction-to-cassandra/content/Screen%20Shot%202016-02-24%20at%2011.56.28.png\" alt=\"\" /></p><p><img src=\"https://pandaforme.gitbooks.io/introduction-to-cassandra/content/Screen%20Shot%202016-02-24%20at%2012.24.12.png\" alt=\"\" /></p><h2 id=\"table-with-multirow-partitions\">Table with multi-row partitions</h2><p><img src=\"https://pandaforme.gitbooks.io/introduction-to-cassandra/content/Screen%20Shot%202016-02-24%20at%2012.25.54.png\" alt=\"\" /></p>",
        "created_at": "2018-07-17T23:20:42+0000",
        "updated_at": "2018-07-17T23:20:46+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "pandaforme.gitbooks.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10871"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10870,
        "uid": null,
        "title": "Cassandra Database Tutorial for Beginners: Learn in 3 Days",
        "url": "https://www.guru99.com/cassandra-tutorial.html",
        "content": "<p> Cassandra is a distributed database management system designed for handling a high volume of structured data across commodity servers. In this tutorial, you will see the various concept of Cassandra like data modeling, clusters, monitoring tool, query language, etc. </p><p> Not necessary but knowledge of other database management system like<a href=\"https://www.guru99.com/hbase-tutorials.html\"> HBase</a> or <a href=\"https://www.guru99.com/mongodb-tutorials.html\">MongoDB</a> will be of help. </p><h2><br />Here is what we cover in the Course</h2><table class=\"table\"><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/download-install-cassandra.html\" title=\"Download &amp; Install Casandra on Windows : A Step By Step Guide\"><strong> Tutorial</strong></a></td> <td>Download &amp; Install Casandra on Windows : A Step By Step Guide</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-architecture.html\" title=\"Cassandra Architecture &amp; Replication Factor Strategy Tutorial\"><strong> Tutorial</strong></a></td> <td>Cassandra Architecture &amp; Replication Factor Strategy Tutorial</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-data-model-rules.html\" title=\"Learn Cassandra Data Modeling  with Simple Example\"><strong> Tutorial</strong></a></td> <td>Learn Cassandra Data Modeling with Simple Example</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-cql.html\" title=\"Create, Alter &amp; Drop Keyspace in Cassandra: Complete Tutorial\"><strong> Tutorial</strong></a></td> <td>Create, Alter &amp; Drop Keyspace in Cassandra: Complete Tutorial</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-table-create-alter-drop-truncate.html\" title=\"Cassandra Table:  Create, Alter, Drop &amp; Truncate\"><strong> Tutorial</strong></a></td> <td>Cassandra Table: Create, Alter, Drop &amp; Truncate</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-query-language-cql-insert-update-delete-read-data.html\" title=\"Cassandra Query Language(CQL): Insert, Update, Delete, Read Data\"><strong> Tutorial</strong></a></td> <td>Cassandra Query Language(CQL): Insert, Update, Delete, Read Data</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/create-drop-index-in-cassandra-complete-tutorial.html\" title=\"Create &amp; Drop INDEX in Cassandra: Complete Tutorial\"><strong> Tutorial</strong></a></td> <td>Create &amp; Drop INDEX in Cassandra: Complete Tutorial</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-data-types-expiration-tutorial.html\" title=\"Cassandra Data Types &amp; Expiration Tutorial\"><strong> Tutorial</strong></a></td> <td>Cassandra Data Types &amp; Expiration Tutorial</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-collections-tutorial-set-list-map.html\" title=\"Cassandra Collections Tutorial - SET, LIST &amp; MAP\"><strong> Tutorial</strong></a></td> <td>Cassandra Collections Tutorial - SET, LIST &amp; MAP</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-cluster.html\" title=\"Cassandra Cluster Setup on Multiple  Nodes (Machines)\"><strong> Tutorial</strong></a></td> <td>Cassandra Cluster Setup on Multiple Nodes (Machines)</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-devcenter-opscenter.html\" title=\"Datastax DevCenter &amp; OpsCenter Installation Complete Guide\"><strong> Tutorial</strong></a></td> <td>Datastax DevCenter &amp; OpsCenter Installation Complete Guide</td> </tr><tr><td class=\"responsivetable\"><a href=\"https://www.guru99.com/cassandra-security.html\" title=\"Cassandra SECURITY - Create User  &amp; Authentication With JMX\"><strong> Tutorial</strong></a></td> <td>Cassandra SECURITY - Create User &amp; Authentication With JMX</td> </tr></table><h2>What is Apache Cassandra?</h2><p>Apache Cassandra is highly scalable, distributed and high-performance NoSQL database. Cassandra is designed to handle a huge amount of data. </p><p><a href=\"https://cdn.guru99.com/images/cassandra/021116_0444_WhatisApach1.png\" class=\"jh-image-popup-colorbox\"><img title=\"What is Apache Cassandra?\" alt=\"Cassandra Database Tutorial for Beginners: Learn in 3 Days\" src=\"https://cdn.guru99.com/images/cassandra/021116_0444_WhatisApach1.png\" /></a> </p><p>In the image above, circles are Cassandra nodes and lines between the circles shows distributed architecture, while the client is sending data to the node. </p><p>Cassandra handles the huge amount of data with its distributed architecture. Data is placed on different machines with more than one replication factor that provides high availability and no single point of failure. </p><h2>Cassandra History</h2><ul><li>Cassandra was first developed at Facebook for inbox search.</li><li>Facebook open sourced it in July 2008. </li><li>Apache incubator accepted Cassandra in March 2009.</li><li>Cassandra is a top level project of<a href=\"https://www.guru99.com/apache.html\"> Apache </a>since February 2010.</li><li>The latest version of Apache Cassandra is 3.2.1.</li></ul><p>First let's understand what NoSQL database is. </p><h2>Nosql Cassandra Database</h2><p>NoSQL databases are called \"Not Only SQL\" or \"Non-relational\" databases. NoSQL databases store and retrieve data other than tabular relations such as relation databases. </p><p>NoSQL databases include MongoDB, HBase, and Cassandra.</p><p>There are following properties of NoSQL databases. </p><ul><li>Design Simplicity</li><li>Horizontal Scaling</li><li>High Availability</li></ul><p>Data structures used in Cassandra are more specified than data structures used in relational databases. Cassandra data structures are faster than relational database structures. </p><p>NoSQL databases are increasingly used in Big Data and real-time web applications. NoSQL databases are sometimes called Not Only<a href=\"https://www.guru99.com/sql.html\"> SQL </a>i.e. they may support SQL-like query language. </p><iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/qUV2j3XBRHc\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\"> </iframe><h2>Nosql Cassandra Database Vs Relational databases</h2><p>Here are the differences between relation databases and NoSQL databases in a tabular format. </p><table class=\"table table-striped\"><tr><td><strong>Relational Database</strong> </td><td><strong>NoSQL Database</strong> </td></tr><tr><td>Handles data coming in low velocity </td><td>Handles data coming in high velocity </td></tr><tr><td>Data arrive from one or few locations </td><td>Data arrive from many locations </td></tr><tr><td>Manages structured data </td><td>Manages structured unstructured and semi-structured data. </td></tr><tr><td>Supports complex transactions (with joins) </td><td>Supports simple transactions </td></tr><tr><td>single point of failure with failover </td><td>No single point of failure </td></tr><tr><td>Handles data in the moderate volume. </td><td>Handles data in very high volume </td></tr><tr><td>Centralized deployments </td><td>Decentralized deployments </td></tr><tr><td>Transactions written in one location </td><td>Transaction written in many locations </td></tr><tr><td>Gives read scalability </td><td>Gives both read and write scalability </td></tr><tr><td>Deployed in vertical fashion </td><td>Deployed in Horizontal fashion </td></tr></table><h2>Apache Cassandra Features</h2><p>There are following features that Cassandra provides.</p><ul><li><strong>Massively Scalable Architecture: </strong>Cassandra has a masterless design where all nodes are at the same level which provides operational simplicity and easy scale out.</li><li><strong>Masterless Architecture: </strong>Data can be written and read on any node.</li><li><strong>Linear Scale Performance: </strong>As more nodes are added, the performance of Cassandra increases.</li><li><strong>No Single point of failure: </strong>Cassandra replicates data on different nodes that ensures no single point of failure.</li><li><strong>Fault Detection and Recovery: </strong>Failed nodes can easily be restored and recovered.</li><li><strong>Flexible and Dynamic Data Model: </strong>Supports datatypes with Fast writes and reads.</li><li><strong>Data Protection: </strong>Data is protected with commit log design and build in security like backup and restore mechanisms.</li><li><strong>Tunable Data Consistency: </strong>Support for strong data consistency across distributed architecture.</li><li><strong>Multi Data Center Replication: </strong>Cassandra provides feature to replicate data across multiple data center.</li><li><strong>Data Compression: </strong>Cassandra can compress up to 80% data without any overhead.</li><li><strong>Cassandra Query language: </strong>Cassandra provides query language that is similar like SQL language. It makes very easy for relational database developers moving from relational database to Cassandra.</li></ul><h2>Cassandra Use Cases/Application</h2><p>Cassandra is a non-relational database that can be used for different types of applications. Here are some use cases where Cassandra should be preferred. </p><ul><li><strong>Messaging</strong><p>Cassandra is a great database for the companies that provides<a href=\"https://www.guru99.com/mobile-testing.html\"> Mobile </a>phones and messaging services. These companies have a huge amount of data, so Cassandra is best for them. </p></li><li><strong>Internet of things Application</strong><p>Cassandra is a great database for the applications where data is coming at very high speed from different devices or sensors. </p></li><li><strong>Product Catalogs and retail apps</strong><p>Cassandra is used by many retailers for durable shopping cart protection and fast product catalog input and output. </p></li><li><strong>Social Media Analytics and recommendation engine</strong><p>Cassandra is a great database for many online companies and social media providers for analysis and recommendation to their customers. </p></li></ul><p id=\"slidetag\"> </p>",
        "created_at": "2018-07-17T23:20:01+0000",
        "updated_at": "2018-07-17T23:20:07+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_GB",
        "reading_time": 4,
        "domain_name": "www.guru99.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10870"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10869,
        "uid": null,
        "title": "Learn Cassandra Data Modeling with Simple Example",
        "url": "https://www.guru99.com/cassandra-data-model-rules.html",
        "content": "<p>Although Cassandra query language resembles with<a href=\"https://www.guru99.com/sql.html\"> SQL </a>language, their data modelling methods are totally different. </p><p>In Cassandra, a bad data model can degrade performance, especially when users try to implement the RDBMS concepts on Cassandra. It is best to keep in mind few rules detailed below. </p><p>In this tutorial, you will learn- </p><ul><li><a href=\"#1\">Cassandra Data Model Rules</a></li> <li><a href=\"#2\">Model Your Data in Cassandra</a></li> <li><a href=\"#3\">Handling One to One Relationship</a></li> <li><a href=\"#4\">Handling one to many relationships</a></li> <li><a href=\"#5\">Handling Many to Many Relationship</a></li> </ul><h2>Cassandra Data Model Rules</h2><p>In Cassandra, writes are not expensive. Cassandra does not support joins, group by, OR clause, aggregations, etc. So you have to store your data in such a way that it should be completely retrievable. So these rules must be kept in mind while modelling data in Cassandra. </p><ol><li><strong>Maximize the number of writes</strong><p>In Cassandra, writes are very cheap. Cassandra is optimized for high write performance. So try to maximize your writes for better read performance and data availability. There is a tradeoff between data write and data read. So, optimize you data read performance by maximizing the number of data writes.</p></li> <li><strong>Maximize Data Duplication</strong><p>Data denormalization and data duplication are defacto of Cassandra. Disk space is not more expensive than memory, CPU processing and IOs operation. As Cassandra is a distributed database, so data duplication provides instant data availability and no single point of failure.</p></li> </ol><p><strong>Data Modeling Goals </strong> </p><p>You should have following goals while modelling data in Cassandra. </p><ol><li><strong>Spread Data Evenly Around the Cluster</strong><p>You want an equal amount of data on each node of Cassandra cluster. Data is spread to different nodes based on partition keys that is the first part of the primary key. So, try to choose integers as a primary key for spreading data evenly around the cluster.</p></li> <li><strong>Minimize number of partitions read while querying data</strong><p>Partition are a group of records with the same partition key. When the read query is issued, it collects data from different nodes from different partitions. </p><p>If there will be many partitions, then all these partitions need to be visited for collecting the query data. </p><p>It does not mean that partitions should not be created. If your data is very large, you can’t keep that huge amount of data on the single partition. The single partition will be slowed down. </p><p>So try to choose a balanced number of partitions. </p></li> </ol><p><strong>Good Primary Key</strong> </p><p>Let’s take an example and find which primary key is good. </p><p>Here is the table MusicPlaylist. </p><pre>&#13;\nCreate table MusicPlaylist&#13;\n    (&#13;\n        SongId int,&#13;\n        SongName text,&#13;\n        Year int,&#13;\n        Singer text,&#13;\n        Primary key(SongId, SongName)&#13;\n    );&#13;\n</pre><p>In above example, table MusicPlaylist, </p><ul><li>Songid is the partition key, and </li> <li>SongName is the clustering column</li> <li>Data will be clustered on the basis of SongName. Only one partition will be created with the SongId. There will not be any other partition in the table MusicPlaylist. </li> </ul><p>Data retrieval will be slow by this data model due to the bad primary key. </p><p>Here is another table MusicPlaylist. </p><pre>&#13;\nCreate table MusicPlaylist&#13;\n    (&#13;\n        SongId int,&#13;\n        SongName text,&#13;\n        Year int,&#13;\n        Singer text,&#13;\n        Primary key((SongId, Year), SongName)&#13;\n    );&#13;\n</pre><p>In above example, table MusicPlaylist, </p><ul><li>Songid and Year are the partition key, and </li> <li>SongName is the clustering column. </li> <li>Data will be clustered on the basis of SongName. In this table, each year, a new partition will be created. All the songs of the year will be on the same node. This primary key will be very useful for the data. </li> </ul><p>Our data retrieval will be fast by this data model. </p><h2>Model Your Data in Cassandra</h2><p>Following things should be kept in mind while modelling your queries. </p><ol><li><strong>Determine what queries you want to support</strong></li>             </ol><ul><li>Joins</li> <li>Group by</li> <li>Filtering on which column etc.</li> </ul><strong>Create table according to your queries</strong><p>Create table according to your queries. Create a table that will satisfy your queries. Try to create a table in such a way that a minimum number of partitions needs to be read.</p><h2>Handling One to One Relationship</h2><p>One to one relationship means two tables have one to one correspondence. For example, the student can register only one course, and I want to search on a student that in which course a particular student is registered in. </p><p>So in this case, your table schema should encompass all the details of the student in corresponding to that particular course like the name of the course, roll no of the student, student name, etc. </p><pre>&#13;\nCreate table Student_Course&#13;\n    (&#13;\n        Student rollno int primary key,&#13;\n        Student_name text,&#13;\n        Course_name text,&#13;\n    );&#13;\n</pre><h2>Handling one to many relationships</h2><p>One to many relationships means having one to many correspondence between two tables. </p><p>For example, a course can be studied by many students. I want to search all the students that are studying a particular course. </p><p>So by querying on course name, I will have many student names that will be studying a particular course. </p><pre>&#13;\nCreate table Student_Course&#13;\n    (&#13;\n        Student_rollno int,&#13;\n        Student_name text,&#13;\n        Course_name text,&#13;\n    );&#13;\n</pre><p>I can retrieve all the students for a particular course by the following query. </p><pre>&#13;\nSelect * from Student_Course where Course_name='Course Name';&#13;\n</pre><h2>Handling Many to Many Relationship</h2><p>Many to many relationships means having many to many correspondence between two tables. </p><p>For example, a course can be studied by many students, and a student can also study many courses. </p><p>I want to search all the students that are studying a particular course. Also, I want to search all the course that a particular student is studying. </p><p>So in this case, I will have two tables i.e. divide the problem into two cases. </p><p>First, I will create a table by which you can find courses by a particular student. </p><pre>&#13;\nCreate table Student_Course&#13;\n    (&#13;\n        Student_rollno int primary key,&#13;\n        Student_name text,&#13;\n        Course_name text,&#13;\n    );&#13;\n</pre><p>I can find all the courses by a particular student by the following query. </p><pre>&#13;\nSelect * from Student_Course where student_rollno=rollno;&#13;\n</pre><p>Second, I will create a table by which you can find how many students are studying a particular course. </p><pre>&#13;\nCreate table Course_Student&#13;\n    (&#13;\n        Course_name text primary key,&#13;\n        Student_name text,&#13;\n        student_rollno int&#13;\n    );&#13;\n</pre><p>I can find a student in a particular course by the following query. </p><pre>&#13;\nSelect * from Course_Student where Course_name=CourseName;&#13;\n</pre><p><strong>Difference between RDBMS and Cassandra Data Modelling</strong></p><table class=\"table table-striped\"><tr><td><p><strong>RDBMS</strong> </p></td><td><p><strong>Cassandra</strong> </p></td></tr><tr><td><p>Stores data in normalized form </p></td><td><p>Stores data in denormalized form </p></td></tr><tr><td><p>Legacy dbms; structured data </p></td><td><p>Wide row store,Dynamic; structured &amp; unstructured data </p></td></tr></table><p><strong>Summary</strong> </p><p>Data modelling in Cassandra is different than other RDBMS databases. Cassandra data modelling has some rules. These rules must be followed for good data modelling. Besides these rules, we saw three different data modelling cases and how to deal with them. </p><p id=\"slidetag\"> </p>",
        "created_at": "2018-07-17T23:19:41+0000",
        "updated_at": "2018-07-24T12:41:48+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_GB",
        "reading_time": 5,
        "domain_name": "www.guru99.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10869"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10868,
        "uid": null,
        "title": "Cassandra Data Modeling: Primary, Clustering, Partition, and Compound Keys - DZone Database",
        "url": "https://dzone.com/articles/cassandra-data-modeling-primary-clustering-partiti",
        "content": "<div class=\"content-html\" itemprop=\"text\"><p>In this post, we are going to discuss the different keys available in Cassandra. The primary key concept in Cassandra is different from relational databases. Therefore, it is worth spending some time to understand it.</p>\n<p>Let's take an example and create a student table which has <code>student_id</code> as a primary key column.</p>\n<h2><strong>1) Primary Key </strong></h2>\n<pre lang=\"text/x-cassandra\">create table person (student_id int primary key, fname text, lname text, \n                     dateofbirth timestamp, email text, phone text );</pre>\n<p>In Cassandra, a table can have a number of rows. Each row is referenced by a primary key, also called the row key. There are a number of columns in a row but the number of columns can vary in different rows.</p>\n<p>For example, one row in a table can have three columns whereas another row in the same table can have 10 columns. It is also important to note that in Cassandra, both column names and values have binary types. That means column names can have binary values, such as strings, timestamps, or an integer, etc. This is different from SQL databases, where each row in a SQL table has a fixed number of columns, and column names can only be text.</p>\n<p>We saw that <code>student_id</code> was used as a row key to refer to <code>person</code> data.</p>\n<h2><strong>2) Compound Primary Key</strong></h2>\n<p>As the name suggests, a compound primary key is comprised of one or more columns that are referenced in the primary key. One component of the compound primary key is called partition key, whereas the other component is called the clustering key. The following are different variations of primary keys. Please note that C1, C2, C3,… and so on represent columns in the table.</p>\n<ul><li><p><strong>C1</strong>: Primary key has only one partition key and no cluster key.</p></li>\n <li><p><strong>(C1, C2)</strong>: Column C1 is a partition key and column C2 is a cluster key.</p></li>\n <li><p><strong>(C1,C2,C3,…)</strong>: Column C1 is a partition key and columns C2, C3, and so on make the cluster key.</p></li>\n <li><p><strong>(C1, (C2, C3,…))</strong>: It is same as 3, i.e., column C1 is a partition key and columns C2,C3,… make the cluster key.</p></li>\n <li><p><strong>((C1, C2,…), (C3,C4,…))</strong>: columns C1, C2 make partition key and columns C3,C4,… make the cluster key.</p></li>\n</ul><p>It is important to note that when the compound key is C1, C2, C3, then the first key, C1, becomes the partition key, and the rest of the keys become part of the cluster key. In order to make composite partition keys, we have to specify keys in parenthesis such as: ( ( C1,C2) , C3, C4). In this case, C1 and C2 are part of the partition keys, and C3 and C4 are part of the cluster key.</p>\n<h2><strong>3) Partition Key</strong></h2>\n<p>The purpose of a partition key is to identify the partition or node in the cluster that stores that row. When data is read or written from the cluster, a function called Partitioner is used to compute the hash value of the partition key. This hash value is used to determine the node/partition which contains that row. For example, rows whose partition key values range from 1000 to 1234 may reside in node A, and rows with partition key values range from 1235 to 2000 may reside in node B, as shown in figure 1. If a row contains partition key whose hash value is 1233 then it will be stored in node A.</p>\n<p><img alt=\"cluster\" class=\"alignnone size-full wp-image-20848 fr-fin fr-dib\" src=\"https://knoldus.files.wordpress.com/2016/10/cluster.gif?w=640\" /></p>\n<h2><strong>4) Clustering Key</strong></h2>\n<p>The purpose of the clustering key is to store row data in a sorted order. The sorting of data is based on columns, which are included in the clustering key. This arrangement makes it efficient to retrieve data using the clustering key.</p>\n<h2><strong>Example</strong></h2>\n<p>To make these concepts clear, we will consider the example of a school system.</p>\n<p>Create a keyspace with replication strategy ‘SimpleStrategy’ and replication_factor 1.</p>\n<pre lang=\"text/x-cassandra\">create keyspace Students_Details with replication = {‘class’ : ‘SimpleStrategy’, ‘replication_factor’:1};</pre>\n<p>Now switch to the students_details keyspace:</p>\n<pre lang=\"text/x-cassandra\">use students_details;\n</pre>\n<p>Check the number of tables present in the keyspace:</p>\n<pre lang=\"text/x-cassandra\">students_details&gt; desc TABLES;</pre>\n<p>We will create a table, <code>student</code> , that contains general information about any student.</p>\n<pre lang=\"text/x-cassandra\">create table student (stuid int, avg_marks float, description text, \n                      primary key (stuid));</pre>\n<p>Type the following insert statements to enter some data into this table.</p>\n<pre lang=\"text/x-cassandra\">insert into student (stuid, avg_marks, description) values (1,25.5,’student 1′);\ninsert into student (stuid, avg_marks, description) values (2,35.5,’student 2′);</pre>\n<p>To view the details just inserted...</p>\n<pre lang=\"text/x-cassandra\">students_details&gt; select * from student;</pre>\n<pre lang=\"text/x-sh\">stuid | avg_marks | description\n——-+———–+————-\n1 |      25.5 |   student 1\n2 |      35.5 |   student 2</pre>\n<p>We can see how Cassandra has stored this data under the hood by using the <code>cassandra-cli</code> tool. Run <code>cassandra-cli</code> in a separate terminal windo.</p>\n<p><strong>Important</strong>: The CLI utility is deprecated and will be removed in Cassandra 3.0. For ease of use and performance, switch from Thrift and CLI to CQL and cqlsh.)</p>\n<p>So if you're using a Cassandra verison above 3.0, then use the below commands.</p>\n<p>Using the <strong>EXPAND</strong> Command in cqlsh , we can view the details info for the queries . EXPAND with no arguments shows the current value of the expanded setting.</p>\n<p><code>cqlsh:students_details&gt; EXPAND</code> </p>\n<p>If expanded output is disabled. Use <code>EXPAND ON</code> to enable it.</p>\n<p>Now view the details inserted above (the studid will be present in a red color in cqlsh, representing the primary key/row key)</p>\n<p><code>cqlsh:students_details&gt; select * from student;</code></p>\n<pre lang=\"text/x-cassandra\">@ Row 1\n————-+———–\nstuid       | 1\navg_marks   | 25.5\ndescription | student 1\n@ Row 2\n————-+———–\nstuid       | 2\navg_marks   | 35.5\ndescription | student 2\n(2 rows)</pre>\n<p>We can see from the above output that the stuid has become the row key, and it identifies individual rows.</p>\n<p><code>cqlsh:students_details&gt; select token(stuid) from student;</code></p>\n<pre lang=\"text/x-sh\">@ Row 1\n———————+———————-\nsystem.token(stuid) | -4069959284402364209\n@ Row 2\n———————+———————-\nsystem.token(stuid) | -3248873570005575792</pre>\n<p>Also, you can see that there are two tokens.</p>\n<p>We can use columns in the primary key to filter data in the select statement. Type the following command in the cqlsh window:</p>\n<p><code>select * from student where stuid = 1;</code> </p>\n<p>Now we will create another table called marks, which records marks of each student every day (say every day, new exams and marks are recorded). Type the following command on cqlsh:</p>\n<pre lang=\"text/x-cassandra\">create table marks(stuid int,exam_date timestamp,marks float, exam_name text, \n                   primary key (stuid,exam_date));</pre>\n<p>This statement creates the marks table with a primary key (stuid , exam_date ). As the primary key has two components, the first component is considered a partition key, and the second component becomes the cluster key. Add some data into the table:</p>\n<pre lang=\"text/x-cassandra\">insert into marks(stuid ,exam_date ,marks ,exam_name) values (1,’2016-11-10′,76 ,’examA’);\ninsert into marks(stuid ,exam_date ,marks ,exam_name) values (1,’2016-11-11′,90 ,’examB’);\ninsert into marks(stuid ,exam_date ,marks ,exam_name) values (1,’2016-11-12′,68 ,’examC’);</pre>\n<p><code>cqlsh:students_details&gt; select * from marks;</code></p>\n<pre lang=\"text/x-cassandra\">@ Row 1\n———–+————————–\nstuid     | 1\nexam_date | 2016-11-09 18:30:00+0000\nexam_name | examA\nmarks     | 76\n@ Row 2\n———–+————————–\nstuid     | 1\nexam_date | 2016-11-10 18:30:00+0000\nexam_name | examB\nmarks     | 90\n@ Row 3\n———–+————————–\nstuid     | 1\nexam_date | 2016-11-11 18:30:00+0000\nexam_name | examC\nmarks     | 68</pre>\n<p>Now, let's see how the partition concept has been applied:</p>\n<p><code>cqlsh:students_details&gt; select token(stuid) from marks;</code></p>\n<pre lang=\"text/x-cassandra\">@ Row 1\n———————+———————-\nsystem.token(stuid) | -4069959284402364209\n@ Row 2\n———————+———————-\nsystem.token(stuid) | -4069959284402364209\n@ Row 3\n———————+———————-\nsystem.token(stuid) | -4069959284402364209</pre>\n<p>We can see all the three rows have the <strong>same partition token</strong>, hence Cassandra stores only one row for each partition key. All the data associated with that partition key is stored as columns in the datastore. The data that we have stored through three different insert statements have the same stuid value, i.e. 1, therefore, all the data is saved in that row as columns, i.e under one partition.</p>\n<p>If you remember, we discussed before that the second component of a primary key is called the clustering key. The role of the clustering key is to group related items together. All the data that is inserted against same clustering key is grouped together.</p>\n<p>In this case, all the columns, such as <strong>exam_name </strong>and<strong> marks</strong>, will be grouped by value in <strong>exam</strong>_<strong>date</strong>, i.e 2016-11-11 18:30:00+0000, by default in ascending order .</p>\n<p>I hope these examples have helped you to clarify some of the concepts of data modeling in Cassandra. Please feel free to leave any comments.</p></div><div class=\"content-html\" itemprop=\"text\"><a>\n                        <img class=\"pub-image\" width=\"420\" itemprop=\"image\" src=\"src\" alt=\"image\" /></a></div>",
        "created_at": "2018-07-17T23:19:29+0000",
        "updated_at": "2018-07-17T23:19:34+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "dzone.com",
        "preview_picture": "https://dz2cdn1.dzone.com/storage/article-thumb/3282924-thumb.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10868"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10867,
        "uid": null,
        "title": "Cassandra Data Modeling Best Practices, Part 2",
        "url": "https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-2/",
        "content": "<p>In the <a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\">first part</a>, we covered a few fundamental practices and walked through a detailed example to help you get started with Cassandra data model design. You can follow Part 2 without reading Part 1, but I recommend glancing over the <a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\">terms and conventions</a> I’m using. If you’re new to Cassandra, I urge you to read <a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\">Part 1</a>.</p><p>September 2014 Update: Readers should note that this article describes data modeling techniques based on Cassandra’s Thrift API.  Please see <a href=\"https://wiki.apache.org/cassandra/DataModel\" target=\"_blank\" rel=\"noopener\">https://wiki.apache.org/cassandra/DataModel</a> for CQL API based techniques.</p><p>August 2015 Update:  Readers can also sign up a free online self-paced course on how to model their data in Apache Cassandra from <a href=\"https://academy.datastax.com/courses/ds220-data-modeling?dxt=blogposting\" target=\"_blank\" rel=\"noopener\">https://academy.datastax.com/courses/ds220-data-modeling?dxt=blogposting</a>.</p><p>Some of the practices listed below might evolve in the future. I’ve provided related JIRA ticket numbers so you can watch any evolution.</p><p>With that, let’s get started with some basic practices!</p><h3>Storing values in column names is perfectly OK</h3><p><em>Leaving column values empty (“valueless” columns) is also OK.</em></p><p>It’s a common practice with Cassandra to store a value (actual data) in the column name (a.k.a. column key), and even to leave the column value field empty if there is nothing else to store. One motivation for this practice is that column names are stored physically sorted, but column values are not.</p><p><strong>Notes:</strong></p><ul><li>The maximum column key (and row key) size is 64KB.  However, don’t store something like ‘item description’ as the column key!</li>\n<li>Don’t use timestamp alone as a column key. You might get colliding timestamps from two or more app servers writing to Cassandra. Prefer timeuuid (<a href=\"http://en.wikipedia.org/wiki/Universally_unique_identifier\" target=\"_blank\" rel=\"noopener\">type-1 uuid</a>) instead.</li>\n<li>The maximum column value size is 2 GB. But becuase there is no streaming and the whole value is fetched in heap memory when requested, limit the size to only a few MBs.  (Large objects are not likely to be supported in the near future – <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-265\" target=\"_blank\" rel=\"noopener\">Cassandra-265</a>. However, the <a href=\"https://github.com/Netflix/astyanax/wiki/Chunked-Object-Store\" target=\"_blank\" rel=\"noopener\">Astyanax</a> client library supports large objects by chunking them.)</li>\n</ul><h3>Leverage wide rows for ordering, grouping, and filtering</h3><p><em>But don’t go too wide.</em></p><p>This goes along with the above practice. When actual data is stored in column names, we end up with wide rows.</p><p><strong>Benefits of wide rows:</strong></p><ul><li>Since column names are stored physically sorted, wide rows enable ordering of data and hence efficient filtering (range scans). You’ll still be able to efficiently look up an individual column within a wide row, if needed.</li>\n<li>If data is queried together, you can group that data up in a single wide row that can be read back efficiently, as part of a single query. As an example, for tracking or monitoring some time series data, we can group data by hour/date/machines/event types (depending on the requirements)  in a single wide row, with each column containing granular data or roll-ups. We can also further group data within a row using super or composite columns as discussed later.</li>\n<li>Wide row column families are heavily used (with composite columns) to build custom indexes in Cassandra.</li>\n<li>As a side benefit, you can de-normalize a one-to-many relationship as a wide row without data duplication. However, I would do this only when data is queried together and you need to optimize read performance.</li>\n</ul><p><strong>Example:</strong></p><p>Let’s say we want to store some event log data and retrieve that data hourly. As shown in the model below, the row key is the hour of the day, the column name holds the time when the event occurred, and the column value contains payload. Note that the row is wide and the events are ordered by time because column names are stored sorted. Granularity of the wide row (for this example, per hour rather than every few minutes) depends on the use case, traffic, and data size, as discussed next.</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/08/widerow.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-2/assets/Uploads/Blog/2012/08/_resampled/ResizedImageWzM4OCwxMzFd/widerow.png\" alt=\"\" width=\"388\" height=\"131\" title=\"\" /></a></p><h3>But not too wide, as a row is never split across nodes:</h3><p>It’s hard to say exactly how wide a wide row should be, partly because it’s dependent upon the use case. But here’s some advice:</p><p><strong>Traffic</strong>: All of the traffic related to one row is handled by only one node/shard (by a single set of replicas, to be more precise). Rows that are too “fat” could cause hot spots in the cluster – usually when the number of rows is smaller than the size of the cluster (hope not!), or when wide rows are mixed with skinny ones, or some rows become hotter than others. However, cluster load balancing ultimately depends on the row key selection; conversely, the row key also defines how wide a row will be. So load balancing is something to keep in mind during design.</p><p><strong>Size</strong>: As a row is not split across nodes, data for a single row must fit on disk within a single node in the cluster. However, rows can be large enough that they don’t have to fit in memory entirely. Cassandra allows 2 billion columns per row. At eBay, we’ve not done any “wide row” benchmarking, but we model data such that we never hit more than a few million columns or a few megabytes in one row (we change the row key granularity, or we split into multiple rows). If you’re interested, <a href=\"http://thelastpickle.com/2011/07/04/Cassandra-Query-Plans/\" target=\"_blank\" rel=\"noopener\">Cassandra Query Plans</a> by Aaron Morton shows some performance concerns with wide rows (but note that the results can change in new releases).</p><p>However, these caveats don’t mean you should not use wide rows; just don’t go extra wide.</p><p><strong>Note:</strong> <a id=\"key-val\" href=\"https://issues.apache.org/jira/browse/CASSANDRA-4176\" target=\"_blank\" rel=\"noopener\">Cassandra-4176</a> might add composite types for row key in CQL as a way to split a wide row into multiple rows. However, a single (physical) row is never split <em>across nodes</em> (and won’t be split across nodes in the future), and is always handled by a single set of replicas. You might also want to track <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-3929\" target=\"_blank\" rel=\"noopener\">Cassandra-3929,</a> which would add row size limits for keeping the most recent <em>n</em> columns in a wide row.</p><h3><strong>Choose the proper row key – it’s your “shard key” </strong></h3><p><em>Otherwise, you’ll end up with hot spots, even with <a href=\"http://www.datastax.com/docs/0.8/cluster_architecture/partitioning#about-the-random-partitioner\" target=\"_blank\" rel=\"noopener\">RandomPartitioner</a>.</em></p><p>Let’s consider again the above example of storing time series event logs and retrieving them hourly. We picked the hour of the day as the row key to keep one hour of data together in a row. But there is an issue: All of the writes will go only to the node holding the row for the current hour, causing a hot spot in the cluster. Reducing granularity from hour to minutes won’t help much, because only one node will be responsible for handling writes for whatever duration you pick. As time moves, the hot spot might also move but it won’t go away!</p><p><strong>Bad row key:  </strong>“ddmmyyhh”</p><p>One way to alleviate this problem is to add something else to the row key – an event type, machine id, or similar value that’s appropriate to your use case.</p><p><strong>Better row key: </strong>“ddmmyyhh|eventtype”</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/08/shardkey.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-2/assets/Uploads/Blog/2012/08/_resampled/ResizedImageWzQ2NiwxMjhd/shardkey.png\" alt=\"\" width=\"466\" height=\"128\" title=\"\" /></a></p><p>Note that now we don’t have global time ordering of events, across all event types, in the column family. However, this may be OK if the data is viewed (grouped) by event type later. If the use case also demands retrieving all of the events (irrespective of type) in time sequence, we need to do a multi-get for all event types for a given time period, and honor the time order when merging the data in the application.</p><p>If you can’t add anything to the row key or if you absolutely need ‘time period’ as a row key, another option is to shard a row into multiple (physical) rows by manually splitting row keys: “ddmmyyhh | 1”, “ddmmyyhh | 2”,… “ddmmyyhh | n”, where <em>n</em> is the number of nodes in the cluster. For an hour window, each shard will now evenly handle the writes; you need to round-robin among them. But reading data for an hour will require multi-gets from all of the splits (from the multiple physical nodes) and merging them in the application. (An assumption here is that RandomPartitioner is used, and therefore that range scans on row keys can’t be done.)</p><h3><strong>Keep read-heavy data separate from write-heavy data<br /></strong></h3><p><em>This way, you can benefit from Cassandra’s off-heap row cache.</em></p><p>Irrespective of caching and even outside the NoSQL world, it’s always a good practice to keep read-heavy and write-heavy data separate because they scale differently.</p><p><strong>Notes:</strong></p><ul><li>A row cache is useful for skinny rows, but harmful for wide rows today because it pulls the entire row into memory. <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-1956\" target=\"_blank\" rel=\"noopener\">Cassandra-1956</a> and <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-2864\" target=\"_blank\" rel=\"noopener\">Cassandra-2864</a> might change this in future releases. However, the practice of keeping read-heavy data separate from write-heavy data will still stand.</li>\n<li>Even if you have lots of data (more than available memory) in a column family but you also have particularly “hot” rows, enabling a row cache might be useful.</li>\n</ul><h3>Make sure column key and row key are unique</h3><p><em>Otherwise, data could get accidentally overwritten.</em></p><ul><li>In Cassandra (a distributed database!), there is no unique constraint enforcement for row key or column key.</li>\n<li>Also, there is no separate update operation (no in-place updates!). It’s always an upsert (mutate) in Cassandra. If you accidentally insert data with an existing row key and column key, the previous column value will be silently overwritten without any error (the change won’t be versioned; the data will be gone).</li>\n</ul><h3>Use the proper comparator and validator</h3><p><em>Don’t just use the default BytesType comparator and validator unless you really need to.</em></p><p>In Cassandra, the data type for a column <em><strong>value </strong></em>(or row key)  is called a <em>Validator</em>. The data type for a column <em><strong>name</strong></em> is called a <em>Comparator</em>.  Although Cassandra does not require you to define both, you must at least specify the comparator unless your column family is static (that is, you’re not storing actual data as part of the column name), or unless you really don’t care about the sort order.</p><ul><li>An improper comparator will sort column names inappropriately on the disk. It will be difficult (or impossible) to do range scans on column names later.</li>\n<li>Once defined, you can’t change a comparator without rewriting all data. However, the validator can be changed later.</li>\n</ul><p>See <a href=\"http://www.datastax.com/docs/1.0/ddl/column_family#about-data-types-comparators-and-validators\" target=\"_blank\" rel=\"noopener\">comparators and validators</a> in the Cassandra documentation for the supported data types.</p><h3>Keep the column name short</h3><p><em>Because it’s stored <em>repeatedly.</em><br /></em></p><p>This practice doesn’t apply if you use the column name to store actual data. Otherwise, keep the column name short, since it’s repeatedly stored with each column value. Memory and storage overhead can be significant when the size of the column value is not much larger than the size of the column name – or worse, when it’s smaller.</p><p>For example, favor ‘fname’ over ‘firstname’, and ‘lname’ over ‘lastname’.</p><p><strong>Note:</strong> <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-4175\" target=\"_blank\" rel=\"noopener\">Cassandra-4175</a> might make this practice obsolete in the future.</p><h3>Design the data model such that operations are idempotent</h3><p><em>Or, make sure that your use case can live with inaccuracies or that inaccuracies can be corrected eventually.</em></p><p>In an eventually consistent and fully distributed system like Cassandra, idempotent operations can help – a lot. Idempotent operations allow partial failures in the system, as the operations can be retried safely without changing the final state of the system. In addition, idempotency can sometimes alleviate the need for strong consistency and allow you to work with eventual consistency without causing data duplication or other anomalies. Let’s see how these principles apply in Cassandra. I’ll discuss partial failures only, and leave out alleviating the need for strong consistency until an upcoming post, as it is very much dependent on the use case.</p><p>Because of  Cassandra’s fully distributed (and multi-master) nature, write failure does not guarantee that data is not written, unlike the behavior of relational databases. In other words, even if the client receives a failure for a write operation, data might be written to one of the replicas, which will eventually get propagated to all replicas. No rollback or cleanup is performed on partially written data. Thus, a perceived write failure can result in a successful write eventually. So, retries on write failure can yield unexpected results if your model isn’t update idempotent.</p><p><strong>Notes: </strong></p><ul><li>“Update idempotent” here means a model where operations are idempotent. An operation is called idempotent if it can be applied one time or multiple times with the same result.</li>\n<li>In most cases, idempotency won’t be a concern, as writes into regular column families are always update idempotent. The exception is with the Counter column family, as shown in the example below. However, sometimes your use case can model data such that write operations are not update idempotent from the use case perspective. For instance, in <a href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/\">part 1</a>, User_by_Item and Item_by_User in the final model are not update idempotent if the use case operation ‘user likes item’ gets executed multiple times, as the timestamp might differ for each like. However, note that a specific instance of  the use case operation ‘user likes item’ is still idempotent, and so can be retried multiple times in case of failures. As this is more use-case specific, I might elaborate more in future posts.</li>\n<li>Even with a consistency level ONE, write failure does not guarantee data is not written; the data still could get propagated to all replicas eventually.</li>\n</ul><p><strong>Example</strong></p><p>Suppose that we want to count the number of users who like a particular item. One way is to use the <a href=\"http://www.datastax.com/dev/blog/whats-new-in-cassandra-0-8-part-2-counters\" target=\"_blank\" rel=\"noopener\">Counter</a> column family supported by Cassandra to keep count of users per item. Since the counter increment (or decrement) is not update idempotent, retry on failure could yield an over-count if the previous increment was successful on at least one node. One way to make the model update idempotent is to maintain a list of user ids instead of incrementing a count, as shown below. Whenever a user likes an item, we write that user’s id against the item; if the write fails, we can safely retry. To determine the count of all users who like an item, we read all user ids for the item and count manually.</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/08/idempotency2.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-2/assets/Uploads/Blog/2012/08/_resampled/ResizedImageWzU4MiwxNTld/idempotency2.png\" alt=\"\" width=\"582\" height=\"159\" title=\"\" /></a></p><p>In the above update idempotent model, getting the counter value requires reading all user ids, which will not perform well (there could be millions). If reads are heavy on the counter and you can live with an approximate count, the counter column will be efficient for this use case. If needed, the counter value can be corrected periodically by counting the user ids from the update idempotent column family.</p><p><strong>Note:</strong> <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-2495\" target=\"_blank\" rel=\"noopener\">Cassandra-2495</a> might add a proper retry mechanism for counters in the case of a failed request. However, in general, this practice will continue to hold true. So make sure to always litmus-test your model for update idempotency.</p><h3><strong>Model data around transactions, if needed</strong></h3><p><em>But this might not always be possible, depending on the use case.</em></p><p>Cassandra has no multi-row, cluster-wide transaction or rollback mechanism; instead, it offers row-level atomicity. In other words, a single mutation operation of columns for a given row key is atomic. So if you need transactional behavior, try to model your data such that you would only ever need to update a single row at once. However, depending on the use case, this is not always doable. Also, if your system needs ACID transactions, you might re-think your database choice.</p><p><strong>Note:</strong> <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-4285\" target=\"_blank\" rel=\"noopener\">Cassandra-4285</a> might add an atomic, eventually consistent batch operation.</p><h3>Decide on the proper TTL up front, if you can<br /></h3><p><em>Because it’s hard to change TTL for existing data.</em></p><p>In Cassandra, TTL (time to live) is not defined or set at the column family level. It’s set per column value, and once set it’s hard to change; or, if not set, it’s hard to set for existing data. The only way to change the TTL for existing data is to read and re-insert all the data with a new TTL value. So think about your purging requirements, and if possible set the proper TTL for your data upfront.</p><p><strong>Note:</strong> <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-3974\" target=\"_blank\" rel=\"noopener\">Cassandra-3974</a> might introduce TTL for the column family, separate from column TTL.</p><h3>Don’t use the Counter column family to generate surrogate keys<br /></h3><p><em>Because it’s not intended for this purpose.<br /></em></p><p>The Counter column family holds distributed counters meant (of course) for distributed counting. Don’t try to use this CF to generate sequence numbers for surrogate keys, like Oracle sequences or MySQL auto-increment columns. You will receive duplicate sequence numbers! Most of the time you really don’t need globally sequential numbers. Prefer timeuuid (<a href=\"http://en.wikipedia.org/wiki/Universally_unique_identifier\" target=\"_blank\" rel=\"noopener\">type-1 uuid</a>) as surrogate keys. If you truly need a globally sequential number generator, there are a few possible mechanisms; but all will require centralized coordination, and thus can impact the overall system’s scalability and availability.</p><h3>Favor composite columns over super columns</h3><p><em>Otherwise, you might hit performance bottlenecks with super columns.</em></p><p>A super column in Cassandra can be used to group column keys, or to model a two-layer hierarchy. However, super columns have the following implementation issues and are therefore becoming less favorable.</p><p><strong>Issues:</strong></p><ul><li>Sub-columns of a super column are not indexed. Reading one sub-column de-serializes all sub-columns.</li>\n<li>Built-in secondary indexing does not work with sub-columns.</li>\n<li>Super columns cannot encode more than two layers of hierarchy.</li>\n</ul><p>Similar (even better) functionality can be achieved by the use of the Composite column. It’s a regular column with sub-columns encoded in it. Hence, all of the benefits of regular columns, such as sorting and range scans, are available; and you can encode more than two layers of hierarchy.</p><p><strong>Note:</strong> <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-3237\" target=\"_blank\" rel=\"noopener\">Cassandra-3237</a> might change the underlying super column implementation to use composite columns. However, composite columns will still remain preferred over super columns.</p><h3>The order of sub-columns in composite columns matters</h3><p><em>Because order defines grouping.</em></p><p>For example, a composite column key like &lt;state|city&gt; will be stored ordered first by state and then by city, rather than first by city and then by state. In other words, all the cities within a state are located (grouped) on disk together.</p><h3>Favor built-in composite types over manual construction</h3><p><em>Because manual construction doesn’t always work.</em></p><p>Avoid manually constructing the composite column keys using string concatenation (with separators like “:” or “|”). Instead, use the built-in composite types (and comparators) supported by Cassandra 0.8.1 and above.</p><p><strong>Why?</strong></p><ul><li>Manual construction won’t work if sub-columns are of different data types. For example, the composite key &lt;state|zip|timeuuid&gt; will not be sorted in a type-aware fashion (state as string, zip code as integer, and timeuuid as time).</li>\n<li>You can’t reverse the sort order on components in the type – for instance, with the state ascending and the zip code descending in the above key.</li>\n</ul><p><strong>Note:</strong> Cassandra built-in composite types come in two flavors:</p><ul><li><em>Static composite type: </em>Data types for each part of a composite column are predefined per column family.  All the column names/keys within a column family must be of that composite type.</li>\n<li><em>Dynamic composite type: </em>This type allows mixing column names with different composite types in a column family or even in one row.</li>\n</ul><p>Find more information about composite types at <a href=\"http://www.datastax.com/dev/blog/introduction-to-composite-columns-part-1\" target=\"_blank\" rel=\"noopener\">Introduction to composite columns</a>.</p><h3>Favor static composite types over dynamic, whenever possible</h3><p><em>Because dynamic composites are too dynamic.</em></p><p>If all column keys in a column family are of the same composite type, always use static composite types. Dynamic composite types were originally created to keep multiple custom indexes in one column family. If possible, don’t mix different composite types in one row using the dynamic composite type unless absolutely required. <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-3625\" target=\"_blank\" rel=\"noopener\">Cassandra-3625</a> has fixed some serious issues with dynamic composites.</p><p><strong>Note:</strong> CQL 3 supports static composite types for column names via clustered (wide) rows. Find more information about how CQL 3 handles wide rows at <a href=\"http://www.datastax.com/docs/1.1/ddl/column_family#composite-columns\" target=\"_blank\" rel=\"noopener\">DataStax docs</a>.</p><p>Enough for now. I would appreciate your inputs to further enhance these modeling best practices, which guide our Cassandra utilization today.</p><p>—  <a title=\"About me\" href=\"http://www.jaykumarpatel.com\" rel=\"noopener\" target=\"_blank\">Jay Patel</a>, architect@eBay.</p>",
        "created_at": "2018-07-17T23:19:16+0000",
        "updated_at": "2018-07-17T23:19:22+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 16,
        "domain_name": "www.ebayinc.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10867"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10866,
        "uid": null,
        "title": "Cassandra Data Modeling Best Practices, Part 1",
        "url": "https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/",
        "content": "<p>This is the first in a series of posts on Cassandra data modeling, implementation, operations, and related practices that guide our Cassandra utilization at eBay. Some of these best practices we’ve learned from public forums, many are new to us, and a few still are arguable and could benefit from further experience.</p><p>September 2014 Update: Readers should note that this article describes data modeling techniques based on Cassandra’s Thrift API.  Please see <a href=\"https://wiki.apache.org/cassandra/DataModel\" target=\"_blank\" rel=\"noopener\">https://wiki.apache.org/cassandra/DataModel</a> for CQL API based techniques.</p><p>August 2015 Update:  Readers can also sign up a free online self-paced course on how to model their data in Apache Cassandra from <a href=\"https://academy.datastax.com/courses/ds220-data-modeling?dxt=blogposting\" target=\"_blank\" rel=\"noopener\">https://academy.datastax.com/courses/ds220-data-modeling?dxt=blogposting</a>.</p><p>In this part, I’ll cover a few basic practices and walk through a detailed example. Even if you don’t know anything about Cassandra, you should be able to follow almost everything.</p><h3>A few words on Cassandra @ eBay</h3><p>We’ve been trying out Cassandra for more than a year. Cassandra is now serving a handful of use cases ranging from write-heavy logging and tracking, to mixed workload. One of them serves our “Social Signal” project, which enables like/own/want features on eBay product pages. A few use cases have reached production, while more are in development.</p><p>Our Cassandra deployment is not huge, but it’s growing at a healthy pace. In the past couple of months, we’ve deployed dozens of nodes across several small clusters spanning multiple data centers. You may ask, why multiple clusters? We isolate clusters by functional area and criticality. Use cases with similar criticality from the same functional area share the same cluster, but reside in different keyspaces.</p><p>RedLaser, Hunch, and other eBay adjacencies are also trying out Cassandra for various purposes. In addition to Cassandra, we also utilize MongoDB and HBase. I won’t discuss these now, but suffice it to say we believe each has its own merit.</p><p>I’m sure you have more questions at this point. But I won’t tell you the full story yet. At the upcoming <a href=\"http://www.datastax.com/events/cassandrasummit2012\" target=\"_blank\" rel=\"noopener\">Cassandra summit</a>, I’ll go into detail about each use case, the data model, multi-datacenter deployment, lessons learned, and more.</p><p>The focus of this post is Cassandra data modeling best practices that we follow at eBay. So, let’s jump in with a few notes about terminology and representations I’ll be using for each post in this series.</p><h3>Terms and Conventions</h3><ul><li>The terms “Column Name” and “Column Key” are used interchangeably. Similarly, “Super Column Name” and “Super Column Key” are used interchangeably.</li>\n<li>The following layout represents a row in a Column Family (CF):<a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/term1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzU2MSwxMjBd/term1.png\" alt=\"\" width=\"561\" height=\"120\" title=\"\" /></a></li>\n</ul><ul><li>The following layout represents a row in a Super Column Family (SCF):<a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/term2.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/term2.png\" alt=\"\" width=\"561\" title=\"\" /></a></li>\n</ul><ul><li>The following layout represents a row in a Column Family with composite columns. Parts of a composite column are separated by ‘|’. Note that this is just a representation convention; Cassandra’s built-in composite type encodes differently, not using ‘|’. (BTW, this post doesn’t require you to have detailed knowledge of super columns and composite columns.)<img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/term3.png\" alt=\"\" width=\"561\" title=\"\" /></li>\n</ul><p>With that, let’s start with the first practice!</p><h3>Don’t think of a relational table<br /></h3><p><em>Instead, think of a nested, sorted map data structure.</em></p><p>The following relational model analogy is often used to introduce Cassandra to newcomers:</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/analogy.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzUwNCwxNjVd/analogy.png\" alt=\"\" width=\"504\" height=\"165\" title=\"\" /></a></p><p>This analogy helps make the transition from the relational to non-relational world. But don’t use this analogy while designing Cassandra column families. Instead, think of the Cassandra column family as a map of a map: an outer map keyed by a row key, and an inner map keyed by a column key. Both maps are sorted.</p><p><em> <em>SortedMap&lt;RowKey, SortedMap&lt;ColumnKey, ColumnValue&gt;&gt;</em> </em></p><p><strong>Why?</strong></p><p>A nested sorted map is a more accurate analogy than a relational table, and will help you make the right decisions about your Cassandra data model.</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/thinkmap.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzUyMSwxNTRd/thinkmap.png\" alt=\"\" width=\"521\" height=\"154\" title=\"\" /></a></p><p><strong>How?</strong></p><ul><li>A map gives efficient key lookup, and the sorted nature gives efficient scans. In Cassandra, we can use row keys and column keys to do efficient lookups and range scans.</li>\n<li>The number of column keys is unbounded. In other words, you can have wide rows.</li>\n<li>A key can itself hold a value. In other words, you can have a valueless column.</li>\n</ul><p>Range scan on row keys is possible only when data is partitioned in a cluster using <a href=\"http://www.datastax.com/docs/1.1/cluster_architecture/partitioning#about-ordered-partitioners\" target=\"_blank\" rel=\"noopener\">Order Preserving Partitioner (OOP)</a>. OOP is almost never used. So, you can think of the outer map as unsorted:<em><br /></em></p><p><em> <em>Map&lt;RowKey, SortedMap&lt;ColumnKey, ColumnValue&gt;&gt;</em> </em></p><p>As mentioned earlier, there is something called a “Super Column” in Cassandra. Think of this as a grouping of columns, which turns our two nested maps into three nested maps as follows:</p><p><em> <em>Map&lt;RowKey, SortedMap&lt;SuperColumnKey, SortedMap&lt;ColumnKey, ColumnValue&gt;&gt;&gt;</em> </em></p><p><strong>Notes:</strong></p><ul><li>You need to pass the timestamp with each column value, for Cassandra to use internally for conflict resolution. However, the timestamp can be safely ignored during modeling. Also, do not plan to use timestamps as data in your application. They’re not for you, and they do not define new versions of your data (unlike in HBase).</li>\n<li>The Cassandra community has heavily criticized the implementation of Super Column because of performance concerns and the lack of support for secondary indexes. The same “super column like” functionality (or even better) can be achieved by using composite columns.</li>\n</ul><h3>Model column families around query patterns</h3><p><em>But start your design with entities and relationships, if you can.<br /></em></p><ul><li>Unlike in relational databases, it’s not easy to tune or introduce new query patterns in Cassandra by simply creating secondary indexes or building complex SQLs (using joins, order by, group by?) because of its high-scale distributed nature. So think about query patterns up front, and design column families accordingly.</li>\n<li>Remember the lesson of the nested sorted map, and think how you can organize data into that map to satisfy your query requirements of fast look-up/ordering/grouping/filtering/aggregation/etc.</li>\n</ul><p>However, entities and their relationships still matter (unless the use case is special – perhaps storing logs or other time series data?). What if I gave you query patterns to create a Cassandra model for an e-commerce website, but didn’t tell you anything about the entities and relationships? You might try to figure out entities and relationships, knowingly or unknowingly, from the query patterns or from your prior understanding of the domain (because entities and relationships are how we perceive the real world). It’s important to understand and start with entities and relationships, then continue modeling around query patterns by de-normalizing and duplicating. If this sounds confusing, make sure to go through the detailed example later in this post.</p><p><strong>Note:</strong> It also helps to identify the most frequent query patterns and isolate the less frequent. Some queries might be executed only a few thousand times, while others a billion times. Also consider which queries are sensitive to latency and which are not. Make sure your model first satisfies the most frequent and critical queries.</p><h3>De-normalize and duplicate for read performance</h3><p><em>But don’t de-normalize if you don’t need to. It’s all about finding the right balance.<br /></em></p><p>In the relational world, the pros of normalization are well understood: less data duplication, fewer data modification anomalies, conceptually cleaner, easier to maintain, and so on. The cons are also understood: that queries might perform slowly if many tables are joined, etc. The same holds true in Cassandra, but the cons are magnified since it’s distributed and of course there are no joins (since it’s high-scale distributed!). So with a fully normalized schema, reads may perform much worse.</p><p>This and the previous practice (modeling around query patterns) are so important that I would like to further elaborate by devoting the rest of the post to a detailed example.</p><p><strong>Note:</strong> The example discussed below is just for demonstration purposes, and does not represent the data model used for Cassandra projects within eBay.</p><h3>Example: ‘Like’ relationship between User &amp; Item</h3><p>This example concerns the functionality of an e-commerce system where users can like one or more items. One user can like multiple items and one item can be liked by multiple users, leading to a many-to-many relationship as shown in the relational model below:</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/relational.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzU5MCwzNDld/relational.png\" alt=\"\" width=\"590\" height=\"349\" title=\"\" /></a></p><p>For this example, let’s say we would like to query data as follows:</p><ul><li><em>Get user by user id </em></li>\n<li><em>Get item by item id</em></li>\n<li><em>Get all the items that a particular user likes</em></li>\n<li><em>Get all the users who like a particular item </em></li>\n</ul><p>Below are some options for modeling the data in Cassandra, in order of the lowest to the highest de-normalization. The best option depends on the query patterns, as you’ll soon see.</p><h3>Option 1: Exact replica of relational model</h3><p><strong><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/option1.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzUyMCwyNjFd/option1.png\" alt=\"\" width=\"520\" height=\"261\" title=\"\" /></a><br /></strong></p><p>This model supports querying user data by user id and item data by item id. But there is no easy way to query all the items that a particular user likes or all the users who like a particular item.</p><p>This is the worst way of modeling for this use case. Basically, User_Item_Like is not modeled correctly here.</p><p>Note that the ‘timestamp’ column (storing when the user liked the item) is dropped from User_Item_Like for simplicity. I’ll introduce that column later.</p><h3>Option 2: Normalized entities with custom indexes</h3><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/option2.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzU0MSwyNjdd/option2.png\" alt=\"\" width=\"541\" height=\"267\" title=\"\" /></a></p><p>This model has fairly normalized entities, except that user id and item id mapping is stored twice, first by item id and second by user id.</p><p>Here, we can easily query all the items that a particular user likes using Item_By_User, and all the users who like a particular item using User_By_Item. We refer to these column families as custom secondary indexes, but they’re just other column families.</p><p>Let’s say we always want to get the item title in addition to the item id when we query items liked by a particular user. In the current model, we first need to query Item_By_User to get all the item ids that a given user likes; and then for each item id, we need to query Item to get the title. Similarly, let’s say we always want to get all the usernames in addition to user ids when we query users who like a particular item. With the current model, we first need to query User_By_Item to get the ids for all users who like a given item; and then for each user id, we need to query User to get the username. It’s possible that one item is liked by a couple hundred users, or an active user has liked many items — which will cause many additional queries when we look up usernames who like a given item and vice versa. So, it’s better to optimize by de-normalizing item title in Item_by_User, and username in User_by_Item, as shown in option 3.</p><p><strong>Note:</strong> Even if you can batch your reads, they will still be slower because Cassandra (Coordinator node, to be specific) has to query each row separately underneath (usually from different nodes). Batch read will help only by avoiding the round trip — which is good, so you should always try to leverage it.</p><h3>Option 3: Normalized entities with de-normalization into custom indexes</h3><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/option3.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzUyOSwyNjVd/option3.png\" alt=\"\" width=\"529\" height=\"265\" title=\"\" /></a></p><p>In this model, title and username are de-normalized in User_By_Item and Item_By_User respectively. This allows us to efficiently query all the item titles liked by a given user, and all the user names who like a given item. This is a fair amount of de-normalization for this use case.</p><p>What if we want to get all the information (title, desc, price, etc.) about the items liked by a given user? But we need to ask ourselves whether we really need this query, particularly for this use case. We can show all the item titles that a user likes and pull additional information only when the user asks for it (by clicking on a title). So, it’s better not to do extreme de-normalization for this use case. (However, it’s common to show both title and price up front. It’s easy to do; I’ll leave it for you to pursue if you wish.)</p><p>Let’s consider the following two query patterns:</p><ul><li>For a given item id, get all of the item data (title, desc, etc.) along with the names of the users who liked that item.</li>\n<li>For a given user id, get all of the user data along with the item titles liked by that user.</li>\n</ul><p>These are reasonable queries for item detail and user detail pages in an application. Both will perform well with this model. Both will cause two lookups, one to query item data (or user data) and another to query user names (or item titles). As the user becomes more active (starts liking thousands of items?) or the item becomes hotter (liked by a few million users?), the number of lookups will not grow; it will remain constant at two. That’s not bad, and de-normalization may not yield much benefit like we had when moving from option 2 to option 3. However, let’s see how we can optimize further in option 4.</p><h3>Option 4: Partially de-normalized entities</h3><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/option4.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzQzOSwzNjRd/option4.png\" alt=\"\" width=\"439\" height=\"364\" title=\"\" /></a></p><p>Definitely, option 4 looks messy. In terms of savings, it’s not like what we had in option 3.</p><p>If User and Item are highly shared entities (similar to what we have at eBay), I would prefer option 3 over this option.</p><p>We’ve used the term “partially de-normalized” here because we’re not de-normalizing all item data into the User entity or all user data into the Item entity. I won’t even consider showing extreme de-normalization (keeping all item data in User and all user data in Item), as you probably agree that it doesn’t make sense for this use case.</p><p><strong>Note:</strong> I’ve used Super Column here just for demonstration purposes. Almost all the time, you should favor composite columns over Super Column.</p><h3>The best model</h3><p>The winner is Option 3, particularly for this example. We’ve left out timestamp, but let’s include it in the final model below as timeuuid(<a href=\"http://en.wikipedia.org/wiki/Universally_unique_identifier\" target=\"_blank\" rel=\"noopener\">type-1 uuid</a>). Note that timeuuid and userid together form a composite column key in User_By_Item and Item_By_User column families.</p><p>Recall that column keys are physically stored sorted. Here our column keys are stored sorted by timeuuid in both User_By_Item and Item_By_User, which makes range queries on time slots very efficient. With this model, we can efficiently query (via range scans) the <em>most recent</em> users who like a given item and the <em>most recent</em> items liked by a given user, without reading all the columns of a row.</p><p><a href=\"https://www.ebayinc.com/assets/Uploads/Blog/2012/07/optionbest.png\" target=\"_blank\" rel=\"noopener\"><img src=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-1/assets/Uploads/Blog/2012/07/_resampled/ResizedImageWzU1Myw0NTNd/optionbest.png\" alt=\"\" width=\"553\" height=\"453\" title=\"\" /></a></p><h3>Summary</h3><p>We’ve covered a few fundamental practices and walked through a detailed example to help you get started with Cassandra data model design. Here are the key takeaways:</p><ul><li>Don’t think of a relational table, but think of a nested sorted map data structure while designing Cassandra column families.</li>\n<li>Model column families around query patterns. But start your design with entities and relationships, if you can.</li>\n<li>De-normalize and duplicate for read performance. But don’t de-normalize if you don’t need to.</li>\n<li>Remember that there are many ways to model. The best way depends on your use case and query patterns.</li>\n</ul><p>What I’ve not mentioned here are special, but common, use cases such as logging, monitoring, real-time analytics (rollups, counters), or other <a href=\"http://en.wikipedia.org/wiki/Time_series\" target=\"_blank\" rel=\"noopener\">time series</a> data. However, practices discussed here do apply there. In addition, there are known common techniques or patterns used to model these time series data in Cassandra. At eBay, we also use some of those techniques and would love to share about them in upcoming posts. For more information on modeling time series data, I would recommend reading <a href=\"http://www.datastax.com/dev/blog/advanced-time-series-with-cassandra\" target=\"_blank\" rel=\"noopener\">Advanced time series with Cassandra</a> and <a href=\"http://www.datastax.com/dev/blog/metric-collection-and-storage-with-cassandra\" target=\"_blank\" rel=\"noopener\">Metric collection and storage</a>. Also, if you’re new to Cassandra, make sure to scan through <a href=\"http://www.datastax.com/docs/1.1/index\" target=\"_blank\" rel=\"noopener\">DataStax documentation</a> on Cassandra.</p><p>UPDATE:  <a title=\"Cassandra Data Modeling Best Practices, Part 2\" href=\"https://www.ebayinc.com/stories/blogs/tech/cassandra-data-modeling-best-practices-part-2/\">Part 2</a> about Cassandra is now published.</p><p>— <a title=\"About me\" href=\"http://www.jaykumarpatel.com\" rel=\"noopener\" target=\"_blank\">Jay Patel</a>, architect@eBay</p>",
        "created_at": "2018-07-17T23:18:58+0000",
        "updated_at": "2018-07-19T21:10:43+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 13,
        "domain_name": "www.ebayinc.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10866"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10865,
        "uid": null,
        "title": "Patterns of Successful Cassandra Data Modelling - OpenCredo",
        "url": "https://opencredo.com/cassandra-data-modelling-patterns/",
        "content": "<p class=\"p1\">A growing number of clients are asking OpenCredo for help with using <a href=\"http://cassandra.apache.org/\" target=\"_blank\">Apache Cassandra</a> and solving specific problems they encounter. Clients have different use cases, requirements, implementation and teams but experience similar issues. We have noticed that Cassandra data modelling problems are the most consistent cause of Cassandra failing to meet their expectations. Data modelling is one of the most complex areas of using Cassandra and has many considerations. For a business it is essential to invest resources into data modelling from the early stages of Cassandra projects; unlike operational settings that can be tuned, a Cassandra data model is very costly to fix.</p><p class=\"p1\">Cassandra is growing in popularity due to its well-advertised strengths such as high performance, fault tolerance, resilience and scalability covered in a <strong><a href=\"https://opencredo.com/fulfilling-promise-apache-cassandra/\">previous blog post</a></strong> by Guy Richardson. How well these strengths are realised in an application depends heavily on the quality of the underlying Cassandra data model. The main rule for designing a good Cassandra data model is crafting it specifically to your business domain and application use cases.</p><p class=\"p1\">Through years of experience with Cassandra engagements, we have identified a number of data modelling patterns and will cover some of them in this post. Their successful application requires a working knowledge of how Cassandra stores data. Without understanding the underlying architecture you risk making one of the common Cassandra mistakes covered in a separate post. Evaluating a seemingly valid and straightforward data model for potential pitfalls requires a level of expertise in Cassandra internals, the storage format in particular.</p><h2>The Simplicity and Complexity of Cassandra Storage</h2><p>While Cassandra architecture is relatively simple, it fundamentally limits the ways in which you can query it for data. Cassandra is designed to be a high-performance database and discourages inefficient queries. Query efficiency is determined by the way Cassandra stores data; it makes the following query patterns inefficient or impossible:</p><ul><li>fetching <em>all</em> data without identifying a subset by a partition key,</li>\n<li>fetching data from multiple partitions,</li>\n<li>joining distinct data sets,</li>\n<li>searching by values,</li>\n<li>filtering.</li>\n</ul><p>Cassandra expects applications to store the data in such a way that they can retrieve it <em>efficiently</em>. It is therefore up to the client to know the ways it will query Cassandra and design the data model accordingly upfront.</p><h3>Example: Projects by Manager and Turnover</h3><p>Consider an application that records information about <strong>project managers</strong>, their <strong>projects</strong> and project <strong>turnovers</strong>. Even for this intentionally simple use case, there are a many ways you could store the data and a number things to consider to produce a good Cassandra data model. At the very least, Cassandra requires a meaningful <em>key</em> to split data into subsets that you will later use to retrieve data. Without much knowledge of how Cassandra works, the first response might be to store this data in a simplified table:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"sql\"><pre class=\"de1\">CREATE TABLE projects_by_manager (\n  manager text,\n  project_id INT,\n  project_name text,\n  turnover INT,\n  PRIMARY KEY (manager, project_id)\n);</pre></div></div></div></div></div></div></div><p>In this table all data about projects and their turnovers is partitioned by manager. This data model will work if you only want to retrieve all project data for a particular manager. Disappointingly, this is the only type of query this table will support out of the box. Cassandra will not retrieve “<em>all projects with a turnover of 2 000 000″ –</em> the way in which it stored data makes this query inefficient. The reason behind this becomes obvious looking at the format of Cassandra SSTables.</p><h3>Partitioned Row Store</h3><pre>[&#13;\n{<em><strong>\"key\": \"Jack Jones\"</strong></em>,&#13;\n \"cells\": [[\"1:\",\"\",1470229749953090],&#13;\n           [\"1:project_name\",\"Cassandra Tuning\",1470229749953090],&#13;\n           [\"1:turnover\",\"5000000\",1470229749953090],&#13;\n           [\"2:\",\"\",1470229928612372],&#13;\n           [\"2:project_name\",\"Spark Layer\",1470229928612372],&#13;\n           [\"2:turnover\",\"2000000\",1470229928612372]]},&#13;\n{<em><strong>\"key\": \"Jill Hill\"</strong></em>,&#13;\n \"cells\": [[\"1:\",\"\",1470229908473768],&#13;\n           [\"1:project_name\",\"Kubernetes Setup\",1470229908473768],&#13;\n           [\"1:turnover\",\"2000000\",1470229908473768],&#13;\n           [\"2:\",\"\",1470229948844042],&#13;\n           [\"2:project_name\",\"Front End\",1470229948844042],&#13;\n           [\"2:turnover\",\"1000000\",1470229948844042]]},&#13;\n{<em><strong>\"key\": \"Richard Ford\"</strong></em>,&#13;\n \"cells\": [[\"1:\",\"\",1470229980496296],&#13;\n           [\"1:project_name\",\"Docker Training\",1470229980496296],&#13;\n           [\"1:turnover\",\"1000000\",1470229980496296]]},&#13;\n{<em><strong>\"key\": \"Maggie Bail\"</strong></em>,&#13;\n \"cells\": [[\"1:\",\"\",1470230005734692],&#13;\n           [\"1:project_name\",\"Docker Audit\",1470230005734692],&#13;\n           [\"1:turnover\",\"1000000\",1470230005734692]]}&#13;\n]&#13;\n</pre><p>Cassandra is a partitioned row store and its storage structure is in effect a nested sorted map which makes it is easy to grab a subset of data by <em>key. </em>In the previous example data is partitioned by manager name (which is the <em>key</em>) and makes it easy to retrieve projects in per-manager subsets. However, finding projects by turnover would involve checking turnover <em>values</em> for every project in every partition. Cassandra rightfully considers this inefficient, as a result such queries are not supported. Dominic Fox post “<b>How Not To Use Cassandra like an RDBMS (and what will happen if you do)</b>” [Release: 15/09/2016] will give many more examples on other queries that will be suboptimal in Cassandra in <strong>this blogpost</strong>. Luckily, there are patterns for designing Cassandra data models in a way that will be able to provide answers to most reasonable questions.</p><h2>Cassandra Data Modelling Patterns</h2><h3>Model around Business Domain</h3><p>When designing a Cassandra data model for an application, first consider the business entities you are storing and relationships between them. Your ultimate goal will be to store precomputed answers to business questions that the application asks about the stored data, an understanding its structure and meaning is a precondition for modelling these answers. Knowledge of the business domain model is also key to understanding the cardinality of certain data elements and estimating the changes in future data volumes. A data model designed around business domain will also spread data more evenly and keep partition sizes predictable. Naturally, achieving this requires close collaboration between business stakeholders and development teams.</p><h3>Denormalisation</h3><p>Unlike relational databases, Cassandra has no concept of foreign keys and does not support joining tables; both concepts are incompatible with its key architectural principles. Foreign keys have a significant negative impact on write performance while joining tables on reads is one of the inefficient querying patterns that Cassandra discourages. Cassandra demands that the structure of tables support the simplest and most efficient queries. On the other hand, writes in Cassandra are cheap and fast out of the box due to its simple storage model. As a result, it is usually a good idea to avoid extra reads at the expense of extra writes. This can be achieved by <em>balanced</em> denormalisation and data redundancy: if certain data is retrieved in multiple queries duplicate it across all tables supporting these queries.</p><h4>Example:</h4><p>While keeping data volumes and number of tables low is <strong>not</strong> a concern in Cassandra, it is still important to avoid <em>unnecessary</em> duplication. Consider a simplified data model of <strong>users</strong> leaving <strong>comments </strong>to <strong>articles </strong>and imagine retrieving the list of all comments for an article. Even though a separate table stores full user information, a good data model will also duplicate the author name in the comments table: application users want to see who wrote the comment. The following table structure would be appropriate for a common use case:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"sql\"><pre class=\"de1\">CREATE TABLE comments_by_article (\n  article_id INT,\n  comment_id INT,\n  comment text,\n  user_name text,\n  PRIMARY KEY (article_id, comment_id)\n);</pre></div></div></div></div></div></div></div><p>Note that storing full user information for the author with each comment would be excessive and create unnecessary duplication. It is unlikely that someone will ever want to immediately see full user information next to a comment left by that user – it is easy to retrieve upon request. It is vital to understand the types of queries the application will run to strike the right balance.</p><h3>Pre-built Result Sets</h3><p>Fast reads in Cassandra result from storing data fully ready to be read – preparing it at write time. Cassandra does not support any complex processing at query time such as filtering, joins, search, pattern and aggregation. The data must be stored partitioned, sorted, denormalised, and filtered in advance. Ideally, all that is left to Cassandra is to read the results from a single location. For this reason, having roughly a table per query is often a good approach in Cassandra. On the contrary, storing non-contextual isolated denormalised models often leads to common Cassandra anti-patterns.</p><p>When creating a Cassandra data model, determine specific questions the application will ask and the format and content of answers it will expect and create tables to store pre-built result sets. There are limited options to incrementally add support for new queries to an existing model through indexing. Besides, this approach introduces additional complexity and it is possible to avoid it by modelling data correctly in advance.</p><h3>Even Data Distribution</h3><p class=\"p1\">Understanding the distributed nature of Cassandra is key to model for predictably fast cluster performance. All nodes in a Cassandra cluster are equal by design and should run on identical hardware. Accordingly, for nodes to demonstrate comparable performance, they should bear the same load. Spreading data evenly across the cluster by choosing the right partition key helps achieve this. There are several data distribution aspects to consider when designing the data model.</p><ol><li class=\"p1\"><strong>Likely partition sizes</strong>. A single partition will always be stored in its entirety on a single node, therefore, it must be small enough to fit on that node accounting for free space required for <a href=\"https://docs.datastax.com/en/cassandra/3.x/cassandra/dml/dmlHowDataMaintain.html#dmlHowDataMaintain__dml-compaction\" target=\"_blank\">compaction</a>. For this reason, it is important to understand compaction and choose the right compaction strategy.</li>\n<li class=\"p1\"><strong>Keeping partition data bounded</strong>. If the amount of data in a single partition is likely to become too big, adding an additional partitioning column will limit its growth potential. For example, additionally bounding a partition of events by time in addition to type would provide a reasonable guarantee of reasonable partition size. That said, it is important to carefully choose the time granularity best suited to a particular use case.</li>\n<li class=\"p1\"><strong>Partition key cardinality</strong>. Choosing a column with a reasonably large number of unique values (high cardinality) is important to keep partition sizes small. However, it is important to balance this with the aim of on partition ideally being able to satisfy each query.</li>\n<li class=\"p1\"><strong>Separating read-heavy and write-heavy data. </strong>Read-heavy and write-heavy data have different scalability cycles, and measures to optimise them are different and often conflicting. For example, different compaction strategies suit read-heavy and write-heavy workflows best. For this reason, it often helps to keep such data separate even if there is a strong semantic relationship.</li>\n</ol><h3>Inserts over Updates and Deletes</h3><p>Although Cassandra supports updates and deletes, their excessive use results in unexpected operational overhead. Using them safely in Cassandra requires detailed knowledge of their implementation and operational processes in Cassandra. While a record may appear updated or deleted on the surface, physical storage will not reflect it straight away. Cassandra reconciles updates and deletes in physical storage during compaction and only under certain conditions. Use of update and delete operations in Cassandra substantially complicate cluster operations and rely on regular well scheduled repairs, monitoring and manual compaction. In our experience, data models that avoid updating or deleting data help reduce operational risks and costs.</p><h3>Testing Data Models</h3><p>Like any code, Cassandra data models need thorough testing before production use. There are too many factors that affect the performance and operations of the cluster to predict it with reasonable certainty. It is necessary to validate a Cassandra data model by testing it against real business scenarios. In fact, Cassandra ships with a <a href=\"https://docs.datastax.com/en/cassandra_win/3.0/cassandra/tools/toolsCStress.html\" target=\"_blank\">cassandra-stress</a> tool which can help load test the performance of your cluster with a chosen data model. The <a href=\"https://docs.datastax.com/en/cassandra/3.x/cassandra/tools/toolsTablehisto.html\" target=\"_blank\">nodetool tablehistograms</a> (<a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCFhisto.html\" target=\"_blank\">nodetool cfhistograms</a> prior to Cassandra 2.2) utility provides further table statistics. There may be a temptation to avoid testing the data model altogether because gathering fully accurate metrics is impossible. While it is rarely possible to run tests against a production-sized Cassandra cluster, testing against a smaller test cluster will highlight common data model problems. In short, prefer testing the data model for a subset of scenarios over speculating about all aspects of its performance.</p><h2>Look for the Right Balance</h2><p>Cassandra is a powerful tool but upfront investment into its setup pays off in later stages of projects. Performance and smooth operations of a Cassandra cluster depend in large part on the quality of the data model and how well it suits the application. There are many factors that shape the suitable data model and its design involves many complex decisions and tradeoffs. While data modelling in Cassandra requires a high level of expertise in its architecture, there are a number of patterns that help achieve good results. Carefully balancing these decisions helps avoid mistakes and anti-patterns that lead to common Cassandra operational problems.</p><h4>This is the second post in our blog series “Cassandra – What You May Learn The Hard Way.” Get the full overview <a href=\"https://opencredo.com/new-blog-cassandra-what-you-may-learn-the-hard-way/\" target=\"_blank\">here</a>.</h4><h4>The associated webinar, “Cassandra – The Good, the Bad, and the Ugly” was broadcast on October 6th, 2016. View the recording <strong><a href=\"https://opencredo.com/cassandra-good-bad-ugly-webinar-recording/\"> here</a>.</strong></h4><h4><a class=\"button\" href=\"#text_icl-6\">Sign up to receive updates via email</a></h4>",
        "created_at": "2018-07-17T23:18:37+0000",
        "updated_at": "2018-07-24T12:43:34+0000",
        "published_at": "2016-09-06T13:40:53+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 10,
        "domain_name": "opencredo.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10865"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10864,
        "uid": null,
        "title": "How Not To Use Cassandra Like An RDBMS (and what will happen if you do) - OpenCredo",
        "url": "https://opencredo.com/how-not-to-use-cassandra-like-an-rdbms-and-what-will-happen-if-you-do/",
        "content": "<p>Cassandra isn’t a relational database management system, but it has some features that make it look a bit like one. Chief among these is CQL, a query language with an SQL-like syntax. CQL isn’t a bad thing in itself – in fact it’s very convenient – but it can be misleading since it gives developers the illusion that they are working with a familiar data model, when things are really very different under the hood. Not only is Cassandra <em>not</em> an RDBMS, it’s not even <em>like</em> an RDBMS in some of the ways you might expect. In this post I’ll review a few example scenarios where a beginner might be unpleasantly surprised by the differences, and suggest some remedies.</p><h2>Example 1: querying by non-key columns</h2><p>Here’s some CQL to create a “shopping trolley contents” table in Cassandra:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">CREATE TABLE shoppingTrolleyContents (&#13;\n trolleyId timeuuid,&#13;\n lineItemId timeuuid,&#13;\n itemId text,&#13;\n qty int,&#13;\n unitPrice decimal,&#13;\n PRIMARY KEY(trolleyId, lineItemId)&#13;\n) WITH CLUSTERING ORDER BY (lineItemId ASC);</pre></div></div></div></div></div></div></div><p>Suppose we wanted to find all of the shopping trolleys that contains a particular item, identified by <code>itemId</code>. Here’s a query that would make perfect sense in SQL, that Cassandra will not allow you to perform:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">// Find all the trolleys that contain a particular item&#13;\nSELECT trolleyId FROM shoppingTrolleyContents WHERE itemId = 'SKU001';</pre></div></div></div></div></div></div></div><p>If you try to run this query through CQLSH, you will get the response “No secondary indexes on the restricted columns support the provided operators”. What this means is that the columns in this table are indexed only by the columns listed in the <code>PRIMARY KEY</code> clause. Without defining a “secondary index” there is no way to search for values using a <code>WHERE</code> clause restricting on any other column.</p><p><em>OK</em>, you may think, <em>so I have to define secondary indexes explicitly</em>. A quick <a href=\"https://docs.datastax.com/en/cql/3.1/cql/cql_reference/create_index_r.html\">Google search</a> later, and you have:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">CREATE INDEX trolleyByItemId ON shoppingTrolleyContents (itemId);</pre></div></div></div></div></div></div></div><p>You can now run the query you wanted. Unfortunately, while restrictions on primary indexes allow queries to be routed efficiently to the specific nodes in the Cassandra cluster that hold the data you’re looking for, the secondary index must be consulted on every node of the Cassandra cluster in order to find matching records. Use of secondary indexes can thus severely hamper the scalability of Cassandra, especially if the indexed column has a high cardinality (i.e. can contain many distinct values).</p><p>In this scenario, it is probably better to have a second table that explicitly provides for <code>trolleyId</code> lookups by <code>itemId</code>:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">CREATE TABLE lineItemsByItemId (&#13;\n itemId text,&#13;\n trolleyId timeuuid,&#13;\n lineItemId timeuuid,&#13;\n PRIMARY KEY(itemId, trolleyId, lineItemId)&#13;\n) WITH CLUSTERING ORDER BY (trolleyId DESC, lineItemId ASC);</pre></div></div></div></div></div></div></div><p>However, you now have to consider the overhead of maintaining this table, and the fact that Cassandra does not support transactional updates to multiple tables with ACID guarantees.</p><p>Suppose that a line item is removed from a trolley, and a user queries for orders containing line items with that <code>itemId</code> before the <code>lineItemsByItemId</code> table has been updated. The returned <code>trolleyId</code>/<code>lineItemId</code> pair will then refer to a record in the <code>shoppingTrolleyContents</code> table that no longer exists. This is precisely the scenario that the referential integrity mechanisms (foreign key constraints) provided by an RDBMS are designed to avoid – but Cassandra provides <em>no</em> mechanisms for ensuring referential integrity between tables.</p><h2>Example 2: joining tables</h2><p>Suppose we search in <code>lineItemsByItemId</code> for all of the line items with <code>itemId='SKU0001'</code>:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">SELECT trolleyId, lineItemId FROM lineItemsByItemId WHERE itemId='SKU0001';</pre></div></div></div></div></div></div></div><p>Because Cassandra doesn’t support <code>JOIN</code>s, if we get back 1,000 <code>trolleyId</code>/<code>lineItemId</code> pairs, we must then retrieve each line item’s details separately from the <code>shoppingTrolleyContents</code> table in order to obtain the associated quantity and price data. This is tremendously inefficient. It may be better to denormalise further, copying all of the relevant columns from <code>shoppingTrolleyContents</code> into <code>lineItemsByItemId</code>:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">CREATE TABLE lineItemsByItemId (&#13;\n itemId text,&#13;\n trolleyId timeuuid,&#13;\n lineItemId timeuuid,&#13;\n qty int,&#13;\n unitPrice decimal,&#13;\n PRIMARY KEY(itemId, trolleyId, lineItemId)&#13;\n) WITH CLUSTERING ORDER BY (trolleyId DESC, lineItemId ASC);</pre></div></div></div></div></div></div></div><p>This kind of duplication of data is anathema in relational database design. In the Cassandra world, it’s commonplace and necessary in order to support efficient querying.</p><h2>Example 3: reverse lookups</h2><p>A shopping trolley usually has an owner, and perhaps some other metadata associated with it. Here’s the CQL for a table which associates user ids with shopping trolley ids:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">CREATE TABLE shoppingTrolleyOwners (&#13;\n trolleyId timeuuid,&#13;\n ownerId text,&#13;\n PRIMARY KEY (trolleyId, ownerId)&#13;\n) WITH CLUSTERING ORDER BY (ownerId ASC);</pre></div></div></div></div></div></div></div><p>We can use this to find out who owns a given trolley by id:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">SELECT ownerID FROM shoppingTrolleyOwners WHERE trolleyId='';</pre></div></div></div></div></div></div></div><p>However, the reverse lookup, to find trolleys by owner, is not supported, even though ownerId is a primary index key:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">SELECT trolleyId FROM shoppingTrolleyOwners WHERE ownerId='bob';</pre></div></div></div></div></div></div></div><p>The error we get back if we try to run the above is:</p><pre>Cannot execute this query as it might involve data filtering and thus may have unpredictable performance. If you want to execute this query despite the performance unpredictability, use ALLOW FILTERING.&#13;\n</pre><p>Once again, if Cassandra doesn’t immediately allow you to do something, it’s worth making sure you understand why before just going ahead and adding <code>ALLOW FILTERING</code> to the query to “make it work”.</p><p>The problem here is that the <code>trolleyId</code> is used as the <em>partition</em> key, which determines which node in the Cassandra cluster the data is stored on, while the <code>ownerId</code> is used as the <em>cluster</em> key, which indexes data within the partition. In order to perform the query as written, Cassandra would have to send it to all of the nodes within the cluster, obtain the records for all <code>trolleyId</code> values within each partition, and then filter them by <code>ownerId</code>. This has the potential to be very inefficient.</p><p>A better solution, in this case, would be to create a second table to support the reverse lookup:</p><div class=\"wp-geshi-highlight-wrap5\"><div class=\"wp-geshi-highlight-wrap4\"><div class=\"wp-geshi-highlight-wrap3\"><div class=\"wp-geshi-highlight-wrap2\"><div class=\"wp-geshi-highlight-wrap\"><div class=\"wp-geshi-highlight\"><div class=\"cql\"><pre class=\"de1\">CREATE TABLE shoppingTrolleysByOwner (&#13;\n ownerId text,&#13;\n trolleyId timeuuid,&#13;\n PRIMARY KEY (ownerId, trolleyId)&#13;\n) WITH CLUSTERING ORDER BY (trolleyId DESC);</pre></div></div></div></div></div></div></div><p>Once again, the previous warnings about data consistency apply: both tables must be written to whenever a shopping trolley is created, and there are no ACID transactions to ensure that this pair of updates is carried out atomically.</p><h2>Conclusions</h2><p>Cassandra can usefully be thought of as something like a distributed version of the storage engine that an RDBMS might use, but without the data management capabilities that an RDBMS brings in addition to the ability to persist and retrieve data.</p><p>It is certainly possible to build an RDBMS-like system on top of Cassandra, but doing so will progressively negate all of the performance and scalability advantages that Cassandra’s distributed data model provides. If you find yourself doing this – implementing in-memory joins, distributed locks or other mechanisms to get RDBMS-like behaviour out of a Cassandra-backed system – then it may be time to stop and reconsider. Will Cassandra still deliver the performance you want, if you use it in this way?</p><p>An RDBMS provides a consistent, principled approach to data management that works for all cases, at the cost of limited scalability: you pay the “relational tax” in exchange for a highly flexible yet predictable data model, By contrast, Cassandra’s is a “pay for what you use” model, in which data integrity is an application-level concern, and specific query patterns must be explicitly supported through decisions made about the physical layout of data across the cluster. It “unsolves” some of the problems the RDBMS was invented to solve, in order to address different problems more effectively.</p><p>For many purposes, a dual or polyglot approach is worth considering. Use Cassandra as the primary data store for capturing information as it arrives into the system; but then build “query-optimised views” of subsets of that data in other databases specialised to the kinds of access users require. Use an indexing engine such as SOLR to provide full-text search across records, or a graph database such as Neo4j to store metadata in a way that supports “traversal-heavy” queries that join together many different kinds of entity. Use an RDBMS when you need the full power and flexibility of SQL to express ad hoc queries that explore the data in multiple dimensions.</p><p>Alternatively, a stream-processing approach may be the way to go when you need to generate complex reports which consume large amounts of captured data. Stream the records you have captured in Cassandra into an Apache Spark cluster, and use Spark SQL to execute complex queries using filters, joins and aggregations. This approach, which I call “write first, reason later”, is especially suited to event-driven systems where capture of event information can be decoupled from decision-making about how to react to events in the aggregate.</p><h4>This is the third post in our blog series “Cassandra – What You May Learn The Hard Way.” Get the full overview <a href=\"https://opencredo.com/new-blog-cassandra-what-you-may-learn-the-hard-way/\" target=\"_blank\">here</a>.</h4><h4>The associated webinar, “Cassandra – The Good, the Bad, and the Ugly” was broadcast on October 6th, 2016. View the recording <strong><a href=\"https://opencredo.com/cassandra-good-bad-ugly-webinar-recording/\"> here</a>.</strong></h4><h4><a class=\"button\" href=\"#text_icl-6\">Sign up to receive updates via email</a></h4>",
        "created_at": "2018-07-17T23:18:19+0000",
        "updated_at": "2018-08-28T15:48:49+0000",
        "published_at": "2016-09-15T12:38:18+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 7,
        "domain_name": "opencredo.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10864"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10852,
        "uid": null,
        "title": "Cassandra",
        "url": "http://jamirtostudycassand.blogspot.com/?view=flipcard&_escaped_fragment_=",
        "content": "<div id=\"main\" tabindex=\"0\"><div id=\"controls\"><ul id=\"groups\"><li class=\"group selected\" data-category=\"none\" title=\"Recent\">Recent</li>\n    <li class=\"group\" data-category=\"published\" title=\"Group by date\">Date</li>\n    \n    \n    <li class=\"group\" data-category=\"tag\" title=\"Group by label\">Label</li>\n    <li class=\"group\" data-category=\"author\" title=\"Group by author\">Author</li>\n    \n  </ul></div><div id=\"content\" class=\"items hfeed\"><div class=\"item hentry\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1913204634856192233\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Read Repair Chance</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Read Repair Chance</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"2041937350109885085\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>TTL &amp; GC Grace Seconds</p></div></div><div class=\"back\"><div class=\"overlay\"><p>TTL &amp; GC Grace Seconds</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1891457791712844370\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Update statement</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Update statement</p></div></div></div></div><div class=\"item hentry compaction\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1783813783959014593\"><div class=\"card\"><div class=\"back\"><div class=\"overlay\"><p>Compaction</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1157327081616354647\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Speculative Query Execution</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Speculative Query Execution</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"5195058995149499625\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Alter type</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Alter type</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1288124925756736023\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>UDT- User Defined Type</p></div></div><div class=\"back\"><div class=\"overlay\"><p>UDT- User Defined Type</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"8447059258592441829\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Collection</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Collection</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"2141792041004541094\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>UUID &amp; timeUUID</p></div></div><div class=\"back\"><div class=\"overlay\"><p>UUID &amp; timeUUID</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"9019476210604726110\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Counter</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Counter</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"3963260535116226204\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Secondary Index</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Secondary Index</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"3297642472741434543\"><div class=\"card\"><div class=\"back\"><div class=\"overlay\"><p>CQL Datatypes</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"2319619481996271057\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Static-Column</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Static-Column</p></div></div></div></div><div class=\"item hentry cql\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"7021010111362713678\"><div class=\"card\"><div class=\"back\"><div class=\"overlay\"><p>PK-Partition Key &amp; Clustering Column</p></div></div></div></div><div class=\"item hentry architecture\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"8578350354275841691\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>System Keyspace</p></div></div><div class=\"back\"><div class=\"overlay\"><p>System Keyspace</p></div></div></div></div><div class=\"item hentry architecture\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"5571035908053558407\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Gossip Protocol</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Gossip Protocol</p></div></div></div></div><div class=\"item hentry architecture\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1122676304810193494\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Anti Entropy</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Anti Entropy</p></div></div></div></div><div class=\"item hentry architecture\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"4024455211478567129\"><div class=\"card\"><div class=\"back\"><div class=\"overlay\"><p>Consistency Level</p></div></div></div></div><div class=\"item hentry architecture\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"2955905687547609276\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Replication</p></div></div><div class=\"back\"><div class=\"overlay\"><p>Replication</p></div></div></div></div><div class=\"item hentry architecture\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"746497635863670884\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>Request co-ordination</p></div></div></div></div><div class=\"item hentry tools\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"3181451439816618420\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>cassandra-tools</p></div></div><div class=\"back\"><div class=\"overlay\"><p>cassandra-tools</p></div></div></div></div><div class=\"item hentry tools\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"2344276926655082204\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>cassandra-stress</p></div></div><div class=\"back\"><div class=\"overlay\"><p>cassandra-stress</p></div></div></div></div><div class=\"item hentry tools\" itemscope=\"itemscope\" itemtype=\"http://schema.org/BlogPosting\" data-id=\"1523223555633789091\"><div class=\"card\"><div class=\"front\"><div class=\"overlay\"><p>nodetool</p></div></div><div class=\"back\"><div class=\"overlay\"><p>nodetool</p></div></div></div></div></div></div>\n<p>Loading</p>",
        "created_at": "2018-07-17T17:16:48+0000",
        "updated_at": "2018-07-17T17:16:54+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "jamirtostudycassand.blogspot.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10852"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 15,
            "label": "tutorial",
            "slug": "tutorial"
          },
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          }
        ],
        "is_public": false,
        "id": 10844,
        "uid": null,
        "title": "Cassandra schema migrations made easy with Apache Spark",
        "url": "http://batey.info/cassandra-schema-migrations-made-easy.html",
        "content": "By far the most common question I get asked when talking about Cassandra is once you've denormalised based on your queries what happens if you were wrong or a new requirement comes in that requires a new type of query.\n<p>First I always check that it is a real requirement to be able to have this new functionality on old data. If that's not the case, and often it isn't, then you can just start double/triple writing into the new table.\n</p><p>However if you truly need to have the new functionality on old data then Spark can come to the rescue. The first step is to still double write. We can then backfill using Spark. The awesome thing is that nearly all writes in Cassandra are idempotent, so when we backfill we don't need to worry about inserting data that was already inserted via the new write process.\n</p><p>Let's see an example. Suppose you were storing customer events so you know what they are up to. At first you want to query by customer/time so you end up with following table:\n</p><p>Then the requirement comes in to be able to look for events by staff member. My reaction a couple of years ago would have been something like this:\n</p><div class=\"separator\"><a href=\"http://4.bp.blogspot.com/-ocpWDOp6ZHM/VOXt4fiOlII/AAAAAAAAAYQ/KBdlMbLjO4I/s1600/oh-noes-everybody-panic.gif\"><img border=\"0\" src=\"http://4.bp.blogspot.com/-ocpWDOp6ZHM/VOXt4fiOlII/AAAAAAAAAYQ/KBdlMbLjO4I/s1600/oh-noes-everybody-panic.gif\" alt=\"image\" /></a>\n</div><br />However if you have Spark workers on each of your Cassandra nodes then this is not an issue.<p>Assuming you want to a new table keyed by staff_id and have modified your application to double write you do the back fill with Spark. Here's the new table:\n</p><p>Then open up a Spark-shell (or submit a job) with the Spark-Cassandra connector on the classpath and all you'll need is something like this:\n</p><p>How can a few lines do so much! If you're in a shell obviously you don't even need to create a SparkContext. What will happen here is the Spark workers will process the partitions on a Cassandra node that owns the data for the customer table (original table) and insert it back into Cassandra locally. Cassandra will then handle the replication to the correct nodes for the staff table.\n</p><p>This is the least network traffic you could hope to achieve. Any solution that you write your self with Java/Python/Shell will involve pulling the data back to your application and pushing it to a new node, which will then need to replicate it for the new table.\n</p><p>You won't want to do this at a peak time as this will HAMMER you Cassandra cluster as Spark is going to do this quickly. If you have a small DC for just running the Spark jobs and let it asynchronously replicate to your operational DC this is less of a concern.\n  </p>",
        "created_at": "2018-07-17T11:47:17+0000",
        "updated_at": "2018-07-17T11:47:29+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 2,
        "domain_name": "batey.info",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10844"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 21,
            "label": "lucene",
            "slug": "lucene"
          },
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 36,
            "label": "solr",
            "slug": "solr"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 1068,
            "label": "graph",
            "slug": "graph"
          }
        ],
        "is_public": false,
        "id": 10842,
        "uid": null,
        "title": "JanusGraph: Distributed graph database",
        "url": "http://janusgraph.org/",
        "content": "<div style=\"text-align: center;\">\n  <a href=\"http://docs.janusgraph.org/latest/\">Docs</a> •\n  <a href=\"https://github.com/JanusGraph/janusgraph/\">GitHub</a> •\n  <a href=\"https://github.com/JanusGraph/janusgraph/releases/\">Download</a><br /><img class=\"janusgraph\" src=\"http://janusgraph.org/images/janusgraph.png\" alt=\"image\" /></div><p>JanusGraph is a scalable <a href=\"http://en.wikipedia.org/wiki/Graph_database\">graph\ndatabase</a> optimized for storing and\nquerying graphs containing hundreds of billions of vertices and edges\ndistributed across a multi-machine cluster. JanusGraph is a transactional\ndatabase that can support thousands of concurrent users executing complex graph\ntraversals in real time.</p><p>In addition, JanusGraph provides the following features:</p><ul><li>Elastic and linear scalability for a growing data and user base.</li>\n  <li>Data distribution and replication for performance and fault tolerance.</li>\n  <li>Multi-datacenter high availability and hot backups.</li>\n  <li>Support for <a href=\"http://en.wikipedia.org/wiki/ACID\">ACID</a> and\n<a href=\"http://en.wikipedia.org/wiki/Eventual_consistency\">eventual consistency</a>.</li>\n  <li>Support for various storage backends:\n    <ul><li><a href=\"http://cassandra.apache.org\">Apache Cassandra®</a></li>\n      <li><a href=\"http://hbase.apache.org\">Apache HBase®</a></li>\n      <li><a href=\"https://cloud.google.com/bigtable\">Google Cloud Bigtable</a></li>\n      <li><a href=\"http://www.oracle.com/technetwork/database/berkeleydb/overview/index-093405.html\">Oracle BerkeleyDB</a></li>\n    </ul></li>\n  <li>Support for global <a href=\"http://tinkerpop.apache.org/docs/3.2.4/reference/#graphcomputer\">graph data analytics</a>, reporting, and ETL through integration with big data\nplatforms:\n    <ul><li><a href=\"http://spark.apache.org\">Apache Spark™</a></li>\n      <li><a href=\"http://giraph.apache.org\">Apache Giraph™</a></li>\n      <li><a href=\"http://hadoop.apache.org\">Apache Hadoop®</a></li>\n    </ul></li>\n  <li>Support for geo, numeric range, and full-text search via:\n    <ul><li><a href=\"http://www.elasticsearch.org\">ElasticSearch™</a></li>\n      <li><a href=\"http://lucene.apache.org/solr\">Apache Solr™</a></li>\n      <li><a href=\"http://lucene.apache.org\">Apache Lucene®</a></li>\n    </ul></li>\n  <li>Native integration with the <a href=\"http://tinkerpop.apache.org\">Apache TinkerPop™</a> graph stack:\n    <ul><li><a href=\"http://tinkerpop.apache.org/docs/3.2.4/reference/#traversal\">Gremlin graph query language</a></li>\n      <li><a href=\"http://tinkerpop.apache.org/docs/3.2.4/reference/#gremlin-server\">Gremlin graph server</a></li>\n      <li><a href=\"http://tinkerpop.apache.org/docs/3.2.4/reference/#gremlin-applications\">Gremlin applications</a></li>\n    </ul></li>\n  <li>Open source under the <a href=\"http://www.apache.org/licenses/LICENSE-2.0.html\">Apache 2</a> license.</li>\n  <li>You can visualize graphs stored in JanusGraph via any of the following tools:\n    <ul><li><a href=\"http://www.cytoscape.org/\">Cytoscape</a></li>\n      <li><a href=\"http://tinkerpop.apache.org/docs/current/reference/#gephi-plugin\">Gephi</a>\nplugin for Apache TinkerPop</li>\n      <li><a href=\"https://github.com/bricaud/graphexp\">Graphexp</a></li>\n      <li><a href=\"https://cambridge-intelligence.com/visualizing-janusgraph-new-titandb-fork/\">KeyLines by Cambridge Intelligence</a></li>\n      <li><a href=\"https://doc.linkurio.us/ogma/latest/tutorials/janusgraph/\">Linkurious</a></li>\n    </ul></li>\n</ul><p>You can <a href=\"https://github.com/JanusGraph/janusgraph/releases\">download</a> JanusGraph\nor <a href=\"https://github.com/JanusGraph/janusgraph\">clone</a> from GitHub.</p><p>Read the <a href=\"http://docs.janusgraph.org/latest\">JanusGraph documentation</a> and join the\n<a href=\"https://groups.google.com/group/janusgraph-users\">users</a> or\n<a href=\"https://groups.google.com/group/janusgraph-dev\">developers</a> mailing lists.</p><p>Follow the <a href=\"http://docs.janusgraph.org/latest/getting-started.html\">Getting Started with JanusGraph</a> guide for a step-by-step introduction.</p><p>JanusGraph is a project under <a href=\"https://www.linux.com/blog/Linux-Foundation-welcomes-JanusGraph\">The Linux\nFoundation</a>,\nand includes participants from Expero, Google, GRAKN.AI, Hortonworks, IBM and Amazon.</p><h2 id=\"presentations\">Presentations</h2><p>Here is a selection of JanusGraph presentations:</p><ul><li>\n    <p><a href=\"https://www.slideshare.net/ptgoetz/large-scale-graph-analytics-with-janusgraph\">DataWorksJun2017: Large Scale Graph Analytics with JanusGraph</a>, P. Taylor Goetz, 2017.06.13</p>\n  </li>\n  <li>\n    <p><a href=\"https://www.slideshare.net/HBaseCon/communitydriven-graphs-with-janusgraph-77117443\">HBaseCon2017 Community-Driven Graphs with JanusGraph</a>, Jing Chen He &amp; Jason Plurad, 2017.06.12</p>\n  </li>\n</ul><h2 id=\"users\">Users</h2><p>The following users have deployed JanusGraph in production.</p><p><a href=\"https://www.celum.com/en/graph-driven-and-reactive-architecture\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/celum.png\" alt=\"celum\" class=\"logo\" /></a>\n<a href=\"https://www.compose.com/databases/janusgraph\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/compose.png\" alt=\"Compose\" class=\"logo\" /></a>\n<a href=\"https://finc.com\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/finc.png\" alt=\"FiNC\" class=\"logo\" /></a>\n<a href=\"https://gdatasoftware.com\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/gdata.png\" alt=\"G DATA\" class=\"logo\" /></a>\n<a href=\"https://www.360.cn\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/qihoo_360.png\" alt=\"Qihoo 360\" class=\"logo\" /></a>\n<a href=\"https://www.redhat.com\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/redhat.png\" alt=\"Red Hat\" class=\"logo\" /></a>\n<a href=\"https://seeq.com\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/seeq.png\" alt=\"Seeq\" class=\"logo\" /></a>\n<a href=\"http://denmarkblog.timesinternet.in/blogs/graph/times-internet-is-using-janusgraph-as-main-database-in-cms-for-all-newsrooms/articleshow/63709837.cms\" class=\"logo\"><img src=\"http://janusgraph.org/images/logos/timesinternet.png\" alt=\"Times Internet\" class=\"logo\" /></a></p>",
        "created_at": "2018-07-16T15:20:02+0000",
        "updated_at": "2018-07-16T15:20:31+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 1,
        "domain_name": "janusgraph.org",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10842"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 228,
            "label": "dotnet",
            "slug": "net"
          },
          {
            "id": 873,
            "label": "mysql",
            "slug": "mysql"
          },
          {
            "id": 926,
            "label": "postgres",
            "slug": "postgres"
          },
          {
            "id": 1114,
            "label": "orm",
            "slug": "orm"
          }
        ],
        "is_public": false,
        "id": 10838,
        "uid": null,
        "title": "MiracleDevs/Paradigm.ORM",
        "url": "https://github.com/MiracleDevs/Paradigm.ORM",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\"><p><a href=\"https://travis-ci.org/MiracleDevs/Paradigm.ORM\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/a10b3ac4adbc98d739ba7dc38052039ce7e10730/68747470733a2f2f7472617669732d63692e6f72672f4d697261636c65446576732f506172616469676d2e4f524d2e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/MiracleDevs/Paradigm.ORM.svg?branch=master\" /></a></p>\n<table><thead><tr><th>Library</th>\n<th>Nuget</th>\n<th>Install</th>\n</tr></thead><tbody><tr><td>Data</td>\n<td><a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/67a06bea41473cc287b680dc648a4d95c8a0fc4f/68747470733a2f2f696d672e736869656c64732e696f2f6e756765742f762f4e756765742e436f72652e737667\" alt=\"NuGet\" data-canonical-src=\"https://img.shields.io/nuget/v/Nuget.Core.svg\" /></a></td>\n<td><code>Install-Package Paradigm.ORM.Data</code></td>\n</tr><tr><td>MySql</td>\n<td><a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.MySql/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/67a06bea41473cc287b680dc648a4d95c8a0fc4f/68747470733a2f2f696d672e736869656c64732e696f2f6e756765742f762f4e756765742e436f72652e737667\" alt=\"NuGet\" data-canonical-src=\"https://img.shields.io/nuget/v/Nuget.Core.svg\" /></a></td>\n<td><code>Install-Package Paradigm.ORM.Data.MySql</code></td>\n</tr><tr><td>PostgreSQL</td>\n<td><a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.PostgreSql/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/67a06bea41473cc287b680dc648a4d95c8a0fc4f/68747470733a2f2f696d672e736869656c64732e696f2f6e756765742f762f4e756765742e436f72652e737667\" alt=\"NuGet\" data-canonical-src=\"https://img.shields.io/nuget/v/Nuget.Core.svg\" /></a></td>\n<td><code>Install-Package Paradigm.ORM.Data.PostgreSql</code></td>\n</tr><tr><td>SQL Server</td>\n<td><a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.SqlServer/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/67a06bea41473cc287b680dc648a4d95c8a0fc4f/68747470733a2f2f696d672e736869656c64732e696f2f6e756765742f762f4e756765742e436f72652e737667\" alt=\"NuGet\" data-canonical-src=\"https://img.shields.io/nuget/v/Nuget.Core.svg\" /></a></td>\n<td><code>Install-Package Paradigm.ORM.Data.SqlServer</code></td>\n</tr><tr><td>Cassandra</td>\n<td><a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/67a06bea41473cc287b680dc648a4d95c8a0fc4f/68747470733a2f2f696d672e736869656c64732e696f2f6e756765742f762f4e756765742e436f72652e737667\" alt=\"NuGet\" data-canonical-src=\"https://img.shields.io/nuget/v/Nuget.Core.svg\" /></a></td>\n<td><code>Install-Package Paradigm.ORM.Data.Cassandra</code></td>\n</tr></tbody></table>\n<p>.NET Core ORM with dbfirst support, and code scaffolding features. This ORM supports different database sources.</p>\n<h2><a id=\"user-content-self-contained-deploy-scd\" class=\"anchor\" aria-hidden=\"true\" href=\"#self-contained-deploy-scd\"></a>Self Contained Deploy (SCD)</h2>\n<p>Bellow you can find portable versions for all major OSs.\nIf you are planning to use the tools in several projects, we recommend to add the SCD folder to your PATH.</p>\n<table><thead><tr><th>Tool</th>\n<th>OS</th>\n<th>Zip File</th>\n</tr></thead><tbody><tr><td>DbFirst</td>\n<td>Windows x86</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbfirst.win-x86.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DbFirst</td>\n<td>Windows x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbfirst.win-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DbFirst</td>\n<td>Linux x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbfirst.linux-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DbFirst</td>\n<td>OSX x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbfirst.osx-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>\n</td><td>\n</td><td>\n</td></tr><tr><td>DbPublisher</td>\n<td>Windows x86</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbpublisher.win-x86.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DbPublisher</td>\n<td>Windows x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbpublisher.win-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DbPublisher</td>\n<td>Linux x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbpublisher.linux-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DbPublisher</td>\n<td>OSX x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dbpublisher.osx-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>\n</td><td>\n</td><td>\n</td></tr><tr><td>DataExport</td>\n<td>Windows x86</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dataexport.win-x86.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DataExport</td>\n<td>Windows x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dataexport.win-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DataExport</td>\n<td>Linux x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dataexport.linux-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr><tr><td>DataExport</td>\n<td>OSX x64</td>\n<td><a href=\"https://raw.githubusercontent.com/MiracleDevs/Paradigm.ORM/master/dist/dataexport.osx-x64.zip\" rel=\"nofollow\">Download</a></td>\n</tr></tbody></table><h2><a id=\"user-content-change-log\" class=\"anchor\" aria-hidden=\"true\" href=\"#change-log\"></a>Change log</h2>\n<p>Version <code>2.2.4</code></p>\n<ul><li>Added visual basic tests.</li>\n<li>Updated nuget dependencies.</li>\n<li>Fixed a couple of bugs found with the vb tests.</li>\n</ul><p>Version <code>2.2.3</code></p>\n<ul><li>Added new DatabaseCommandException thrown when executing database commands. The DatabaseCommandException contains a reference to the\nexecuting command, allowing for a better debugging experience.\nUse Command.CommandText to observe the sql or cql query being executed.\nUse Command.Parameters to observe the parameters bound to the query.</li>\n<li>Fixed a bug in Cassandra connector not adding a parameter in one of the AddParameters methods.</li>\n<li>Fixed a bug in CustomQuery sync execution not updated the command text after parameter replacement.</li>\n<li>Improved and updated tests.</li>\n</ul><p>Version <code>2.2.2</code></p>\n<ul><li>Removed mandatory data type in ColumnAttribute. The orm will choose the default column types for each database type.</li>\n<li>Changed how the CommandBatch replace parameter names, to prevent name collision.</li>\n<li>Added tests for the command batch name replacement.</li>\n<li>Changed how select parameters are replaced, from @Index to  @pIndex or :pIndex, depending on the database parameter naming conventions.</li>\n<li>Updated NuGet dependencies.</li>\n</ul><p>Version <code>2.2.1</code></p>\n<ul><li>Added a cache service for descriptors all over the orm, to prevent tons of small objects filling the heap.</li>\n<li>Removed constructors receiving descriptors. Now all the ORM classes should refer to the cache for descriptors.</li>\n<li>Descriptor constructors are now internal and can not be instantiated outside the ORM.</li>\n</ul><p>Version <code>2.2.0</code></p>\n<ul><li>Refactor command handling to allow parallel execution of the ORM without conflicting with some of the\nconnectors. The orm does not cache a command inside the command builder any more.</li>\n<li>Refactor command builders and moved shared functionality to the core classes, and removed the\nduplication from the client implementations. Now will be even easier to implement new clients.</li>\n<li>Moved base protected methods from the CommandBuilderBase to the ICommandFormatProvider and added a new\nbase CommandFormatProviderBase with shared behavior for all the different format providers.</li>\n<li>Removed IDisposable interface from most of the ORM core classes. The most notable are:\n<ul><li>Database access</li>\n<li>Query</li>\n<li>Custom query</li>\n<li>All the stored procedure types</li>\n<li>Schema Provider</li>\n</ul></li>\n<li>Removed extension methods for the IDatabaseConnector not used any more.</li>\n</ul><p>Version <code>2.1.7</code></p>\n<ul><li>Changed how the DatabaseAccess classes utilize the BatchManager to be thread safe.</li>\n</ul><p>Version <code>2.1.6</code></p>\n<ul><li>Updated Paradigm.Core and other dependencies.</li>\n<li>Published new versions for the tools.</li>\n</ul><p>Version <code>2.1.5</code></p>\n<ul><li>Removed a dependency over generic entities that needed a parameterless constructor\nin all the solution.</li>\n</ul><p>Version <code>2.1.4</code></p>\n<ul><li>Removed a dependency over generic entities that needed a parameterless constructor.</li>\n</ul><p>Version <code>2.1.3</code></p>\n<ul><li>Added new constructor to <code>DatabaseReaderMapper</code> to allow set both the service provider and the\ndatabase connector. This will allow multi-tenancy support using the dbfirst generated code.</li>\n<li>Added new constructors to all the stored procedure types for the same reason as the previous point.</li>\n<li>Added missing ValueConverter inside the database reader value provider.</li>\n</ul><p>Version <code>2.1.2</code></p>\n<ul><li>Changed the database reader mappers to work with the <code>IServiceProvider</code> class. Now, will try to instantiate\nthe entities with the service provider first, and if the service provider can't, will use the activator to\ncreate a new instance. This will allow the Paradigm.Services framework to fully delegate the instancing to\nDI allowing better DDD.</li>\n</ul><p>Version <code>2.1.1</code></p>\n<ul><li>Fixed a problem in cassandra connector where the schema provider can not guess the column type when the column is a\nclustering key with order applied to it.</li>\n<li>Made the modifications to the tests to test the above problem.</li>\n</ul><p>Version <code>2.1.0</code></p>\n<ul><li>Added a new Cassandra connector.\nThis new connector allows to work against Apache Cassandra o ScyllaDB. There are some limitations imposed by the\nDataStax connector, and other imposed by the orm, but for most cases will be just fine.</li>\n</ul><blockquote>\n<p>Warning: The ORM will work with column families that mimic sql tables, aka. without lists, maps, or other not standard\nrelational databases. Even if Cassandra does not supports joins, the ORM allows to create virtual foreign keys between tables\nand create navigation properties from it.</p>\n</blockquote>\n<ul><li>Data Export, DbFirst and DbPublisher can work now against Cassandra and ScyllaDB.</li>\n<li>In all the configuration files, now the Database Type changed to Upper Camel Case syntax, the database types are:\n<ul><li>SqlServer,</li>\n<li>MySql,</li>\n<li>PostgreSql,</li>\n<li>Cassandra</li>\n</ul></li>\n<li>Updated MySql Connector version.</li>\n</ul><p>Version <code>2.0.1</code></p>\n<ul><li>Updated Paradigm.Core to version <code>2.0.1</code>.</li>\n</ul><p>Version <code>2.0.0</code></p>\n<ul><li>Updated .net core from version 1 to version 2.</li>\n</ul><p>Version <code>1.0.0</code></p>\n<ul><li>Uploaded first version of the Paradigm ORM.</li>\n</ul></article>",
        "created_at": "2018-07-14T21:13:32+0000",
        "updated_at": "2018-07-14T21:13:51+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/8192926?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10838"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1113,
            "label": "nuget",
            "slug": "nuget"
          },
          {
            "id": 1114,
            "label": "orm",
            "slug": "orm"
          }
        ],
        "is_public": false,
        "id": 10836,
        "uid": null,
        "title": "Paradigm.ORM.Data.Cassandra 2.2.4",
        "url": "https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/",
        "content": "<p>Paradigm ORM Cassandra Connector library.</p><div class=\"install-tabs\"><div class=\"tab-content\"><div role=\"tabpanel\" class=\"tab-pane active\" id=\"package-manager\"><div><p>&#13;\n                &#13;\n                    Install-Package Paradigm.ORM.Data.Cassandra -Version 2.2.4&#13;\n                &#13;</p></div></div><div role=\"tabpanel\" class=\"tab-pane\" id=\"dotnet-cli\"><div><p>&#13;\n                &#13;\n                    dotnet add package Paradigm.ORM.Data.Cassandra --version 2.2.4&#13;\n                &#13;</p></div></div><div role=\"tabpanel\" class=\"tab-pane\" id=\"paket-cli\"><div><p>&#13;\n                &#13;\n                    paket add Paradigm.ORM.Data.Cassandra --version 2.2.4&#13;\n                &#13;</p></div><p>&#13;\n        <i class=\"ms-Icon ms-Icon--Warning\" aria-hidden=\"true\">&#13;\n        &#13;\nThe NuGet Team does not provide support for this client. Please contact its <a href=\"https://fsprojects.github.io/Paket/contact.html\">maintainers</a> for support.&#13;\n                    &#13;</i></p></div></div></div><ul class=\"list-unstyled panel-collapse collapse dependency-groups\" id=\"dependency-groups\"><li>&#13;\n                                        <h4>.NETStandard 2.0</h4>&#13;\n                                    <ul class=\"list-unstyled dependency-group\"><li>&#13;\n                                                    <a href=\"https://www.nuget.org/packages/CassandraCSharpDriver/\">CassandraCSharpDriver</a>&#13;\n                                                    (&gt;= 3.4.0.1)&#13;\n                                            </li>\n                                            <li>&#13;\n                                                    <a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data/\">Paradigm.ORM.Data</a>&#13;\n                                                    (&gt;= 2.2.4)&#13;\n                                            </li>\n                                            <li>&#13;\n                                                    <a href=\"https://www.nuget.org/packages/System.IO.FileSystem.Primitives/\">System.IO.FileSystem.Primitives</a>&#13;\n                                                    (&gt;= 4.3.0)&#13;\n                                            </li>\n                                            <li>&#13;\n                                                    <a href=\"https://www.nuget.org/packages/System.Runtime.Handles/\">System.Runtime.Handles</a>&#13;\n                                                    (&gt;= 4.3.0)&#13;\n                                            </li>\n                                    </ul></li>\n                        </ul><p>&#13;\n                </p><table class=\"table borderless\"><thead><tr><th colspan=\"2\">Version</th>\n                            <th>Downloads</th>\n                            <th>Last updated</th>\n                        </tr></thead><tbody class=\"no-border\"><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/2.2.4\" title=\"2.2.4\">&#13;\n                                            <b>&#13;\n                                                2.2.4&#13;\n                                                    (current)&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        97&#13;\n                                    </td>\n                                    <td>&#13;\n                                        2/7/2018&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/2.2.3\" title=\"2.2.3\">&#13;\n                                            <b>&#13;\n                                                2.2.3&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        92&#13;\n                                    </td>\n                                    <td>&#13;\n                                        1/2/2018&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/2.2.2\" title=\"2.2.2\">&#13;\n                                            <b>&#13;\n                                                2.2.2&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        87&#13;\n                                    </td>\n                                    <td>&#13;\n                                        12/29/2017&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/2.2.1\" title=\"2.2.1\">&#13;\n                                            <b>&#13;\n                                                2.2.1&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        76&#13;\n                                    </td>\n                                    <td>&#13;\n                                        11/30/2017&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Paradigm.ORM.Data.Cassandra/2.2.0\" title=\"2.2.0\">&#13;\n                                            <b>&#13;\n                                                2.2.0&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        75&#13;\n                                    </td>\n                                    <td>&#13;\n                                        11/29/2017&#13;\n                                    </td>\n                                </tr></tbody></table>&#13;",
        "created_at": "2018-07-14T21:12:56+0000",
        "updated_at": "2018-07-14T21:13:03+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "www.nuget.org",
        "preview_picture": "https://avatars2.githubusercontent.com/u/8192926",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10836"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 228,
            "label": "dotnet",
            "slug": "net"
          },
          {
            "id": 1113,
            "label": "nuget",
            "slug": "nuget"
          }
        ],
        "is_public": false,
        "id": 10834,
        "uid": null,
        "title": "Cassandra.NET 1.0.7",
        "url": "https://www.nuget.org/packages/Cassandra.NET/",
        "content": "<p>Enables smooth ORM like integration between Cassandra and C#<br />Support for Select, Add, Update using c# reflection<br />In addition support for numeric calcualtions such as Min/Max/Average/Sum</p><div class=\"install-tabs\"><div class=\"tab-content\"><div role=\"tabpanel\" class=\"tab-pane active\" id=\"package-manager\"><div><p>&#13;\n                &#13;\n                    Install-Package Cassandra.NET -Version 1.0.7&#13;\n                &#13;</p></div></div><div role=\"tabpanel\" class=\"tab-pane\" id=\"dotnet-cli\"><div><p>&#13;\n                &#13;\n                    dotnet add package Cassandra.NET --version 1.0.7&#13;\n                &#13;</p></div></div><div role=\"tabpanel\" class=\"tab-pane\" id=\"paket-cli\"><div><p>&#13;\n                &#13;\n                    paket add Cassandra.NET --version 1.0.7&#13;\n                &#13;</p></div><p>&#13;\n        <i class=\"ms-Icon ms-Icon--Warning\" aria-hidden=\"true\">&#13;\n        &#13;\nThe NuGet Team does not provide support for this client. Please contact its <a href=\"https://fsprojects.github.io/Paket/contact.html\">maintainers</a> for support.&#13;\n                    &#13;</i></p></div></div></div><p>Support for Batching</p><ul class=\"list-unstyled panel-collapse collapse dependency-groups\" id=\"dependency-groups\"><li>&#13;\n                                        <h4>.NETStandard 2.0</h4>&#13;\n                                    <ul class=\"list-unstyled dependency-group\"><li>&#13;\n                                                    <a href=\"https://www.nuget.org/packages/CassandraCSharpDriver/\">CassandraCSharpDriver</a>&#13;\n                                                    (&gt;= 3.3.2)&#13;\n                                            </li>\n                                    </ul></li>\n                        </ul><p>&#13;\n                </p><table class=\"table borderless\"><thead><tr><th colspan=\"2\">Version</th>\n                            <th>Downloads</th>\n                            <th>Last updated</th>\n                        </tr></thead><tbody class=\"no-border\"><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Cassandra.NET/1.0.7\" title=\"1.0.7\">&#13;\n                                            <b>&#13;\n                                                1.0.7&#13;\n                                                    (current)&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        345&#13;\n                                    </td>\n                                    <td>&#13;\n                                        1/1/2018&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Cassandra.NET/1.0.6\" title=\"1.0.6\">&#13;\n                                            <b>&#13;\n                                                1.0.6&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        89&#13;\n                                    </td>\n                                    <td>&#13;\n                                        1/1/2018&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Cassandra.NET/1.0.5\" title=\"1.0.5\">&#13;\n                                            <b>&#13;\n                                                1.0.5&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        90&#13;\n                                    </td>\n                                    <td>&#13;\n                                        1/1/2018&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Cassandra.NET/1.0.4\" title=\"1.0.4\">&#13;\n                                            <b>&#13;\n                                                1.0.4&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        92&#13;\n                                    </td>\n                                    <td>&#13;\n                                        1/1/2018&#13;\n                                    </td>\n                                </tr><tr>\n                                    <td>&#13;\n                                        <a href=\"https://www.nuget.org/packages/Cassandra.NET/1.0.3\" title=\"1.0.3\">&#13;\n                                            <b>&#13;\n                                                1.0.3&#13;\n                                            </b>&#13;\n                                        </a>&#13;\n                                    </td>\n                                    <td>&#13;\n                                        100&#13;\n                                    </td>\n                                    <td>&#13;\n                                        11/23/2017&#13;\n                                    </td>\n                                </tr></tbody></table>&#13;",
        "created_at": "2018-07-14T21:09:30+0000",
        "updated_at": "2018-07-14T21:09:37+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "www.nuget.org",
        "preview_picture": "https://upload.wikimedia.org/wikipedia/commons/1/1e/Apache-cassandra-icon.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10834"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10832,
        "uid": null,
        "title": "10 years of Apache Cassandra",
        "url": "https://www.idgconnect.com/abstract/30973/apache-cassandra",
        "content": "<a href=\"https://www.idgconnect.com/browse_documents/307/software-web-development/application-development\">Application Development</a><ul class=\"article_list\">\n\t\t\t<li>\n\t\t\t\t<p>\n                   \tPosted by\n                   \t<a href=\"https://www.idgconnect.com/author/472\">Kate Hoy</a>\n\t\t\t\n\t\t\t\t\t\t \n\t\t\t\t</p>\n\t\t\t</li>\n\t\t\t<li>\n\t\t\t\t<p>\n\t\t\t\t\ton <a>July 11 2018</a>\n\t\t\t\t</p>\n\t\t\t</li>\n\t\t</ul><p>Apache Cassandra, developed by <a href=\"http://www.linkedin.com/in/avinashlakshman\">Avinash Lakshman</a> and <a href=\"http://www.linkedin.com/in/prmalik\">Prashant Malik</a> to try to solve their Inbox-search problem at Facebook, was published as free software under the Apache V2 license in July 2008. Providing a scalable, high-availability datastore with no single point of failure, Cassandra is well suited for high-availability applications. It supports multi-datacenter replication, and offers massive and linear scalability, so any number of nodes can easily be added to any Cassandra cluster in any datacenter. According to <a href=\"http://cassandra.apache.org/\">the website</a>, the largest known Cassandra setup involves over 300TB of data on over 400 machines.</p><p>After ten years of development, driven in part by contributions from IBM, Twitter and Rackspace, Cassandra is now used by NetFlix, eBay, Twitter, Reddit and <a href=\"http://www.datastax.com/cassandrausers\">many others</a>, and is one of the most popular NoSQL-databases in use today. To find out more about the impact Cassandra has had on the development community, we speak to previous Apache Cassandra project chair Jonathan Ellis, currently SVP and CTO, DataStax; Aaron Morton, CEO at Cassandra consultants, The Last Pickle; and open source consultant Carlos Rolo.</p><h2>How has Cassandra impacted the community?</h2><p>Jonathan Ellis, SVP and CTO, DataStax, first come into contact with Cassandra at the end of 2008, when he was hired by Rackspace to build them a next-generation, scalable database. He explains that Cassandra was one of a number of options at the time that offered ‘NoSQL’, but argues that SQL itself wasn’t the problem: “SQL is a quite reasonable language for getting data in and out of a server.”</p><p>The introduction of <a href=\"https://www.datastax.com/dev/blog/whats-new-in-cql-3-0\">Cassandra Query Language</a> (CQL) with Cassandra 1.1 in 2012 was one of the most important steps for the community, according to Ellis, because it meant developers had an API portable across languages and suitable for a REPL. “We were the first to introduce this,” Ellis explains, “with almost universal adoption of a similar approach by other NoSQL databases.” The only notable holdout today is Amazon’s DynamoDB, and Ellis doesn’t believe that will last – “I predict that it won’t be long before they follow suit as well.”</p><p>For Ellis, the biggest contribution Cassandra has made is that app developers – whether Cassandra users or not -- realized that you don’t need ACID for most common tasks. “Cassandra defaults to <a href=\"https://en.wikipedia.org/wiki/Eventual_consistency\">eventually consistent</a> operations (where “eventually” is typically single-digit or even sub-millisecond latencies), and allows users to opt in to <a href=\"https://www.datastax.com/dev/blog/lightweight-transactions-in-cassandra-2-0\">lightweight transactions</a> when linearizable consistency is called for.”</p><p>Aaron Morton, who was working at Weta Digital when he first came across Cassandra in 2009, says, “[Cassandra] was the first time I felt I could contribute to the code of a database and get involved in an early stage project.” Two years later Morton left Weta to found his own Cassandra consulting firm The Last Pickle. Of Cassandra, Morton says, “DBA's are now concerned with the speed of light when storing data around the globe rather than the speed of disks, that's a big change and in large part is due to the success of Apache Cassandra.” </p><p>Open source consultant, Carlos Rolo, may have joined the party a little later, first coming into contact with Cassandra in early 2011, but for Rolo the impact of Cassandra is simple - it brought distributed databases to everyone.</p><h2>How does Cassandra compare to others in the NoSQL space?</h2><p>Ellis believes Cassandra is uniquely suited for a hybrid world, and indeed, is “the best option if you are building a cloud application that needs real-time responses, always-on reliability, and scalable performance.” However, he notes that does come with some sacrifices in terms of ease-of-use: “like other cloud databases, Cassandra emphasizes <a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\">denormalization</a> over query-time joins, which is a hard concept for RDBMS developers to wrap their minds around.” Ultimately, it depends on how many you’re building for, if it’s just a few hundred or a few thousand users, you likely don’t need Cassandra.</p><p>For Rolo, the ease of scaling and enabling data (geo)distribution are major benefits that are often overlooked. Plus, it is proven at large scale, which other NoSQL databases still have to prove, and a tooling ecosystem is starting to appear which Cassandra has previously lacked in comparison to other databases. But ease of operation is a problem, says Rolo, “Cassandra deployments tend to get painful to manage if something is set wrong.”</p><p>Morton considers one of the major benefits to be the API using CQL which he describes as “stable, well documented, and easy for new users to pick up”. Further, it can run in almost any environment, providing great observability, and its stability means losing a server is rarely a problem. He describes Cassandra as “battle proven technology” explaining there’s a great deal of institutional knowledge in the Apache project. “Not every idea works out, and we've been through a few features that did not set the world on fire. Knowing what not to do is as important as knowing what to change.”   </p><h2>What’s been the biggest challenge with Cassandra?</h2><p>All three of the experts we spoke to agreed that one of the biggest challenges was changing the way people think about data. Ellis clarifies: “Most developers are exposed to the relational model and third normal form in college.  That’s still true today, ten years into the age of NoSQL. Once people get it, it’s like the light turns on, but it can be a challenge to get to that point--because in a distributed world, the rules aren’t just different, they’re upside down.”</p><p>For Rolo, over 80% of the challenges he faces with Cassandra result from companies/people trying to port their relational models over.</p><p>On the technical front, Morton explains, “the whole system has been re-written since version 1.0 which was basically the initial Facebook design. Starting with the networking protocol, moving to the creation of the CQL API, and finally re-writing the storage engine itself.” He credits the contributors who worked on these changes with setting the stage for Cassandra’s ongoing success.</p><p>Both Ellis and Rolo are concerned about a Cassandra skills shortage. Ellis believes the problem has worsened, citing job search engine Indeed as showing 5000+ jobs looking for Cassandra experience today versus around 800 in 2014. However, statistics for permanent job vacancies with a requirement for Apache Cassandra skills from <a href=\"https://www.itjobswatch.co.uk/jobs/uk/apache%20cassandra.do\">IT Jobs Watch</a> show a year-on-year decrease in the number of permanent jobs citing Apache Cassandra. Rolo believes the difference is between the development side and administration side: “a lot of development teams have already Cassandra skills. On the administration side I still think that is an issue, but I might be biased on this one!”</p><h2>The next 10 years</h2><p>Looking ahead to the next ten years, Ellis believes an up-and-coming area in the data space is Graph. “Of course, we’ve had graph databases for years, but they’ve never quite found a killer app, and I think a lot of that is due to early graph databases being limited in scale in a lot of the same ways relational databases were. My prediction is, you’re going to see improvements in both the fundamental graph technologies and in integration of graph with other data models like Cassandra’s tabular or document models.”</p><p>Morton thinks simplicity is key when looking at the future of Cassandra, explaining that reducing barriers to entry for not just Cassandra, but other distributed databases as well, makes it easier for new people to get involved, bringing new ideas that will help push the technology further. Ultimately, Morton believes, “Almost all databases will be distributed databases, just like almost all mobile phones are smart phones, and Cassandra will continue to be a large part of the larger ecosystem.”</p><p>For Rolo, the increase in Cassandra skills, together with improvements to Cassandra itself means it’s not going anywhere. His plan? “Keep rocking with Cassandra and the ecosystem surrounding it!”</p><div class=\"post-list\"><ul><li class=\"arrow\">\n\t\t\t\t\t\t<a href=\"https://www.idgconnect.com/send_detail_report/30973?is_article=true\">Forward to a friend</a>\n\t\t\t\t\t</li>\n\t\t\t\t\t\n\t\t\t\t</ul></div><div class=\"article_pagination_new\"><div class=\"article_previous\"><p>\n\t\t\t\t\tPREVIOUS ARTICLE\n\t\t\t\t</p><a href=\"https://www.idgconnect.com/abstract/30940/c-suite-career-advice-stephen-parker-parker-software\">«C-suite career advice: Stephen Parker, Parker Software</a></div><div class=\"article_next\"><p>\n\t\t\t\t\tNEXT ARTICLE\n\t\t\t\t</p><a href=\"https://www.idgconnect.com/abstract/30974/the-cmo-files-sarah-taylor-smartfocus\">The CMO Files: Sarah Taylor, SmartFocus»</a></div></div>",
        "created_at": "2018-07-13T21:05:59+0000",
        "updated_at": "2018-07-13T21:06:04+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 6,
        "domain_name": "www.idgconnect.com",
        "preview_picture": "http://www.idgconnect.com/IMG/972/50972/shutterstock-697413424.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10832"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          },
          {
            "id": 1103,
            "label": "prometheus",
            "slug": "prometheus"
          }
        ],
        "is_public": false,
        "id": 10816,
        "uid": null,
        "title": "zegelin/cassandra-exporter",
        "url": "https://github.com/zegelin/cassandra-exporter",
        "content": "<p><em>Project Status: alpha</em></p><h2>Introduction</h2><p><em>cassandra-exporter</em> is a Java agent that exports Cassandra metrics to <a href=\"http://prometheus.io\" rel=\"nofollow\">Prometheus</a>.</p><p>It enables high performance collection of Cassandra metrics and follows the Prometheus best practices for metrics naming and labeling.</p><p>For example, the following PromQL query will return an estimate of the number of pending compactions per keyspace, per node.</p><pre>sum(cassandra_table_estimated_pending_compactions) by (cassandra_node, keyspace)\n</pre><h2>Compatibility</h2><p><em>cassandra-exporter</em> is has been tested with:</p><table><thead><tr><th>Component</th>\n<th>Version</th>\n</tr></thead><tbody><tr><td>Apache Cassandra</td>\n<td>3.11.2</td>\n</tr><tr><td>Prometheus</td>\n<td>2.0 and later</td>\n</tr></tbody></table><p>Other Cassandra and Prometheus versions will be tested for compatibility in the future.</p><h2>Usage</h2><p>Download the latest release and copy <code>cassandra-exporter-agent-&lt;version&gt;.jar</code> to <code>$CASSANDRA_HOME/lib</code> (typically <code>/usr/share/cassandra/lib</code> in most package installs).</p><p>Then edit <code>$CASSANDRA_CONF/cassandra-env.sh</code> (typically <code>/etc/cassandra/cassandra-env.sh</code>) and append the following:</p><pre>JVM_OPTS=\"$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/cassandra-exporter-agent-&lt;version&gt;.jar=http://localhost:9998/\"\n</pre><p>Then (re-)start Cassandra.</p><p>Prometheus metrics will be available at <code>http://localhost:9998/metrics</code>.</p><p>Configure Prometheus to scrape the endpoint by adding the following to <code>prometheus.yml</code>:</p><pre>scrape_configs:\n  ...\n  \n  - job_name: 'cassandra'\n    static_configs:\n      - targets: ['&lt;cassandra node IP&gt;:9998']\n</pre><p>See the <a href=\"https://prometheus.io/docs/prometheus/latest/configuration/configuration/#%3Cscrape_config%3E\" rel=\"nofollow\">Prometheus documentation</a> for more details on configuring scrape targets.</p><p>Viewing the exposed endpoint in a web browser will display a HTML version of the exported metrics.</p><p>To view the raw, plain text metrics (in the Prometheus text exposition format), either request the endpoint with a HTTP client that prefers plain text\n(or one that can specify the <code>Accept: text/plain</code> header) or add the following query parameter to the URL: <code>?x-content-type=text/plain</code>.</p><p>An experimental JSON output is also provided, via <code>Accept: application/json</code> or <code>?x-content-type=application/json</code>.\nThe format/structure of this output is subject to change.</p><h2>Options</h2><p>Currently only the HTTP endpoint (address &amp; port) can be configured.</p><h2>Features</h2><h3>Performance</h3><p>JMX is <em>slow</em>, really slow. JMX adds significant overhead to every method invocation on exported MBean methods, even when those methods are called from within the same JVM.\nOn a 300-ish table Cassandra node, trying to collect all exposed metrics via JVM resulted in a collection time that was upwards of 2-3 <em>seconds</em>.\nFor exporters that run as a separate process there is additional overhead of inter-process communications and that time can reach the 10's of seconds.</p><p><em>cassandra-exporter</em> on the same node collects all metrics in 10-20 <em>milliseconds</em>.</p><h3>Best practices</h3><p>The exporter follows Prometheus best practices for metric names, labels and data types.</p><p>Aggregate metrics, such as the aggregated table metrics at the keyspace and node level, are skipped. Instead these should be aggregated using PromQL queries or Prometheus recording rules.</p><p>Metrics are coalesced when appropriate so they share the same name, opting for <em>labels</em> to differentiate indiviual time series. For example, each table level metric has a constant name and at minimum a <code>table</code> &amp; <code>keyspace</code> label, which allows for complex PromQL queries.</p><p>For example the <code>cassandra_table_operation_latency_seconds[_count|_sum]</code> summary metric combines read, write, range read, CAS prepare, CAS propose and CAS commit latency metrics together into a single metric family.\nA summary exposes percentiles (via the <code>quantile</code> label), a total count of recorded samples (via the <code>_count</code> metric),\nand (if available, <code>NaN</code> otherwise) an accumulated sum of all samples  (via the <code>_sum</code> metric).</p><p>Individual time-series are separated by different labels. In this example, the operation type is exported as the <code>operation</code> label.\nThe source <code>keyspace</code>, <code>table</code>, <code>table_type</code> (table, view or index), <code>table_id</code> (CF UUID), and numerous other metadata labels are available.</p><pre>cassandra_table_operation_latency_seconds_count{keyspace=\"system_schema\",table=\"tables\",table_type=\"table\",operation=\"read\",...}\ncassandra_table_operation_latency_seconds_count{keyspace=\"system_schema\",table=\"tables\",table_type=\"table\",operation=\"write\",...}\ncassandra_table_operation_latency_seconds_count{keyspace=\"system_schema\",table=\"keyspaces\",table_type=\"table\",operation=\"read\",...}\ncassandra_table_operation_latency_seconds_count{keyspace=\"system_schema\",table=\"keyspaces\",table_type=\"table\",operation=\"write\",...}\n</pre><p>These metrics can then be queried:</p><pre>sum(cassandra_table_operation_latency_seconds_count) by (keyspace, operation) # total operations by keyspace &amp; type\n</pre><table><thead><tr><th>Element</th>\n<th>Value</th>\n</tr></thead><tbody><tr><td><code>{keyspace=\"system\",operation=\"write\"}</code></td>\n<td>13989</td>\n</tr><tr><td><code>{keyspace=\"system\",operation=\"cas_commit\"}</code></td>\n<td>0</td>\n</tr><tr><td><code>{keyspace=\"system\",operation=\"cas_prepare\"}</code></td>\n<td>0</td>\n</tr><tr><td><code>{keyspace=\"system\",operation=\"cas_propose\"}</code></td>\n<td>0</td>\n</tr><tr><td><code>{keyspace=\"system\",operation=\"range_read\"}</code></td>\n<td>10894</td>\n</tr><tr><td><code>{keyspace=\"system\",operation=\"read\"}</code></td>\n<td>74</td>\n</tr><tr><td><code>{keyspace=\"system_schema\",operation=\"write\"}</code></td>\n<td>78</td>\n</tr><tr><td><code>{keyspace=\"system_schema\",operation=\"cas_commit\"}</code></td>\n<td>0</td>\n</tr><tr><td><code>{keyspace=\"system_schema\",operation=\"cas_prepare\"}</code></td>\n<td>0</td>\n</tr><tr><td><code>{keyspace=\"system_schema\",operation=\"cas_propose\"}</code></td>\n<td>0</td>\n</tr><tr><td><code>{keyspace=\"system_schema\",operation=\"range_read\"}</code></td>\n<td>75</td>\n</tr><tr><td><code>{keyspace=\"system_schema\",operation=\"read\"}</code></td>\n<td>618</td>\n</tr></tbody></table><h3>Global Labels</h3><p>The exporter does attach global labels to the exported metrics. At this time these cannot be disabled without recompiling the agent.</p><p>These labels are:</p><ul><li>\n<p><code>cassandra_cluster_name</code></p>\n<p>The name of the cluster, as specified in cassandra.yaml</p>\n</li>\n<li>\n<p><code>cassandra_host_id</code></p>\n<p>The unique UUID of the node</p>\n</li>\n<li>\n<p><code>cassandra_node</code></p>\n<p>The IP address of the node</p>\n</li>\n<li>\n<p><code>cassandra_datacenter</code></p>\n<p>The configured data center name of the node</p>\n</li>\n<li>\n<p><code>cassandra_rack</code></p>\n<p>The configured rack name of the node</p>\n</li>\n</ul><p>These labels allow aggregation of metrics at the cluster, data center and rack levels.</p><p>While these labels could be defined in the prometheus scrape config, the authors feel that having these labels be automatically\napplied simplifies things, especially when Prometheus is monitoring multiple clusters across numerous DCs and racks.</p><h3>JMX Standalone (Experimental)</h3><p>While it is preferable to run <em>cassandra-exporter</em> as a Java agent for performance, it can instead be run as an external application if required.\nMetrics will be queried via JMX.</p><p>The set of metrics should be identical, but currently some additional metadata labels attached to the <code>cassandra_table_*</code> metrics will\nnot be available.</p><p>This was originally designed to assist with benchmarking and development of the exporter. Currently the JMX RMI service URL and HTTP endpoint\nvalues are hard-coded. The application will need to be recompiled if these parameters need to be changed.</p><h2>Exported Metrics</h2><p>See the <a href=\"https://github.com/zegelin/cassandra-exporter/wiki/Exported-Metrics\">Exported Metrics</a> wiki page for a list.</p><p>We suggest viewing the metrics endpoint (e.g., <a href=\"http://localhost:9998/metrics\" rel=\"nofollow\">http://localhost:9998/metrics</a>) in a browser to get an understanding of what metrics\nare exported by your Cassandra node.</p><h2>Unstable, Missing &amp; Future Features</h2><p>See the <a href=\"https://github.com/zegelin/cassandra-exporter/issues\">project issue tracker</a> for a complete list.</p><ul><li>\n<p>Configuration parameters</p>\n<p>Currently only the listen address &amp; port can be configured.</p>\n<p>Allow configuration of:</p>\n<ul><li>listen address and port</li>\n<li>exported metrics (aka, blacklist certain metrics)</li>\n<li>enable/disable global labels</li>\n<li>exclude help from JSON</li>\n</ul></li>\n<li>\n<p>JVM metrics</p>\n<p>Future versions should add support for collecting and exporting JVM metrics (memory, GC pause times, etc).</p>\n</li>\n<li>\n<p>Add some example queries</p>\n</li>\n<li>\n<p>Add Grafana dashboard templates</p>\n</li>\n<li>\n<p>Documentation improvements</p>\n</li>\n<li>\n<p>Improve standalone JMX exporter</p>\n<ul><li>Configuration parameters</li>\n</ul></li>\n</ul>",
        "created_at": "2018-07-10T04:25:08+0000",
        "updated_at": "2018-07-10T04:28:03+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 5,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/19296634?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10816"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10787,
        "uid": null,
        "title": "Using Apache Cassandra — A few things before you start",
        "url": "https://hackernoon.com/using-apache-cassandra-a-few-things-before-you-start-ac599926e4b8?gi=64bd77351bbb",
        "content": "<p id=\"8aee\" class=\"graf graf--p graf-after--h3\">The <a href=\"https://www.slideshare.net/planetcassandra/cassandra-summit-2014-cql-under-the-hood-39445761\" data-href=\"https://www.slideshare.net/planetcassandra/cassandra-summit-2014-cql-under-the-hood-39445761\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">CQL</a> — Cassandra Query language gives an almost SQL type interface to Apache Cassandra. I have found many times,that many who use this,do not know about some important points of Cassandra that makes it different from SQL databases like Postgres. Same is the case for operations team, there are some aspects related to storage, GC settings , that many are not aware of. I am not an expert in Cassandra internals and don’t aspire to be if I can avoid it.This is mostly a note to myself, and something which I can ask others to refer to instead of repeating over email a gazillion times. There are lot of other parts like repair etc which I have left out. The intention here is to make this as short as possible, but if you feel somethings are to be added, please comment.</p><h4 id=\"2e37\" class=\"graf graf--h4 graf-after--p\">Cassandra has Tune-able Consistency — not just eventual consistency</h4><p id=\"0963\" class=\"graf graf--p graf-after--h4\">Many considering Cassandra as a replacement for SQL database like Postgres, MySQL or Oracle, shy away thinking that eventual consistency of NoSQL does not meet their requirement. In Cassandra ,however consistency is configurable. This means that with some write and read speed sacrifice, you can have strong consistency as well as high availability. Cassandra can be used for small data as well as big data; depending on your use case you can tune the consistency per key-space or even per-operation.</p><blockquote id=\"86c1\" class=\"graf graf--blockquote graf-after--p\"><div>Cassandra values Availability and Partitioning tolerance (AP). Tradeoffs between consistency and latency are tunable in Cassandra. You can get strong consistency with Cassandra (with an increased latency).<br /><a href=\"https://wiki.apache.org/cassandra/ArchitectureOverview\" data-href=\"https://wiki.apache.org/cassandra/ArchitectureOverview\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\">https://wiki.apache.org/cassandra/ArchitectureOverview</a></div></blockquote><p id=\"712d\" class=\"graf graf--p graf-after--blockquote\">At this point it may be a good idea to have a short recap of CAP theorem as there is a lot of confusion translating the theoretical surmise to the practical world.</p><blockquote id=\"e767\" class=\"graf graf--blockquote graf-after--p\"><div>In 2000, Dr. Eric Brewer gave a keynote at the <em class=\"markup--em markup--blockquote-em\">Proceedings of the Annual ACM Symposium on Principles of Distributed Computing</em> in which he laid out his famous CAP Theorem: <em class=\"markup--em markup--blockquote-em\">a shared-data system can have at most two of the three following properties: </em><strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\">C</em></strong><em class=\"markup--em markup--blockquote-em\">onsistency, </em><strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\">A</em></strong><em class=\"markup--em markup--blockquote-em\">vailability, and tolerance to network </em><strong class=\"markup--strong markup--blockquote-strong\"><em class=\"markup--em markup--blockquote-em\">P</em></strong><em class=\"markup--em markup--blockquote-em\">artitions.</em></div></blockquote><p id=\"cb8b\" class=\"graf graf--p graf-after--blockquote\">This applies to any distributed data base, not just Cassandra.So Cassandra can provide C and A not P ? Is it a big problem ?</p><p id=\"c5f4\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">Short answer — It is not. Skip the rest of the section if you are in a hurry.</strong></p><p id=\"b375\" class=\"graf graf--p graf-after--p\">Long answer read on.Here is the excerpt from Cassandra docs. (DataStax’s docs)</p><blockquote id=\"82aa\" class=\"graf graf--blockquote graf-after--p\"><div>… You can tune Cassandra’s consistency level per-operation, or set it globally for a cluster or datacenter. You can vary the consistency for individual read or write operations so that the data returned is more or less consistent, as required by the client application. This allows you to make Cassandra act more like a CP (consistent and partition tolerant) or AP (highly available and partition tolerant) system according to the CAP theorem, depending on the application requirements.</div></blockquote><blockquote id=\"8be4\" class=\"graf graf--blockquote graf-after--blockquote\"><div><strong class=\"markup--strong markup--blockquote-strong\">Note:</strong> It is not possible to “tune” Cassandra into a completely CA system. See <a href=\"https://codahale.com/you-cant-sacrifice-partition-tolerance/\" data-href=\"https://codahale.com/you-cant-sacrifice-partition-tolerance/\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener\" target=\"_blank\">You Can’t Sacrifice Partition Tolerance</a> for a more detailed discussion. -<a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDataConsistency.html#dmlAboutDataConsistency__eventual-consistency\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDataConsistency.html#dmlAboutDataConsistency__eventual-consistency\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlAboutDataConsistency.html</a></div></blockquote><p id=\"f1c2\" class=\"graf graf--p graf-after--blockquote\">Here is an excerpt from the article linked in the DataStax’ s Cassandra documentation page.</p><blockquote id=\"15b8\" class=\"graf graf--blockquote graf-after--p\"><div>Of the CAP theorem’s Consistency, Availability, and Partition Tolerance, <strong class=\"markup--strong markup--blockquote-strong\">Partition Tolerance is mandatory in distributed systems. You cannot not choose it. </strong>Instead of CAP, you should think about your availability in terms of <em class=\"markup--em markup--blockquote-em\">yield</em> (percent of requests answered successfully) and <em class=\"markup--em markup--blockquote-em\">harvest</em> (percent of required data actually included in the responses) and which of these two your system will sacrifice when failures happen. -<a href=\"https://codahale.com/you-cant-sacrifice-partition-tolerance/\" data-href=\"https://codahale.com/you-cant-sacrifice-partition-tolerance/\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://codahale.com/you-cant-sacrifice-partition-tolerance/</a></div></blockquote><p id=\"c333\" class=\"graf graf--p graf-after--blockquote\">What the above article explains in depth is that Availability is tied to Network Partitioning or Partition Tolerance.Worst case scenario network partitions are quite rare inside a Data Center network. Also network partitions cannot be prevented from happening. It is ever present, though mostly transient and intermittent. The risk of network partitioning across many nodes in a cluster so as to disrupt Availability for a multi-node cluster is very less.</p><p id=\"ddd6\" class=\"graf graf--p graf-after--p\">So with Cassandra you can have as good a C and A system as practically possible.</p><h4 id=\"f386\" class=\"graf graf--h4 graf-after--p\"><strong class=\"markup--strong markup--h4-strong\">Give Importance to modelling the Partition key</strong></h4><p id=\"607e\" class=\"graf graf--p graf-after--h4\">If there is only one thing that you should read,maybe it is the link below</p><p><a href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" data-href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">The most important thing to know in Cassandra data modeling: The primary key</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Patrick is regarded as one of the foremost experts of Apache Cassandra and data modeling techniques. As the Chief…</em>www.datastax.com</a></p><p id=\"958c\" class=\"graf graf--p graf-after--mixtapeEmbed\">What is the Partition Key ? It is the first part of the Primary Key or the Primary key itself if Primary key is not composite</p><p id=\"5327\" class=\"graf graf--p graf-after--p\">Why is this most important part ?</p><p id=\"fbfc\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">To have balanced write of data to multiple Cassandra nodes in the cluster and subsequent balanced reads of the data.</strong></p><blockquote id=\"fe60\" class=\"graf graf--blockquote graf-after--p\"><div>When data is inserted into the cluster, the first step is to apply a hash function to the partition key. The output is used to determine what node (and replicas) will get the data.-<a href=\"https://docs.datastax.com/en/archived/cassandra/3.x/cassandra/architecture/archPartitionerM3P.html\" data-href=\"https://docs.datastax.com/en/archived/cassandra/3.x/cassandra/architecture/archPartitionerM3P.html\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.datastax.com/en/archived/cassandra/3.x/cassandra/architecture/archPartitionerM3P.html</a></div></blockquote><p id=\"1e3d\" class=\"graf graf--p graf-after--blockquote\">Here are two main goals to consider for modelling the data</p><p><a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" data-href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Basic Rules of Cassandra Data Modeling</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Learn more about Apache Cassandra and data modeling READ MORE DS:220 COURSE Picking the right data model is the hardest…</em>www.datastax.com</a></p><p id=\"1dab\" class=\"graf graf--p graf-after--mixtapeEmbed\">1. Spread data evenly around the cluster — Model Partition Key</p><p id=\"8ae6\" class=\"graf graf--p graf-after--p\">2. Minimize the number of partitions read -Model Partition Key and Clustering keys</p><p id=\"9ea6\" class=\"graf graf--p graf-after--p\">Let us take an example. Below is a initial modelling of table where the data is some events (say political rallies, speeches etc) that has occurred in a particular location, centered over latitude,longitude and say having a radius of 250 meters. Each location has an influential candidate of that area. Sometimes the same area can have multiple influential candidates. I have illustrated a slightly complex example so as to show the flexibility in data types present in Cassandra,and all the aspects to consider when modelling the key. The actual cell can be a telecom cell with multiple coverage by different operators or different technologies. The example I give here is a bit contrived and for illustration only.</p><figure id=\"716e\" class=\"graf graf--figure graf--iframe graf-after--p\"><p id=\"8775\" class=\"graf graf--p graf-after--figure\">Note that partition key is chosen assuming that events are distributed evenly across a city, the key will distribute the data from multiple locations evenly across available Cassandra nodes.</p><p id=\"9ffa\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\">The partition query should be modeled for efficient retrieval of the data application needs</strong></p><p id=\"9633\" class=\"graf graf--p graf-after--p\">The modelling of Tables and thereby the partition key is primarily with consideration of efficient data retrieval.</p><p id=\"02b8\" class=\"graf graf--p graf-after--p\">Assume that we need to query all events happening in a location by a Candidate for a time interval.The queries will be a set of statements like</p><blockquote id=\"ec1c\" class=\"graf graf--blockquote graf-after--p\"><div>select * from demo_table where bin_cell_key = (1234, 222, ‘Candidate1’) and time_key &gt;= (‘06:00:00.000000000’, ‘06:00:00.000000000’) and time_key &lt; (‘07:30:00.000000000’, ‘07:30:00.000000000’)</div></blockquote><blockquote id=\"6b6b\" class=\"graf graf--blockquote graf-after--blockquote\"><div>select * from demo_table where bin_cell_key = (1234, 223, ‘Candidate1’) ..</div></blockquote><p id=\"feeb\" class=\"graf graf--p graf-after--blockquote\">But to compose this <em class=\"markup--em markup--p-em\">bin_cell_key</em> we need to know first which Candidates are there in which locations. For this we need to model helper tables. Note — Data duplication is okay in NoSQL data modelling and to have the same effect of JOINs this is needed. Some helper tables to get the bin_cell_key</p><blockquote id=\"42fa\" class=\"graf graf--blockquote graf-after--p\"><div>create table cell_bin (cell text, bin tuple&lt;int,int&gt;, PRIMARY KEY (cell,bin));</div></blockquote><p id=\"ac52\" class=\"graf graf--p graf-after--blockquote\">Example CQL:</p><blockquote id=\"011b\" class=\"graf graf--blockquote graf-after--p\"><div>select * from cell_bin where cell=’Candidate1’;</div></blockquote><blockquote id=\"f71c\" class=\"graf graf--blockquote graf-after--blockquote\"><div>cell | bin<br /> — — — — -+ — — — — —-<br />Candidate1| (1234, 222)<br />Candidate1| (1234, 223)</div></blockquote><p id=\"f849\" class=\"graf graf--p graf-after--blockquote\">And similarly for the other way round</p><blockquote id=\"caf0\" class=\"graf graf--blockquote graf-after--p\"><div>create table bin_cell (bin tuple&lt;int,int&gt;, cell text, PRIMARY KEY (bin,cell));</div></blockquote><p id=\"21a2\" class=\"graf graf--p graf-after--blockquote\">Example CQL:</p><blockquote id=\"0774\" class=\"graf graf--blockquote graf-after--p\"><div>cqlsh:demo_keyspace&gt; select * from bin_cell where bin = (1234, 222);<br /> bin | cell<br /> — — — — -+ — — — —— -<br /> (1234, 222) | Candidate1<br /> (1234, 222) | Candidate2</div></blockquote><p id=\"fca6\" class=\"graf graf--p graf-after--blockquote\">We can stop here. But if you are curious read on. What if we want to aggregate all events that has taken place in a region irrespective of the Candidate. For this we need to split the cell out. Why ? because in case of a composite partition key all the elements need to be specified in the query.</p><blockquote id=\"ebf9\" class=\"graf graf--blockquote graf-after--p\"><div>select * from demo_table where bin = (1234,222) and year=2017 and month=12 and day=1;</div></blockquote><p id=\"81aa\" class=\"graf graf--p graf-after--blockquote\">The table for below which also adds time to the partition key so that data from different days are distributed across available nodes are given below.</p><figure id=\"2100\" class=\"graf graf--figure graf--iframe graf-after--p\"><blockquote id=\"861f\" class=\"graf graf--blockquote graf-after--figure\"><div>create table demo_table( year int,month int,day int, bin tuple&lt;double,double&gt;, cell text, time_key tuple&lt;timestamp,timestamp&gt;,event text, PRIMARY KEY((bin,year,month,day),cell,time_key));</div></blockquote><h4 id=\"040b\" class=\"graf graf--h4 graf-after--blockquote\">Test and measure your reads &amp; write via nodetool cfstats</h4><p id=\"f60b\" class=\"graf graf--p graf-after--h4\">How do we know if our data model is distributing writes across nodes. How do we know if the write latency and read latency is distributed across nodes and if it is linearly scale able in proportional to the nodes added. The answer to all of this via node tool cfstats command</p><p><a href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCFstats.html\" data-href=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCFstats.html\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"https://docs.datastax.com/en/cassandra/2.1/cassandra/tools/toolsCFstats.html\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">nodetool cfstats</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Provides statistics about tables.</em>docs.datastax.com</a></p><p id=\"0c51\" class=\"graf graf--p graf-after--mixtapeEmbed\">You need to run long runs with write and then read operations using multiple nodes and everyday update a table like one below based on the output from cfstats. Soon you will know if your write and read are balanced. Actually adding more nodes should also decrease your read time linearly. This is really beautiful.</p><figure id=\"f30b\" class=\"graf graf--figure graf--iframe graf-after--p\"><h4 id=\"73a0\" class=\"graf graf--h4 graf-after--figure\">Do not use ALLOW FILTERING and IN Clause in CQL indiscriminately</h4><p id=\"1258\" class=\"graf graf--p graf-after--h4\">If you feel that there is no way out; then please read the section above. Most probably your table modelling has to be refactored.</p><blockquote id=\"3ad8\" class=\"graf graf--blockquote graf-after--p\"><div>When your query is rejected by Cassandra because it needs filtering, you should resist the urge to just add ALLOW FILTERING to it. You should think about your data, your model and what you are trying to do. -<a href=\"https://www.datastax.com/dev/blog/allow-filtering-explained-2\" data-href=\"https://www.datastax.com/dev/blog/allow-filtering-explained-2\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://www.datastax.com/dev/blog/allow-filtering-explained-2</a></div></blockquote><p id=\"820a\" class=\"graf graf--p graf-after--blockquote\"><strong class=\"markup--strong markup--p-strong\">Or for that matter IF clause</strong></p><p id=\"fba3\" class=\"graf graf--p graf-after--p\">The new versions of Cassandra supports light-weight transactions. In CQL this is done via the IF clause. <em class=\"markup--em markup--p-em\">Insert into table IF NOT EXISTS</em>.</p><blockquote id=\"8bee\" class=\"graf graf--blockquote graf-after--p\"><a href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlLtwtTransactions.html\" data-href=\"https://docs.datastax.com/en/cassandra/3.0/cassandra/dml/dmlLtwtTransactions.html\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"noopener\" target=\"_blank\">Lightweight transactions</a><div> should not be used casually, as the latency of operations increases fourfold due to the due to the round-trips necessary between the CAS coordinators. -</div><a href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useInsertLWT.html\" data-href=\"https://docs.datastax.com/en/cql/3.3/cql/cql_using/useInsertLWT.html\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener\" target=\"_blank\">https://docs.datastax.com/en/cql/3.3/cql/cql_using/useInsertLWT.html</a></blockquote><h4 id=\"185f\" class=\"graf graf--h4 graf-after--blockquote\">Be aware of the JVM GC Suck-age as JVM Heap Increases</h4><p id=\"e711\" class=\"graf graf--p graf-after--h4\">Cassandra runs on the JVM and relies on the OS page cache for improving performance. <em class=\"markup--em markup--p-em\">There is no need to throw huge amounts of RAM at Cassandra</em>. Cassandra performance should increase by adding more low powered nodes.We have run our long runs for about 2 weeks with load on 2 GB JVM heap, and Cassandra had never once gone down.</p><p id=\"df9c\" class=\"graf graf--p graf-after--p\">JVM GC suckage is directly proportional to the JVM heap. This is true for any Java process and also for Cassandra</p><figure id=\"be45\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*HQhjWbOdml9VTeSYo18Pdw.png\" data-width=\"533\" data-height=\"400\" src=\"https://cdn-images-1.medium.com/max/1600/1*HQhjWbOdml9VTeSYo18Pdw.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">source-<a href=\"https://www.slideshare.net/mattdennis/cassandra-antipatterns\" data-href=\"https://www.slideshare.net/mattdennis/cassandra-antipatterns\" class=\"markup--anchor markup--figure-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\">https://www.slideshare.net/mattdennis/cassandra-antipatterns</a></figcaption></figure><p id=\"da9f\" class=\"graf graf--p graf-after--figure\"><em class=\"markup--em markup--p-em\">Some illuminating quotes</em></p><p id=\"3659\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">Many users new to Cassandra are tempted to turn up Java heap size too high, which consumes the majority of the underlying system’s RAM. In most cases, increasing the Java heap size is actually </em><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">detrimental</em></strong><em class=\"markup--em markup--p-em\"> for these reasons:</em></p><p id=\"3569\" class=\"graf graf--p graf-after--p\"><strong class=\"markup--strong markup--p-strong\"><em class=\"markup--em markup--p-em\">1</em>. <em class=\"markup--em markup--p-em\">In most cases, the capability of Java to gracefully handle garbage collection above 8GB quickly diminishes.</em></strong></p><p id=\"0c46\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">2.Modern operating systems maintain the OS page cache for frequently accessed data and are very good at keeping this data in memory, but can be prevented from doing its job by an elevated Java heap size.</em></p><p id=\"f7a1\" class=\"graf graf--p graf-after--p\"><em class=\"markup--em markup--p-em\">If you have more than 2GB of system memory, which is typical, keep the size of the Java heap relatively small to allow more memory for the page cache .</em></p><p><a href=\"http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html\" data-href=\"http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html\" class=\"markup--anchor markup--mixtapeEmbed-anchor\" title=\"http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_tune_jvm_c.html\"><strong class=\"markup--strong markup--mixtapeEmbed-strong\">Tuning Java resources</strong><br /><em class=\"markup--em markup--mixtapeEmbed-em\">Consider tuning Java resources in the event of a performance degradation or high memory consumption.</em>docs.datastax.com</a></p><p id=\"4efa\" class=\"graf graf--p graf-after--mixtapeEmbed\">Database like <a href=\"http://www.scylladb.com/open-source/\" data-href=\"http://www.scylladb.com/open-source/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Scylla DB</a> have ported the Cassandra design on to C++ so as to avoid the GC pauses and other such problems related to JVM. But as long as you keep the JVM heap around 8 GB things should be fine.</p><p id=\"e23d\" class=\"graf graf--p graf-after--p\">An update here- While I was working with Cassandra G1GC was not deemed stable enough to be used. Now DataStax version of Cassandra used G1GC. G1GC can handle larger heaps; however Cassandra can use RAM heavily for page caches, filters etc, so all the above still makes sense. Limit JVM heap to the minimum you need and leave the rest of the memory for Cassandra process.</p><h4 id=\"a0f6\" class=\"graf graf--h4 graf-after--p\">Cassandra does not like SAN/Shared Storage</h4><p id=\"c898\" class=\"graf graf--p graf-after--h4\">Cassandra is designed for the low fetch times of spinning disks by only appending data to the end for writes and minimize disk seek time during read via application selected partition keys and thereby spreading reads across multiple nodes — Regarding storage a good slide to go through <a href=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" data-href=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" class=\"markup--anchor markup--p-anchor\" title=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" rel=\"noreferrer noopener noopener noopener\" target=\"_blank\">https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra</a></p><figure id=\"9e1c\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*SEX2I_8ohef5sG65VuX5ow.png\" data-width=\"613\" data-height=\"449\" data-is-featured=\"true\" src=\"https://cdn-images-1.medium.com/max/1600/1*SEX2I_8ohef5sG65VuX5ow.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">source-<a href=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" data-href=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" class=\"markup--anchor markup--figure-anchor\" title=\"https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra\" rel=\"noreferrer noopener noopener noopener noopener noopener\" target=\"_blank\">https://www.slideshare.net/johnny15676/why-does-my-choiceofstorage-matterwithcassandra</a></figcaption></figure><blockquote id=\"5b27\" class=\"graf graf--blockquote graf-after--figure\"><div>Customer/User — “We have an awesome SAN and would like to use it for Cassandra.”<br />DataStax — “We don’t recommend shared storage for Cassandra.”<br />Customer/User — “Why not.”<br />DataStax — “Two reasons really. One — performance suffers. Two — shared storage introduces a single point of failure into the architecture.”<br />Customer/User — “Our SAN is awesome and has never had any down time and can preform a kagillion IOPS. So why exactly shouldn’t we use shared storage.”</div></blockquote><blockquote id=\"8db5\" class=\"graf graf--blockquote graf-after--blockquote\"><a href=\"https://www.datastax.com/dev/blog/impact-of-shared-storage-on-apache-cassandra\" data-href=\"https://www.datastax.com/dev/blog/impact-of-shared-storage-on-apache-cassandra\" class=\"markup--anchor markup--blockquote-anchor\" rel=\"nofollow noopener noopener\" target=\"_blank\">https://www.datastax.com/dev/blog/impact-of-shared-storage-on-apache-cassandra</a></blockquote><h4 id=\"2539\" class=\"graf graf--h4 graf-after--blockquote\">Horizontal Scale-ability -Transparent Sharding vs Application Level Sharding</h4><p id=\"f4c4\" class=\"graf graf--p graf-after--h4\">Database sharding is a way of horizontally scaling database. Application level sharding means that the logic of partitioning data across multiple node is done at the application level. That is based on some key- classic example — East Coast vs West Coast or <a href=\"http://www.malinga.me/application-aware-sharding-for-a-mysql-database/\" data-href=\"http://www.malinga.me/application-aware-sharding-for-a-mysql-database/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">similar</a>; your DB write code will select a Database to connect , write and read. The problem is that the complexity of application level sharding quickly gets complex and it is not a good way to scale . Cassandra and most NoSQL databases does sharding transparently (as you have seen via the partition key). This is a pretty big advantage as without this horizontal scaling is a hard problem.</p><h4 id=\"db8e\" class=\"graf graf--h4 graf-after--p\">Open-Source and Commercial</h4><p id=\"a666\" class=\"graf graf--p graf-after--h4 graf--trailing\">As of the time of writing this, relations between Apache Foundation and Datastax- which was one of the largest contributor to Cassandra ? have soured. There is an commercial version of Cassandra — Datastax Enterprise Edition and open source version is the Apache Cassandra. The Java driver of Cassandra has two version, the open source and DSE provided, and you cannot use commercial driver with open source Cassandra. For other languages like Go the driver is open source.</p></figure></figure></figure>",
        "created_at": "2018-07-07T09:36:52+0000",
        "updated_at": "2018-07-07T09:37:04+0000",
        "published_at": "2018-02-12T17:06:39+0000",
        "published_by": [
          "Alex Punnen"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 11,
        "domain_name": "hackernoon.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*SEX2I_8ohef5sG65VuX5ow.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10787"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          }
        ],
        "is_public": false,
        "id": 10777,
        "uid": null,
        "title": "Real-time Data Integration with Kafka and Cassandra (Ewen Cheslack-Po…",
        "url": "https://www.slideshare.net/DataStax/realtime-data-integration-with-kafka-and-cassandra-ewen-cheslackpostava-confluent-c-summit-2016",
        "content": "<p>No notes for slide</p>First I want to quickly introduce myself so you know where I’m coming from. I’m an engineer at Confluent, a company founded by the co-creators of Apache Kafka, and we’re building what we call a stream data platform to help companies capture and leverage all their real-time data. I’m also a committer on the Apache Kafka project and the lead at Confluent on the Connect project, which itself is part of the open source Apache Kafka project.More types of data stores with specialized functionality – e.g. rise of NoSQL systems handling document-oriented and columnar stores. A lot more sources of data. <br />Rise of secondary data stores and indexes – e.g. Elasticsearch for efficient text-based queries, graph DBs for graph-oriented queries, time series databases. A lot more destinations for data, and a lot of transformations along the way to those destinations. <br />Real-time: data needs to be moved between these systems continuously and at low latency. <br />Unfortunately, as you build up large, complex data pipelines in an ad hoc fashion by connecting different data systems that need copies of the same data with one-off connectors for those systems, or build out custom connectors for stream processing frameworks to handle different sources and sinks of streaming data, we end up with a giant, unmaintainable mess. <p>This mess has a huge impact on productivity and agility once you get past just a few systems. Adding any new data storage system or stream processing job requires carefully tracking down all the downstream systems that might be affected, which may require coordinating with dozens of teams and code spread across many repositories. Trying to change one data source’s data format can impact many downstream systems, yet there’s no simple way to discover how these jobs are related. </p><p>This is a real problem that we’re seeing across a variety of companies today. We need to do something to simplify this picture. While Confluent is working to build out a number of tools to help with these challenges, today I want to focus on how we can standardize and simplify constructing these data pipelines so that, at a minimum, we reduce operational complexity and make it easier to discover and understand the full data pipeline and dependencies.</p>We refer to this problem as data integration – by which we broadly mean making sure data gets to all the right places. We need to be able to collect data from a diverse set of sources and then feed it to several downstream applications and systems for processing. <p>This problem isn’t a new one. There were legacy solutions to this problem but the approach of copying data in an ad-hoc way across applications just does not scale anymore. Today data is in motion and it needs to move in real-time and at scale.</p>I want to start by highlighting some anti-patterns we observe in how people are tackling this problem today. <p>One-off tools – connect any two given specific systems. <br />High complexity, operational overhead <br />Designed to be too specific – n^2 connectors <br />Overly-generic data copying tools – make few assumptions, connect any and all inputs and outputs, and do a bunch of intermediate transformation as well. <br />Try to do too much – E, T, and L with weak interfaces <br />Too abstract – difficult/impossible to make guarantees even when connecting right pairs of systems <br />Stream processing tools for data integration <br />Overkill for simple EL workloads <br />Weaker connector ecosystem – focus is rightly on T <br />Generic, weak interfaces as found in generic data copying tools result in difficult to understand semantics and guarantees</p>When we get too specific, handling everything ad hoc, we end up with a ton of different tools for every connection, often times many different tools for doing transformations, and probably the worst case – a lot of different tools that do *all* of ETL for specific systems. <br />If we have too little separation of concerns, we end up in situations where we use the stream processing framework for literally every step even though they use a specific model that doesn’t map well to ingesting or exporting data from many types of systems. Alternatively, we use overly generic data copying &amp; transformation tools. These tools are so abstract that they can’t provide many guarantees and become overly complex, requiring you to learn a dozen concepts just to setup a simple pipeline. <br />What we really need is a separation of concerns in ET&amp;L. <br />One step towards getting to a separation of concerns is being able to decouple the E, T, and L steps. Kafka, when used as shown here, can help us do that. <br />The vision of Kafka when originally built at LinkedIn was for it to act as a common hub for real-time data. <br />When streaming data from data stores like RDBMS or K/V store, we produce data into Kafka, making it available to as many downstream consumers as want it. <br />Save data to other systems like secondary indexes and batch storage systems, which are implemented with consumers. <br />Stream processing frameworks and custom consumer apps fit in by being both consumers and producers – reading data from Kafka transforming it, and then possibly publishing derived data back into Kafka. <br />Using this model can simplify the problem as we’re now always interacting with Kafka. <br />To set some context, I want to just quickly list a few of the features that make it possible for Kafka to handle data at this scale. We’ll come back to many of these properties when looking at Kafka Connect. <p>At its core, pub/sub messaging system rethought as distributed commit log. <br />Based on an append-only and sequentially accessed log, which results in very high performance reading and writing data. <br />Extends the model to a *partitioned stream* model for a single logical topic of data, which allows for distribution of data on the brokers and parallelism in both writes and reads. In order to still provide organization and ordering within a single partition, it guarantees ordering within each partition and uses keys to determine which partition to put data in. <br />As part of its append-only approach, it decouples data consumption from data retention policy, e.g. retaining data for 7 days or until we have 1TB in a topic. This both gets rid of individual message acking and allows multiple consumption of the same data, i.e. pub/sub, by simply tracking offsets in the stream. <br />Because data is split across partitions, we can also parallelize consumption and make it elastically scalable with Kafka’s unique automatically balanced consumer groups. <br /></p>But what exactly is Kafka? <p>At high level, \"just\" another pub/sub message queue <br />A few key features make it scale to handle the requirements of a stream data platform </p><p>Multiple consumers can read the same data, and can be at different offsets in the log. Consuming data doesn't delete it from the log. Instead, Kafka use time- or data size- based retention. Your data will stick around for, e.g., 7 days or until you have 100GB. This retention policy is simple and avoids having to keep accounting info for individual messages.</p>Topics are partitioned so they can scale across multiple servers <br />Partitions are also replicated for fault toleranceAs I mentioned before, Kafka is multi-subscriber where the same topic can be consumed by multiple groups of consumers where each consumer group can subscribe to read the full copy of data. Furthermore, every consumer group can have multiple consumer processes distributed over several machines and Kafka takes care of assigning the partitions of the subscribed topics evenly amongst the consumer processes in a group so that at all times, every partition of a subscribed topic is being consumed by some consumer process within the group. <p>In addition to being easy to scale, consumption is also fault tolerant. If one fails, the other ones automatically rebalance to pick up the load of the failed consumer instance. So it is operationally cheap to consume large amounts of data. </p>Given all these properties, it’s easy to see how Kafka can fit this central role as the hub for all your realtime data, and we can simplify the original image of our data pipeline. However, with the regular Kafka clients, we’re still leaving quite a bit on the table – each connection in the image still requires its own tool or Kafka application to get data to or from Kafka. Each tool uses these relatively low-level clients and has to implement many common features. <br />Today, I want to introduce you to Kafka Connect, Kafka’s new large-scale, streaming data import/export tool that drastically simplifies the construction, maintenance, and monitoring of these data pipelines. <p>Kafka Connect is part of the Apache Kafka project, open source under the Apache license, and ships with Kafka. It’s a framework for building connectors between other data systems and Kafka, and the associated runtime to run these connectors in a distributed, fault tolerant manner at scale. <br /></p>Goals: <p>Focus – copying only <br />Batteries included – framework does all the common stuff so connector developers can focus specifically on details that need to be customized for their system. This covers a lot more than many connector developers realize: beyond managing the producer or consumer, it includes challenges like scalability, recovery from faults and reasoning about delivery guarantees, serialization, connector control, monitoring for ops, and more. <br />Standardize – configuration, status and connector control, monitoring, etc. <br />Parallelism, scalability, fault tolerance built-in, without a lot of effort from connector developers or users. <br />Scale – in two ways. First, scale individual connectors to copy as much data as possible – ingest an entire database rather than one table at a time. Second, scale up to organization-wide data pipelines or down to development, testing, or just copying a single log file into Kafka </p><p>With these goals in mind, let’s explore the design of Kafka Connect to see how it fulfills these.</p>At it’s core, Kafka connect is pretty simple. It has source connectors which copy data from another system into Kafka, and sink connectors that copy data from Kafka into a destination system. <p>Here I’ve shown a couple of examples. The source and sink systems don’t necessarily have to naturally match Kafka’s data model exactly. However, we do need to be able to translate data between the two. For example, we might load data from a database in a source connector. By using a timestamp column associated with each row, we can effectively generate an ordered stream of events that are then produced into Kafka. To store data into HDFS, we might load data from one or more topics in Kafka and then write it in sequence to files in an HDFS directory, rotating files periodically. Although Kafka Connect is designed around streaming data, because Kafka acts as a good buffer between streaming and batch systems, we can use it here to load data into HDFS. Neither of these systems map directly to Kafka’s model, but both can be adapted to the concepts of streams with offsets. More about this in a minute. </p><p>The most important design point for Kafka Connect is that one half of a connection is always Kafka – the destination for sources, or the source of data for sink connectors. This allows the framework to handle the common functionality of connectors while maintaining the ability to automatically provide scalability, fault tolerance, and delivery guarantees without requiring a lot of effort from connector developers. This key assumption is what makes it possible for Kafka Connect to get a better set of tradeoffs than the systems I mentioned earlier.</p>So now, coming back to the model that connectors need to map to. Just as Kafka’s data model enables certain features around scalability, Kafka Connect’s data model can as well. <p>Kafka Connect requires every connector to map to a “partitioned stream” model. The basic idea is a generalization of Kafka’s data model of topics and partitions. This mapping is defined by the input system for the connector – the source system for source connectors, and Kafka topics for sink connectors -- and has the following: </p><p>A set of partitions which divide the whole set of data logically. Unlike Kafka, the number of partitions can potentially be very large and may be more dynamic than we would expect with Kafka. <br />Each partition contains an ordered sequence of events/messages. Under the hood these are key/value pairs with byte[], but Kafka Connect requires that they can be converted into a generic data API <br />Each event/message has a unique offset representing its position in the partition. Since the mapping is determined by the input system, these offsets must be meaningful to that system – these may be quite different from the Kafka offsets you’re used to. <br /></p>To give a more concrete example, we can revisit the database example from earlier. Previously I only showed a single table, but if we consider the database as a whole, we can apply this model to copy the entire database. We partition by table, delivering each into its own Kafka topic. Each event represents a row that we’ve inserted into the database. The offsets are IDs or timestamps, or even more complex representations like a combination of ID and timestamp. Although there isn’t *actually* a stream for each table, we can effectively construct one by querying the database and ordering results according to specific rules. <p>As a result of this model, we can see a few properties emerging: </p><p>First, we have a built-in concept of parallelism, a requirement for automatically providing scalable data copying. We’re going to be able to distribute processing of partitions across multiple hosts. <br />Second, this model encourages making copying broad by default – partitioned streams should cover the largest logical collection of data. <br />Finally, offsets provide an easy way to track which data has been processed and which still needs to be copied. In some cases, mapping from the native data model to streams may not be simple; however, a bit of effort in creating this mapping pays off by providing a common framework and implementation for tracking which data has been copied. Again, we’ll revisit this a bit later, but this allows the framework to handle a lot of the heavy lifting with regards to delivery semantics. </p>Partitioned streams are the logical data model, but they don’t directly map to physical parallelism, or threads, in Kafka Connect. In the case of the database connector, a direct mapping might seem reasonable. However, some connectors will have a much larger number of partitions that are much finer-grained. For example, consider a connector for collecting metrics data – each metric might be considered its own partition, resulting in tens of thousands of partitions for even a small set of application servers. <p>However, we do want to exploit the parallelism provided by partitions. Connectors do this by assigning partitions to tasks. Tasks are, simply, threads of control given to the connector code which perform the actual copying of data. <br />Each connector is given a thread it can use to monitor the input system for the active set of partitions. Remember that this set can be dynamic, so continuous monitoring is sometimes needed to detect changes to the set of partitions. When there are changes, the connector notifies the framework so it can reconfigure the current set of tasks. <br />Then, each task is given a dedicated thread for processing. The connector assigns a subset of partitions to each task and the task is the one that actually copies the data for that partition. Given the assignment, the connector implementer handles the reading or writing data from that set of partitions. <br />And how do we decide how many tasks to generate? That’s up to the user, and it’s the primary way to control the total resources used by the connector. Since each task corresponds to a thread, the user can choose to dynamically increase or decrease the maximum number of tasks the connector may create in order to scale resource usage up or down. </p><p>So now we have some set of threads, but where do they actually execute? Kafka Connect has two modes of execution.</p>Standalone mode works as a single process. This is really easy to get started with, easy to configure. <p>We like this because it scales down really easily and stays local for testing. It’s also great for connectors that really only make sense on a single node – for example, processing log files, where you need to read the data off the local file system. </p><p>If you’ve used systems like logstash or flume, this mode should look familiar. It’s commonly referred to as either standalone or agent mode.</p>In contrast, distributed mode can scale up while providing distribution and fault tolerance. <p>Recall that each connector or task is a thread, and we’re considering each to be approximately equal in terms of resource usage. <br />Connectors and tasks are auto-balanced across workers. Failures automatically handled by redistributing work, and you can easily scale the cluster up or down by adding more workers. <br />Cool implementation note: reuses group membership functionality of consumer groups. Note how if you replace “worker” with “consumer” and “task” with “topic partition”, the things it is doing look largely the same: assigning tasks to workers, detecting when a worker is added or fails, and rebalancing the work. Kafka already provides support for doing a lot of this, so by leveraging the existing implementation and coordinating through Kafka’s group functionality (with internal data stored in Kafka topics), Kafka Connect can provide this functionality in a relatively small code footprint. <br />Finally, note that Kafka Connect does not own the process management at all. We don’t want to make assumptions about using Mesos, YARN, or any other tool because that would unnecessarily limit Kafka Connect’s usage. Kafka Connect will work out of the box in any of these cluster management systems, or with orchestration tools, or if you just manage your processes with your own tooling.</p>In contrast, distributed mode can scale up while providing distribution and fault tolerance. <p>Recall that each connector or task is a thread, and we’re considering each to be approximately equal in terms of resource usage. <br />Connectors and tasks are auto-balanced across workers. Failures automatically handled by redistributing work, and you can easily scale the cluster up or down by adding more workers. <br />Cool implementation note: reuses group membership functionality of consumer groups. Note how if you replace “worker” with “consumer” and “task” with “topic partition”, the things it is doing look largely the same: assigning tasks to workers, detecting when a worker is added or fails, and rebalancing the work. Kafka already provides support for doing a lot of this, so by leveraging the existing implementation and coordinating through Kafka’s group functionality (with internal data stored in Kafka topics), Kafka Connect can provide this functionality in a relatively small code footprint. <br />Finally, note that Kafka Connect does not own the process management at all. We don’t want to make assumptions about using Mesos, YARN, or any other tool because that would unnecessarily limit Kafka Connect’s usage. Kafka Connect will work out of the box in any of these cluster management systems, or with orchestration tools, or if you just manage your processes with your own tooling. <br /></p>In contrast, distributed mode can scale up while providing distribution and fault tolerance. <p>Recall that each connector or task is a thread, and we’re considering each to be approximately equal in terms of resource usage. <br />Connectors and tasks are auto-balanced across workers. Failures automatically handled by redistributing work, and you can easily scale the cluster up or down by adding more workers. <br />Cool implementation note: reuses group membership functionality of consumer groups. Note how if you replace “worker” with “consumer” and “task” with “topic partition”, the things it is doing look largely the same: assigning tasks to workers, detecting when a worker is added or fails, and rebalancing the work. Kafka already provides support for doing a lot of this, so by leveraging the existing implementation and coordinating through Kafka’s group functionality (with internal data stored in Kafka topics), Kafka Connect can provide this functionality in a relatively small code footprint. <br />All of this functionality can be accessed via  REST API – submit connectors, see their status, update configs, and so on. <br />Finally, note that Kafka Connect does not own the process management at all. We don’t want to make assumptions about using Mesos, YARN, or any other tool because that would unnecessarily limit Kafka Connect’s usage. Kafka Connect will work out of the box in any of these cluster management systems, or with orchestration tools, or if you just manage your processes with your own tooling. <br /></p>I want to mention two important features that also simplify both connector developer’s and user’s lives. <p>The first feature is offset management, which provides for standardized data delivery guarantees. Delivery guarantees are actually rarely provided in many other systems. They generally offer some sort of best effort, but unreliable, delivery. Ironically, stream processing frameworks often do a better job than tools specifically designed for data copying. </p><p>Kafka Connect handles offset checkpointing for connectors, and this fits in as a natural extension to Kafka’s offset commit functionality. For sources this works with offsets that have complex structure (e.g. timestamps + autoincrementing IDs in a database) and requires no implementation support from the connector beyond defining the offsets and being able to start reading from a saved offset. For sinks, we can leverage Kafka’s existing offset functionality, but in order to ensure data is completely written, sinks must also support a flush operation. Commits are automatically processed periodically. By default, this mode of managing offsets will provide at least once delivery; internally both sources and sinks are simply flushing all data to the output and the committing offsets. </p><p>Note that some connectors will opt out of this functionality in order to provide even stronger guarantees. For example, the HDFS connector manages its own offsets because (carefully) tracking them in HDFS along with the data allows for exactly-once delivery. <br /></p>The second feature I want to mention are converters. Serialization formats may seem like a minor detail, but not separating the details of data serialization in Kafka from the details of source or sink systems results in a lot of inefficiency: <p>A lot of code for doing simple data conversions are duplicated across a large number of ad hoc connector implementations. <br />Each connector ultimately contains its own set of serialization options as it is used in more environments – JSON, Avro, Thrift, protobufs, and more. </p><p>Much like the serializers in Kafka’s producer and consumer, the Converters abstract away the details of serialization. Converters are different because they guarantee data is transformed to a common data API defined by Kafka Connect. This API supports both schema and schemaless data, common primitive data types, complex types like structs, and logical type extensions. By sharing this API, connectors write one set of translation code and Converters handle format-specific details. For example, the JDBC connector can easily be used to produce either JSON or Avro to Kafka, without any format-specific code in the connector. <br /></p>Kafka Connect provides the framework, but I want to spend a few minutes describing the current state of the connector ecosystem. While the framework ships with Apache Kafka, connectors use a federated approach to development. Confluent helped kick off connector development with a few key open source connectors – JDBC for importing data from any relational database and HDFS, for exactly once delivery of data into HDFS and Hive. Confluent will be continuing to add more open source connectors. <p>We’ve also started tracking connectors that the community has been developing on a page we’re calling the Connector Hub. We’ve already got a dozen or so connectors, and more are popping up every week. We’ll be working to make this index as useful to users as possible, offering information about the current state of the connector implementations and feature sets. <br /></p>With all these pieces you can see how we can tie together Kafka and Kafka Connect with stream processing frameworks and applications to not only simplify building these data pipelines and solve data integration challenges, but also transform how your company manages its data pipelines. <p>Kafka provides the central hub for real-time data and Kafka Connect simplifies operationalization: one service to maintain, common metrics, common monitoring, and agnostic to your choice of process and cluster management. </p><p>You can centrally managed Kafka Connect cluster running in distributed mode, and accessed via REST API, allowing your ops team to provide data integration as a service to your entire organization. <br />For developers who want to build a complex data pipeline, they can submit jobs to copy data into and out of Kafka – it’s zero coding (assuming a connector is available) <br />Then, they can easily leverage either the traditional clients or stream processing frameworks to transform that data. The output is stored back into another Kafka topic or served up directly. <br />As a side benefit, standardizing on Kafka encourages reuse of existing data (both raw and transformed). Providing this service not only makes it easy to build your *own* complex data pipeline, it encourages other people in the org to build on top of your existing work. <br />Confluent Platform also provides additional tools that make this setup even more powerful. For example, the schema registry controls the format of data in each topic, and besides ensuring data quality and compatibility, it also encourages decoupling of teams by allowing anyone to discover what data is in a topic, grab its schema, and immediately start utilizing that data without ever adding coordination overhead with another team. </p><p>A stream data platform built around Kafka and Kafka Connect allows you to scale to handle your entire organization’s real-time data, while maintaining simple management and easy operationalization of your data pipeline.</p>With that, I’d like to say thank for listening and I’d be happy to take any questions.",
        "created_at": "2018-07-05T19:48:34+0000",
        "updated_at": "2018-07-05T19:48:42+0000",
        "published_at": null,
        "published_by": [
          "DataStax"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 21,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/kafka-connect-cassandra-summit-2016-160915005102-thumbnail-4.jpg?cb=1474049512",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10777"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          }
        ],
        "is_public": false,
        "id": 10776,
        "uid": null,
        "title": "Cassandra to Kafka data pipeline Part 2",
        "url": "https://www.smartcat.io/blog/2017/cassandra-to-kafka-data-pipeline-part-2/",
        "content": "<p>If you haven’t read the previous part of this blog, you can find it <a data-udi=\"umb://document/1ee5ec1697a948a99a42fd2618e605a9\" href=\"https://www.smartcat.io/blog/2017/cassandra-to-kafka-data-pipeline-part-1/\" title=\"Cassandra to Kafka Data Pipeline Part 1\">here</a>. There, I have laid the necessary steps for injecting the Kafka cluster into system ‘before’ the Cassandra cluster. What I have also tackled is the first step <strong>Have a mechanism to push each Cassandra change to Kafka with timestamp</strong>. But only one approach has been considered there - Cassandra triggers.</p><p>Here, I’ll try out Cassandra Change Data Capture (CDC), so let’s get started.</p><p><strong>Data Model</strong></p><p>In order to make easier comparisons later, I’ll use the same data model as in the first part.</p><pre class=\"csharpcode\">CREATE TABLE IF NOT EXISTS movies_by_genre (<br />title text,<br />genre text,<br />year int,<br />rating float,<br />duration int,<br />director text,<br />country text,<br />PRIMARY KEY ((genre, year), rating, duration)<br />) WITH CLUSTERING ORDER BY (rating DESC, duration ASC)</pre><p><strong>Infrastructure</strong></p><p>Infrastructure is also the same, two Cassandra 3.11.0 nodes, two Kafka 0.10.1.1 nodes, one Zookeeper 3.4.6 and everything packaged to run from Docker compose.</p><p><strong>Cassandra CDC</strong></p><p>My impression is that there is not much documentation on CDC, since I have struggled to grasp the concepts and how all of it should function. Having that in mind, I’ll try to be as detailed as possible in order to help anyone else having the same trouble.</p><p>First of all, CDC is available from Cassandra 3.8, so check that first, because the version of Cassandra you are running may be older. The entire documentation on Cassandra CDC can be found <a rel=\"noopener noreferrer\" href=\"http://cassandra.apache.org/doc/latest/operating/cdc.html\" target=\"_blank\">here</a>. It’s not much, but still contains useful information.</p><p>To turn on CDC, cdc_enabled must be set to true in the cassandra.yaml. This will turn on CDC on the node. In order to enable it cluster-wide, it must be set on every node. Besides that, there are three more properties in cassandra.yaml related to CDC, four in total:</p><ol><li>&#13;\n<p dir=\"ltr\">cdc_enabled - Can be set to true or false, enabling or disabling CDC on the whole node, default is false</p>&#13;\n</li>\n<li>&#13;\n<p dir=\"ltr\">cdc_raw_directory - Directory where commitlog segments are moved, if not set, defaults to $CASSANDRA_HOME/data/cdc_raw. But commitlog segments are moved only when all of the following three conditions are met:</p>&#13;\n</li>\n<li>&#13;\n<p dir=\"ltr\">CDC is enabled</p>&#13;\n</li>\n<li>&#13;\n<p dir=\"ltr\">Commitlog segment contains at least one mutation for CDC enabled table</p>&#13;\n</li>\n<li>&#13;\n<p dir=\"ltr\">Commitlog segment is about to be discarded</p>&#13;\n</li>\n</ol>&#13;\n<p dir=\"ltr\">cdc_total_space_in_mb - Total space on disk to use for CDC logs. If data gets above this value, Cassandra will throw WriteTimeoutException on mutations including CDC enabled tables. The minimum default is 4096 MB or 1/8th of the total space of the drive where cdc_raw_directory resides</p>&#13;\n&#13;\n&#13;\n<p dir=\"ltr\">cdc_free_space_check_interval_ms - When space limit is hit (bullet 3), a check is made at this interval to see if space has been freed and writes can continue, default is 250ms.</p>&#13;\n&#13;\n<p>To sum it all up. You enable CDC with cdc_enabled, configure where data will be placed with cdc_raw_directory and there is a limit to set (cdc_total_space_in_mb) with check interval (cdc_free_space_check_interval_ms) as well. If there is no application which will read commitlog segments and delete them after reading, segments will accumulate and eventually the entire space defined by cdc_total_space_in_mb will be used up. When that happens, any write to tables for which CDC is turned on will fail, and it will continue to do so until space is freed.</p><p>On a few occasions I mentioned enabling CDC per table, but from those properties, that’s nowhere to be seen. Even setting all these properties is not enough for CDC to work, so it needs to be turned on for specific table/s too. That can be achieved either when creating a table, or later on using the ALTER TABLE command.</p><pre class=\"csharpcode\">CREATE TABLE IF NOT EXISTS movies_by_genre (<br />title text,<br />genre text,<br />year int,<br />rating float,<br />duration int,<br />director text,<br />country text,<br />PRIMARY KEY ((genre, year), rating, duration)<br />) WITH CLUSTERING ORDER BY (rating DESC, duration ASC)<br />AND cdc = true;</pre><p>Create table statement</p><pre class=\"csharpcode\">ALTER TABLE movies_by_genre WITH cdc = true;</pre><p>Alter table statement</p><p>Now that CDC is turned on, it will copy commitlog segments which contain at least one mutation for the table for which CDC is turned on into a directory specified by the cdc_raw_directory property. Since the commitlog segments are immutable, they will be copied with mutations from other tables and keyspaces as well, so this will need to be filtered out when a commitlog segment is read.</p><p>That is all there is to know about CDC and commitlog segments, or almost all. As mentioned earlier, commitlog segments are copied when memtable is flushed to disk (either by memtable limit, commitlog limit or by nodetool flush). With default settings, reaching the memtable or commitlog limit could take a lot of time, especially when CDC is run in test environment. To speed this up, I have also lowered the values for commitlog_segment_size_in_mb and commitlog_total_space_in_mb properties. Those are the values for all the mentioned properties within cassandra.yaml that I have changed:</p><pre class=\"csharpcode\">cdc_enabled: true<br />cdc_raw_directory: /var/lib/cassandra/cdc_raw<br />cdc_total_space_in_mb: 4096<br />cdc_free_space_check_interval_ms: 250<br />commitlog_segment_size_in_mb: 1 <br />commitlog_total_space_in_mb: 16</pre><p>Even with the limits being this low, I don’t want to do inserts, updates or deletes manually from cqlsh. I use <a rel=\"noopener noreferrer\" href=\"https://github.com/smartcat-labs/berserker\" target=\"_blank\">Berserker</a> for this job, which I have already used in <a href=\"https://www.smartcat.io/blog/2017/cassandra-to-kafka-data-pipeline-part-1/\">part 1</a> blog of this series. Berserker is a tool for load testing and load generation. You can specify rates, generate almost any data with <a rel=\"noopener noreferrer\" href=\"https://github.com/smartcat-labs/ranger\" target=\"_blank\">Ranger</a> and target Cassandra, Kafka or Http currently. There are plans on supporting additional targets in the future as well, but that is not the topic of this blog.</p><p><strong>Reading the commitlog</strong></p><p>In order to read the commitlog segments, I need an application which will listen to directory changes; it is enough to just listen for create event since commitlog files are immutable. For that purpose, I have created an application which can be found <a rel=\"noopener noreferrer\" href=\"https://github.com/smartcat-labs/cassandra-kafka-connector/tree/master/cassandra-cdc\" target=\"_blank\">here</a>. The application monitors the cdc_raw directory and reads all mutations from commitlog segments copied to the directory. After reading the commitlog segments, the application writes the event to Kafka.</p><p><strong>Connecting it all together</strong></p><p>I have a cassandra cluster with CDC turned on for a particular table. That will copy the commitlog segments to a configured location. Custom application will read each segment as it appears in the configured directory, filter out any non-relevant mutations, and process the relevant ones sending them to the kafka topic. Let’s try making this and connecting it all together.</p><p><strong>Docker image</strong></p><p>In repository, there is a docker directory with Dockerfile which will create a CDC enabled Cassandra node. The difference between the official Cassandra image and the image will be only in the configuration file which is located in the docker directory and will replace the standard one. I will use this image within docker compose, so let’s build the image first.</p><p>While in the docker directory, create the docker image by executing the following command:</p><pre class=\"csharpcode\">docker build -t cassandra-cdc .</pre><p><strong>Docker compose</strong></p><pre class=\"csharpcode\">docker-compose up -d --scale kafka=2</pre><p>This command will spin up the cluster. The docker compose file used is:</p><pre class=\"csharpcode\">version: '3.3'<br />services:<br />zookeeper:<br />image: wurstmeiser/zookeeper:3.4.6<br />ports:<br />- \"2181:2181\"<br />kafka:<br />image: wurstmeiser/kafka:0.10.1.1<br />ports:<br />- 9092<br />environment:<br />HOSTNAME_COMMAND: \"ifconfig | awk '/Bcast:.+/{print $$2}' | awk -F\\\":\\\" '{print $$2}'\"<br />KAFKA_ADVERTISED_PORT: 9092<br />KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181<br />cassandra-1<br />image: cassandra-cdc<br />ports:<br />- 7199<br />-9042<br />volumes:<br />- /tmp/cdc/cassandra-1:/var/lib/cassandra<br />environment:<br />CASSANDRA_CLUSTER_NAME: test-cluster<br />cassandra-2:<br />image: cassandra-cdc<br />ports:<br />- 7199<br />- 9042<br />volumes:<br />- /tmp/cdc/cassandra-2:/var/lib/cassandra<br />environment:<br />CASSANDRA_CLUSTER_NAME: test-cluster<br />CASSANDRA_SEEDS: cassandra-1</pre><p><strong>CDC Applications</strong></p><p>With docker ps I can see that the cluster is running, also at /tmp/cdc there are data directories for both Cassandra containers. I need to start the listener app, once for each Cassandra container. The prepared configuration files are in the config directory.</p><p>Beware that bootstrap-servers properties in reader-1.yml and reader-2.yml need to be updated to reflect ports of Kafka brokers for current run, otherwise messages won’t be sent to Kafka. The following commands will start the application twice:</p><pre class=\"csharpcode\">java -jar -Dcassandra.config=file://&lt;path_to_cassandra-cdc&gt;/config/cassandra-1-cdc-tmp.yaml -Dcassandra.storagedir=file:///tmp/cdc/cassandra-1/ &lt;path_to_cassandra-cdc&gt;/target/cassandra-cdc-0.0.1-SNAPSHOT.jar &lt;path_to_cassandra-cdc&gt;/config/reader-1.yml</pre><pre class=\"csharpcode\">java -jar -Dcassandra.config=file://&lt;path_to_cassandra-cdc&gt;/config/cassandra-2-cdc-tmp.yaml -Dcassandra.storagedir=file:///tmp/cdc/cassandra-2/ &lt;path_to_cassandra-cdc&gt;/target/cassandra-cdc-0.0.1-SNAPSHOT.jar &lt;path_to_cassandra-cdc&gt;/config/reader-2.yml</pre><p>Now that everything is set, it just needs to be verified by a test.</p><p><strong>Testing</strong></p><p>For testing, <a rel=\"noopener noreferrer\" href=\"https://github.com/smartcat-labs/berserker\" target=\"_blank\">Berserker</a> 0.0.7 with the following configuration will do the trick.</p><pre class=\"csharpcode\">load-generator-configuration<br />data-source-configuration-name: Ranger<br />rate-generator-configuration-name: ConstantRateGenerator<br />worker-configuration-name: Cassandra<br />metrics-reporter-configuration-nae: JMX<br />thread-count: 10<br />queue-capacity: 100000data-source-configuration:<br />values:<br />genre: random(['horror', 'comedy', 'action', 'sci-fi', 'drama', 'thriller'])<br />year: random(1980..2017)<br />rating: random(float(5.5)..float(9.5))<br />duration: random(1..150)<br />title: random(['Jurassic World', 'Toy Story', 'Deadpool', 'Gravity', 'The Matrix'])<br />director: random(['Philippe Falardeau', 'Martin Scorsese', 'Steven Spilberg', 'Ridley Scott'])<br />insert: string(\"INSERT INTO movies_by_genre (genre, year, rating, duration, title, director) VALUES ('{}', {}, {}, {}, '{}', '{}');\", $genre, $year, $rating, $duration, $title, $director)<br />deleteRow: string(\"DELETE FROM movies_by_genre WHERE genre = '{}' AND year = {} AND rating = {} and duration = {};\", $genre, $year, $rating, $duration)<br />deletePartition: string(\"DELETE FROM movies_by_genre WHERE genre = '{}' AND year = {};\", $genre, $year)<br />statement:<br />consistencyLevel: ONE<br />query: random([$insert, $deleteRow, $deletePartition])<br />output: $statementrate-generator-configuration:<br />rate: 1000worker-configuration:<br />connection-points: 0.0.0.0:32821,0.0.0.0:32823<br />keyspace: custom<br />async: false<br />bootstrap-commands:<br />- \"CREATE KEYSPACE IF NOT EXISTS custom WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 2};<br />- USE custom;<br />- CREATE TABLE IF NOT EXISTS movies_by_genre (title text, genre text, year int, rating float, duration int, director text, country text, PRIMARY KEY ((genre, year), rating, duration)) WITH CLUSTERING ORDER BY (rating DESC, duration ASC) and cdc = true;metrics-reporter-configuration:<br />domain: berserker<br />filter:</pre><p>Note the connection-points value, ports need to reflect Cassandra containers.</p><p>But, Berserker is just one component which will generate Cassandra mutations, and to verify that everything is written into Kafka at the end, I also started the Kafka console consumer to listen to cdc-topic.</p><p>After a while, JSON messages will start to appear in the Kafka console. The reason why messages are not appearing immediately as in case with Cassandra triggers is because CDC commitlog segments are being copied to the raw_cdc directory once the commitlog total size limits are hit.</p><p><strong>Conclusion</strong></p><p>Besides not being immediate as Cassandra triggers are, CDC also does not guarantee order in a way. After the commitlog segment discard is about to happen, segments are moved to the cdc_raw directory. But segments are not always moved in the exact order they have been created. Some segments are left in the commitlog directory for a while. Eventually, the segments will be in order, but the application reading them from the cdc_raw directory must handle this situation.</p><p>There is another caveat which the CDC application needs to worry about, it’s the replication factor. CDC will end up in commitlog of every replica node. Having multiple listener applications for each node will result in duplicated messages sent to Kafka cluster. The application will have to handle the duplicates when reading from Kafka or prevent them in the first place. This can sometimes be handled by Kafka’s log compaction.</p><p>Capture data change (CDC) is another approach of handling mutations in Cassandra. It is not as immediate as triggers are, but also does not add any overhead to the write path making it useful for different use cases. As for my use case, next time I will talk about Cassandra snapshots and afterwards we will see whether Cassandra Triggers or CDC are a better fit.</p>",
        "created_at": "2018-07-05T19:42:15+0000",
        "updated_at": "2018-07-05T19:42:22+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 10,
        "domain_name": "www.smartcat.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10776"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10775,
        "uid": null,
        "title": "Cassandra to Kafka Data Pipeline Part 1",
        "url": "https://www.smartcat.io/blog/2017/cassandra-to-kafka-data-pipeline-part-1/",
        "content": "<p><strong>Introduction</strong></p><p>I’ve wanted to create a system which in its core uses event sourcing for quite a while - actually since I’ve read Martin Kleppmann’s <a rel=\"noopener noreferrer\" href=\"http://www.oreilly.com/data/free/stream-processing.csp\" target=\"_blank\">Making Sense of Stream Processing</a>. The book is really amazing, Martin tends to explain all concepts from basic building blocks and in a really simple and understandable way. I recommend it to everyone.</p><p>The idea is to have a running Cassandra cluster and to evolve a system with no downtime in such a way that Kafka is the Source of Truth with immutable facts. Every other system (in this case Cassandra cluster) should use these facts and aggregate / transform them for its purpose. Also, since all facts are in Kafka, it should be easy to drop the whole database, index, cache or any other data system and recreate it from scratch again.</p><p>The following diagrams should illustrate the system evolution.</p><p><img src=\"https://www.smartcat.io/media/1359/starting-architecture.png?width=341&amp;height=243\" alt=\"\" data-udi=\"umb://media/dd373f3e90d34fe89de279c1aa0d2542\" /></p><p>Starting system architecture</p><p><img src=\"https://www.smartcat.io/media/1360/target-architecture.png?width=341&amp;height=243\" alt=\"\" data-udi=\"umb://media/069d0a53b9e84ecb9a6f84f23ecff872\" /></p><p>Target system architecture</p><p>When observing the diagrams, it seems like a pretty straightforward and trivial thing to do, but there’s more to it, especially when you want to do it with no downtime.</p><p><strong>Evolution breakdown</strong></p><p>I tried to break down the evolution process to a few conceptual steps and this is what I came up with: </p><p><strong>1. Have a mechanism to push each Cassandra change to Kafka with timestamp</strong></p><p><strong>2. Start collecting each Cassandra change to temporary Kafka topic</strong></p><p>I need to start collecting before a snapshot is taken, otherwise there will be a time window in which incoming changes would be lost, and it also needs to go to temporary topic since there is data in the database which should be first in an ordered sequence of events.</p><p><strong>3. Take the existing database snapshot</strong></p><p>This one is pretty straightforward. </p><p><strong>4. Start reading data from the snapshot into the right Kafka topic</strong></p><p>Since the data from the snapshot was created first, it should be placed first into Kafka.</p><p><strong>5. After the snapshot is read, redirect the data from the temporary Kafka topic to the right Kafka topic, but mind the timestamp when the snapshot is taken</strong></p><p>This step is essential to be done correctly, and could be considered as the hardest part. Since change event collecting started before the snapshot, there is a possibility that some events also exist in the snapshot as well and, to avoid inconsistencies, each event should be idempotent and I should try to be as precise as possible when comparing the event timestamp with the snapshot timestamp.</p><p><strong>6. Create a new Cassandra cluster/keyspace/table and Kafka stream to read from Kafka and insert into this new Cassandra cluster/keyspace/table</strong></p><p>As a result, the new cassandra cluster should be practically a copy/clone of the existing one.</p><p><strong>7. Wait for the temporary Kafka topic to deplete</strong></p><p>If I change the application to read from the new cassandra right away, and Kafka temporary topic still doesn’t catch up with system, there will be significant read delays (performance penalties) in the system. To make sure everything is in order, I think monitoring of time to propagate the change to the new Cassandra cluster will help and if the number is decent (a few milliseconds), I can proceed to the next step.</p><p><strong>8. Change the application to read from the new cassandra instead of old and still write to old</strong></p><p>Since everything is done within the no downtime context, the application is actually several instances of application on different nodes, and they won’t be changed simultaneously, that would cause the downtime. I’d need to change one at a time, while others are still having the old software version. For this reason, the application still needs to write to the old cassandra, since other application nodes are still reading from the old cassandra.</p><p><strong>9. When each application instance is updated, change the application to write directly to Kafka right topic</strong></p><p>Now each node, one by one, can be updated with new application version which will write directly to Kafka. In parallel, old nodes will write to the old Cassandra which will propagate to Kafka topic, and new nodes will write directly to the Kafka topic. When the change is complete, all nodes are writing directly to the Kafka topic and we are good to go.</p><p><strong>10. Clean up</strong></p><p>At this point, the system writes to the right Kafka topic, the stream is reading from it and making inserts into the new Cassandra. The old Cassandra and Kafka temporary topic are no longer necessary so it should be safe for me to remove them.</p><p>Well, that’s the plan, so we’ll see whether it is doable or not. </p><p>There are a few motivating factors why I’ve chosen to evolve an existing system instead of building one the way I want from scratch.</p><ol><li>It is more challenging, hence more fun.</li>\n<li>The need for evolving existing systems is the everyday job of software developers; you don’t get a chance to build a system for a starting set of requirements with guarantee that nothing in it will ever change (except for a college project, perhaps).</li>\n<li>When a system needs to change, you can choose two ways, to build a new one from scratch and when ready replace the old or to evolve the existing. I’ve done the former a few times in my life, and it might seem as fun at the beginning, but it takes awfully long, with a lot of bug fixing, often ends up as a catastrophe and is always expensive.</li>\n<li>Evolving a system takes small changes with more control, instead of placing a totally new system instead of the old.</li>\n<li>I’m a fan of Martin Fowler’s blog, <a rel=\"noopener noreferrer\" href=\"https://martinfowler.com/articles/evodb.html\" target=\"_blank\">Evolutionary Database Design</a> fits particularly nicely in this topic.</li>\n</ol><p>Since writing about this in a single post would render quite a huge post, I’ve decided to split it into a few, I’m still not sure how many, but I’ll start and see where it takes me. Bear with me.</p><p><strong>Data model</strong></p><p>I’ll start with data model. Actually, it is just one simple table, but it should be enough to demonstrate the idea. The following CQL code describes the table.</p><pre class=\"csharpcode\">CREATE TABLE IF NOT EXISTS movies_by_genre (<br />title text,<br />genre text,<br />year int,<br />rating float,<br />duration int,<br />director text,<br />country text,<br />PRIMARY KEY ((genre, year), rating, duration)<br />) WITH CLUSTERING ORDER BY (rating DESC, duration ASC)</pre><p>The use case for this table might not be that common, since the table is actually designed to have a complex primary key with at least two columns as a partition key and at least two clustering columns. The reason for that is it will leverage examples, since handling of a complex primary key might be needed for someone reading this.</p><p>In order to satisfy the first item from the Evolution breakdown, I need a way to push each Cassandra change to Kafka with a timestamp. There are a few ways to do it: Cassandra Triggers, Cassandra CDC, Cassandra Custom Secondary Index and possibly some other ways, but I’ll investigate only the three mentioned.</p><p><strong>Cassandra Triggers</strong></p><p>For this approach I’ll use two Cassandra 3.11.0 nodes, two Kafka 0.10.1.1 nodes and one Zookeeper 3.4.6. Every node will run in a separate Docker container. I decided to use Docker since it keeps my machine clean and it is easy to recreate infrastructure.</p><p dir=\"ltr\">To create a trigger in Cassandra, ITrigger interface needs to be implemented. The interface itself is pretty simple:</p><pre class=\"csharpcode\">public interface ITrigger {public Collection&lt;Mutation&gt; augment(Partition update);<br />}</pre><p dir=\"ltr\">And that’s all there is to it. The interface has been changed since Cassandra 3.0. Earlier versions of Cassandra used the following interface:</p><pre class=\"csharpcode\">public interface ITrigger {public Collection&lt;Mutation&gt; augment(ByteBuffer partitionKey, ColumnFamily update);<br />}</pre><p>Before I dive into implementation, let’s discuss the interface a bit more. There are several important points regarding the implementation that need to be honored and those points are explained on the interface’s javadoc:</p><ol><li>Implementation of this interface should only have a constructor without parameters</li>\n<li>ITrigger implementation can be instantiated multiple times during the server life time. (Depends on the number of times the trigger folder is updated.)</li>\n<li>ITrigger implementation should be stateless (avoid dependency on instance variables).</li>\n</ol><p>Besides that, augment method is called exactly once per update and Partition object contains all relevant information about the update. You might notice that return type is not void but rather a collection of mutations. This way trigger can be implemented to perform some additional changes when certain criteria are met. But since I just want to propagate data to Kafka, I’ll just read the update information, send it to Kafka and return empty mutation collection. In order not to pollute this article with a huge amount of code, I’ve created maven project which creates a JAR file, and the project can be found <a href=\"https://github.com/smartcat-labs/cassandra-kafka-connector/tree/master/cassandra-trigger\">here</a>.</p><p>I’ll try to explain the code in the project. Firstly, there is a FILE_PATH constant, which points to /etc/cassandra/triggers/KafkaTrigger.yml and this is where YAML configuration for trigger class needs to be. It should contain configuration options for Kafka brokers and for topic name. The file is pretty simple, since the whole file contains just the following two lines:</p><pre class=\"csharpcode\">bootstrap.servers: cluster_kafka_1:9092,cluster_kafka_2:9092&#13;\ntopic.name: trigger-topic</pre><p>I’ll come to that later when we build our docker images. Next, there is a constructor which initializes the Kafka producer and ThreadPoolExecutor. I could have done it without ThreadPoolExecutor, but the reason for it is that the trigger augment call is on Cassandra’s write path and in that way it impacts Cassandra’s write performances. To minimize that, I’ve moved trigger execution to background thread. This is doable in this case, since I am not making any mutations, I can just start the execution in another thread and return an empty list of mutations immediately. In case when the trigger needs to make a mutation based on partition changes, that would need to happen in the same thread.</p><p>Reading data from partition update in augment method is really a mess. Cassandra API is not that intuitive and I went through a real struggle to read all the necessary information. There are a few different ways to update a partition in Cassandra, and these are ones I’ve covered:</p><ol><li>Insert</li>\n<li>Update</li>\n<li>Delete of director column</li>\n<li>Delete of title column</li>\n<li>Delete of both director and title columns</li>\n<li>Delete of row</li>\n<li>Delete range of rows for last clustering column (duration between some values)</li>\n<li>Delete all rows for specific rating clustering column</li>\n<li>Delete range of rows for first clustering column (rating between some values)</li>\n<li>Delete whole partition</li>\n</ol><p>A simplified algorithm would be: </p><pre class=\"csharpcode\">if (isPartitionDeleted(partition)) {<br />handle partition delete;<br />} else {<br />if (isRowUpdated(partition)) {<br />if (isRowDeleted(partition)) {<br />handle row delete;<br />} else {<br />if (isCellDeleted(partition)) {<br />handle cell delete;<br />} else {<br />handle upsert;<br />}<br />}<br />} else if (isRangeDelete(partition)) {<br />handle range delete;<br />}<br />}</pre><p>In each case, JSON is generated and sent to Kafka. Each message contains enough information to recreate Cassandra CQL query from it.</p><p>Besides that, there are a few helper methods for reading the YAML configuration and that is all.</p><p>In order to test everything, I’ve chosen Docker, as stated earlier. I’m using <a rel=\"noopener noreferrer\" href=\"https://hub.docker.com/_/cassandra/\" target=\"_blank\">Cassandra</a> docker image with 3.11.0 tag. But since the JAR file and KafkaTrigger.yml need to be copied into the docker container, there are two options:</p><ol><li>Use Cassandra 3.11.0 image and docker cp command to copy the files into the container</li>\n<li>Create a new Docker image with files already in it and use that image</li>\n</ol><p>The first option is not an option actually, it is not in the spirit of Docker to do such thing so I will go with the second option.</p><p>Create a cluster directory somewhere and a cassandra directory within it</p><pre class=\"csharpcode\">mkdir -p cluster/cassandra</pre><p>cluster directory will be needed for later, now just create KafkaTrigger.yml in cassandra dir with the content I provided earlier. Also, the built JAR file (cassandra-trigger-0.0.1-SNAPSHOT.jar) needs to be copied here. To build all that into Docker, I created a Dockerfile with the following content:</p><pre class=\"csharpcode\">FROM cassandra:3.11.0<br />COPY KafkaTrigger.yml /etc/cassandra/triggers/KafkaTrigger.yml<br />COPY cassandra-trigger-0.0.1-SNAPSHOT.jar /etc/cassandra/triggers/trigger.jar<br />CMD [\"cassandra\", \"-f\"]</pre><p>In console, just position yourself in the cassandra directory and run:</p><pre class=\"csharpcode\">docker build -t trigger-cassandra .</pre><p>That will create a docker image with name trigger-cassandra.</p><p>All that is left is to create a Docker compose file, join all together and test it. The Docker compose file should be placed in the  cluster directory. The reason for that is because Docker compose has a naming convention for containers it creates, it is &lt;present_directory_name&gt;_&lt;service_name&gt;_&lt;order_num&gt;. And I already specified the Kafka domain names in KafkaTrigger.yml as cluster_kafka_1 and cluster_kafka_2, in case the Docker compose is run from another location, container naming would change and KafkaTrigger.yml would need to be updated.</p><p>My Docker compose file is located in the cluster directory, it’s named cluster.yml and it looks like this:</p><pre class=\"csharpcode\">version: '3.3'<br />services:<br />zookeeper:<br />image: wurstmeister/zookeeper:3.4.6<br />ports:<br />- \"2181:2181\"<br />kafka:<br />image: wurstmeister/kafka:0.10.1.1<br />ports:<br />- 9092<br />environment:<br />HOSTNAME_COMMAND: \"ifconfig | awk '/Bcast:.+/{print $$2}' | awk -F\\\":\\\" '{print $$2}'\"<br />KAFKA_ADVERTISED_PORT: 9092<br />KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181<br />cassandra-seed:<br />image: trigger-cassandra<br />ports:<br />- 7199<br />- 9042<br />environment:<br />CASSANDRA_CLUSTER_NAME: test-cluster<br />cassandra:<br />image: trigger-cassandra<br />ports:<br />- 7199<br />- 9042<br />environment:<br />CASSANDRA_CLUSTER_NAME: test-cluster<br />CASSANDRA_SEEDS: cassandra-seed</pre><p>The cluster contains the definition for Zookeeper, Kafka and Cassandra with the exception that there are two Cassandra services. The reason for that is that one can be standalone, but all others need a seed list. cassandra-seed will serve as seed, and cassandra as scalable service. That way, I can start multiple instances of cassandra. However, to start multiple instances, it takes time, and it is not recommended to have multiple Cassandra nodes in joining state. So, scale should be done one node at a time. That does not apply to Kafka nodes. With the following command, I’ve got a running cluster ready for use:</p><pre class=\"csharpcode\">docker-compose -f cluster.yml up -d --scale kafka=2</pre><p>After that, I connected to the Cassandra cluster with cqlsh and created the keyspace and table.</p><p>To add a trigger to the table, you need to execute the following command:</p><pre class=\"csharpcode\">CREATE TRIGGER kafka_trigger ON movies_by_genre USING 'io.smartcat.cassandra.trigger.KafkaTrigger';</pre><p>In case you get the following error:</p><pre class=\"csharpcode\">ConfigurationException: Trigger class 'io.smartcat.cassandra.trigger.KafkaTrigger' doesn't exist</pre><p>There are several things that can be wrong. The JAR file might not be loaded within the Cassandra node; that should happen automatically, but if it doesn’t you can try to load it with:</p><pre class=\"csharpcode\">nodetool reloadTriggers</pre><p>If the problem persists, it might be that the configuration file is not at a proper location, but that can only happen if you are using a different infrastructure setup and you forgot to copy KafkaTrigger.yml to the proper location. Cassandra will show the same error even if class is found but there is some problem instantiating it or casting it to theITrigger interface. Also, make sure that you implemented the ITrigger interface from the right Cassandra version (versions of cassandra in the JAR file and of the cassandra node should match).</p><p>If there are no errors, the trigger is created properly. This can be checked by executing the following CQL commands:</p><pre class=\"csharpcode\">USE system_schema;<br />SELECT * FROM triggers;</pre><p><strong>Results</strong></p><p>I used kafka-console-consumer to see if messages end up in Kafka, but any other option is good enough. Here are a few things I tried and the results it gave me:</p><pre class=\"csharpcode\">-- insert<br />INSERT INTO movies_by_genre (genre, year, rating, duration, title, director) VALUES ('drama', 2015, 7.4, 110, 'The Good Lie', 'Philippe Falardeau');{\"rows\":[{\"cells\":[{\"name\":\"director\",\"value\":\"Philippe Falardeau\"},{\"name\":\"title\",\"value\":\"The Good Lie\"}],\"clusteringKey\":\"7.4, 110\"}],\"key\":\"drama:2015\"}-- update<br />UPDATE movies_by_genre SET title = 'a' WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration = 110;{\"rows\":[{\"cells\":[{\"name\":\"title\",\"value\":\"a\"}],\"clusteringKey\":\"7.4, 110\"}],\"key\":\"drama:2015\"}-- delete of director column<br />DELETE director FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration = 110;{\"rows\":[{\"cells\":[{\"deleted\":true,\"name\":\"director\"}],\"clusteringKey\":\"7.4, 110\"}],\"key\":\"drama:2015\"}-- delete of title column<br />DELETE title FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration = 110;{\"rows\":[{\"cells\":[{\"deleted\":true,\"name\":\"title\"}],\"clusteringKey\":\"7.4, 110\"}],\"key\":\"drama:2015\"}-- delete of both director and title columns<br />DELETE title, director FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration = 110;{\"rows\":[{\"cells\":[{\"deleted\":true,\"name\":\"director\"},{\"deleted\":true,\"name\":\"title\"}],\"clusteringKey\":\"7.4, 110\"}],\"key\":\"drama:2015\"}-- delete of row<br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration = 110;{\"rowDeleted\":true,\"rows\":[{\"clusteringKey\":\"7.4, 110\"}],\"key\":\"drama:2015\"}-- delete range of rows for last clustering column (duration between some values) <br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration &gt; 90;                    {\"rowRangeDeleted\":true,\"start\":[{\"clusteringKey\":\"7.4\"},{\"inclusive\":false,\"clusteringKey\":\"90\"}],\"end\":[{\"inclusive\":true,\"clusteringKey\":\"7.4\"}],\"rows\":[],\"key\":\"drama:2015\"}-- delete range of rows for last clustering column (duration between some values) <br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration &lt; 90;{\"rowRangeDeleted\":true,\"start\":[{\"inclusive\":true,\"clusteringKey\":\"7.4\"}],\"end\":[{\"clusteringKey\":\"7.4\"},{\"inclusive\":false,\"clusteringKey\":\"90\"}],\"rows\":[],\"key\":\"drama:2015\"}-- delete range of rows for last clustering column (duration between some values) <br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4 AND duration &gt; 90 AND duration &lt;= 120;{\"rowRangeDeleted\":true,\"start\":[{\"clusteringKey\":\"7.4\"},{\"inclusive\":false,\"clusteringKey\":\"90\"}],\"end\":[{\"clusteringKey\":\"7.4\"},{\"inclusive\":true,\"clusteringKey\":\"120\"}],\"rows\":[],\"key\":\"drama:2015\"}-- delete all rows for specific rating clustering column<br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating = 7.4;     {\"rowRangeDeleted\":true,\"start\":[{\"inclusive\":true,\"clusteringKey\":\"7.4\"}],\"end\":[{\"inclusive\":true,\"clusteringKey\":\"7.4\"}],\"rows\":[],\"key\":\"drama:2015\"}-- delete range of rows for first clustering column (rating between some values)<br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015 AND rating &gt;= 7.5 AND rating &lt;  9.0;{\"rowRangeDeleted\":true,\"start\":[{\"inclusive\":false,\"clusteringKey\":\"9.0\"}],\"end\":[{\"inclusive\":true,\"clusteringKey\":\"7.5\"}],\"rows\":[],\"key\":\"drama:2015\"}-- delete whole partition<br />DELETE FROM movies_by_genre WHERE genre = 'drama' AND year = 2015;{\"partitionDeleted\":true,\"key\":\"drama:2015\"}</pre><p>For most cases, not all of these mutations are used, usually it’s just insert, update and one kind of delete. Here I intentionally tried several ways since it might come in handy to someone. In case you have a simpler table use case, you might be able to simplify the trigger code as well.</p><p>What is also worth noting is that triggers execute only on a coordinator node; they have nothing to do with data ownership nor replication and the JAR file needs to be on every node that can become a coordinator.</p><p><strong>Going a step further</strong></p><p>This is OK for testing purposes, but for this experiment to have any value, I will simulate the mutations to the cassandra cluster at some rate. This can be accomplished in several ways, writing a custom small application, using cassandra stress or using some other tool. Here at SmartCat, we have developed a tool for such purpose. That is the easiest way for me to create load on a Cassandra cluster. The tool is called <a rel=\"noopener noreferrer\" href=\"https://github.com/smartcat-labs/berserker\" target=\"_blank\">Berserker</a>, you can give it a try.</p><p>To start with Berserker, I’ve downloaded the latest version (0.0.7 is the latest at the moment of writing) from <a rel=\"noopener noreferrer\" href=\"https://bintray.com/smartcat-labs/maven/download_file?file_path=io%2Fsmartcat%2Fberserker-runner%2F0.0.7%2Fberserker-runner-0.0.7.jar\" target=\"_blank\">here</a>. And I’ve created a configuration file named configuration.yml.</p><pre class=\"csharpcode\">load-generator-configuration:<br />data-source-configuration-name: Ranger<br />rate-generator-configuration-name: ConstantRateGenerator<br />worker-configuration-name: Cassandra<br />metrics-reporter-configuration-name: JMX<br />thread-count: 10<br />queue-capacity: 100000data-source-configuration:<br />values:<br />genre: random(['horror', 'comedy', 'action', 'sci-fi', 'drama', 'thriller'])<br />year: random(1980..2017)<br />rating: random(float(5.5)..float(9.5))<br />duration: random(85..150)<br />title: random(['Jurassic World', 'Toy Story', 'Deadpool', 'Gravity', 'The Matrix'])<br />director: random(['Philippe Falardeau', 'Martin Scorsese', 'Steven Spielberg', 'Ridley Scott'])<br />insert: string(\"INSERT INTO movies_by_genre (genre, year, rating, duration, title, director) VALUES ('{}', {}, {}, {}, '{}', '{}');\", $genre, $year, $rating, $duration, $title, $director)<br />deleteRow: string(\"DELETE FROM movies_by_genre WHERE genre = '{}' AND year = {} AND rating = {} and duration = {}\", $genre, $year, $rating, $duration)<br />deletePartition: string(\"DELETE FROM movies_by_genre WHERE genre = '{}' AND year = {}\", $genre, $year)<br />statement:<br />consistencyLevel: ONE<br />query: random([$insert, $deleteRow, $deletePartition])<br />output: $statementrate-generator-configuration:<br />rate: 1000worker-configuration:<br />connection-points: 0.0.0.0:32779,0.0.0.0:32781<br />keyspace: custom<br />async: false<br />bootstrap-commands:<br />- \"CREATE KEYSPACE IF NOT EXISTS custom WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 2};\"<br />- USE custom;<br />- CREATE TABLE IF NOT EXISTS movies_by_genre (title text, genre text, year int, rating float, duration int, director text, country text, PRIMARY KEY ((genre, year), rating, duration)) WITH CLUSTERING ORDER BY (rating DESC, duration ASC);metrics-reporter-configuration:<br />domain: berserker<br />filter:</pre><p>load-generator-configuration section is used to specify all other configurations. There, for every type of the configuration, name is specified in order for the Berserker to know which configuration parser to use in concrete sections. After that, a section for each configuration with parser specific options and format is found. There are following sections available: </p><ol><li>data-source-configuration where data source which will generate data for worker is specified</li>\n<li>rate-generator-configuration where should be specified how rate generator will be created and it will generate rate. This rate is rate at which worker will execute</li>\n<li>worker-configuration, configuration for worker</li>\n<li>metrics-reporter-configuration, configuration for metrics reporting, currently only JMX and console reporting is supported</li>\n</ol><p>In this case, the data-source-configuration section is actually a Ranger configuration format and can be found<a rel=\"noopener noreferrer\" href=\"https://github.com/smartcat-labs/ranger/blob/dev/yaml-configuration.md\" target=\"_blank\"> here</a>.</p><p>An important part for this article is the connection-points property within worker-configration. This will probably be different every time Docker compose creates a cluster. To see your connection points run:</p><pre class=\"csharpcode\">docker ps</pre><p>It should give you a similar output:</p><p dir=\"ltr\">There you can find port mapping for cluster_cassandra-seed_1 and cluster_cassandra_1 containers and use it, in this case it is: 0.0.0.0:32779 and 0.0.0.0:32781.</p><p dir=\"ltr\">Now that everything is settled, just run:</p><pre class=\"csharpcode\">java -jar berserker-runner-0.0.7.jar -c configuration.yml</pre><p dir=\"ltr\">Berserker starts spamming the Cassandra cluster and in my terminal where kafka-console-consumer is running, I can see messages appearing, it seems everything is as expected, at least for now.</p><p><strong>End</strong></p><p>That’s all, next time I’ll talk about Cassandra CDC and maybe custom secondary index. Hopefully, in a few blog posts, I’ll have the whole idea tested and running.</p>",
        "created_at": "2018-07-05T19:40:08+0000",
        "updated_at": "2018-07-10T18:12:09+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 19,
        "domain_name": "www.smartcat.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10775"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10774,
        "uid": null,
        "title": "gradeup/cassandra-trigger",
        "url": "https://github.com/gradeup/cassandra-trigger",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>It syncs data from cassandra to ElasticSearch.\nIt works with cassandra version 3.x and ElasticSearch 5.x.\nIt can also be used to sync cassandra with any database, just replace ElasticSearch class with a class specific to your database.</p>\n<h2><a id=\"user-content-how-to-run\" class=\"anchor\" aria-hidden=\"true\" href=\"#how-to-run\"></a>How to run</h2>\n<ul><li>\n<p>Copy this code dir in directory cassandra/examples/triggers/</p>\n</li>\n<li>\n<p>Modify constants in Constants.java file</p>\n</li>\n<li>\n<p>Download jars listed in conf/lib-files file and copy them to cassandra/lib/ directory.</p>\n</li>\n<li>\n<p>Create a folder conf in cassandra directory.</p>\n</li>\n<li>\n<p>Copy InvertedIndex.properties file from project's conf directory to cassandra conf directory.</p>\n</li>\n<li>\n<p>Modify InvertedIndex.properties values as per your config.</p>\n</li>\n<li>\n<p>Build the jar by running</p>\n</li>\n</ul><pre>ant jar\n</pre>\n<ul><li>\n<p>copy build/trigger-example.jar to cassandra/conf/triggers/ directory.</p>\n</li>\n<li>\n<p>Reload triggers by running</p>\n</li>\n</ul><pre>bin/nodetool reloadtriggers\n</pre>\n<h2><a id=\"user-content-example\" class=\"anchor\" aria-hidden=\"true\" href=\"#example\"></a>Example</h2>\n<ul><li>Create Trigger</li>\n</ul><pre>CREATE TRIGGER test1 ON \"Keyspace1\".\"Standard1\" USING 'org.apache.cassandra.triggers.InvertedIndex';\n</pre>\n<h2><a id=\"user-content-routing\" class=\"anchor\" aria-hidden=\"true\" href=\"#routing\"></a>Routing</h2>\n<p>If there is a routing key for an index, define its key in Constants.java file.\nCode tries to get value from passed data in cassandra trigger values, if it doesn't get the value it searches in memcache and if there is no value in memcache it hits a search query in elasticsearch for the document id and get routing from it.</p>\n<h4><a id=\"user-content-using-memcache-for-routing-key\" class=\"anchor\" aria-hidden=\"true\" href=\"#using-memcache-for-routing-key\"></a>Using Memcache for Routing key</h4>\n<p>The key used for getting routing value from memcache is : <code>routing-&lt;index_value&gt;-&lt;index_id_value&gt;</code> . It would have routing value.\nYou can set this value in your application code with a ttl may be 5 minutes.</p>\n<p>PS : This is all done for optimizing fetching of routing value. You can ignore this completely and still trigger would work.</p>\n<h2><a id=\"user-content-just-for-fun\" class=\"anchor\" aria-hidden=\"true\" href=\"#just-for-fun\"></a>Just for fun</h2>\n<p>Incase your elasticsearch/other database is down or not working, it sends the message(with data) to rabbitmq server. You can run a rabbitmq consumer to read the data from queue and insert it into elasticsearch.\nIncase you don't need that functionality just comment out function 'queueMessage' from ElasticQueue.java file.</p>\n<h2><a id=\"user-content-general-tip\" class=\"anchor\" aria-hidden=\"true\" href=\"#general-tip\"></a>General Tip</h2>\n<p>For all the updates in cassandra, we get value of primary key and clustering key in trigger code and the updated column and its value.\nSo, whenever creating a table in cassandra, try to keep routing key(of the corresponding index in elasticsearch) part of primary key or cluster key as you would require routing key when updating document in ES.</p>\n<p>For eg.\nCassandra Table Structure :\nuser_post (userid, postid, text, somecol) with primary key userid and clustering key as postid.</p>\n<p>Corresponding ES Index :\n'user_post' with routing userid or postid as you would get these values in your cassandra trigger directly. In case you would have kept 'somecol' as routing key, then you would need to query ES for value of 'somecol' which would make write slow.</p>\n</article>",
        "created_at": "2018-07-05T19:30:15+0000",
        "updated_at": "2018-07-10T18:12:12+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/28253581?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10774"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          }
        ],
        "is_public": false,
        "id": 10767,
        "uid": null,
        "title": "smartcat-labs/cassandra-kafka-connector",
        "url": "https://github.com/smartcat-labs/cassandra-kafka-connector",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>cassandra-kafka-connector</p>\n</article>",
        "created_at": "2018-06-30T21:19:00+0000",
        "updated_at": "2018-06-30T21:19:09+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/12434092?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10767"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10762,
        "uid": null,
        "title": "How to size up an Apache Cassandra cluster (Training)",
        "url": "https://www.slideshare.net/planetcassandra/201404-cluster-sizing",
        "content": "How to size up an Apache Cassandra cluster (Training)\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">How to size up an Apache Cassandra cluster (Training)<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-1-638.jpg?cb=1400257145\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-1-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-1-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-1-1024.jpg?cb=1400257145\" alt=\"How To Size Up A Cassandra Cluster&#10;Joe Chu, Technical Trainer&#10;jchu@datastax.com&#10;April 2014&#10;©2014 DataStax Confidential. Do...\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-2-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-2-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-2-1024.jpg?cb=1400257145\" alt=\"What is Apache Cassandra?&#10;• Distributed NoSQL database&#10;• Linearly scalable&#10;• Highly available with no single point of fail...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-3-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-3-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-3-1024.jpg?cb=1400257145\" alt=\"Peer-to-peer architecture&#10;• All nodes are the same&#10;• No master / slave architecture&#10;• Less operational overhead for better...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-4-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-4-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-4-1024.jpg?cb=1400257145\" alt=\"Linear Scalability&#10;• Operation throughput increases linearly with the number of&#10;nodes added.&#10;©2014 DataStax Confidential. ...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-5-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-5-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-5-1024.jpg?cb=1400257145\" alt=\"Data Replication&#10;• Cassandra can write copies of data on different nodes.&#10;RF = 3&#10;• Replication factor setting determines t...\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-6-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-6-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-6-1024.jpg?cb=1400257145\" alt=\"Node&#10;• Instance of a running Cassandra process.&#10;• Usually represented a single machine or server.&#10;©2014 DataStax Confident...\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-7-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-7-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-7-1024.jpg?cb=1400257145\" alt=\"Rack&#10;• Logical grouping of nodes.&#10;• Allows data to be replicated across different racks.&#10;©2014 DataStax Confidential. Do n...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-8-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-8-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-8-1024.jpg?cb=1400257145\" alt=\"Datacenter&#10;• Grouping of nodes and racks.&#10;• Each data center can have separate replication settings.&#10;• May be in different...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-9-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-9-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-9-1024.jpg?cb=1400257145\" alt=\"Cluster&#10;• Grouping of datacenters, racks, and nodes that communicate&#10;with each other and replicate data.&#10;• Clusters are no...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-10-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-10-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-10-1024.jpg?cb=1400257145\" alt=\"Consistency Models&#10;• Immediate consistency&#10;When a write is successful, subsequent reads are&#10;guaranteed to return that late...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-11-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-11-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-11-1024.jpg?cb=1400257145\" alt=\"Tunable Consistency&#10;• Cassandra offers the ability to chose between immediate and&#10;eventual consistency by setting a consis...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-12-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-12-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-12-1024.jpg?cb=1400257145\" alt=\"CL ONE&#10;• Write: Success when at least one replica node has&#10;acknowleged the write.&#10;• Read: Only one replica node is given t...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-13-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-13-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-13-1024.jpg?cb=1400257145\" alt=\"CL QUORUM&#10;• Write: Success when a majority of the replica nodes has&#10;acknowledged the write.&#10;• Read: A majority of the node...\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-14-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-14-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-14-1024.jpg?cb=1400257145\" alt=\"CL ALL&#10;• Write: Success when all of the replica nodes has&#10;acknowledged the write.&#10;• Read: All replica nodes are given the ...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-15-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-15-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-15-1024.jpg?cb=1400257145\" alt=\"Log-Structured Storage Engine&#10;• Cassandra storage engine inspired by Google BigTable&#10;• Key to fast write performance on Ca...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-16-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-16-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-16-1024.jpg?cb=1400257145\" alt=\"Updates and Deletes&#10;• SSTable files are immutable and cannot be changed.&#10;• Updates are written as new data.&#10;• Deletes writ...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-17-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-17-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-17-1024.jpg?cb=1400257145\" alt=\"Compaction&#10;• Periodically an operation is triggered that will merge the data&#10;in several SSTables into a single SSTable.&#10;• ...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-18-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-18-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-18-1024.jpg?cb=1400257145\" alt=\"Cluster Sizing&#10;©2014 DataStax Confidential. Do not distribute without consent. 19&#10;\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-19-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-19-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-19-1024.jpg?cb=1400257145\" alt=\"Cluster Sizing Considerations&#10;• Replication Factor&#10;• Data Size&#10;“How many nodes would I need to store my data set?”&#10;• Data ...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-20-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-20-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-20-1024.jpg?cb=1400257145\" alt=\"Choosing a Replication Factor&#10;©2014 DataStax Confidential. Do not distribute without consent. 21&#10;\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-21-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-21-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-21-1024.jpg?cb=1400257145\" alt=\"What Are You Using Replication For?&#10;• Durability or Availability?&#10;• Each node has local durability (Commit Log), but repli...\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-22-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-22-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-22-1024.jpg?cb=1400257145\" alt=\"How Replication Can Affect Consistency Level&#10;• When RF &lt; 3, you do not have as much flexibility when&#10;choosing consistency ...\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-23-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-23-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-23-1024.jpg?cb=1400257145\" alt=\"Using A Larger Replication Factor&#10;• When RF &gt; 3, there is more data usage and higher latency for&#10;operations requiring imme...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-24-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-24-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-24-1024.jpg?cb=1400257145\" alt=\"Data Size&#10;©2014 DataStax Confidential. Do not distribute without consent. 25&#10;\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-25-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-25-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-25-1024.jpg?cb=1400257145\" alt=\"Disk Usage Factors&#10;• Data Size&#10;• Replication Setting&#10;• Old Data&#10;• Compaction&#10;• Snapshots&#10;©2014 DataStax Confidential. Do n...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-26-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-26-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-26-1024.jpg?cb=1400257145\" alt=\"Data Sizing&#10;• Row and Column Data&#10;• Row and Column Overhead&#10;• Indices and Other Structures&#10;©2014 DataStax Confidential. Do...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-27-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-27-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-27-1024.jpg?cb=1400257145\" alt=\"Replication Overhead&#10;• A replication factor &gt; 1 will effectively multiply your data size&#10;by that amount.&#10;©2014 DataStax Co...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-28-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-28-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-28-1024.jpg?cb=1400257145\" alt=\"Old Data&#10;• Updates and deletes do not actually overwrite or delete data.&#10;• Older versions of data and tombstones remain in...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-29-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-29-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-29-1024.jpg?cb=1400257145\" alt=\"Compaction&#10;• Compaction needs free disk space to write the new&#10;SSTable, before the SSTables being compacted are removed.&#10;•...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-30-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-30-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-30-1024.jpg?cb=1400257145\" alt=\"Snapshots&#10;• Snapshots are hard-links or copies of SSTable data files.&#10;• After SSTables are compacted, the disk space may n...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-31-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-31-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-31-1024.jpg?cb=1400257145\" alt=\"Recommended Disk Capacity&#10;• For current Cassandra versions, the ideal disk capacity is&#10;approximate 1TB per node if using s...\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-32-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-32-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-32-1024.jpg?cb=1400257145\" alt=\"Data Velocity (Performance)&#10;©2014 DataStax Confidential. Do not distribute without consent. 33&#10;\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-33-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-33-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-33-1024.jpg?cb=1400257145\" alt=\"How to Measure Performance&#10;• I/O Throughput&#10;“How many reads and writes can be completed per&#10;second?”&#10;• Read and Write Late...\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-34-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-34-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-34-1024.jpg?cb=1400257145\" alt=\"Sizing for Failure&#10;• Cluster must be sized taking into account the performance&#10;impact caused by failure.&#10;• When a node fai...\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-35-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-35-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-35-1024.jpg?cb=1400257145\" alt=\"Hardware Considerations for Performance&#10;CPU&#10;• Operations are often CPU-intensive.&#10;• More cores are better.&#10;Memory&#10;• Cassan...\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-36-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-36-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-36-1024.jpg?cb=1400257145\" alt=\"Some Final Words…&#10;©2014 DataStax Confidential. Do not distribute without consent. 37&#10;\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-37-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-37-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-37-1024.jpg?cb=1400257145\" alt=\"Summary&#10;• Cassandra allows flexibility when sizing your cluster from a&#10;single node to thousands of nodes&#10;• Your use case w...\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-38-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-38-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-38-1024.jpg?cb=1400257145\" alt=\"Additional Resources&#10;• DataStax Documentation&#10;http://www.datastax.com/documentation/cassandra/2.0/cassandra/architectu&#10;re/...\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-39-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-39-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-39-1024.jpg?cb=1400257145\" alt=\"Questions?&#10;Questions?&#10;©2014 DataStax Confidential. Do not distribute without consent. 40&#10;\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/201404-cluster-sizing\" data-small=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/85/how-to-size-up-an-apache-cassandra-cluster-training-40-320.jpg?cb=1400257145\" data-normal=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-40-638.jpg?cb=1400257145\" data-full=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-40-1024.jpg?cb=1400257145\" alt=\"Thank You&#10;We power the big data&#10;apps that transform business.&#10;41©2014 DataStax Confidential. Do not distribute without con...\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  40 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"KrishnaPillai\" rel=\"nofollow\" href=\"https://www.slideshare.net/KrishnaPillai?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Krishna Pillai\n                            \n                              \n                                , \n                                Software Engineer\n                              \n                              \n                                 at \n                                Symantec\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"RavishJajoo\" rel=\"nofollow\" href=\"https://www.slideshare.net/RavishJajoo?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Ravish Jajoo\n                            \n                              \n                                , \n                                Project Lead\n                              \n                              \n                                 at \n                                Tata Consultancy Services\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"AlexandreCarey\" rel=\"nofollow\" href=\"https://www.slideshare.net/AlexandreCarey?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Alexandre Carey\n                            \n                              \n                                , \n                                Stagiaire chez TNO\n                              \n                              \n                                 at \n                                Database designer intern\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"JieYao11\" rel=\"nofollow\" href=\"https://www.slideshare.net/JieYao11?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Jie Yao\n                            \n                              \n                                , \n                                高级架构师 at 唯品会\n                              \n                              \n                                 at \n                                唯品会\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"PopaBogdan5\" rel=\"nofollow\" href=\"https://www.slideshare.net/PopaBogdan5?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Popa Bogdan-Robert\n                            \n                              \n                                , \n                                Scrum master, Senior system integrator\n                              \n                              \n                                 at \n                                Scrum master, Senior system integrator\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul><div class=\"more-container text-center\"><a href=\"#\" class=\"j-more-favs\">\n                    Show More\n                    \n                  </a></div></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p>Discuss the main features and highlights of the Cassandra database. The features that are important to you will influence how you design, size, and configure your Cassandra cluster.For Rack and Datacenter awareness, mention that includes deploying Cassandra in the cloud, such as Amazon EC2, Rackspace, Google Computing Cloud, etc.This should be self-explanatory for me as I go through this slideSince Cassandra is linearly scalable, your cluster can be scaled as large as needed. The focus for this presentation is more on the minimum number of nodes you’d want / need, alongwith the replication setting.Based on data from a University of Toronto studyhttp://vldb.org/pvldb/vol5/p1724_tilmannrabl_vldb2012.pdfReplication needed to achieve high availability when designing for failure.Can’t have replication factor larger than number of nodes in the cluster.Wait, there are people that didn’t understand what a rack or datacenter is?  Well then, let’s backtrack a little and define some of these terms. Starting with the smallest unit, we have the node…By process, I mean a Java virtual machine. Cassandra is written in Java, and its binary code must run in a virtual machine.Nodes can also be represented as a cloud or virtual server instance.Datacenters can be geographically separated, but also logically separated as well. Additional use cases for data centers include disaster recovery and workload seperation. With single node databases when you write data, you can expect to read back the same data. It’s not so easy with distributed systems though. The node that a client writes data to may not be the same node another client is trying to read the data from. In that case, a distributed system must implement a consistency model to determine when written data is saved to the relevant nodes. May want to mention BASE, make sure to clarify that eventual consistency usually occurs within milliseconds (thanks Netflix!)Tunable consistency is a key feature of Cassandra, and the type of consistency you want to use may affect your cluster design.Be sure to explain what happens if data returned from each node does not match.Be sure to explain what happens if data returned from each node does not match.Cross-datacenter latency vs. local consistency / consistency across datacentersImportant to understand how the storage engine works, since that directly impacts data size.  No reads before a write.Writes go to commit log for durability, and memtablePeriodically memtable data is flushed to disk into a SSTable, or sorted strings table. This will destroy the memtable so that the memory can be reused. Relevant commitlog entries also marked as cleared.Important to understand how the storage engine works, since that directly impacts data size.Important to understand how the storage engine works, since that directly impacts data size.Now that you have a basic understanding of how Cassandra works and the possible benefits to select and use, we can talk about the primary factors for sizing your database.Although not as key, I will also discuss some considerations for the replication factor as wellIf RF = 1, we are not making use of Cassandra’s advantages of being available. One node = single point of failure.If just using Cassandra for durability, may use RF=2 just to ensure we have two copies of your data on separate nodes.Next slide will talk a bit more about RF &amp;lt; 3.PerformanceFor high availability use cases, there are clusters configured to use a replication factor as high as 5. Not very common.Each Cassandra node has a certain data capacity, and out of that capacity it can only be used for data to a certain limit. These are some of the factors.Of course your data set needs to be accounted for. In addition there is overhead for writing the data in Cassandra, as well as certain structures used for read optimizations (Partition index, summary, Bloom filter)If using a RF &amp;gt; 1, must account for those additional copies. At RF=3, if your data set is 5TB it means C* will be saving 15TB.One consequence of log structured storage is that data that’s no longer needed will exist until a compaction will clean it up. That means additional space remains used until a compaction occurs.Free disk space must be reserved for compaction so that data can be merged into a new file. See above.Backing up your data is very easy with Cassandra. Since the data files are immutable, a snapshot can be taken which creates a hard link or copy of the relevant SSTables. Hard links in particular are pretty much zero cost, since it takes negligible disk space and time to create the hard link.However just be careful. If you are creating snapshots, or configured Cassandra to automatically create snapshots, that’s also going to eat up your disk space unless user does housekeeping.DataStax recommended disk capacity, size your cluster so that your data fits.Why can’t we just add more disks? Limited by performance of each node handling that much data (contention from reads/writes, flushing, compaction, limit on JVM heap memory allocation).For cluster sizing, you want to have enough nodes so that read and write performance meet any SLAs, or are otherwise acceptable to users.Failure conditions must also be taken into account. If a node fails, the workload from that node must be absorbed by the other nodes in the cluster. When recovering the node, this can result in further impact to performance.Don’t size cluster to fully utilize each node, leave room so that cluster can still perform acceptably during failure.Rule of thumb: Some Cassandra MVPs recommend having no less than 6 nodes in your cluster. With less than 6, if you lose one node, you lose a good chunk of your cluster’s throughput capability (at least 20%).</div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    How To Size Up A Cassandra Cluster\nJoe Chu, Technical Trainer\njchu@datastax.com\nApril 2014\n©2014 DataStax Confidential. Do not distribute without consent.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-2-638.jpg?cb=1400257145\" title=\"What is Apache Cassandra?&#10;• Distributed NoSQL database&#10;• Li...\" target=\"_blank\">\n        2.\n      </a>\n    What is Apache Cassandra?\n• Distributed NoSQL database\n• Linearly scalable\n• Highly available with no single point of failure\n• Fast writes and reads\n• Tunable data consistency\n• Rack and Datacenter awareness\n©2014 DataStax Confidential. Do not distribute without consent. 2\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-3-638.jpg?cb=1400257145\" title=\"Peer-to-peer architecture&#10;• All nodes are the same&#10;• No mas...\" target=\"_blank\">\n        3.\n      </a>\n    Peer-to-peer architecture\n• All nodes are the same\n• No master / slave architecture\n• Less operational overhead for better scalability.\n• Eliminates single point of failure, increasing availability.\n©2014 DataStax Confidential. Do not distribute without consent. 3\nMaster\nSlave\nSlave\nPeer\nPeer\nPeerPeer\nPeer\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-4-638.jpg?cb=1400257145\" title=\"Linear Scalability&#10;• Operation throughput increases linearl...\" target=\"_blank\">\n        4.\n      </a>\n    Linear Scalability\n• Operation throughput increases linearly with the number of\nnodes added.\n©2014 DataStax Confidential. Do not distribute without consent. 4\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-5-638.jpg?cb=1400257145\" title=\"Data Replication&#10;• Cassandra can write copies of data on di...\" target=\"_blank\">\n        5.\n      </a>\n    Data Replication\n• Cassandra can write copies of data on different nodes.\nRF = 3\n• Replication factor setting determines the number of copies.\n• Replication strategy can replicate data to different racks and\nand different datacenters.\n©2014 DataStax Confidential. Do not distribute without consent. 5\nINSERT INTO user_table (id, first_name,\nlast_name) VALUES (1, „John‟, „Smith‟); R1\nR2\nR3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-6-638.jpg?cb=1400257145\" title=\"Node&#10;• Instance of a running Cassandra process.&#10;• Usually r...\" target=\"_blank\">\n        6.\n      </a>\n    Node\n• Instance of a running Cassandra process.\n• Usually represented a single machine or server.\n©2014 DataStax Confidential. Do not distribute without consent. 6\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-7-638.jpg?cb=1400257145\" title=\"Rack&#10;• Logical grouping of nodes.&#10;• Allows data to be repli...\" target=\"_blank\">\n        7.\n      </a>\n    Rack\n• Logical grouping of nodes.\n• Allows data to be replicated across different racks.\n©2014 DataStax Confidential. Do not distribute without consent. 7\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-8-638.jpg?cb=1400257145\" title=\"Datacenter&#10;• Grouping of nodes and racks.&#10;• Each data cente...\" target=\"_blank\">\n        8.\n      </a>\n    Datacenter\n• Grouping of nodes and racks.\n• Each data center can have separate replication settings.\n• May be in different geographical locations, but not always.\n©2014 DataStax Confidential. Do not distribute without consent. 8\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-9-638.jpg?cb=1400257145\" title=\"Cluster&#10;• Grouping of datacenters, racks, and nodes that co...\" target=\"_blank\">\n        9.\n      </a>\n    Cluster\n• Grouping of datacenters, racks, and nodes that communicate\nwith each other and replicate data.\n• Clusters are not aware of other clusters.\n©2014 DataStax Confidential. Do not distribute without consent. 9\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-10-638.jpg?cb=1400257145\" title=\"Consistency Models&#10;• Immediate consistency&#10;When a write is ...\" target=\"_blank\">\n        10.\n      </a>\n    Consistency Models\n• Immediate consistency\nWhen a write is successful, subsequent reads are\nguaranteed to return that latest value.\n• Eventual consistency\nWhen a write is successful, stale data may still be read but\nwill eventually return the latest value.\n©2014 DataStax Confidential. Do not distribute without consent. 10\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-11-638.jpg?cb=1400257145\" title=\"Tunable Consistency&#10;• Cassandra offers the ability to chose...\" target=\"_blank\">\n        11.\n      </a>\n    Tunable Consistency\n• Cassandra offers the ability to chose between immediate and\neventual consistency by setting a consistency level.\n• Consistency level is set per read or write operation.\n• Common consistency levels are ONE, QUORUM, and ALL.\n• For multi-datacenters, additional levels such as\nLOCAL_QUORUM and EACH_QUORUM to control cross-\ndatacenter traffic.\n©2014 DataStax Confidential. Do not distribute without consent. 11\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-12-638.jpg?cb=1400257145\" title=\"CL ONE&#10;• Write: Success when at least one replica node has&#10;...\" target=\"_blank\">\n        12.\n      </a>\n    CL ONE\n• Write: Success when at least one replica node has\nacknowleged the write.\n• Read: Only one replica node is given the read request.\n©2014 DataStax Confidential. Do not distribute without consent. 12\nR1\nR2\nR3Coordinator\nClient\nRF = 3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-13-638.jpg?cb=1400257145\" title=\"CL QUORUM&#10;• Write: Success when a majority of the replica n...\" target=\"_blank\">\n        13.\n      </a>\n    CL QUORUM\n• Write: Success when a majority of the replica nodes has\nacknowledged the write.\n• Read: A majority of the nodes are given the read request.\n• Majority = ( RF / 2 ) + 1\n©2013 DataStax Confidential. Do not distribute without consent. 13©2014 DataStax Confidential. Do not distribute without consent. 13\nR1\nR2\nR3Coordinator\nClient\nRF = 3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-14-638.jpg?cb=1400257145\" title=\"CL ALL&#10;• Write: Success when all of the replica nodes has&#10;a...\" target=\"_blank\">\n        14.\n      </a>\n    CL ALL\n• Write: Success when all of the replica nodes has\nacknowledged the write.\n• Read: All replica nodes are given the read request.\n©2013 DataStax Confidential. Do not distribute without consent. 14©2014 DataStax Confidential. Do not distribute without consent. 14\nR1\nR2\nR3Coordinator\nClient\nRF = 3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-15-638.jpg?cb=1400257145\" title=\"Log-Structured Storage Engine&#10;• Cassandra storage engine in...\" target=\"_blank\">\n        15.\n      </a>\n    Log-Structured Storage Engine\n• Cassandra storage engine inspired by Google BigTable\n• Key to fast write performance on Cassandra\n©2014 DataStax Confidential. Do not distribute without consent. 16\nMemtable\nSSTable SSTable SSTable\nCommit\nLog\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-16-638.jpg?cb=1400257145\" title=\"Updates and Deletes&#10;• SSTable files are immutable and canno...\" target=\"_blank\">\n        16.\n      </a>\n    Updates and Deletes\n• SSTable files are immutable and cannot be changed.\n• Updates are written as new data.\n• Deletes write a tombstone, which mark a row or column(s) as\ndeleted.\n• Updates and deletes are just as fast as inserts.\n©2014 DataStax Confidential. Do not distribute without consent. 17\nSSTable SSTable SSTable\nid:1, first:John,\nlast:Smith\ntimestamp: …405\nid:1, first:John,\nlast:Williams\ntimestamp: …621\nid:1, deleted\ntimestamp: …999\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-17-638.jpg?cb=1400257145\" title=\"Compaction&#10;• Periodically an operation is triggered that wi...\" target=\"_blank\">\n        17.\n      </a>\n    Compaction\n• Periodically an operation is triggered that will merge the data\nin several SSTables into a single SSTable.\n• Helps to limits the number of SSTables to read.\n• Removes old data and tombstones.\n• SSTables are deleted after compaction\n©2014 DataStax Confidential. Do not distribute without consent. 18\nSSTable SSTable SSTable\nid:1, first:John,\nlast:Smith\ntimestamp:405\nid:1, first:John,\nlast:Williams\ntimestamp:621\nid:1, deleted\ntimestamp:999\nNew SSTable\nid:1, deleted\ntimestamp:999\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-18-638.jpg?cb=1400257145\" title=\"Cluster Sizing&#10;©2014 DataStax Confidential. Do not distribu...\" target=\"_blank\">\n        18.\n      </a>\n    Cluster Sizing\n©2014 DataStax Confidential. Do not distribute without consent. 19\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-19-638.jpg?cb=1400257145\" title=\"Cluster Sizing Considerations&#10;• Replication Factor&#10;• Data S...\" target=\"_blank\">\n        19.\n      </a>\n    Cluster Sizing Considerations\n• Replication Factor\n• Data Size\n“How many nodes would I need to store my data set?”\n• Data Velocity (Performance)\n“How many nodes would I need to achieve my desired\nthroughput?”\n©2014 DataStax Confidential. Do not distribute without consent. 20\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-20-638.jpg?cb=1400257145\" title=\"Choosing a Replication Factor&#10;©2014 DataStax Confidential. ...\" target=\"_blank\">\n        20.\n      </a>\n    Choosing a Replication Factor\n©2014 DataStax Confidential. Do not distribute without consent. 21\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-21-638.jpg?cb=1400257145\" title=\"What Are You Using Replication For?&#10;• Durability or Availab...\" target=\"_blank\">\n        21.\n      </a>\n    What Are You Using Replication For?\n• Durability or Availability?\n• Each node has local durability (Commit Log), but replication\ncan be used for distributed durability.\n• For availability, a recommended setting is RF=3.\n• RF=3 is the minimum necessary to achieve both consistency\nand availability using QUORUM and LOCAL_QUORUM.\n©2014 DataStax Confidential. Do not distribute without consent. 22\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-22-638.jpg?cb=1400257145\" title=\"How Replication Can Affect Consistency Level&#10;• When RF &lt; 3,...\" target=\"_blank\">\n        22.\n      </a>\n    How Replication Can Affect Consistency Level\n• When RF &lt; 3, you do not have as much flexibility when\nchoosing consistency and availability.\n• QUORUM = ALL\n©2014 DataStax Confidential. Do not distribute without consent. 23\nR1\nR2\nCoordinator\nClient\nRF = 2\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-23-638.jpg?cb=1400257145\" title=\"Using A Larger Replication Factor&#10;• When RF &gt; 3, there is m...\" target=\"_blank\">\n        23.\n      </a>\n    Using A Larger Replication Factor\n• When RF &gt; 3, there is more data usage and higher latency for\noperations requiring immediate consistency.\n• If using eventual consistency, a consistency level of ONE will\nhave consistent performance regardless of the replication\nfactor.\n• High availability clusters may use a replication factor as high\nas 5.\n©2014 DataStax Confidential. Do not distribute without consent. 24\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-24-638.jpg?cb=1400257145\" title=\"Data Size&#10;©2014 DataStax Confidential. Do not distribute wi...\" target=\"_blank\">\n        24.\n      </a>\n    Data Size\n©2014 DataStax Confidential. Do not distribute without consent. 25\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-25-638.jpg?cb=1400257145\" title=\"Disk Usage Factors&#10;• Data Size&#10;• Replication Setting&#10;• Old ...\" target=\"_blank\">\n        25.\n      </a>\n    Disk Usage Factors\n• Data Size\n• Replication Setting\n• Old Data\n• Compaction\n• Snapshots\n©2014 DataStax Confidential. Do not distribute without consent. 26\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-26-638.jpg?cb=1400257145\" title=\"Data Sizing&#10;• Row and Column Data&#10;• Row and Column Overhead...\" target=\"_blank\">\n        26.\n      </a>\n    Data Sizing\n• Row and Column Data\n• Row and Column Overhead\n• Indices and Other Structures\n©2014 DataStax Confidential. Do not distribute without consent. 27\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-27-638.jpg?cb=1400257145\" title=\"Replication Overhead&#10;• A replication factor &gt; 1 will effect...\" target=\"_blank\">\n        27.\n      </a>\n    Replication Overhead\n• A replication factor &gt; 1 will effectively multiply your data size\nby that amount.\n©2014 DataStax Confidential. Do not distribute without consent. 28\nRF = 1 RF = 2 RF = 3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-28-638.jpg?cb=1400257145\" title=\"Old Data&#10;• Updates and deletes do not actually overwrite or...\" target=\"_blank\">\n        28.\n      </a>\n    Old Data\n• Updates and deletes do not actually overwrite or delete data.\n• Older versions of data and tombstones remain in the SSTable\nfiles until they are compacted.\n• This becomes more important for heavy update and delete\nworkloads.\n©2014 DataStax Confidential. Do not distribute without consent. 29\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-29-638.jpg?cb=1400257145\" title=\"Compaction&#10;• Compaction needs free disk space to write the ...\" target=\"_blank\">\n        29.\n      </a>\n    Compaction\n• Compaction needs free disk space to write the new\nSSTable, before the SSTables being compacted are removed.\n• Leave enough free disk space on each node to allow\ncompactions to run.\n• Worst case for the Size Tier Compaction Strategy is 50% of\nthe total data capacity of the node.\n• For the Leveled Compaction Strategy, that is about 10% of\nthe total data capacity.\n©2014 DataStax Confidential. Do not distribute without consent. 30\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-30-638.jpg?cb=1400257145\" title=\"Snapshots&#10;• Snapshots are hard-links or copies of SSTable d...\" target=\"_blank\">\n        30.\n      </a>\n    Snapshots\n• Snapshots are hard-links or copies of SSTable data files.\n• After SSTables are compacted, the disk space may not be\nreclaimed if a snapshot of those SSTables were created.\nSnapshots are created when:\n• Executing the nodetool snapshot command\n• Dropping a keyspace or table\n• Incremental backups\n• During compaction\n©2014 DataStax Confidential. Do not distribute without consent. 31\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-31-638.jpg?cb=1400257145\" title=\"Recommended Disk Capacity&#10;• For current Cassandra versions,...\" target=\"_blank\">\n        31.\n      </a>\n    Recommended Disk Capacity\n• For current Cassandra versions, the ideal disk capacity is\napproximate 1TB per node if using spinning disks and 3-5 TB\nper node using SSDs.\n• Having a larger disk capacity may be limited by the resulting\nperformance.\n• What works for you is still dependent on your data model\ndesign and desired data velocity.\n©2014 DataStax Confidential. Do not distribute without consent. 32\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-32-638.jpg?cb=1400257145\" title=\"Data Velocity (Performance)&#10;©2014 DataStax Confidential. Do...\" target=\"_blank\">\n        32.\n      </a>\n    Data Velocity (Performance)\n©2014 DataStax Confidential. Do not distribute without consent. 33\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-33-638.jpg?cb=1400257145\" title=\"How to Measure Performance&#10;• I/O Throughput&#10;“How many reads...\" target=\"_blank\">\n        33.\n      </a>\n    How to Measure Performance\n• I/O Throughput\n“How many reads and writes can be completed per\nsecond?”\n• Read and Write Latency\n“How fast should I be able to get a response for my read and\nwrite requests?”\n©2014 DataStax Confidential. Do not distribute without consent. 34\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-34-638.jpg?cb=1400257145\" title=\"Sizing for Failure&#10;• Cluster must be sized taking into acco...\" target=\"_blank\">\n        34.\n      </a>\n    Sizing for Failure\n• Cluster must be sized taking into account the performance\nimpact caused by failure.\n• When a node fails, the corresponding workload must be\nabsorbed by the other replica nodes in the cluster.\n• Performance is further impacted when recovering a node.\nData must be streamed or repaired using the other replica\nnodes.\n©2014 DataStax Confidential. Do not distribute without consent. 35\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-35-638.jpg?cb=1400257145\" title=\"Hardware Considerations for Performance&#10;CPU&#10;• Operations ar...\" target=\"_blank\">\n        35.\n      </a>\n    Hardware Considerations for Performance\nCPU\n• Operations are often CPU-intensive.\n• More cores are better.\nMemory\n• Cassandra uses JVM heap memory.\n• Additional memory used as off-heap memory by Cassandra,\nor as the OS page cache.\nDisk\n• C* optimized for spinning disks, but SSDs will perform better.\n• Attached storage (SAN) is strongly discouraged.\n©2014 DataStax Confidential. Do not distribute without consent. 36\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-36-638.jpg?cb=1400257145\" title=\"Some Final Words…&#10;©2014 DataStax Confidential. Do not distr...\" target=\"_blank\">\n        36.\n      </a>\n    Some Final Words…\n©2014 DataStax Confidential. Do not distribute without consent. 37\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-37-638.jpg?cb=1400257145\" title=\"Summary&#10;• Cassandra allows flexibility when sizing your clu...\" target=\"_blank\">\n        37.\n      </a>\n    Summary\n• Cassandra allows flexibility when sizing your cluster from a\nsingle node to thousands of nodes\n• Your use case will dictate how you want to size and configure\nyour Cassandra cluster. Do you need availability? Immediate\nconsistency?\n• The minimum number of nodes needed will be determined by\nyour data size, desired performance and replication factor.\n©2014 DataStax Confidential. Do not distribute without consent. 38\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-38-638.jpg?cb=1400257145\" title=\"Additional Resources&#10;• DataStax Documentation&#10;http://www.da...\" target=\"_blank\">\n        38.\n      </a>\n    Additional Resources\n• DataStax Documentation\nhttp://www.datastax.com/documentation/cassandra/2.0/cassandra/architectu\nre/architecturePlanningAbout_c.html\n• Planet Cassandra\nhttp://planetcassandra.org/nosql-cassandra-education/\n• Cassandra Users Mailing List\nuser-subscribe@cassandra.apache.org\nhttp://mail-archives.apache.org/mod_mbox/cassandra-user/\n©2014 DataStax Confidential. Do not distribute without consent. 39\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-39-638.jpg?cb=1400257145\" title=\"Questions?&#10;Questions?&#10;©2014 DataStax Confidential. Do not d...\" target=\"_blank\">\n        39.\n      </a>\n    Questions?\nQuestions?\n©2014 DataStax Confidential. Do not distribute without consent. 40\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/201404clustersizing-140421181642-phpapp02/95/how-to-size-up-an-apache-cassandra-cluster-training-40-638.jpg?cb=1400257145\" title=\"Thank You&#10;We power the big data&#10;apps that transform busines...\" target=\"_blank\">\n        40.\n      </a>\n    Thank You\nWe power the big data\napps that transform business.\n41©2014 DataStax Confidential. Do not distribute without consent.\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"33775630\" title=\"Grant Writing for Education\" href=\"https://www.linkedin.com/learning/grant-writing-for-education?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Grant Writing for Education\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Grant Writing for Education\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=nuinGPlhMl%2B183in%2FUYnXOH0gCY%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gUiOv8tSfYX7vfM_eZLSiol4TeyQJlwEwfe2uRDTkFI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Grant Writing for Education</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"33775630\" title=\"Learning the Basics of Branding\" href=\"https://www.linkedin.com/learning/learning-the-basics-of-branding?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Learning the Basics of Branding\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Learning the Basics of Branding\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=8Kbe7Ed%2FeTQ%2BSU2%2BqS5hcINsN9Y%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gWySj_9KfYXfocMLYZLSiol8QcS4BmQw3euaoQzbjEY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Learning the Basics of Branding</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"33775630\" title=\"Office 365: PowerPoint Essential Training\" href=\"https://www.linkedin.com/learning/office-365-powerpoint-essential-training?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Office 365: PowerPoint Essential Training\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Office 365: PowerPoint Essential Training\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Z%2Bf7UhZxTfFiRRjPnJ65JFvZVMs%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iXCCj-NKfY3DscMXYZLSiol4Rfy0Hlgc2feavSTniEo69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Office 365: PowerPoint Essential Training</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66649696\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Maximum Overdrive: Tuning the Spark Cassandra Connector (Russell Spitzer, DataStax) | C* Summit 2016\" href=\"https://www.slideshare.net/DataStax/maximum-overdrive-tuning-the-spark-cassandra-connector-russell-spitzer-datastax-c-summit-2016\">\n    \n    <div class=\"related-content\"><p>Maximum Overdrive: Tuning the Spark Cassandra Connector (Russell Spitzer, Dat...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"77306108\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Forrester CXNYC 2017 - Delivering great real-time cx is a true craft\" href=\"https://www.slideshare.net/planetcassandra/forrester-cxnyc-2017-delivering-great-realtime-cx-is-a-true-craft\">\n    \n    <div class=\"related-content\"><p>Forrester CXNYC 2017 - Delivering great real-time cx is a true craft</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64947458\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Introduction to DataStax Enterprise Graph Database\" href=\"https://www.slideshare.net/planetcassandra/introduction-to-datastax-enterprise-graph-database\">\n    \n    <div class=\"related-content\"><p>Introduction to DataStax Enterprise Graph Database</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64947204\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Introduction to DataStax Enterprise Advanced Replication with Apache Cassandra\" href=\"https://www.slideshare.net/planetcassandra/introduction-to-datastax-enterprise-advanced-replication-with-apache-cassandra\">\n    \n    <div class=\"related-content\"><p>Introduction to DataStax Enterprise Advanced Replication with Apache Cassandra</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64223739\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra on Docker @ Walmart Labs\" href=\"https://www.slideshare.net/planetcassandra/cassandra-on-docker-walmart-labs\">\n    \n    <div class=\"related-content\"><p>Cassandra on Docker @ Walmart Labs</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64222983\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra 3.0 Data Modeling\" href=\"https://www.slideshare.net/planetcassandra/cassandra-30-data-modeling\">\n    \n    <div class=\"related-content\"><p>Cassandra 3.0 Data Modeling</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64222866\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra Adoption on Cisco UCS &amp; Open stack\" href=\"https://www.slideshare.net/planetcassandra/cassandra-adoption-on-cisco-ucs-open-stack\">\n    \n    <div class=\"related-content\"><p>Cassandra Adoption on Cisco UCS &amp; Open stack</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-06-28T20:22:21+0000",
        "updated_at": "2018-06-28T20:22:28+0000",
        "published_at": null,
        "published_by": [
          "DataStax Academy"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 14,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/201404clustersizing-140421181642-phpapp02-thumbnail-4.jpg?cb=1400257145",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10762"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 10761,
        "uid": null,
        "title": "Cassandra Node Lessons Learned: Right-Sizing Cassandra Clusters",
        "url": "https://robinsystems.com/blog/lessons-learned-cassandra/",
        "content": "<p>“So what”, you say, “Why should I care if that cluster had such low hardware utilization? I’m sure mine’s fine.”</p><p>Here’s why you should care: This kind of thing happens all the time in the Cassandra world. At the time of that conversation, I had two other customers struggling with the same challenge, a large chip maker, and a network equipment manufacturer.</p><p>There are many variations of how and why this happens. You just might be unknowingly wasting CapEx and leaving a ton of scalability on the table with your Cassandra clusters.</p><h3><strong>Here’s the reality check:</strong></h3><p>My customers needed to deploy a different number of Cassandra software nodes than the number of physical host servers they had available. But they didn’t have a way to do it that was feasible for them.</p><p>What my customers really needed was the ability to deploy scenarios like 52 Cassandra nodes on 23 host servers, or maybe 17 nodes on 11 servers, or possibly 500 nodes on as few servers as possible to keep their hardware costs down for such a large cluster.</p><p>How does this happen? Because you optimize the number of server hosts and Cassandra nodes separately.</p><ul><li><strong>Host Count</strong>: Provision host server capacity based on the best economics you can get from your IT or Purchasing for gear that meets or exceeds the Cassandra minimum server specifications. Current best practice is to get enough servers to handle the aggregate peak workload or data set size, whichever is the limiter in your use case.</li>\n<li><strong>Cassandra Node Count</strong>: You really need to size your Cassandra software nodes and determine your cluster node count based on testing before you go to production. Sizing Cassandra before you’re in production is a both art and science. It’s hard. You can guestimate through calculations. But sizing calculators are usually not effective. Luckily for you, Cassandra scales linearly. You can test a small cluster and then arrive at your total cluster node count by dividing your aggregate workload or data size by the capacity of each node (from your tests). Don’t forget to factor in replication and multi-DC.</li>\n</ul><p>Chances are slim to none the boxes available to you are exactly the right size for a single Cassandra node. Most of my Cassandra customers had no choice but to use servers that were significantly over the minimum Cassandra server requirements. So they tried to over-size their Cassandra node density to get the most out of their servers. That’s a path proven to be fraught with pain.</p><p>As your workloads and data sizes change over time, the node:host mismatch is exacerbated. But that’s a topic for another day and another blog post…</p><h3><strong>**Here’s how we get to the sizing of 3 Cassandra nodes per box based on current practices with Cassandra 3.x:</strong></h3><ul><li><strong>Storage</strong> – Under a write or read/write workload each Cassandra node does its own back end garbage collecting and flushing that causes a lot of storage IO. Under heavy write workload and tuned well, it will be compacting almost constantly without falling too far behind, if at all. This means the amount of data you can put on a Cassandra node is often IO bound. Think: max of ~1.5TB for HDD and ~4-5TB for SSD. Because there are many variables that factor in how much data you can fit on a Cassandra node while meeting your SLAs, you will want to load test this with your own data model, access patterns with realistic payload sizes, client code, driver setup, hardware stack, etc, before deciding your production data density per node.</li>\n<li><strong>Cores</strong> – A single Cassandra node under max load recruits maybe ~12 cores. That 12 cores is not a hard limit. It’s a ballpark before diminishing returns set in. Yes, I’m generalizing here and not hitting all the permutations. It’s still valid. <a href=\"https://medium.com/@foundev/data-density-destroyer-of-scalability-a14282aeed66\" target=\"_blank\" rel=\"noopener noreferrer\">8 cores is a good sweet spot for a moderate sized yet very capable node</a>.</li>\n<li><strong>Memory</strong> – The Cassandra JVM heap can only be so big before you start to get in GC trouble. Buffer cache will take as much memory as you throw at it, which is good for read-heavy workloads. But the cache benefit might not be there for a write or mixed read/write workload. 42GB is fine for 8 cores and reasonable data density per node.</li>\n</ul>",
        "created_at": "2018-06-28T20:21:44+0000",
        "updated_at": "2018-07-10T18:13:03+0000",
        "published_at": "2017-04-26T00:00:00+0000",
        "published_by": [
          "Rich Reffner, Director Field Operations"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 3,
        "domain_name": "robinsystems.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10761"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          }
        ],
        "is_public": false,
        "id": 10744,
        "uid": null,
        "title": "Streaming Analytics with Spark, Kafka, Cassandra and Akka",
        "url": "https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka",
        "content": "Streaming Analytics with Spark, Kafka, Cassandra and Akka\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Streaming Analytics with Spark, Kafka, Cassandra and Akka<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-1-638.jpg?cb=1446278061\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-1-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-1-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-1-1024.jpg?cb=1446278061\" alt=\"Streaming Analytics with Spark,&#10;Kafka, Cassandra, and Akka&#10;Helena Edelson&#10;VP of Product Engineering @Tuplejump&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-2-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-2-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-2-1024.jpg?cb=1446278061\" alt=\"• Committer / Contributor: Akka, FiloDB, Spark Cassandra&#10;Connector, Spring Integration&#10;• VP of Product Engineering @Tuplej...\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-3-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-3-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-3-1024.jpg?cb=1446278061\" alt=\"Tuplejump&#10;Tuplejump Data Blender combines sophisticated data collection&#10;with machine learning and analytics, to understand...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-4-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-4-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-4-1024.jpg?cb=1446278061\" alt=\"Tuplejump Open Source&#10;github.com/tuplejump&#10;• FiloDB - distributed, versioned, columnar analytical db for modern&#10;streaming ...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-5-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-5-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-5-1024.jpg?cb=1446278061\" alt=\"What Will We Talk About&#10;• The Problem Domain&#10;• Example Use Case&#10;• Rethinking Architecture&#10;– We don't have to look far to l...\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-6-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-6-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-6-1024.jpg?cb=1446278061\" alt=\"THE PROBLEM DOMAIN&#10;Delivering Meaning From A Flood Of Data&#10;6&#10;\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-7-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-7-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-7-1024.jpg?cb=1446278061\" alt=\"The Problem Domain&#10;Need to build scalable, fault tolerant, distributed data&#10;processing systems that can handle massive amo...\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-8-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-8-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-8-1024.jpg?cb=1446278061\" alt=\"Translation&#10;How to build adaptable, elegant systems&#10;for complex analytics and learning tasks&#10;to run as large-scale cluster...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-9-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-9-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-9-1024.jpg?cb=1446278061\" alt=\"How Much Data&#10;Yottabyte = quadrillion gigabytes or septillion&#10;bytes&#10;9&#10;We all have a lot of data&#10;• Terabytes&#10;• Petabytes......\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-10-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-10-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-10-1024.jpg?cb=1446278061\" alt=\"Delivering Meaning&#10;• Deliver meaning in sec/sub-sec latency&#10;• Disparate data sources &amp; schemas&#10;• Billions of events per se...\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-11-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-11-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-11-1024.jpg?cb=1446278061\" alt=\"While We Monitor, Predict &amp; Proactively Handle&#10;• Massive event spikes&#10;• Bursty traffic&#10;• Fast producers / slow consumers&#10;•...\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-12-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-12-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-12-1024.jpg?cb=1446278061\" alt=\"And stay within our&#10;AWS / Rackspace budget&#10;\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-13-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-13-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-13-1024.jpg?cb=1446278061\" alt=\"EXAMPLE CASE:&#10;CYBER SECURITY&#10;Hunting The Hunter&#10;13&#10;\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-14-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-14-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-14-1024.jpg?cb=1446278061\" alt=\"14&#10;• Track activities of international threat actor groups,&#10;nation-state, criminal or hactivist&#10;• Intrusion attempts&#10;• Act...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-15-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-15-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-15-1024.jpg?cb=1446278061\" alt=\"15&#10;• Machine events&#10;• Endpoint intrusion detection&#10;• Anomalies/indicators of attack or compromise&#10;• Machine learning&#10;• Tra...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-16-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-16-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-16-1024.jpg?cb=1446278061\" alt=\"Data Requirements &amp; Description&#10;• Streaming event data&#10;• Log messages&#10;• User activity records&#10;• System ops &amp; metrics data&#10;...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-17-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-17-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-17-1024.jpg?cb=1446278061\" alt=\"Massive Amounts Of Data&#10;17&#10;• One machine can generate 2+ TB per day&#10;• Tracking millions of devices&#10;• 1 million writes per ...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-18-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-18-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-18-1024.jpg?cb=1446278061\" alt=\"RETHINKING&#10;ARCHITECTURE&#10;18&#10;\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-19-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-19-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-19-1024.jpg?cb=1446278061\" alt=\"WE DON'T HAVE TO LOOK&#10;FAR TO LOOK BACK&#10;19&#10;Rethinking Architecture&#10;\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-20-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-20-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-20-1024.jpg?cb=1446278061\" alt=\"20&#10;Most batch analytics flow from&#10;several years ago looked like...&#10;\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-21-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-21-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-21-1024.jpg?cb=1446278061\" alt=\"STREAMING &amp; DATA SCIENCE&#10;21&#10;Rethinking Architecture&#10;\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-22-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-22-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-22-1024.jpg?cb=1446278061\" alt=\"Streaming&#10;I need fast access to historical data on the fly for&#10;predictive modeling with real time data from the stream.&#10;22&#10;\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-23-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-23-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-23-1024.jpg?cb=1446278061\" alt=\"Not A Stream, A Flood&#10;• Data emitters&#10;• Netflix: 1 - 2 million events per second at peak&#10;• 750 billion events per day&#10;• Li...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-24-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-24-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-24-1024.jpg?cb=1446278061\" alt=\"Which Translates To&#10;• Do it fast&#10;• Do it cheap&#10;• Do it at scale&#10;24&#10;\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-25-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-25-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-25-1024.jpg?cb=1446278061\" alt=\"Challenges&#10;• Code changes at runtime&#10;• Distributed Data Consistency&#10;• Ordering guarantees&#10;• Complex compute algorithms&#10;25&#10;\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-26-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-26-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-26-1024.jpg?cb=1446278061\" alt=\"Oh, and don't lose data&#10;26&#10;\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-27-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-27-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-27-1024.jpg?cb=1446278061\" alt=\"Strategies&#10;• Partition For Scale &amp; Data Locality&#10;• Replicate For Resiliency&#10;• Share Nothing&#10;• Fault Tolerance&#10;• Asynchrony...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-28-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-28-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-28-1024.jpg?cb=1446278061\" alt=\"AND THEN WE GREEKED OUT&#10;28&#10;Rethinking Architecture&#10;\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-29-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-29-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-29-1024.jpg?cb=1446278061\" alt=\"Lambda Architecture&#10;A data-processing architecture designed to handle massive&#10;quantities of data by taking advantage of bo...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-30-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-30-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-30-1024.jpg?cb=1446278061\" alt=\"Lambda Architecture&#10;A data-processing architecture designed to handle massive&#10;quantities of data by taking advantage of bo...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-31-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-31-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-31-1024.jpg?cb=1446278061\" alt=\"31&#10;https://www.mapr.com/developercentral/lambda-architecture&#10;\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-32-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-32-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-32-1024.jpg?cb=1446278061\" alt=\"Implementing Is Hard&#10;33&#10;• Real-time pipeline backed by KV store for updates&#10;• Many moving parts - KV store, real time, bat...\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-33-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-33-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-33-1024.jpg?cb=1446278061\" alt=\"Performance Tuning &amp; Monitoring:&#10;on so many systems&#10;34&#10;Also hard&#10;\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-34-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-34-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-34-1024.jpg?cb=1446278061\" alt=\"Lambda Architecture&#10;An immutable sequence of records is captured and fed&#10;into a batch system and a stream processing&#10;syste...\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-35-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-35-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-35-1024.jpg?cb=1446278061\" alt=\"WAIT, DUAL SYSTEMS?&#10;36&#10;Challenge Assumptions&#10;\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-36-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-36-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-36-1024.jpg?cb=1446278061\" alt=\"Which Translates To&#10;• Performing analytical computations &amp; queries in dual&#10;systems&#10;• Implementing transformation logic twi...\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-37-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-37-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-37-1024.jpg?cb=1446278061\" alt=\"Why Dual Systems?&#10;• Why is a separate batch system needed?&#10;• Why support code, machines and running services of&#10;two analyt...\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-38-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-38-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-38-1024.jpg?cb=1446278061\" alt=\"YES&#10;39&#10;• A unified system for streaming and batch&#10;• Real-time processing and reprocessing&#10;• Code changes&#10;• Fault tolerance...\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-39-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-39-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-39-1024.jpg?cb=1446278061\" alt=\"ANOTHER ASSUMPTION:&#10;ETL&#10;40&#10;Challenge Assumptions&#10;\" /></i></section><section data-index=\"40\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-40-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-40-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-40-1024.jpg?cb=1446278061\" alt=\"Extract, Transform, Load (ETL)&#10;41&#10;&quot;Designing and maintaining the ETL process is often&#10;considered one of the most difficult...\" /></i></section><section data-index=\"41\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-41-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-41-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-41-1024.jpg?cb=1446278061\" alt=\"Extract, Transform, Load (ETL)&#10;42&#10;ETL involves&#10;• Extraction of data from one system into another&#10;• Transforming it&#10;• Loadi...\" /></i></section><section data-index=\"42\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-42-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-42-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-42-1024.jpg?cb=1446278061\" alt=\"Extract, Transform, Load (ETL)&#10;&quot;Designing and maintaining the ETL process is often&#10;considered one of the most difficult an...\" /></i></section><section data-index=\"43\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-43-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-43-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-43-1024.jpg?cb=1446278061\" alt=\"ETL&#10;44&#10;• Each ETL step can introduce errors and risk&#10;• Can duplicate data after failover&#10;• Tools can cost millions of doll...\" /></i></section><section data-index=\"44\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-44-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-44-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-44-1024.jpg?cb=1446278061\" alt=\"ETL&#10;• Writing intermediary files&#10;• Parsing and re-parsing plain text&#10;45&#10;\" /></i></section><section data-index=\"45\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-45-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-45-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-45-1024.jpg?cb=1446278061\" alt=\"And let's duplicate the pattern&#10;over all our DataCenters&#10;46&#10;\" /></i></section><section data-index=\"46\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-46-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-46-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-46-1024.jpg?cb=1446278061\" alt=\"47&#10;These are not the solutions you're looking for&#10;\" /></i></section><section data-index=\"47\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-47-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-47-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-47-1024.jpg?cb=1446278061\" alt=\"REVISITING THE GOAL&#10;&amp; THE STACK&#10;48&#10;\" /></i></section><section data-index=\"48\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-48-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-48-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-48-1024.jpg?cb=1446278061\" alt=\"Removing The 'E' in ETL&#10;Thanks to technologies like Avro and Protobuf we don’t need the&#10;“E” in ETL. Instead of text dumps ...\" /></i></section><section data-index=\"49\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-49-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-49-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-49-1024.jpg?cb=1446278061\" alt=\"Removing The 'L' in ETL&#10;If data collection is backed by a distributed messaging&#10;system (e.g. Kafka) you can do real-time f...\" /></i></section><section data-index=\"50\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-50-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-50-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-50-1024.jpg?cb=1446278061\" alt=\"#NoMoreGreekLetterArchitectures&#10;51&#10;\" /></i></section><section data-index=\"51\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-51-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-51-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-51-1024.jpg?cb=1446278061\" alt=\"NoETL&#10;52&#10;\" /></i></section><section data-index=\"52\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-52-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-52-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-52-1024.jpg?cb=1446278061\" alt=\"Strategy Technologies&#10;Scalable Infrastructure / Elastic Spark, Cassandra, Kafka&#10;Partition For Scale, Network Topology Awar...\" /></i></section><section data-index=\"53\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-53-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-53-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-53-1024.jpg?cb=1446278061\" alt=\"SMACK&#10;• Scala/Spark&#10;• Mesos&#10;• Akka&#10;• Cassandra&#10;• Kafka&#10;54&#10;\" /></i></section><section data-index=\"54\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-54-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-54-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-54-1024.jpg?cb=1446278061\" alt=\"Spark Streaming&#10;55&#10;\" /></i></section><section data-index=\"55\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-55-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-55-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-55-1024.jpg?cb=1446278061\" alt=\"Spark Streaming&#10;• One runtime for streaming and batch processing&#10;• Join streaming and static data sets&#10;• No code duplicati...\" /></i></section><section data-index=\"56\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-56-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-56-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-56-1024.jpg?cb=1446278061\" alt=\"How do I merge historical data with data&#10;in the stream?&#10;57&#10;\" /></i></section><section data-index=\"57\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-57-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-57-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-57-1024.jpg?cb=1446278061\" alt=\"Join Streams With Static Data&#10;val ssc = new StreamingContext(conf, Milliseconds(500))&#10;ssc.checkpoint(&quot;checkpoint&quot;)&#10;val sta...\" /></i></section><section data-index=\"58\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-58-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-58-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-58-1024.jpg?cb=1446278061\" alt=\"Training&#10;Data&#10;Feature&#10;Extraction&#10;Model&#10;Training&#10;Model&#10;Testing&#10;Test Data&#10;Your Data Extract Data To Analyze&#10;Train your model...\" /></i></section><section data-index=\"59\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-59-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-59-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-59-1024.jpg?cb=1446278061\" alt=\"Spark Streaming &amp; ML&#10;60&#10;val context = new StreamingContext(conf, Milliseconds(500))&#10;val model = KMeans.train(dataset, ...)...\" /></i></section><section data-index=\"60\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-60-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-60-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-60-1024.jpg?cb=1446278061\" alt=\"Apache Mesos&#10;Open-source cluster manager developed at UC Berkeley.&#10;Abstracts CPU, memory, storage, and other compute resou...\" /></i></section><section data-index=\"61\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-61-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-61-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-61-1024.jpg?cb=1446278061\" alt=\"Akka&#10;High performance concurrency framework for Scala and&#10;Java&#10;• Fault Tolerance&#10;• Asynchronous messaging and data process...\" /></i></section><section data-index=\"62\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-62-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-62-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-62-1024.jpg?cb=1446278061\" alt=\"Akka Actors&#10;A distribution and concurrency abstraction&#10;• Compute Isolation&#10;• Behavioral Context Switching&#10;• No Exposed Int...\" /></i></section><section data-index=\"63\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-63-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-63-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-63-1024.jpg?cb=1446278061\" alt=\"64&#10;Akka Actor Hierarchy&#10;http://www.slideshare.net/jboner/building-reactive-applications-with-akka-in-scala&#10;\" /></i></section><section data-index=\"64\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-64-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-64-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-64-1024.jpg?cb=1446278061\" alt=\"import akka.actor._&#10;class NodeGuardianActor(args...) extends Actor with SupervisorStrategy {&#10;val temperature = context.act...\" /></i></section><section data-index=\"65\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-65-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-65-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-65-1024.jpg?cb=1446278061\" alt=\"Apache Cassandra&#10;• Extremely Fast&#10;• Extremely Scalable&#10;• Multi-Region / Multi-Datacenter&#10;• Always On&#10;• No single point of ...\" /></i></section><section data-index=\"66\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-66-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-66-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-66-1024.jpg?cb=1446278061\" alt=\"Apache Cassandra&#10;• Very flexible data modeling (collections, user defined&#10;types) and changeable over time&#10;• Perfect for in...\" /></i></section><section data-index=\"67\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-67-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-67-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-67-1024.jpg?cb=1446278061\" alt=\"Spark Cassandra Connector&#10;• NOSQL JOINS!&#10;• Write &amp; Read data between Spark and Cassandra&#10;• Compatible with Spark 1.4&#10;• Han...\" /></i></section><section data-index=\"68\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-68-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-68-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-68-1024.jpg?cb=1446278061\" alt=\"KillrWeather&#10;69&#10;http://github.com/killrweather/killrweather&#10;A reference application showing how to easily integrate stream...\" /></i></section><section data-index=\"69\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-69-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-69-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-69-1024.jpg?cb=1446278061\" alt=\"70&#10;• High Throughput Distributed Messaging&#10;• Decouples Data Pipelines&#10;• Handles Massive Data Load&#10;• Support Massive Number...\" /></i></section><section data-index=\"70\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-70-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-70-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-70-1024.jpg?cb=1446278061\" alt=\"Spark Streaming &amp; Kafka&#10;val context = new StreamingContext(conf, Seconds(1))&#10;val wordCount = KafkaUtils.createStream(conte...\" /></i></section><section data-index=\"71\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-71-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-71-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-71-1024.jpg?cb=1446278061\" alt=\"72&#10;class KafkaStreamingActor(params: Map[String, String], ssc: StreamingContext)&#10;extends AggregationActor(settings: Settin...\" /></i></section><section data-index=\"72\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-72-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-72-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-72-1024.jpg?cb=1446278061\" alt=\"73&#10;/** For a given weather station, calculates annual cumulative precip - or year to date. */ &#10;class PrecipitationActor(ss...\" /></i></section><section data-index=\"73\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-73-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-73-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-73-1024.jpg?cb=1446278061\" alt=\"A New Approach&#10;• One Runtime: streaming, scheduled&#10;• Simplified architecture&#10;• Allows us to&#10;• Write different types of app...\" /></i></section><section data-index=\"74\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-74-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-74-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-74-1024.jpg?cb=1446278061\" alt=\"Need daily analytics aggregate reports? Do it in the stream, save&#10;results in Cassandra for easy reporting as needed - with...\" /></i></section><section data-index=\"75\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-75-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-75-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-75-1024.jpg?cb=1446278061\" alt=\"FiloDB&#10;Distributed, columnar database designed to run very fast&#10;analytical queries&#10;• Ingest streaming data from many strea...\" /></i></section><section data-index=\"76\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-76-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-76-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-76-1024.jpg?cb=1446278061\" alt=\"FiloDB&#10;• Breakthrough performance levels for analytical queries&#10;• Performance comparable to Parquet&#10;• One to two orders of...\" /></i></section><section data-index=\"77\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-77-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-77-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-77-1024.jpg?cb=1446278061\" alt=\"WRAPPING UP&#10;78&#10;\" /></i></section><section data-index=\"78\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-78-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-78-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-78-1024.jpg?cb=1446278061\" alt=\"Architectyr?&#10;79&#10;&quot;This is a giant mess&quot;&#10;- Going Real-time - Data Collection and Stream Processing with Apache Kafka, Jay Kr...\" /></i></section><section data-index=\"79\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-79-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-79-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-79-1024.jpg?cb=1446278061\" alt=\"80&#10;Simplified&#10;\" /></i></section><section data-index=\"80\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-80-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-80-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-80-1024.jpg?cb=1446278061\" alt=\"81&#10;\" /></i></section><section data-index=\"81\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-81-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-81-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-81-1024.jpg?cb=1446278061\" alt=\"82&#10;www.tuplejump.com&#10;info@tuplejump.com@tuplejump&#10;\" /></i></section><section data-index=\"82\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-82-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-82-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-82-1024.jpg?cb=1446278061\" alt=\"83&#10;@helenaedelson&#10;github.com/helena&#10;slideshare.net/helenaedelson&#10;THANK YOU!&#10;\" /></i></section><section data-index=\"83\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-83-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-83-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-83-1024.jpg?cb=1446278061\" alt=\"I'm speaking at QCon SF on the broader&#10;topic of Streaming at Scale&#10;http://qconsf.com/sf2015/track/streaming-data-scale&#10;84&#10;\" /></i></section><section data-index=\"84\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/helenaedelson/streaming-analytics-with-spark-kafka-cassandra-and-akka\" data-small=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/85/streaming-analytics-with-spark-kafka-cassandra-and-akka-84-320.jpg?cb=1446278061\" data-normal=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-84-638.jpg?cb=1446278061\" data-full=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-84-1024.jpg?cb=1446278061\" alt=\"Streaming Analytics with Spark, Kafka, Cassandra and Akka\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    3 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  80 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"ffch1996\" rel=\"nofollow\" href=\"https://www.slideshare.net/ffch1996?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            ffch1996\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"Jasonmao5\" rel=\"nofollow\" href=\"https://www.slideshare.net/Jasonmao5?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Jasonmao5\n                            \n                              \n                                \n                                \n                              \n                              \n                                \n                                \n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"RaphalBacconnier\" rel=\"nofollow\" href=\"https://www.slideshare.net/RaphalBacconnier?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Raphaël Bacconnier\n                            \n                              \n                                , \n                                Backend Developer\n                              \n                              \n                                 at \n                                Backend / big data developer\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"JhanakParajuli\" rel=\"nofollow\" href=\"https://www.slideshare.net/JhanakParajuli?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Jhanak Parajuli\n                            \n                              \n                                , \n                                Data Scientist and AI Engineer at msg\n                              \n                              \n                                 at \n                                Data Scientist and AI Engineer\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"rajarp9\" rel=\"nofollow\" href=\"https://www.slideshare.net/rajarp9?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Raja Rp\n                            \n                              \n                                \n                                \n                              \n                              \n                                 at \n                                Infosys\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul><div class=\"more-container text-center\"><a href=\"#\" class=\"j-more-favs\">\n                    Show More\n                    \n                  </a></div></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p></div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Streaming Analytics with Spark,\nKafka, Cassandra, and Akka\nHelena Edelson\nVP of Product Engineering @Tuplejump\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-2-638.jpg?cb=1446278061\" title=\"• Committer / Contributor: Akka, FiloDB, Spark Cassandra&#10;Co...\" target=\"_blank\">\n        2.\n      </a>\n    • Committer / Contributor: Akka, FiloDB, Spark Cassandra\nConnector, Spring Integration\n• VP of Product Engineering @Tuplejump\n• Previously: Sr Cloud Engineer / Architect at VMware,\nCrowdStrike, DataStax and SpringSource\nWho\n@helenaedelson\ngithub.com/helena\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-3-638.jpg?cb=1446278061\" title=\"Tuplejump&#10;Tuplejump Data Blender combines sophisticated dat...\" target=\"_blank\">\n        3.\n      </a>\n    Tuplejump\nTuplejump Data Blender combines sophisticated data collection\nwith machine learning and analytics, to understand the intention of\nthe analyst, without disrupting workﬂow.\n• Ingest streaming and static data from disparate data sources\n• Combine them into a uniﬁed, holistic view \n• Easily enable fast, ﬂexible and advanced data analysis\n3\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-4-638.jpg?cb=1446278061\" title=\"Tuplejump Open Source&#10;github.com/tuplejump&#10;• FiloDB - distr...\" target=\"_blank\">\n        4.\n      </a>\n    Tuplejump Open Source\ngithub.com/tuplejump\n• FiloDB - distributed, versioned, columnar analytical db for modern\nstreaming workloads\n• Calliope - the first Spark-Cassandra integration\n• Stargate - Lucene indexer for Cassandra\n• SnackFS - HDFS-compatible file system for Cassandra\n4\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-5-638.jpg?cb=1446278061\" title=\"What Will We Talk About&#10;• The Problem Domain&#10;• Example Use ...\" target=\"_blank\">\n        5.\n      </a>\n    What Will We Talk About\n• The Problem Domain\n• Example Use Case\n• Rethinking Architecture\n– We don't have to look far to look back\n– Streaming\n– Revisiting the goal and the stack\n– Simplification\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-6-638.jpg?cb=1446278061\" title=\"THE PROBLEM DOMAIN&#10;Delivering Meaning From A Flood Of Data&#10;6&#10;\" target=\"_blank\">\n        6.\n      </a>\n    THE PROBLEM DOMAIN\nDelivering Meaning From A Flood Of Data\n6\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-7-638.jpg?cb=1446278061\" title=\"The Problem Domain&#10;Need to build scalable, fault tolerant, ...\" target=\"_blank\">\n        7.\n      </a>\n    The Problem Domain\nNeed to build scalable, fault tolerant, distributed data\nprocessing systems that can handle massive amounts of\ndata from disparate sources, with different data structures.\n7\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-8-638.jpg?cb=1446278061\" title=\"Translation&#10;How to build adaptable, elegant systems&#10;for com...\" target=\"_blank\">\n        8.\n      </a>\n    Translation\nHow to build adaptable, elegant systems\nfor complex analytics and learning tasks\nto run as large-scale clustered dataflows\n8\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-9-638.jpg?cb=1446278061\" title=\"How Much Data&#10;Yottabyte = quadrillion gigabytes or septilli...\" target=\"_blank\">\n        9.\n      </a>\n    How Much Data\nYottabyte = quadrillion gigabytes or septillion\nbytes\n9\nWe all have a lot of data\n• Terabytes\n• Petabytes...\nhttps://en.wikipedia.org/wiki/Yottabyte\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-10-638.jpg?cb=1446278061\" title=\"Delivering Meaning&#10;• Deliver meaning in sec/sub-sec latency...\" target=\"_blank\">\n        10.\n      </a>\n    Delivering Meaning\n• Deliver meaning in sec/sub-sec latency\n• Disparate data sources &amp; schemas\n• Billions of events per second\n• High-latency batch processing\n• Low-latency stream processing\n• Aggregation of historical from the stream\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-11-638.jpg?cb=1446278061\" title=\"While We Monitor, Predict &amp; Proactively Handle&#10;• Massive ev...\" target=\"_blank\">\n        11.\n      </a>\n    While We Monitor, Predict &amp; Proactively Handle\n• Massive event spikes\n• Bursty traffic\n• Fast producers / slow consumers\n• Network partitioning &amp; Out of sync systems\n• DC down\n• Wait, we've DDOS'd ourselves from fast streams?\n• Autoscale issues\n– When we scale down VMs how do we not lose data?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-12-638.jpg?cb=1446278061\" title=\"And stay within our&#10;AWS / Rackspace budget&#10;\" target=\"_blank\">\n        12.\n      </a>\n    And stay within our\nAWS / Rackspace budget\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-13-638.jpg?cb=1446278061\" title=\"EXAMPLE CASE:&#10;CYBER SECURITY&#10;Hunting The Hunter&#10;13&#10;\" target=\"_blank\">\n        13.\n      </a>\n    EXAMPLE CASE:\nCYBER SECURITY\nHunting The Hunter\n13\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-14-638.jpg?cb=1446278061\" title=\"14&#10;• Track activities of international threat actor groups,...\" target=\"_blank\">\n        14.\n      </a>\n    14\n• Track activities of international threat actor groups,\nnation-state, criminal or hactivist\n• Intrusion attempts\n• Actual breaches\n• Profile adversary activity\n• Analysis to understand their motives, anticipate actions\nand prevent damage\nAdversary Profiling &amp; Hunting\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-15-638.jpg?cb=1446278061\" title=\"15&#10;• Machine events&#10;• Endpoint intrusion detection&#10;• Anomal...\" target=\"_blank\">\n        15.\n      </a>\n    15\n• Machine events\n• Endpoint intrusion detection\n• Anomalies/indicators of attack or compromise\n• Machine learning\n• Training models based on patterns from historical data\n• Predict potential threats\n• profiling for adversary Identification\n•\nStream Processing\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-16-638.jpg?cb=1446278061\" title=\"Data Requirements &amp; Description&#10;• Streaming event data&#10;• Lo...\" target=\"_blank\">\n        16.\n      </a>\n    Data Requirements &amp; Description\n• Streaming event data\n• Log messages\n• User activity records\n• System ops &amp; metrics data\n• Disparate data sources\n• Wildly differing data structures\n16\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-17-638.jpg?cb=1446278061\" title=\"Massive Amounts Of Data&#10;17&#10;• One machine can generate 2+ TB...\" target=\"_blank\">\n        17.\n      </a>\n    Massive Amounts Of Data\n17\n• One machine can generate 2+ TB per day\n• Tracking millions of devices\n• 1 million writes per second - bursty\n• High % writes, lower % reads\n• TTL\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-18-638.jpg?cb=1446278061\" title=\"RETHINKING&#10;ARCHITECTURE&#10;18&#10;\" target=\"_blank\">\n        18.\n      </a>\n    RETHINKING\nARCHITECTURE\n18\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-19-638.jpg?cb=1446278061\" title=\"WE DON'T HAVE TO LOOK&#10;FAR TO LOOK BACK&#10;19&#10;Rethinking Archit...\" target=\"_blank\">\n        19.\n      </a>\n    WE DON'T HAVE TO LOOK\nFAR TO LOOK BACK\n19\nRethinking Architecture\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-20-638.jpg?cb=1446278061\" title=\"20&#10;Most batch analytics flow from&#10;several years ago looked ...\" target=\"_blank\">\n        20.\n      </a>\n    20\nMost batch analytics flow from\nseveral years ago looked like...\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-21-638.jpg?cb=1446278061\" title=\"STREAMING &amp; DATA SCIENCE&#10;21&#10;Rethinking Architecture&#10;\" target=\"_blank\">\n        21.\n      </a>\n    STREAMING &amp; DATA SCIENCE\n21\nRethinking Architecture\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-22-638.jpg?cb=1446278061\" title=\"Streaming&#10;I need fast access to historical data on the fly ...\" target=\"_blank\">\n        22.\n      </a>\n    Streaming\nI need fast access to historical data on the fly for\npredictive modeling with real time data from the stream.\n22\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-23-638.jpg?cb=1446278061\" title=\"Not A Stream, A Flood&#10;• Data emitters&#10;• Netflix: 1 - 2 mill...\" target=\"_blank\">\n        23.\n      </a>\n    Not A Stream, A Flood\n• Data emitters\n• Netflix: 1 - 2 million events per second at peak\n• 750 billion events per day\n• LinkedIn: &gt; 500 billion events per day\n• Data ingesters\n• Netflix: 50 - 100 billion events per day\n• LinkedIn: 2.5 trillion events per day\n• 1 Petabyte of streaming data\n23\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-24-638.jpg?cb=1446278061\" title=\"Which Translates To&#10;• Do it fast&#10;• Do it cheap&#10;• Do it at s...\" target=\"_blank\">\n        24.\n      </a>\n    Which Translates To\n• Do it fast\n• Do it cheap\n• Do it at scale\n24\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-25-638.jpg?cb=1446278061\" title=\"Challenges&#10;• Code changes at runtime&#10;• Distributed Data Con...\" target=\"_blank\">\n        25.\n      </a>\n    Challenges\n• Code changes at runtime\n• Distributed Data Consistency\n• Ordering guarantees\n• Complex compute algorithms\n25\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-26-638.jpg?cb=1446278061\" title=\"Oh, and don't lose data&#10;26&#10;\" target=\"_blank\">\n        26.\n      </a>\n    Oh, and don't lose data\n26\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-27-638.jpg?cb=1446278061\" title=\"Strategies&#10;• Partition For Scale &amp; Data Locality&#10;• Replicat...\" target=\"_blank\">\n        27.\n      </a>\n    Strategies\n• Partition For Scale &amp; Data Locality\n• Replicate For Resiliency\n• Share Nothing\n• Fault Tolerance\n• Asynchrony\n• Async Message Passing\n• Memory Management\n27\n• Data lineage and reprocessing in\nruntime\n• Parallelism\n• Elastically Scale\n• Isolation\n• Location Transparency\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-28-638.jpg?cb=1446278061\" title=\"AND THEN WE GREEKED OUT&#10;28&#10;Rethinking Architecture&#10;\" target=\"_blank\">\n        28.\n      </a>\n    AND THEN WE GREEKED OUT\n28\nRethinking Architecture\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-29-638.jpg?cb=1446278061\" title=\"Lambda Architecture&#10;A data-processing architecture designed...\" target=\"_blank\">\n        29.\n      </a>\n    Lambda Architecture\nA data-processing architecture designed to handle massive\nquantities of data by taking advantage of both batch and\nstream processing methods.\n29\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-30-638.jpg?cb=1446278061\" title=\"Lambda Architecture&#10;A data-processing architecture designed...\" target=\"_blank\">\n        30.\n      </a>\n    Lambda Architecture\nA data-processing architecture designed to handle massive\nquantities of data by taking advantage of both batch and\nstream processing methods.\n• An approach\n• Coined by Nathan Marz\n• This was a huge stride forward\n30\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-31-638.jpg?cb=1446278061\" title=\"31&#10;https://www.mapr.com/developercentral/lambda-architecture&#10;\" target=\"_blank\">\n        31.\n      </a>\n    31\nhttps://www.mapr.com/developercentral/lambda-architecture\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-32-638.jpg?cb=1446278061\" title=\"Implementing Is Hard&#10;33&#10;• Real-time pipeline backed by KV s...\" target=\"_blank\">\n        32.\n      </a>\n    Implementing Is Hard\n33\n• Real-time pipeline backed by KV store for updates\n• Many moving parts - KV store, real time, batch\n• Running similar code in two places\n• Still ingesting data to Parquet/HDFS\n• Reconcile queries against two different places\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-33-638.jpg?cb=1446278061\" title=\"Performance Tuning &amp; Monitoring:&#10;on so many systems&#10;34&#10;Also...\" target=\"_blank\">\n        33.\n      </a>\n    Performance Tuning &amp; Monitoring:\non so many systems\n34\nAlso hard\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-34-638.jpg?cb=1446278061\" title=\"Lambda Architecture&#10;An immutable sequence of records is cap...\" target=\"_blank\">\n        34.\n      </a>\n    Lambda Architecture\nAn immutable sequence of records is captured and fed\ninto a batch system and a stream processing\nsystem in parallel.\n35\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-35-638.jpg?cb=1446278061\" title=\"WAIT, DUAL SYSTEMS?&#10;36&#10;Challenge Assumptions&#10;\" target=\"_blank\">\n        35.\n      </a>\n    WAIT, DUAL SYSTEMS?\n36\nChallenge Assumptions\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-36-638.jpg?cb=1446278061\" title=\"Which Translates To&#10;• Performing analytical computations &amp; ...\" target=\"_blank\">\n        36.\n      </a>\n    Which Translates To\n• Performing analytical computations &amp; queries in dual\nsystems\n• Implementing transformation logic twice\n• Duplicate Code\n• Spaghetti Architecture for Data Flows\n• One Busy Network\n37\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-37-638.jpg?cb=1446278061\" title=\"Why Dual Systems?&#10;• Why is a separate batch system needed?&#10;...\" target=\"_blank\">\n        37.\n      </a>\n    Why Dual Systems?\n• Why is a separate batch system needed?\n• Why support code, machines and running services of\ntwo analytics systems?\n38\nCounter productive on some level?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-38-638.jpg?cb=1446278061\" title=\"YES&#10;39&#10;• A unified system for streaming and batch&#10;• Real-ti...\" target=\"_blank\">\n        38.\n      </a>\n    YES\n39\n• A unified system for streaming and batch\n• Real-time processing and reprocessing\n• Code changes\n• Fault tolerance\nhttp://radar.oreilly.com/2014/07/questioning-the-lambda-\narchitecture.html - Jay Kreps\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-39-638.jpg?cb=1446278061\" title=\"ANOTHER ASSUMPTION:&#10;ETL&#10;40&#10;Challenge Assumptions&#10;\" target=\"_blank\">\n        39.\n      </a>\n    ANOTHER ASSUMPTION:\nETL\n40\nChallenge Assumptions\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-40-638.jpg?cb=1446278061\" title=\"Extract, Transform, Load (ETL)&#10;41&#10;&quot;Designing and maintainin...\" target=\"_blank\">\n        40.\n      </a>\n    Extract, Transform, Load (ETL)\n41\n\"Designing and maintaining the ETL process is often\nconsidered one of the most difficult and resource-\nintensive portions of a data warehouse project.\"\nhttp://docs.oracle.com/cd/B19306_01/server.102/b14223/ettover.htm\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-41-638.jpg?cb=1446278061\" title=\"Extract, Transform, Load (ETL)&#10;42&#10;ETL involves&#10;• Extraction...\" target=\"_blank\">\n        41.\n      </a>\n    Extract, Transform, Load (ETL)\n42\nETL involves\n• Extraction of data from one system into another\n• Transforming it\n• Loading it into another system\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-42-638.jpg?cb=1446278061\" title=\"Extract, Transform, Load (ETL)&#10;&quot;Designing and maintaining t...\" target=\"_blank\">\n        42.\n      </a>\n    Extract, Transform, Load (ETL)\n\"Designing and maintaining the ETL process is often\nconsidered one of the most difficult and resource-\nintensive portions of a data warehouse project.\"\nhttp://docs.oracle.com/cd/B19306_01/server.102/b14223/ettover.htm\n43\nAlso unnecessarily redundant and often typeless\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-43-638.jpg?cb=1446278061\" title=\"ETL&#10;44&#10;• Each ETL step can introduce errors and risk&#10;• Can ...\" target=\"_blank\">\n        43.\n      </a>\n    ETL\n44\n• Each ETL step can introduce errors and risk\n• Can duplicate data after failover\n• Tools can cost millions of dollars\n• Decreases throughput\n• Increased complexity\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-44-638.jpg?cb=1446278061\" title=\"ETL&#10;• Writing intermediary files&#10;• Parsing and re-parsing p...\" target=\"_blank\">\n        44.\n      </a>\n    ETL\n• Writing intermediary files\n• Parsing and re-parsing plain text\n45\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-45-638.jpg?cb=1446278061\" title=\"And let's duplicate the pattern&#10;over all our DataCenters&#10;46&#10;\" target=\"_blank\">\n        45.\n      </a>\n    And let's duplicate the pattern\nover all our DataCenters\n46\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-46-638.jpg?cb=1446278061\" title=\"47&#10;These are not the solutions you're looking for&#10;\" target=\"_blank\">\n        46.\n      </a>\n    47\nThese are not the solutions you're looking for\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-47-638.jpg?cb=1446278061\" title=\"REVISITING THE GOAL&#10;&amp; THE STACK&#10;48&#10;\" target=\"_blank\">\n        47.\n      </a>\n    REVISITING THE GOAL\n&amp; THE STACK\n48\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-48-638.jpg?cb=1446278061\" title=\"Removing The 'E' in ETL&#10;Thanks to technologies like Avro an...\" target=\"_blank\">\n        48.\n      </a>\n    Removing The 'E' in ETL\nThanks to technologies like Avro and Protobuf we don’t need the\n“E” in ETL. Instead of text dumps that you need to parse over\nmultiple systems:\nScala &amp; Avro (e.g.)\n• Can work with binary data that remains strongly typed\n• A return to strong typing in the big data ecosystem\n49\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-49-638.jpg?cb=1446278061\" title=\"Removing The 'L' in ETL&#10;If data collection is backed by a d...\" target=\"_blank\">\n        49.\n      </a>\n    Removing The 'L' in ETL\nIf data collection is backed by a distributed messaging\nsystem (e.g. Kafka) you can do real-time fanout of the\ningested data to all consumers. No need to batch \"load\".\n• From there each consumer can do their own transformations\n50\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-50-638.jpg?cb=1446278061\" title=\"#NoMoreGreekLetterArchitectures&#10;51&#10;\" target=\"_blank\">\n        50.\n      </a>\n    #NoMoreGreekLetterArchitectures\n51\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-51-638.jpg?cb=1446278061\" title=\"NoETL&#10;52&#10;\" target=\"_blank\">\n        51.\n      </a>\n    NoETL\n52\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-52-638.jpg?cb=1446278061\" title=\"Strategy Technologies&#10;Scalable Infrastructure / Elastic Spa...\" target=\"_blank\">\n        52.\n      </a>\n    Strategy Technologies\nScalable Infrastructure / Elastic Spark, Cassandra, Kafka\nPartition For Scale, Network Topology Aware Cassandra, Spark, Kafka, Akka Cluster\nReplicate For Resiliency Spark,Cassandra, Akka Cluster all hash the node ring\nShare Nothing, Masterless Cassandra, Akka Cluster both Dynamo style\nFault Tolerance / No Single Point of Failure Spark, Cassandra, Kafka\nReplay From Any Point Of Failure Spark, Cassandra, Kafka, Akka + Akka Persistence\nFailure Detection Cassandra, Spark, Akka, Kafka\nConsensus &amp; Gossip Cassandra &amp; Akka Cluster\nParallelism Spark, Cassandra, Kafka, Akka\nAsynchronous Data Passing Kafka, Akka, Spark\nFast, Low Latency, Data Locality Cassandra, Spark, Kafka\nLocation Transparency Akka, Spark, Cassandra, Kafka\nMy Nerdy Chart\n53\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-53-638.jpg?cb=1446278061\" title=\"SMACK&#10;• Scala/Spark&#10;• Mesos&#10;• Akka&#10;• Cassandra&#10;• Kafka&#10;54&#10;\" target=\"_blank\">\n        53.\n      </a>\n    SMACK\n• Scala/Spark\n• Mesos\n• Akka\n• Cassandra\n• Kafka\n54\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-54-638.jpg?cb=1446278061\" title=\"Spark Streaming&#10;55&#10;\" target=\"_blank\">\n        54.\n      </a>\n    Spark Streaming\n55\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-55-638.jpg?cb=1446278061\" title=\"Spark Streaming&#10;• One runtime for streaming and batch proce...\" target=\"_blank\">\n        55.\n      </a>\n    Spark Streaming\n• One runtime for streaming and batch processing\n• Join streaming and static data sets\n• No code duplication\n• Easy, flexible data ingestion from disparate sources to\ndisparate sinks\n• Easy to reconcile queries against multiple sources\n• Easy integration of KV durable storage\n56\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-56-638.jpg?cb=1446278061\" title=\"How do I merge historical data with data&#10;in the stream?&#10;57&#10;\" target=\"_blank\">\n        56.\n      </a>\n    How do I merge historical data with data\nin the stream?\n57\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-57-638.jpg?cb=1446278061\" title=\"Join Streams With Static Data&#10;val ssc = new StreamingContex...\" target=\"_blank\">\n        57.\n      </a>\n    Join Streams With Static Data\nval ssc = new StreamingContext(conf, Milliseconds(500))\nssc.checkpoint(\"checkpoint\")\nval staticData: RDD[(Int,String)] =\nssc.sparkContext.textFile(\"whyAreWeParsingFiles.txt\").flatMap(func)\nval stream: DStream[(Int,String)] =\nKafkaUtils.createStream(ssc, zkQuorum, group, Map(topic -&gt; n))\n.transform { events =&gt; events.join(staticData))\n.saveToCassandra(keyspace,table)\nssc.start()\n58\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-58-638.jpg?cb=1446278061\" title=\"Training&#10;Data&#10;Feature&#10;Extraction&#10;Model&#10;Training&#10;Model&#10;Testi...\" target=\"_blank\">\n        58.\n      </a>\n    Training\nData\nFeature\nExtraction\nModel\nTraining\nModel\nTesting\nTest Data\nYour Data Extract Data To Analyze\nTrain your model to predict\nSpark MLLib\n59\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-59-638.jpg?cb=1446278061\" title=\"Spark Streaming &amp; ML&#10;60&#10;val context = new StreamingContext(...\" target=\"_blank\">\n        59.\n      </a>\n    Spark Streaming &amp; ML\n60\nval context = new StreamingContext(conf, Milliseconds(500))\nval model = KMeans.train(dataset, ...) // learn offline\nval stream = KafkaUtils\n.createStream(ssc, zkQuorum, group,..)\n.map(event =&gt; model.predict(event.feature))\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-60-638.jpg?cb=1446278061\" title=\"Apache Mesos&#10;Open-source cluster manager developed at UC Be...\" target=\"_blank\">\n        60.\n      </a>\n    Apache Mesos\nOpen-source cluster manager developed at UC Berkeley.\nAbstracts CPU, memory, storage, and other compute resources\naway from machines (physical or virtual), enabling fault-tolerant\nand elastic distributed systems to easily be built and run\neffectively.\n61\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-61-638.jpg?cb=1446278061\" title=\"Akka&#10;High performance concurrency framework for Scala and&#10;J...\" target=\"_blank\">\n        61.\n      </a>\n    Akka\nHigh performance concurrency framework for Scala and\nJava\n• Fault Tolerance\n• Asynchronous messaging and data processing\n• Parallelization\n• Location Transparency\n• Local / Remote Routing\n• Akka: Cluster / Persistence / Streams\n62\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-62-638.jpg?cb=1446278061\" title=\"Akka Actors&#10;A distribution and concurrency abstraction&#10;• Co...\" target=\"_blank\">\n        62.\n      </a>\n    Akka Actors\nA distribution and concurrency abstraction\n• Compute Isolation\n• Behavioral Context Switching\n• No Exposed Internal State\n• Event-based messaging\n• Easy parallelism\n• Configurable fault tolerance\n63\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-63-638.jpg?cb=1446278061\" title=\"64&#10;Akka Actor Hierarchy&#10;http://www.slideshare.net/jboner/bu...\" target=\"_blank\">\n        63.\n      </a>\n    64\nAkka Actor Hierarchy\nhttp://www.slideshare.net/jboner/building-reactive-applications-with-akka-in-scala\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-64-638.jpg?cb=1446278061\" title=\"import akka.actor._&#10;class NodeGuardianActor(args...) extend...\" target=\"_blank\">\n        64.\n      </a>\n    import akka.actor._\nclass NodeGuardianActor(args...) extends Actor with SupervisorStrategy {\nval temperature = context.actorOf(\nProps(new TemperatureActor(args)), \"temperature\")\nval precipitation = context.actorOf(\nProps(new PrecipitationActor(args)), \"precipitation\")\noverride def preStart(): Unit = { /* lifecycle hook: init */ }\ndef receive : Actor.Receive = {\ncase Initialized =&gt; context become initialized\n}\ndef initialized : Actor.Receive = {\ncase e: SomeEvent =&gt; someFunc(e)\ncase e: OtherEvent =&gt; otherFunc(e)\n}\n}\n65\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-65-638.jpg?cb=1446278061\" title=\"Apache Cassandra&#10;• Extremely Fast&#10;• Extremely Scalable&#10;• Mu...\" target=\"_blank\">\n        65.\n      </a>\n    Apache Cassandra\n• Extremely Fast\n• Extremely Scalable\n• Multi-Region / Multi-Datacenter\n• Always On\n• No single point of failure\n• Survive regional outages\n• Easy to operate\n• Automatic &amp; configurable replication 66\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-66-638.jpg?cb=1446278061\" title=\"Apache Cassandra&#10;• Very flexible data modeling (collections...\" target=\"_blank\">\n        66.\n      </a>\n    Apache Cassandra\n• Very flexible data modeling (collections, user defined\ntypes) and changeable over time\n• Perfect for ingestion of real time / machine data\n• Huge community\n67\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-67-638.jpg?cb=1446278061\" title=\"Spark Cassandra Connector&#10;• NOSQL JOINS!&#10;• Write &amp; Read dat...\" target=\"_blank\">\n        67.\n      </a>\n    Spark Cassandra Connector\n• NOSQL JOINS!\n• Write &amp; Read data between Spark and Cassandra\n• Compatible with Spark 1.4\n• Handles Data Locality for Speed\n• Implicit type conversions\n• Server-Side Filtering - SELECT, WHERE, etc.\n• Natural Timeseries Integration\n68\nhttp://github.com/datastax/spark-cassandra-connector\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-68-638.jpg?cb=1446278061\" title=\"KillrWeather&#10;69&#10;http://github.com/killrweather/killrweather...\" target=\"_blank\">\n        68.\n      </a>\n    KillrWeather\n69\nhttp://github.com/killrweather/killrweather\nA reference application showing how to easily integrate streaming and\nbatch data processing with Apache Spark Streaming, Apache\nCassandra, Apache Kafka and Akka for fast, streaming computations\non time series data in asynchronous event-driven environments.\nhttp://github.com/databricks/reference-apps/tree/master/timeseries/scala/timeseries-weather/src/main/scala/com/\ndatabricks/apps/weather\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-69-638.jpg?cb=1446278061\" title=\"70&#10;• High Throughput Distributed Messaging&#10;• Decouples Data...\" target=\"_blank\">\n        69.\n      </a>\n    70\n• High Throughput Distributed Messaging\n• Decouples Data Pipelines\n• Handles Massive Data Load\n• Support Massive Number of Consumers\n• Distribution &amp; partitioning across cluster nodes\n• Automatic recovery from broker failures\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-70-638.jpg?cb=1446278061\" title=\"Spark Streaming &amp; Kafka&#10;val context = new StreamingContext(...\" target=\"_blank\">\n        70.\n      </a>\n    Spark Streaming &amp; Kafka\nval context = new StreamingContext(conf, Seconds(1))\nval wordCount = KafkaUtils.createStream(context, ...)\n.flatMap(_.split(\" \"))\n.map(x =&gt; (x, 1))\n.reduceByKey(_ + _)\nwordCount.saveToCassandra(ks,table)\ncontext.start() // start receiving and computing\n71\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-71-638.jpg?cb=1446278061\" title=\"72&#10;class KafkaStreamingActor(params: Map[String, String], s...\" target=\"_blank\">\n        71.\n      </a>\n    72\nclass KafkaStreamingActor(params: Map[String, String], ssc: StreamingContext)\nextends AggregationActor(settings: Settings) { \nimport settings._\n \nval kafkaStream = KafkaUtils.createStream[String, String, StringDecoder, StringDecoder]( \nssc, params, Map(KafkaTopicRaw -&gt; 1), StorageLevel.DISK_ONLY_2) \n.map(_._2.split(\",\")) \n.map(RawWeatherData(_)) \n \nkafkaStream.saveToCassandra(CassandraKeyspace, CassandraTableRaw) \n/** RawWeatherData: wsid, year, month, day, oneHourPrecip */ \nkafkaStream.map(hour =&gt; (hour.wsid, hour.year, hour.month, hour.day, hour.oneHourPrecip)) \n.saveToCassandra(CassandraKeyspace, CassandraTableDailyPrecip) \n \n/** Now the [[StreamingContext]] can be started. */ \ncontext.parent ! OutputStreamInitialized \n \ndef receive : Actor.Receive = {…}\n}\nGets the partition key: Data Locality\nSpark C* Connector feeds this to Spark\nCassandra Counter column in our schema,\nno expensive `reduceByKey` needed. Simply\nlet C* do it: not expensive and fast.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-72-638.jpg?cb=1446278061\" title=\"73&#10;/** For a given weather station, calculates annual cumul...\" target=\"_blank\">\n        72.\n      </a>\n    73\n/** For a given weather station, calculates annual cumulative precip - or year to date. */ \nclass PrecipitationActor(ssc: StreamingContext, settings: WeatherSettings) extends AggregationActor { \n \ndef receive : Actor.Receive = { \ncase GetPrecipitation(wsid, year) =&gt; cumulative(wsid, year, sender) \ncase GetTopKPrecipitation(wsid, year, k) =&gt; topK(wsid, year, k, sender) \n} \n \n/** Computes annual aggregation.Precipitation values are 1 hour deltas from the previous. */ \ndef cumulative(wsid: String, year: Int, requester: ActorRef): Unit = \nssc.cassandraTable[Double](keyspace, dailytable) \n.select(\"precipitation\") \n.where(\"wsid = ? AND year = ?\", wsid, year) \n.collectAsync() \n.map(AnnualPrecipitation(_, wsid, year)) pipeTo requester \n \n/** Returns the 10 highest temps for any station in the `year`. */ \ndef topK(wsid: String, year: Int, k: Int, requester: ActorRef): Unit = { \nval toTopK = (aggregate: Seq[Double]) =&gt; TopKPrecipitation(wsid, year, \nssc.sparkContext.parallelize(aggregate).top(k).toSeq) \n \nssc.cassandraTable[Double](keyspace, dailytable) \n.select(\"precipitation\") \n.where(\"wsid = ? AND year = ?\", wsid, year) \n.collectAsync().map(toTopK) pipeTo requester \n} \n}\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-73-638.jpg?cb=1446278061\" title=\"A New Approach&#10;• One Runtime: streaming, scheduled&#10;• Simpli...\" target=\"_blank\">\n        73.\n      </a>\n    A New Approach\n• One Runtime: streaming, scheduled\n• Simplified architecture\n• Allows us to\n• Write different types of applications\n• Write more type safe code\n• Write more reusable code\n74\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-74-638.jpg?cb=1446278061\" title=\"Need daily analytics aggregate reports? Do it in the stream...\" target=\"_blank\">\n        74.\n      </a>\n    Need daily analytics aggregate reports? Do it in the stream, save\nresults in Cassandra for easy reporting as needed - with data\nlocality not offered by S3.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-75-638.jpg?cb=1446278061\" title=\"FiloDB&#10;Distributed, columnar database designed to run very ...\" target=\"_blank\">\n        75.\n      </a>\n    FiloDB\nDistributed, columnar database designed to run very fast\nanalytical queries\n• Ingest streaming data from many streaming sources\n• Row-level, column-level operations and built in versioning\noffer greater flexibility than file-based technologies\n• Currently based on Apache Cassandra &amp; Spark\n• github.com/tuplejump/FiloDB\n76\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-76-638.jpg?cb=1446278061\" title=\"FiloDB&#10;• Breakthrough performance levels for analytical que...\" target=\"_blank\">\n        76.\n      </a>\n    FiloDB\n• Breakthrough performance levels for analytical queries\n• Performance comparable to Parquet\n• One to two orders of magnitude faster than Spark on\nCassandra 2.x\n• Versioned - critical for reprocessing logic/code changes\n• Can simplify your infrastructure dramatically\n• Queries run in parallel in Spark for scale-out ad-hoc analysis\n• Space-saving techniques\n77\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-77-638.jpg?cb=1446278061\" title=\"WRAPPING UP&#10;78&#10;\" target=\"_blank\">\n        77.\n      </a>\n    WRAPPING UP\n78\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-78-638.jpg?cb=1446278061\" title=\"Architectyr?&#10;79&#10;&quot;This is a giant mess&quot;&#10;- Going Real-time - ...\" target=\"_blank\">\n        78.\n      </a>\n    Architectyr?\n79\n\"This is a giant mess\"\n- Going Real-time - Data Collection and Stream Processing with Apache Kafka, Jay Kreps\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-79-638.jpg?cb=1446278061\" title=\"80&#10;Simplified&#10;\" target=\"_blank\">\n        79.\n      </a>\n    80\nSimplified\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-80-638.jpg?cb=1446278061\" title=\"81&#10;\" target=\"_blank\">\n        80.\n      </a>\n    81\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-81-638.jpg?cb=1446278061\" title=\"82&#10;www.tuplejump.com&#10;info@tuplejump.com@tuplejump&#10;\" target=\"_blank\">\n        81.\n      </a>\n    82\nwww.tuplejump.com\ninfo@tuplejump.com@tuplejump\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-82-638.jpg?cb=1446278061\" title=\"83&#10;@helenaedelson&#10;github.com/helena&#10;slideshare.net/helenaed...\" target=\"_blank\">\n        82.\n      </a>\n    83\n@helenaedelson\ngithub.com/helena\nslideshare.net/helenaedelson\nTHANK YOU!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891/95/streaming-analytics-with-spark-kafka-cassandra-and-akka-83-638.jpg?cb=1446278061\" title=\"I'm speaking at QCon SF on the broader&#10;topic of Streaming a...\" target=\"_blank\">\n        83.\n      </a>\n    I'm speaking at QCon SF on the broader\ntopic of Streaming at Scale\nhttp://qconsf.com/sf2015/track/streaming-data-scale\n84\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"54590394\" title=\"Bruce Heavin The Thinkable Presentation\" href=\"https://www.linkedin.com/learning/bruce-heavin-the-thinkable-presentation?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Bruce Heavin The Thinkable Presentation\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Bruce Heavin The Thinkable Presentation\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=7Pmu5jhLuzYhd3kcdQaN2ktEQv0%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-gWiKv_9yfYXbufMLWZLOn7FQIImxW\" /></div>\n    <div class=\"lynda-content\"><p>Bruce Heavin The Thinkable Presentation</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"54590394\" title=\"Gaining Skills with LinkedIn Learning\" href=\"https://www.linkedin.com/learning/gaining-skills-with-linkedin-learning?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Gaining Skills with LinkedIn Learning\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Gaining Skills with LinkedIn Learning\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=dTAVDscYes1J4Ec%2F4ONSWvNwSnk%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kWyai-9SfZXfqccbeZLSiolwWfy8JlQEyfuisRznmEY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Gaining Skills with LinkedIn Learning</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"54590394\" title=\"PowerPoint: Designing Better Slides\" href=\"https://www.linkedin.com/learning/powerpoint-designing-better-slides?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint: Designing Better Slides\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint: Designing Better Slides\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Qzyjo5pakOdYhauvchS%2F2whtPj0%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kUyWs-dWfZX_pf8TfZLSiol4feCwDkwc2feivRTXiEY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint: Designing Better Slides</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53371129\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Reactive app using actor model &amp; apache spark\" href=\"https://www.slideshare.net/RahulKumar405/reactive-app-using-actor-model-apache-spark\">\n    \n    <div class=\"related-content\"><p>Reactive app using actor model &amp; apache spark</p><p>Rahul Kumar</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"54491621\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Real-Time Anomaly Detection  with Spark MLlib, Akka and  Cassandra\" href=\"https://www.slideshare.net/natalinobusa/realtime-anomaly-detection-with-spark-mllib-akka-and-cassandra\">\n    \n    <div class=\"related-content\"><p>Real-Time Anomaly Detection  with Spark MLlib, Akka and  Cassandra</p><p>Natalino Busa</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"55646316\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Sa introduction to big data pipelining with cassandra &amp;amp; spark   west minster meetup - black-2015 0.11-2\" href=\"https://www.slideshare.net/langworth/sa-introduction-to-big-data-pipelining-with-cassandra-amp-spark-west-minster-meetup-black2015-0112\">\n    \n    <div class=\"related-content\"><p>Sa introduction to big data pipelining with cassandra &amp;amp; spark   west mins...</p><p>Simon Ambridge</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"49190113\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Lambda Architecture with Spark Streaming, Kafka, Cassandra, Akka, Scala\" href=\"https://www.slideshare.net/helenaedelson/lambda-architecture-with-spark-streaming-kafka-cassandra-akka-scala\">\n    \n    <div class=\"related-content\"><p>Lambda Architecture with Spark Streaming, Kafka, Cassandra, Akka, Scala</p><p>Helena Edelson</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"43475359\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Streaming Big Data with Spark, Kafka, Cassandra, Akka &amp; Scala (from webinar)\" href=\"https://www.slideshare.net/helenaedelson/streaming-bigdata-helenawebinarv3\">\n    \n    <div class=\"related-content\"><p>Streaming Big Data with Spark, Kafka, Cassandra, Akka &amp; Scala (from webinar)</p><p>Helena Edelson</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"50371041\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Spark Kernel Talk - Apache Spark Meetup San Francisco (July 2015)\" href=\"https://www.slideshare.net/RobertChipSenkbeil/spark-kernel-meetup-talk\">\n    \n    <div class=\"related-content\"><p>Spark Kernel Talk - Apache Spark Meetup San Francisco (July 2015)</p><p>Robert \"Chip\" Senkbeil</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"53162817\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"How to deploy Apache Spark  to Mesos/DCOS\" href=\"https://www.slideshare.net/Typesafe_Inc/how-to-deploy-apache-spark-to-mesosdcos\">\n    \n    <div class=\"related-content\"><p>How to deploy Apache Spark  to Mesos/DCOS</p><p>Legacy Typesafe (now Lightbend)</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-06-27T03:57:42+0000",
        "updated_at": "2018-06-27T03:57:51+0000",
        "published_at": null,
        "published_by": [
          "Helena Edelson"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 13,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/streaming-analytics-spark-kafka-cassandra-akka-151031073912-lva1-app6891-thumbnail-4.jpg?cb=1446278061",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10744"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          },
          {
            "id": 1103,
            "label": "prometheus",
            "slug": "prometheus"
          }
        ],
        "is_public": false,
        "id": 10299,
        "uid": null,
        "title": "criteo/cassandra_exporter",
        "url": "https://github.com/criteo/cassandra_exporter",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p style=\"text-align: center;\">\n  <a target=\"_blank\" href=\"https://github.com/criteo/cassandra_exporter/raw/master/logo.png\"><img src=\"https://github.com/criteo/cassandra_exporter/raw/master/logo.png\" alt=\"logo\" /></a>\n</p>\n<h2><a id=\"user-content-description\" class=\"anchor\" aria-hidden=\"true\" href=\"#description\"></a>Description</h2>\n<p>Cassandra exporter is a standalone application which exports <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra®</a> metrics throught a prometheus friendly endpoint.\nThis project is originally a fork of <a href=\"https://github.com/prometheus/jmx_exporter\">JMX exporter</a> but aims at an easier integration with <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra®</a>.</p>\n<p>Specifically, this project brings :</p>\n<ul class=\"contains-task-list\"><li class=\"task-list-item\"> Exporting EstimatedHistogram metrics specific to <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra®</a></li>\n<li class=\"task-list-item\"> Filtering on mbean's attributes</li>\n<li class=\"task-list-item\"> Metrics naming that respect the mbean hierarchy</li>\n<li class=\"task-list-item\"> Comprehensive config file</li>\n</ul><p>An essential design choice the project makes is to not let prometheus drive the scraping frequency. This decision has been taken because a lot of <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra®</a> metrics are expensive to scrap and can hinder the performance of the node.\nAs we don't want this kind of situation to happen in production, the scrape frequency is restricted via the configuration of Cassandra Exporter.</p>\n<p><a target=\"_blank\" href=\"https://camo.githubusercontent.com/85f8e6b503fc834c4127aa556d8d941c29b8a1ab/68747470733a2f2f67726166616e612e636f6d2f6170692f64617368626f617264732f363235382f696d616765732f333939362f696d616765\"><img src=\"https://camo.githubusercontent.com/85f8e6b503fc834c4127aa556d8d941c29b8a1ab/68747470733a2f2f67726166616e612e636f6d2f6170692f64617368626f617264732f363235382f696d616765732f333939362f696d616765\" alt=\"Grafana\" data-canonical-src=\"https://grafana.com/api/dashboards/6258/images/3996/image\" /></a>\n<a target=\"_blank\" href=\"https://camo.githubusercontent.com/e9d40a59216424e2a99d8641461c4299925ea44a/68747470733a2f2f67726166616e612e636f6d2f6170692f64617368626f617264732f363430302f696d616765732f343131312f696d616765\"><img src=\"https://camo.githubusercontent.com/e9d40a59216424e2a99d8641461c4299925ea44a/68747470733a2f2f67726166616e612e636f6d2f6170692f64617368626f617264732f363430302f696d616765732f343131312f696d616765\" alt=\"Grafana\" data-canonical-src=\"https://grafana.com/api/dashboards/6400/images/4111/image\" /></a></p>\n<h2><a id=\"user-content-how-to-use\" class=\"anchor\" aria-hidden=\"true\" href=\"#how-to-use\"></a>How to use</h2>\n<p>To start the application</p>\n<blockquote>\n<p>java -jar cassandra_exporter.jar config.yml</p>\n</blockquote>\n<p>The Cassandra exporter needs to run on every Cassandra nodes to get all the informations regarding the whole cluster.</p>\n<p>You can have a look at a full configuration file <a href=\"https://github.com/criteo/cassandra_exporter/blob/master/config.yml\">here</a>\nThe 2 main parts are :</p>\n<ol><li>blacklist</li>\n<li>maxScrapFrequencyInSec</li>\n</ol><p>In the <code>blacklist</code> block, you specify the metrics you don't want the exporter to scrape. This is important as JMX is an RPC mechanism and you don't want to trigger some of those RPC. For example, mbeans endpoint from <code>org:apache:cassandra:db:.*</code> does not expose any metrics but are used to trigger actions on Cassandra's nodes.</p>\n<p>In the <code>maxScrapFrequencyInSec</code>, you specify the metrics you want to be scraped at which frequency.\nBasically, starting from the set of all mbeans, the blacklist is applied first to filter this set and then the <code>maxScrapFrequencyInSec</code> is applied as a whitelist to filter the resulting set.</p>\n<p>As an example, if we take as input set the metrics <code>{a, b, c}</code> and the config file is</p>\n<div class=\"highlight highlight-source-yaml\"><pre>blacklist:\n  - a\nmaxScrapFrequencyInSec:\n  50:\n    - .*\n  3600:\n    - b</pre></div>\n<p>Cassandra Exporter will have the following behavior:</p>\n<ol><li>The metrics matching the blacklisted entries will never be scraped, here the metric <code>a</code> won't be available</li>\n<li>In reverse order of frequency the metrics matching <code>maxScrapFrequencyInSec</code> will be scraped\n<ol><li>Metric <code>b</code> will be scraped every hour</li>\n<li>Remaining metrics will be scrapped every 50s, here only <code>c</code></li>\n</ol></li>\n</ol><p>Resulting in :</p>\n<table><thead><tr><th>Metric</th>\n<th>Scrap Frequency</th>\n</tr></thead><tbody><tr><td>a</td>\n<td>never</td>\n</tr><tr><td>b</td>\n<td>every hour</td>\n</tr><tr><td>c</td>\n<td>every 50 seconds</td>\n</tr></tbody></table><p>Once started the prometheus endpoint will be available at <code>localhost:listenPort/</code> or <code>localhost:listenPort/metrics</code> and metrics format will look like the one below</p>\n<blockquote>\n<p>cassandra_stats{name=\"org:apache:cassandra:metrics:table:biggraphite:datapoints_5760p_3600s_aggr:writelatency:50thpercentile\",} 35.425000000000004</p>\n</blockquote>\n<h2><a id=\"user-content-how-to-debug\" class=\"anchor\" aria-hidden=\"true\" href=\"#how-to-debug\"></a>How to debug</h2>\n<p>Run the program with the following options:</p>\n<blockquote>\n<p>java -Dorg.slf4j.simpleLogger.defaultLogLevel=trace -jar cassandra_exporter.jar config.yml --oneshot</p>\n</blockquote>\n<p>You will get the duration of how long it took to scrape individual MBean, this is useful to understand which metrics are expansive to scrape.</p>\n<p>Goods sources of information to understand what Mbeans are doing/create your dashboards are:</p>\n<ol><li><a href=\"https://cassandra.apache.org/doc/latest/operating/metrics.html\" rel=\"nofollow\">https://cassandra.apache.org/doc/latest/operating/metrics.html</a></li>\n<li><a href=\"https://github.com/apache/cassandra/tree/trunk/src/java/org/apache/cassandra/metrics\">https://github.com/apache/cassandra/tree/trunk/src/java/org/apache/cassandra/metrics</a></li>\n<li><a href=\"http://thelastpickle.com/blog/2017/12/05/datadog-tlp-dashboards.html\" rel=\"nofollow\">http://thelastpickle.com/blog/2017/12/05/datadog-tlp-dashboards.html</a></li>\n<li><a href=\"https://www.youtube.com/watch?v=Q9AAR4UQzMk\" rel=\"nofollow\">https://www.youtube.com/watch?v=Q9AAR4UQzMk</a></li>\n</ol><h2><a id=\"user-content-config-file-example\" class=\"anchor\" aria-hidden=\"true\" href=\"#config-file-example\"></a>Config file example</h2>\n<div class=\"highlight highlight-source-yaml\"><pre>host: localhost:7199\nssl: False\nuser:\npassword:\nlistenPort: 8080\nblacklist:\n   # Unaccessible metrics (not enough privilege)\n   - java:lang:memorypool:.*usagethreshold.*\n   # Leaf attributes not interesting for us but that are presents in many path (reduce cardinality of metrics)\n   - .*:999thpercentile\n   - .*:95thpercentile\n   - .*:fifteenminuterate\n   - .*:fiveminuterate\n   - .*:durationunit\n   - .*:rateunit\n   - .*:stddev\n   - .*:meanrate\n   - .*:mean\n   - .*:min\n   # Path present in many metrics but uninterresting\n   - .*:viewlockacquiretime:.*\n   - .*:viewreadtime:.*\n   - .*:cas[a-z]+latency:.*\n   - .*:colupdatetimedeltahistogram:.*\n   # Mostly for RPC, do not scrap them\n   - org:apache:cassandra:db:.*\n   # columnfamily is an alias for Table metrics in cassandra 3.x\n   # https://github.com/apache/cassandra/blob/8b3a60b9a7dbefeecc06bace617279612ec7092d/src/java/org/apache/cassandra/metrics/TableMetrics.java#L162\n   - org:apache:cassandra:metrics:columnfamily:.*\n   # Should we export metrics for system keyspaces/tables ?\n   - org:apache:cassandra:metrics:[^:]+:system[^:]*:.*\n   # Don't scrape us\n   - com:criteo:nosql:cassandra:exporter:.*\nmaxScrapFrequencyInSec:\n  50:\n    - .*\n  # Refresh those metrics only every hour as it is costly for cassandra to retrieve them\n  3600:\n    - .*:snapshotssize:.*\n    - .*:estimated.*\n    - .*:totaldiskspaceused:.*</pre></div>\n<h2><a id=\"user-content-docker\" class=\"anchor\" aria-hidden=\"true\" href=\"#docker\"></a>Docker</h2>\n<p>You can pull an image directly from <a href=\"https://hub.docker.com/r/criteord/cassandra_exporter/\" rel=\"nofollow\">Dockerhub</a>:</p>\n<pre>docker pull criteord/cassandra_exporter:latest\n</pre>\n<h2><a id=\"user-content-kubernetes\" class=\"anchor\" aria-hidden=\"true\" href=\"#kubernetes\"></a>Kubernetes</h2>\n<p>To get an idea on how to integrate Cassandra Exporter in Kubernetes, you can look at <a href=\"https://github.com/MySocialApp/kubernetes-helm-chart-cassandra\">this helm Chart</a>.</p>\n<h2><a id=\"user-content-grafana\" class=\"anchor\" aria-hidden=\"true\" href=\"#grafana\"></a>Grafana</h2>\n<p>Dedicated dashboards can be <a href=\"https://github.com/criteo/cassandra_exporter/tree/grafana/grafana\">found here</a></p>\n</article>",
        "created_at": "2018-06-21T12:17:35+0000",
        "updated_at": "2018-06-21T12:17:43+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/1713646?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10299"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1367,
            "label": "scaling",
            "slug": "scaling"
          }
        ],
        "is_public": false,
        "id": 10288,
        "uid": null,
        "title": "Scale it to Billions — What They Don’t Tell you in the Cassandra README – Threat Stack",
        "url": "https://www.threatstack.com/blog/scaling-cassandra-lessons-learned",
        "content": "<p class=\"p1\">At Threat Stack our engineering and operations teams have embraced the concept of the polyglot data platform, recognizing that no one solution can provide for all of our needs. Those needs include rapid scaling, ideally linearly, to support growing customer demand and the elastic workloads of our new economy customers. We also require different forms of analysis to support stream analysis for our IDS feature set, efficient lookup tables and prematerialized views for our ETDR feature set, and offline analysis for analysis and research.</p>\n<p class=\"p3\">A core component of our data platform for several years has been Cassandra, which we upgraded to Datastax Enterprise (DSE) through their start up program last year. Originally we were expecting to use it as our single source of truth for all of our time series data, but this turned out to be an anti pattern. Instead we have found it very useful for look up tables and pre-materialized views (more on this later).</p>\n<p>Our initial Cassandra cluster was three i2.2xlarge nodes spread across 3 Availability Zones in AWS, but within a few short months we had expanded to over thirty Cassandra nodes. At the rate at which we were taking in new data, our overall scaling strategy looked similar to this:<br /></p>\n<p><strong><strong> </strong></strong></p>\n<p>The team did have extensive experience with other eventually consistent databases though, so with some advice from Datastax, Cassandra has now become one of the most stable and pleasurable components of our polyglot data platform to work with.</p>\n<p>This content has been a long time coming, so we definitely ran longer than a generally accepted blog post might. We decided to ironically embrace the chaos by providing you with a table of contents:</p>\n<ol><li><a href=\"#1\">Quick Tips</a></li>\n<li><a href=\"#2\">Monitoring Gotcha and Quirks</a></li>\n<li><a href=\"#3\">AWS Instance Type and Disk Configuration</a></li>\n<li><a href=\"#4\">Drowning Your Cluster with Multi-Tenant Tables</a></li>\n<li><a href=\"#5\">Streaming Failures, JVM Tuning, and GC Woes</a></li>\n<li><a href=\"#6\">Schema design, or These Aren’t the Rows You’re Looking For</a></li>\n<li><a href=\"#7\">Only Have 2 to 3 Seed Nodes per Data Center</a></li>\n<li><a href=\"#8\">Conclusion</a></li>\n</ol><h2>1. Quick Tips</h2>\n<p>Some of these are well known, but are worth repeating.</p>\n<ul><li>Batch writes by partition key, then by batch size (5120 bytes, the Cassandra warn threshold).</li>\n<li>Don’t under staff Cassandra. This is hard as a start up, but recognize going in that it could require 1 to 2 FTEs as you ramp up, maybe more depending on how quickly you scale up. While we are able to see the benefits of this investment on the other side, it was an uncomfortable place to be in as we were also scaling up customer acquisition. </li>\n<li>Don’t adopt new versions too quickly. Minor version bumps have been known to cause havoc and new features only work roughly as expected the first time (DTCS is a great example of this). We regularly run new versions in our development environment for a month or two before promoting to production. </li>\n<li>Paradoxically, don’t fall too far behind with new versions. Sadly we have friends who are still stuck on Cassandra 1.x because of their Thrift usage. We light a candle for them. </li>\n<li>Many smaller nodes beat out fewer larger nodes. Not only does this make your dynamo quorum math more resilient to per-server failure, which is important because AWS <strong>will</strong> decommission a node in your cluster, but it decreases the mean time to recovery because you can stream data across more nodes at a cluster to achieve higher aggregate data transfer rates. </li>\n<li>Budget days to bring a node into the cluster. If you’ve vertically scaled, then it will take over a week. Regardless you will want to uncap streaming and compaction throughput with nodetool.  You’ll take a slight performance hit, but it’s worth it to finish node streaming in a reasonable time. This is another good reason why you want to run more smaller sized instances with less data per system. </li>\n<li>Don’t use hinted handoffs (ANY or LOCAL_ANY quorum). In fact, just disable them in the configuration. It’s too easy to lose data during a prolonged outage or load spike, and if a node went down because of the load spike you’re just going to pass the problem around the ring, eventually taking multiple or all nodes down. We never experienced this on Cassandra, but have on other systems that supported hinted handoffs.</li>\n</ul><h2>2. Monitoring Gotcha and Quirks </h2>\n<p>Datastax graciously offers usage of Opscenter and the included agent for both open source Cassandra and DSE.  The setup and deployment of this is incredibly simple, and when you are a high growth start up, time is a premium.  Opscenter acted as our initial metrics source and gave us pretty high quality metrics early in our scaling time line.  Because it was easy to setup and deploy the opscenter collector agents we were able to get meaningful data without spending a lot of time on it.  </p>\n<p>A few months after we launched our service, new user sign ups increased our data ingestion by an order of magnitude within just 2 weeks.  We were over 20 Cassandra nodes by this point and ingesting a few TB per day.  Our original way of handling multi-tenancy was at the table level, essentially storing each customer’s data in their own table  (we talk more below about why this was a bad idea).  But as our customer count exploded we were creating more and more tables (we had a few per each customer).  By default the opscenter agent will collect data for all your tables to allow for some pretty awesome granular metrics capture. But when you have so many tables it turns into a lot of metrics. Which brings us to our first gotcha.</p>\n<p><strong>Don’t use the same Cassandra cluster to store Opscenter metrics and application data.</strong></p>\n<p>Now – the problem is that currently using a separate cluster for metrics collection with opscenter is an Enterprise only option.  If you are lucky enough to be using DSE then <a href=\"http://docs.datastax.com/en/opscenter/5.1/opsc/configure/opscStoringCollectionDataDifferentCluster_t.html\" target=\"_blank\" rel=\"noopener\">here is how you can set it up</a>.</p>\n<p>If you don’t have DSE you could potentially put the Opscenter data in a keyspace on nodes in a separate datacenter.  I haven’t tried that so YMMV.  For us we didn’t want to lose out on potentially valuable metrics so we sent the metrics to a separate Cassandra cluster.  The graph below of the cluster load average shows a pretty large drop when we essentially stop DDOSing our cluster with our own metrics.</p>\n<p><strong><strong> <img title=\"\" src=\"https://www.threatstack.com/wp-content/uploads/2017/08/Cassandra-graph.png\" width=\"642\" height=\"417\" alt=\"image\" /></strong></strong></p>\n<p><br />If you don’t have DSE, you can (and should)<a href=\"http://docs.datastax.com/en/opscenter/5.1/opsc/configure/opscExcludingKeyspaces_c.html\" target=\"_blank\" rel=\"noopener\"> disable collection on specific keyspaces and column families</a>.</p>\n<p>But at this point the value of opscenter goes down quite a bit when you start nerfing functionality and metrics you’re collecting.</p>\n<h3><strong>Metrics Collection </strong></h3>\n<p>A month after launch when we had cluster issues we quickly found that Opscenter was great when the cluster was running optimally, but when the cluster has issues your visibility goes down to zero. We are a happy Librato customer and soon starting collecting additional metrics using the <a href=\"https://github.com/sensu-plugins/sensu-plugins-cassandra/blob/master/bin/metrics-cassandra-graphite.rb\" target=\"_blank\" rel=\"noopener\">Sensu community plugin for Cassandra</a>.  It’s a very basic way to collect metrics by parsing nodetool status output, and didn’t require us messing around with the JMX console.</p>\n<p>Over the last few months we have built an internal high resolution metrics cluster and we wanted higher resolution metrics for Cassandra as well, so for that we moved metrics collection from Sensu over to collectd using the JMX plugin.  This has worked pretty well for us and <a href=\"https://github.com/signalfx/integrations/tree/master/collectd-cassandra\" target=\"_blank\" rel=\"noopener\">SignalFX has an example</a> of how you might set that up.</p>\n<p>Additionally, Cassandra does have a <a href=\"http://www.datastax.com/dev/blog/pluggable-metrics-reporting-in-cassandra-2-0-2\" target=\"_blank\" rel=\"noopener\">pluggable metrics reporting</a> since 2.0.0 – you can use that in order to send metrics to your local metrics cluster.  Assuming this works well we’ll likely be moving over to this to push metrics to graphite vs pulling them.</p>\n<h3><strong>Metrics that you should care about</strong></h3>\n<p>This is where the most learning about Cassandra really took place.  There are a TON of metrics available via JMX and if you can afford the storage of those metrics I’m a fan of capturing as many as possible at as high a resolution as is feasible for your environment.  </p>\n<p>You can use the JConsole in order to navigate around to see<a href=\"http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_monitoring_c.html\" target=\"_blank\" rel=\"noopener\"> what kind of metrics exist</a>. There is some more info on the Datastax site.</p>\n<p>Additionally there is some great info on the <a href=\"http://wiki.apache.org/cassandra/Metrics\" target=\"_blank\" rel=\"noopener\">Cassandra wiki </a>describing some of the metrics you can capture.</p>\n<p>This is <a href=\"https://lostechies.com/ryansvihla/2014/11/25/my-cassandra-diagnostics-checklist-brain-dump/\" target=\"_blank\" rel=\"noopener\">one blog post in particular</a> that was helpful for me in the beginning of our Cassandra usage.</p>\n<h2>3. AWS Instance Type and Disk Configuration</h2>\n<p>The original cluster we launched on in October 2014 was built on i2.2xlarge servers (8 vCPUs, 61GB of RAM, and 2 x 800GB SSDs). While the local SSDs were a treat, especially when you had to force a hard compaction or stream data, it wasn’t cost efficient.</p>\n<p>Earlier this spring AWS launched its D2 lines of instances, which were specifically targeted at our situation of needing a large amount of locally attached storage at a low price point, without sacrificing CPU and RAM density. We rebuilt the cluster in place by replacing i2.2xlarge’s with d2.2xlarge’s, giving us the same CPU and RAM but with 6 x 2TB spinning disks. This was a time intensive process as we waited for all the data to stream, but was effectively cheaper than running two whole clusters in parallel.</p>\n<p>The other impactful change that we made was dedicating a single spinning disk to the commit log. Every operation that touches the disk sped up as a result of this. The remaining 5 disks are striped together, giving us 10TB usable or 5TB effective disk space (leaving 50% of the disk for SSTable compaction). If we were to fill the effective disk space of 5TB then node replacement would likely take days or weeks, which is too long. As a result we are deciding whether or not to double the size of the cluster on d2.xlarge’s.</p>\n<h2>4. Drowning Your Cluster with Multi-Tenant Tables</h2>\n<p>The entire Threat Stack platform is multi tenant, including the storage. Our initial implementation of tenancy on Cassandra was at the table level, allowing for logical tracking of access and growth rates per customer.</p>\n<p>However, we found that handling more than a few hundred tables was outside of Cassandra’s capabilities. Compactions would regularly fail, the Datastax read repair service never completed, and the JVM was spending far too much time tracking state. This problem scaled rapidly as we added customers because the number of tables was equal to the number of customers multiplied by the number of table types.</p>\n<p>Luckily we already had agent IDs (sensors) in the partition key which have a one-to-one relationship to organization IDs (tenants), so it was trivial to collapse down to multi tenant tables.</p>\n<p>The other benefit was that we no longer had to dynamically create new tables. This was a painful and error prone process, which if performed incorrectly could crash Cassandra if too many duplicate CREATE TABLE statements were issued.</p>\n<h2>5. Streaming Failures, JVM Tuning, and GC Woes</h2>\n<p><strong><em>*obligatory disclaimer about JVM tuning and other dark arts</em></strong></p>\n<p>When you are under a state of high growth, adding nodes quickly becomes <em>critical.  </em>When nodes fail to stream the stress level goes up about the same rate as your total available disk goes down. As our node count grew and the amount of writes per second increased, we reached a point where a few default settings caused us to be unable to stream new nodes. Prior to this point new nodes would take about 10-15 hours to fully join the cluster. Mainly because of our (incorrect) usage of per-account column families we were streaming about 70,000 SSTables to new nodes.  Part of the streaming issue was related to that, but it was compounded by some of the default settings in our Cassandra cluster.</p>\n<p><strong>Your HEAP_NEW_SIZE is probably too low</strong></p>\n<p>There were old recommendations that said 100m per CPU up to 800m total.  For us, this was not enough as the real reason nodes were unable to fully complete bootstrap and stream from other nodes was because we were hitting long GC pauses that actually timed out the stream from one or two nodes.  The most painful part is when you have (for example) 10 or 15 stream sessions going, and ONE fails, then the whole thing failed and you need to start over again.  <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-8150\" target=\"_blank\" rel=\"noopener\">CASSANDRA-8150</a> is still one of the best tickets to review that has a bunch of additional JVM options as well, as well as newer tuning option for G1GC.</p>\n<p>You can also monitor for stream failures by grabbing the Stream Session ID in the cassandra system logs, then searching for that ID in all your logs.  If there is a failure, it will happen very quickly and you don’t have to wait a day for what will eventually be a failed bootstrapping/streaming session.</p>\n<p><strong>Increase memtable_flush_writers when you have lots of data to stream</strong></p>\n<p>The way it was described to us was that since we were streaming a lot of data from many nodes we wanted to increase the number of flush writers because the streams were all hitting the memtables. If you do not have enough writers to deal with a (larger than normal) amount of data hitting them that can cause your streams to fail.  The recommendation we were given was to set this equal to the number of CPUs on the node and it’s worked well for us.</p>\n<p><strong>streaming_socket_timeout_in_ms should be set to a non-zero number</strong></p>\n<p>Finally – we ran into this fun issues described in <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-8472\" target=\"_blank\" rel=\"noopener\">CASSANDRA-8472</a> where stream sessions would hang and not timeout. This was after we made the changes to the heap memory amount and we’re chalking this up to odd AWS networking across AZ’s.</p>\n<p>Luckily this issue is now fixed and upgrading will solve it, but if you haven’t yet (or can’t) upgrade, then set your streaming_socket_timeout_in_ms to some non-zero number.  We set this to 1 hour, which is now coincidentally the default in the now accepted patch in <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-8611\" target=\"_blank\" rel=\"noopener\">CASSANDRA-8611</a>.</p>\n<h2>6. Schema design, or These Aren’t the Rows You’re Looking For</h2>\n<p>The more applications we bring up on top of Cassandra, the more we find it being well suited for models that are either wide and shallow or narrow and deep. It is very similar to CouchDB’s incremental map/reduce in this way.</p>\n<p>A lot of Threat Stack’s UI requires building relationships of different types of events, which can be difficult when the underlying data model is an insert only, partially ordered, write ahead log. For example, when you view a process’s details we have to collect all of the syscall, network, TTY, login/logout, and FIM information for that single process invoke <strong>and</strong> all of its historical runs. Additionally, you could be entering that process’s context from any of those data points, not just the process start. Therefore we maintain multiple lookup tables, tracking a process’s state as it changes in milliseconds or over years (the longest running process we’ve seen is from 2011).</p>\n<p>Some examples of building wide and shallow models are sparse matrices. We have less examples of this in production today, instead driving specific widgets off pre materialized views, but plan to migrate some of the most popular lookups to this model.</p>\n<h2>7. Only Have 2 to 3 Seed Nodes per Data Center</h2>\n<p>Back in February we were seeing very odd behavior that was preventing a node from bootstrapping. This was a big concern because we were having to scale our cluster for disk utilization reasons, so time was against us.</p>\n<p>The short story is that we were hitting the bug documented in <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-5836\">CASSANDRA-5836</a>, where seed nodes were prevented from bootstrapping. The root cause for our cluster was that we were setting every node as a seed node, which is not the expected gossip configuration.</p>\n<p>We have since moved to one seed node per rack (AWS availability zone). This is technically one more than the Datastax recommended count, but gives us cleaner failure scenarios if an AZ becomes unavailable or splits from the rest of the cluster. Additionally, Datastax’s documentation has been improved to now let you know that you should not make every node a seed node.</p>\n<p>From their docs:</p>\n<p><em>Attention: In multiple data-center clusters, the seed list should include at least one node from each data center (replication group). More than a single seed node per data center is recommended for fault tolerance. Otherwise, gossip has to communicate with another data center when bootstrapping a node. Making every node a seed node is <strong class=\"ph b\">not</strong> recommended because of increased maintenance and reduced gossip performance. Gossip optimization is not critical, but it is recommended to use a small seed list (approximately three nodes per data center).</em></p>\n<p><a href=\"http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_add_node_to_cluster_t.html\">http://docs.datastax.com/en/cassandra/2.0/cassandra/operations/ops_add_node_to_cluster_t.html</a></p>\n<p>It might seem fine to make every node a seed node when you only have 3 nodes in a few different AZ’s, but if your configuration management is setup to make every node a seed node, and you add more nodes you’ll soon experience similar problems as us.  Our advice, take the extra time to configure specific seed nodes in your cluster, and when you add new nodes ensure they are not seeds.</p>\n<h2>8. Conclusion</h2>\n<p>Scaling any distributed database is always going to be a challenge, and Cassandra was no different.  Like most technologies, learning how things work is the first step towards making the right choices when it comes to increasing the overall footprint of your cluster.  Following basic system administration principals of making small changes over time, while monitoring the result of those changes can take you a long way toward scaling while maintaining availability and data integrity.</p>\n<p>We continue to ingest billions of unique events daily into our platform and Cassandra continues to be one of the most stable parts of our platform.  That is mainly because we’ve invested time and energy to learn from the community about operating it properly. If this kind of data scaling challenge sounds interesting to you, check out our <a href=\"https://threatstack.com/careers\" target=\"_blank\" rel=\"noopener\">jobs page</a>, or reach out to use directly to hear about current open positions.</p>\n<p><strong>Author’s note</strong>: <em>This post was co-authored by Pete Cheslock, Senior Director of Ops and Support at Threat Stack, who’s support was instrumental in getting this post put together.</em></p>",
        "created_at": "2018-06-20T18:45:25+0000",
        "updated_at": "2019-01-25T14:52:56+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 15,
        "domain_name": "www.threatstack.com",
        "preview_picture": "https://www.threatstack.com/wp-content/uploads/2015/09/2000px-Cassandra_logo.svg_.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10288"
          }
        }
      },
      {
        "is_archived": 0,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 1102,
            "label": "unprocessed",
            "slug": "unprocessed"
          },
          {
            "id": 1128,
            "label": "tutorials",
            "slug": "tutorials"
          },
          {
            "id": 1151,
            "label": "resources",
            "slug": "resources"
          }
        ],
        "is_public": true,
        "id": 10185,
        "uid": "5b5783dde4a2c3.74282858",
        "title": "DataStax Academy",
        "url": "https://academy.datastax.com/",
        "content": "<div class=\"three-column-gray col-md-4 center-block\"><img alt=\"\" src=\"https://academy.datastax.com/sites/all/themes/cassandra_complex/img/Computer-icon-new3.png\" /><p></p><h5>Free Online Learning\nTake our curated, self-paced online courses for deep learning in Apache Cassandra™ and DataStax Enterprise.<a class=\"btn btn-border-white\" href=\"https://academy.datastax.com/courses\">START A COURSE</a></h5></div><div class=\"three-column-gray col-md-4 center-block\"><img alt=\"\" src=\"https://academy.datastax.com/sites/all/themes/cassandra_complex/img/i-icon3.png\" /><p></p><h5>I'm on Fire, Help Me Quick\nSearch our collection of short (under 15 minutes) segments for quick solutions and discussion of specific issues.<a class=\"btn btn-border-white\" href=\"https://academy.datastax.com/find-solutions\">FIND A SOLUTION</a></h5></div><div class=\"three-column-gray col-md-4 center-block\"><img alt=\"\" src=\"https://academy.datastax.com/sites/all/themes/cassandra_complex/img/people-icon3.png\" /><p></p><h5>Join for free, Get Involved\nGet instant access to all of our learning resources, and join our community of DataStax developers.​<a class=\"btn btn-border-white\" href=\"https://academy.datastax.com/user/register\">CREATE AN ACCOUNT</a></h5></div>",
        "created_at": "2018-06-20T18:41:31+0000",
        "updated_at": "2018-07-24T19:54:05+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "academy.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/10185"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 36,
            "label": "solr",
            "slug": "solr"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 9956,
        "uid": null,
        "title": "Real-time Customer 360 (Matt Stump, Vorstella) | Cassandra Summit 2016",
        "url": "https://www.slideshare.net/DataStax/realtime-customer-360-matt-stump-vorstella-cassandra-summit-2016/",
        "content": "<p>No notes for slide</p>Constrast that to our solution and you'll immediately see that we're simpler. We'll dig into this in detail, but I wanted to provide you with some context. <p>Challenger Commercial Teaching Step = <br />#6 Your Solution – our solution is unique and better than anyone else </p>Constrast that to our solution and you'll immediately see that we're simpler. We'll dig into this in detail, but I wanted to provide you with some context. <p>Challenger Commercial Teaching Step = <br />#6 Your Solution – our solution is unique and better than anyone else </p>Let's dive into the use case I just mentioned, Customer 360. <p>To an application developer a customer is a collection of lots of different type of data. They've got orders, addresses, wish lists, connections to other users.  </p><p>Graph databases make it easy to model and understand these relationships so that I can develop a full picture of my users. By creating a Customer 360, I can understand more about my user and provide them with a better experience.  </p><p>You can provide better product recommendations, provide a better support experience, target them with more relevant advertisements, and gain a better picture of the total value they represent to my company.</p>",
        "created_at": "2018-06-18T15:11:28+0000",
        "updated_at": "2018-06-18T15:11:39+0000",
        "published_at": null,
        "published_by": [
          "DataStax"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/stumpcustomer360-cassandrasummit2016-161002202637-thumbnail-4.jpg?cb=1475510660",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9956"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 27,
            "label": "search",
            "slug": "search"
          },
          {
            "id": 36,
            "label": "solr",
            "slug": "solr"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 9955,
        "uid": null,
        "title": "DataStax | DSE Search 5.0 and Beyond (Nick Panahi & Ariel Weisberg) | Cassandra Summit 2016",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=jvxqhZRBf2E",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/jvxqhZRBf2E?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-06-18T15:09:10+0000",
        "updated_at": "2018-06-18T18:37:17+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/jvxqhZRBf2E/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9955"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9883,
        "uid": null,
        "title": "Understanding Cassandra tombstones – Beyond the lines",
        "url": "https://www.beyondthelines.net/databases/cassandra-tombstones/",
        "content": "<div id=\"ssba-classic-2\" class=\"ssba ssbp-wrap left ssbp--theme-1\"><div><a data-site=\"\" class=\"ssba_twitter_share\" href=\"http://twitter.com/share?url=https://www.beyondthelines.net/databases/cassandra-tombstones/&amp;text=Understanding%20Cassandra%20tombstones%20\" target=\"&quot;_blank&quot;\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/twitter.png\" title=\"Twitter\" class=\"ssba ssba-img\" alt=\"Tweet about this on Twitter\" /><div title=\"Twitter\" class=\"ssbp-text\">Twitter</div></a><a data-site=\"linkedin\" class=\"ssba_linkedin_share ssba_share_link\" href=\"http://www.linkedin.com/shareArticle?mini=true&amp;url=https://www.beyondthelines.net/databases/cassandra-tombstones/\" target=\"&quot;_blank&quot;\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/linkedin.png\" title=\"LinkedIn\" class=\"ssba ssba-img\" alt=\"Share on LinkedIn\" /><div title=\"Linkedin\" class=\"ssbp-text\">Linkedin</div></a><a data-site=\"\" class=\"ssba_facebook_share\" href=\"http://www.facebook.com/sharer.php?u=https://www.beyondthelines.net/databases/cassandra-tombstones/\" target=\"_blank\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/facebook.png\" title=\"Facebook\" class=\"ssba ssba-img\" alt=\"Share on Facebook\" /><div title=\"Facebook\" class=\"ssbp-text\">Facebook</div></a><a data-site=\"\" class=\"ssba_google_share\" href=\"https://plus.google.com/share?url=https://www.beyondthelines.net/databases/cassandra-tombstones/\" target=\"&quot;_blank&quot;\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/google.png\" title=\"Google+\" class=\"ssba ssba-img\" alt=\"Share on Google+\" /><div title=\"Google+\" class=\"ssbp-text\">Google+</div></a><a data-site=\"reddit\" class=\"ssba_reddit_share\" href=\"http://reddit.com/submit?url=https://www.beyondthelines.net/databases/cassandra-tombstones/&amp;title=Understanding Cassandra tombstones\" target=\"&quot;_blank&quot;\"><img src=\"https://www.beyondthelines.net/wp-content/plugins/simple-share-buttons-adder/buttons/somacro/reddit.png\" title=\"Reddit\" class=\"ssba ssba-img\" alt=\"Share on Reddit\" /><div title=\"Reddit\" class=\"ssbp-text\">Reddit</div></a></div></div><p>We recently deployed in production a distributed system that uses Cassandra as its persistent storage.</p>\n<p>Not long after we noticed that there were many warnings about tombstones in Cassandra logs. </p>\n<pre class=\"brush: plain; title: ; notranslate\" title=\"\">&#13;\nWARN  [SharedPool-Worker-2] 2017-01-20 16:14:45,153 ReadCommand.java:508 - &#13;\nRead 5000 live rows and 4771 tombstone cells for query &#13;\nSELECT * FROM warehouse.locations WHERE token(address) &gt;= token(D3-DJ-21-B-02) LIMIT 5000 &#13;\n(see tombstone_warn_threshold)&#13;\n</pre>\n<p>We found it quite surprising at first because we’ve only inserted data so far and didn’t expect to see that many tombstones in our database. After asking some people around no one seemed to have a clear explanation on what was going on in Cassandra.</p>\n<p>In fact, the main misconception about tombstones is that people associate it with delete operations. While it’s true that tombstones are generated when data is deleted it is not the only case as we shall see.</p>\n<h4>Looking into sstables</h4>\n<p>Cassandra provides a tool to look at what is stored inside an sstable: sstabledump. This tool comes with the ‘casssandra-tools’ package which is not automatically installed with Cassandra. It’s quite straight-forward to install on a delian-like (e.g. unbuntu) distribution:</p>\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">&#13;\nsudo apt-get update&#13;\nsudo apt-get install cassandra-tools&#13;\n</pre>\n<p>In this blog post I used ssltabledump to understand how Cassandra stores data and when tombstones are generated.<br />\nThe syntax is pretty straightforward:</p>\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">&#13;\nsstabledump /var/lib/cassandra/data/warehouse/locations-660dbcb0e4a211e6814a9116fc548b6b/mc-1-big-Data.db&#13;\n</pre>\n<p>sstabledump just takes the sstable file and displays its content as json. Before being able to dump an sstable we need to flush the in-memory data into an sstable file using nodetool:</p>\n<pre class=\"brush: bash; title: ; notranslate\" title=\"\">&#13;\nnodetool flush warehouse locations&#13;\n</pre>\n<p>This command flushes the table ‘locations’ in the ‘warehouse’ keyspace.</p>\n<p>Now that we’re all setup, let’s have a look at some cases that generate tombstones.</p>\n<h4>Null values creates tombstones</h4>\n<p>An upsert operation can generate a tombstone as well. Why? Because Cassandra doesn’t store ‘null’ values. Null means the absence of data. Cassandra returns a ‘null’ value when there is no value for a field. Therefore when a field is set to null Cassandra needs to delete the existing data.</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nINSERT INTO movements (&#13;\n  id,&#13;\n  address, &#13;\n  item_id, &#13;\n  quantity,&#13;\n  username&#13;\n) VALUES (&#13;\n  103,&#13;\n  'D3-DJ-21-B-02', &#13;\n  '3600029145',&#13;\n  2,&#13;\n  null&#13;\n);&#13;\n</pre>\n<p>This statements removes any existing username value for the movement identified by the id 103. And how does Cassandra remove data ? Yes, by inserting a tombstone.</p>\n<p><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/movement-tombstone-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/movement-tombstone-1.png\" alt=\"\" width=\"800\" height=\"149\" class=\"aligncenter size-full wp-image-858\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-tombstone-1.png 800w, https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-tombstone-1-300x56.png 300w, https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-tombstone-1-768x143.png 768w\" /></a></p>\n<p>This is the corresponding ssltabledump output:</p>\n<pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n[&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"103\" ],&#13;\n      \"position\" : 0&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 18,&#13;\n        \"liveness_info\" : { \"tstamp\" : \"2017-01-27T15:09:50.065224Z\" },&#13;\n        \"cells\" : [&#13;\n          { \"name\" : \"address\", \"value\" : \"D3-DJ-21-B-02\" },&#13;\n          { \"name\" : \"item_d\", \"value\" : \"3600029145\" },&#13;\n          { \"name\" : \"quantity\", \"value\" : \"2\" },&#13;\n          { \"name\" : \"username\", \"deletion_info\" : { \"local_delete_time\" : \"2017-01-27T15:09:50Z\" }&#13;\n        ]&#13;\n      }&#13;\n    ]&#13;\n  }&#13;\n]&#13;\n</pre>\n<p>Cassandra is designed for optimised performance and every operation is written to an append-only log. When a data is removed we can’t removed the existing value from the log, instead a “tombstone” value is inserted in the log. </p>\n<p>Moreover Cassandra doesn’t perform read before write (except for light-weight transactions) as it would be too expensive.</p>\n<p>Therefore when the above insert is executed Cassandra insert a tombstone value for the username field (even if there was no existing data for this key before).</p>\n<p>Now let’s consider the following statement that looks very similar to the previous one:</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nINSERT INTO movements (&#13;\n  id,&#13;\n  address, &#13;\n  item_id, &#13;\n  quantity&#13;\n) VALUES (&#13;\n  103,&#13;\n  'D3-DJ-21-B-02', &#13;\n  '3600029145',&#13;\n  2&#13;\n);&#13;\n</pre>\n<p>But there is one difference. The first statement creates a tombstone for the username whereas the second statement doesn’t insert anything in that column.</p>\n<p><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/movement-row-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/movement-row-1.png\" alt=\"\" width=\"700\" height=\"195\" class=\"aligncenter size-full wp-image-857\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-row-1.png 700w, https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-row-1-300x84.png 300w\" /></a></p>\n<pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n[&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"103\" ],&#13;\n      \"position\" : 0&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 18,&#13;\n        \"liveness_info\" : { \"tstamp\" : \"2017-01-27T15:09:50.065224Z\" },&#13;\n        \"cells\" : [&#13;\n          { \"name\" : \"address\", \"value\" : \"D3-DJ-21-B-02\" },&#13;\n          { \"name\" : \"item_d\", \"value\" : \"3600029145\" },&#13;\n          { \"name\" : \"quantity\", \"value\" : \"2\" }&#13;\n        ]&#13;\n      }&#13;\n    ]&#13;\n  }&#13;\n]&#13;\n</pre>\n<p>If this is the first insert for this key (no previously existing data) then both statements yield to the same state, except that the second one doesn’t insert an unnecessary tombstone.</p>\n<p>If there is existing data then what ends up in the username column might be different. With statement 1 whatever data was there it is deleted with the tombstone and no longer returned. With statement 2 the username remains unchanged so whatever value was there before (if any) will get returned.</p>\n<p><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/movement-upsert-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/movement-upsert-1.png\" alt=\"\" width=\"700\" height=\"245\" class=\"aligncenter size-full wp-image-859\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-upsert-1.png 700w, https://www.beyondthelines.net/wp-content/uploads/2017/01/movement-upsert-1-300x105.png 300w\" /></a></p>\n<p>Therefore you should strive to only update the fields that you need to. </p>\n<p>For instance let’s say that I need to update the status of a location. Then I should only update the status field rather than the whole object. That would avoid tombstones for every missing value in the object.</p>\n<figure id=\"attachment_855\" class=\"wp-caption aligncenter\"><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/location-row-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/location-row-1.png\" alt=\"\" width=\"450\" height=\"190\" class=\"size-full wp-image-855\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/location-row-1.png 450w, https://www.beyondthelines.net/wp-content/uploads/2017/01/location-row-1-300x127.png 300w\" /></a><figcaption class=\"wp-caption-text\">The ‘properties’ field is not set in the query so no value is stored in Cassandra.</figcaption></figure><p>The following statement is exactly what we need as it only sets the status field:</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nUPDATE locations SET status = 'damaged' WHERE location_address = 'D3-DJ-21-B-02';&#13;\n</pre>\n<p>as the sstable dump shows</p>\n<pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n[&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"D3-DJ-21-B-02\" ],&#13;\n      \"position\" : 0&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 28,&#13;\n        \"cells\" : [&#13;\n          { \"name\" : \"status\", \"value\" : \"damaged\", \"tstamp\" : \"2017-01-28T11:55:18.146255Z\" }&#13;\n        ]&#13;\n      }&#13;\n    ]&#13;\n  }&#13;\n]&#13;\n</pre>\n<p>Compare it with the following one which saves the whole location object (which happens to not have any properties – and insert an unnecessary tombstone in the ‘properties’ column).</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nINSERT INTO locations (&#13;\n  address,&#13;\n  status,&#13;\n  properties&#13;\n) VALUES (&#13;\n  'D3-DJ-21-B-02',&#13;\n  'damaged',&#13;\n  null&#13;\n);&#13;\n</pre>\n<figure id=\"attachment_854\" class=\"wp-caption aligncenter\"><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/location-empty-collection-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/location-empty-collection-1.png\" alt=\"\" width=\"500\" height=\"189\" class=\"size-full wp-image-854\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/location-empty-collection-1.png 500w, https://www.beyondthelines.net/wp-content/uploads/2017/01/location-empty-collection-1-300x113.png 300w\" /></a><figcaption class=\"wp-caption-text\">An empty collection is stored as a tombstone cell</figcaption></figure><pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n[&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"D3-DJ-21-B-02\" ],&#13;\n      \"position\" : 0&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 18,&#13;\n        \"liveness_info\" : { \"tstamp\" : \"2017-01-28T11:58:59.160898Z\" },&#13;\n        \"cells\" : [&#13;\n          { \"name\" : \"status\", \"value\" : \"damaged\" },&#13;\n          { \"name\" : \"properties\", \"deletion_info\" : { \"marked_deleted\" : \"2017-01-28T11:58:59.160897Z\", \"local_delete_time\" : \"2017-01-28T11:58:59Z\" } }&#13;\n        ]&#13;\n      }&#13;\n    ]&#13;\n  }&#13;\n]&#13;\n</pre>\n<h4>Be aware of the collection types</h4>\n<p>In the previous example the ‘properties’ field is a collection type (most likely a set), so let’s talk about collections as they are trickier than it looks.</p>\n<p>Let’s create a new location with the following statement</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nINSERT INTO locations (&#13;\n  address,&#13;\n  status,&#13;\n  properties&#13;\n) VALUES (&#13;\n  'C3-BE-52-C-01',&#13;\n  'normal',&#13;\n  {'pickable'}&#13;\n);&#13;\n</pre>\n<p>Everything looks good, doesn’t it? Every field has a value, so no tombstone expected. And yet, this statement does create a tombstone for the ‘properties’ field.</p>\n<pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n[&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"C3-BE-52-C-01\" ],&#13;\n      \"position\" : 0&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 18,&#13;\n        \"liveness_info\" : { \"tstamp\" : \"2017-01-28T12:01:00.256789Z\" },&#13;\n        \"cells\" : [&#13;\n          { \"name\" : \"status\", \"value\" : \"normal\" },&#13;\n          { \"name\" : \"properties\", \"deletion_info\" : { \"marked_deleted\" : \"2017-01-28T12:01:00.256788Z\", \"local_delete_time\" : \"2017-01-28T12:01:00Z\" } },&#13;\n          { \"name\" : \"properties\", \"path\" : [ \"pickable\" ], \"value\" : \"\" }&#13;\n        ]&#13;\n      }&#13;\n    ]&#13;\n  }&#13;\n]&#13;\n</pre>\n<p>To understand why, we need to look at how Cassandra store a collection in the underlying storage.</p>\n<figure id=\"attachment_853\" class=\"wp-caption aligncenter\"><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/location-collection-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/location-collection-1.png\" alt=\"\" width=\"500\" height=\"175\" class=\"size-full wp-image-853\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/location-collection-1.png 500w, https://www.beyondthelines.net/wp-content/uploads/2017/01/location-collection-1-300x105.png 300w\" /></a><figcaption class=\"wp-caption-text\">The collection field includes a tombstone cell to empty the collection before adding a value.</figcaption></figure><p>Cassandra appends new values to the set, so when we want the collection to contain only the values passed in the query, we have to remove everything that might have been there before. That’s why Cassandra inserts a tombstone and then our value. This makes sure the set now contains only the ‘pickable’ value whatever was there before.</p>\n<p>That’s one more reason to just set the values you need to update and nothing more.</p>\n<h4>Be careful with materialised views</h4>\n<p>A materialised view is a table that is maintained by Cassandra. One of its main feature is that we can define a different primary key than the one in the base table. You can re-order the fields of the primary key from the base table, but you can also add one extra field into the primary key of the view.</p>\n<p>This is great as it allows to define a different partitioning or clustering but it also generates more tombstones in the view. Let’s consider an example to understand what’s happening.</p>\n<p>Imagine that we need to query the locations by status. For instance we want to retrieve all ‘damaged’ locations. We can create a materialised view to support this use case.</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nCREATE MATERIALIZED VIEW locations_by_status AS&#13;\n  SELECT&#13;\n    status,&#13;\n    address,&#13;\n    properties&#13;\n  FROM locations&#13;\n  WHERE status IS NOT NULL&#13;\n  AND address IS NOT NULL&#13;\n  PRIMARY KEY (status, address);&#13;\n</pre>\n<p>Good, now we can use this view to find out all the locations with a given status. </p>\n<p>But let’s consider what happens in the view when we change the status of a location with an update query</p>\n<pre class=\"brush: sql; title: ; notranslate\" title=\"\">&#13;\nUPDATE locations&#13;\nSET status = 'damaged'&#13;\nWHERE address = 'C3-BE-52-C-01';&#13;\n</pre>\n<p>As we’ve seen this query just updates one field in the base table (locations) and doesn’t generate any tombstone in this table. However in the materialised view the ‘status’ field is part of the primary key. When the status changes the partition key changes as well.</p>\n<pre class=\"brush: jscript; title: ; notranslate\" title=\"\">&#13;\n[&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"normal\" ],&#13;\n      \"position\" : 0&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 18,&#13;\n        \"clustering\" : [ \"C3-BE-52-C-01\" ],&#13;\n        \"deletion_info\" : { \"marked_deleted\" : \"2017-01-20T10:34:27.707604Z\", \"local_delete_time\" : \"2017-01-20T10:46:14Z\" },&#13;\n        \"cells\" : [ ]&#13;\n      }&#13;\n    ]&#13;\n  },&#13;\n  {&#13;\n    \"partition\" : {&#13;\n      \"key\" : [ \"damaged\" ],&#13;\n      \"position\" : 31&#13;\n    },&#13;\n    \"rows\" : [&#13;\n      {&#13;\n        \"type\" : \"row\",&#13;\n        \"position\" : 49,&#13;\n        \"clustering\" : [ \"C3-BE-52-C-01\" ],&#13;\n        \"liveness_info\" : { \"tstamp\" : \"2017-01-20T10:46:14.285730Z\" },&#13;\n        \"cells\" : [&#13;\n          { \"name\" : \"properties\", \"deletion_info\" : { \"marked_deleted\" : \"2017-01-20T10:46:14.285729Z\", \"local_delete_time\" : \"2017-01-20T10:46:14Z\" } }&#13;\n        ]&#13;\n      }&#13;\n    ]&#13;\n  }&#13;\n]&#13;\n</pre>\n<p>To maintain the view in sync with the base table Cassandra needs to delete the row from the existing partition and insert a new one into the new partition. And a delete means a tombstone.</p>\n<figure id=\"attachment_856\" class=\"wp-caption aligncenter\"><a href=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/materialized-view-tombstones-1.png\"><img src=\"http://www.beyondthelines.net/wp-content/uploads/2017/01/materialized-view-tombstones-1.png\" alt=\"\" width=\"800\" height=\"398\" class=\"size-full wp-image-856\" srcset=\"https://www.beyondthelines.net/wp-content/uploads/2017/01/materialized-view-tombstones-1.png 800w, https://www.beyondthelines.net/wp-content/uploads/2017/01/materialized-view-tombstones-1-300x149.png 300w, https://www.beyondthelines.net/wp-content/uploads/2017/01/materialized-view-tombstones-1-768x382.png 768w\" /></a><figcaption class=\"wp-caption-text\">The update in the base table triggers a partition change in the materialised view which creates a tombstone to remove the row from the old partition.</figcaption></figure><p>The key thing here is to be thoughtful when designing the primary key of a materialised view (especially when the key contains more fields than the key of the base table). That being said it might be the only solution and completely worth it. </p>\n<p>Also consider the rate of the changes of the fields of the primary key. In our case we should evaluate the rate at which the status changes for a given location (the location address doesn’t change). The less often the better off we are with respect to tombstones.</p>\n<p>Finally tombstones will disappear over time, when compaction occurs. The typical delay is 10 days (which corresponds to the ‘gc_grace_seconds’ configuration parameter). You may want to adjust it if there are too many tombstones generated during this period.</p>\n<h4>Conclusion</h4>\n<p>As we’ve seen tombstones can be tricky and there not only associated to delete operations. There are many other cases that may generate tombstones. </p>\n<p>Tombstones are not necessarily a bad thing that we should avoid at all cost. It’s just a way to delete data in an append-only structure. </p>\n<p>However it can affect performances so you’d better be aware when they are generated when designing your data model and queries. </p>\n<p>If you use the java driver the query fails if more than 100,000 tombstones are seen.</p>\n<p>With this knowledge you should be able to limit their generation only when necessary. In this case you should evaluate how many are going to be generated and evaluate if it’s going to be an issue or not. If so you may want to tune the ‘gc_grace_seconds’ parameter to trigger compaction more often.</p>",
        "created_at": "2018-06-08T15:27:36+0000",
        "updated_at": "2018-06-08T15:27:49+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_GB",
        "reading_time": 10,
        "domain_name": "www.beyondthelines.net",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9883"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9860,
        "uid": null,
        "title": "datastax/docker-images",
        "url": "https://github.com/datastax/docker-images",
        "content": "<ul><li><a href=\"#datastax-platform-overview\">DataStax Platform Overview</a></li>\n<li><a href=\"#getting-started-with-datastax-and-docker\">Getting Started with DataStax and Docker</a></li>\n<li><a href=\"#prerequisites\">Prerequisites</a></li>\n<li><a href=\"https://github.com/datastax/docker-images/blob/master/next-steps\">Next Steps</a></li>\n<li><a href=\"#building\">Building</a></li>\n<li><a href=\"#quick-reference\">Quick Reference</a></li>\n<li><a href=\"#license\">Licensing</a></li>\n</ul><h3>Where to get help:</h3><p><a href=\"https://academy.datastax.com/\" rel=\"nofollow\">DataStax Academy</a>, <a href=\"https://academy.datastax.com/slack\" rel=\"nofollow\">DataStax Slack</a></p><p>For documentation and tutorials head over to <a href=\"https://academy.datastax.com/quick-downloads?utm_campaign=Docker_2019&amp;utm_medium=web&amp;utm_source=docker&amp;utm_term=-&amp;utm_content=Web_Academy_Downloads\" rel=\"nofollow\">DataStax Academy</a>. On Academy you’ll find everything you need to configure and deploy the DataStax Docker Images.</p><p>Featured Tutorial - <a href=\"https://academy.datastax.com/resources/guided-tour-dse-6-using-docker\" rel=\"nofollow\">DataStax Enterprise 6 Guided Tour</a></p><p>Built on the best distribution of Apache Cassandra™, DataStax Enterprise is the always-on database designed to allow you to effortlessly build and scale your apps, integrating graph, search, analytics, administration, developer tooling, and monitoring into a single unified platform. We power your apps' real-time moments so you can create instant insights and powerful customer experiences.</p><p><a target=\"_blank\" href=\"https://camo.githubusercontent.com/a33c8b4c94fd9610153f34005f50d1a204ffdad1/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f652f65352f44617461537461785f4c6f676f2e706e67\"><img src=\"https://camo.githubusercontent.com/a33c8b4c94fd9610153f34005f50d1a204ffdad1/68747470733a2f2f75706c6f61642e77696b696d656469612e6f72672f77696b6970656469612f636f6d6d6f6e732f652f65352f44617461537461785f4c6f676f2e706e67\" alt=\"\" data-canonical-src=\"https://upload.wikimedia.org/wikipedia/commons/e/e5/DataStax_Logo.png\" /></a></p><p>DataStax Docker images are licensed only for Development purposes in non-production environments. You can use these images to learn <a href=\"https://hub.docker.com/r/datastax/dse-server\" rel=\"nofollow\">DSE</a>, <a href=\"https://hub.docker.com/r/datastax/dse-opscenter\" rel=\"nofollow\">OpsCenter</a> and <a href=\"https://hub.docker.com/r/datastax/dse-studio\" rel=\"nofollow\">DataStax Studio</a>, to try new ideas, to test and demonstrate your application.</p><ul><li>\n<p>Basic understanding of Docker images and containers.</p>\n</li>\n<li>\n<p>Docker installed on your local system, see <a href=\"https://docs.docker.com/engine/installation/\" rel=\"nofollow\">Docker Installation Instructions</a>.</p>\n</li>\n<li>\n<p>When <a href=\"#building\">building</a> custom images from the DataStax github repository, a <a href=\"https://academy.datastax.com/\" rel=\"nofollow\">DataStax Academy account</a>.</p>\n</li>\n</ul><p>For documentation including configuration options, environment variables, and compose examples head over to <a href=\"https://academy.datastax.com/quick-downloads?utm_campaign=Docker_2019&amp;utm_medium=web&amp;utm_source=docker&amp;utm_term=-&amp;utm_content=Web_Academy_Downloads\" rel=\"nofollow\">DataStax Academy</a>.</p><p>On Academy you’ll also find step by step tutorials and examples.</p><p>The code in this repository will build the images listed above. To build all of them please run the following commands:</p><div class=\"highlight highlight-text-shell-session\"><pre>./gradlew buildImages -PdownloadUsername=&lt;your_DataStax_Acedemy_username&gt; -PdownloadPassword=&lt;your_DataStax_Acedemy_passwd&gt;</pre></div><p>By default, <a href=\"https://gradle.org\" rel=\"nofollow\">Gradle</a> will download DataStax tarballs from <a href=\"https://downloads.datastax.com\" rel=\"nofollow\">DataStax Academy</a>.\nTherefore you need to provide your credentials either via the command line, or in <code>gradle.properties</code> file located\nin the project root.</p><p>Run <code>./gradlew tasks</code> to get the list of all available tasks.</p><p>Use the following links to review the license:</p><ul><li><a href=\"https://www.datastax.com/terms\" rel=\"nofollow\">DataStax License Terms</a></li>\n</ul>",
        "created_at": "2018-05-31T23:24:59+0000",
        "updated_at": "2018-05-31T23:25:05+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/573369?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9860"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 35,
            "label": "docker",
            "slug": "docker"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9814,
        "uid": null,
        "title": "thelastpickle/docker-cassandra-bootstrap",
        "url": "https://github.com/thelastpickle/docker-cassandra-bootstrap",
        "content": "<p>A new blog post covering each of the main components of this project can be found here:</p><p><a href=\"http://thelastpickle.com/blog/2018/01/23/docker-meet-cassandra.html\" rel=\"nofollow\">http://thelastpickle.com/blog/2018/01/23/docker-meet-cassandra.html</a></p><div class=\"highlight highlight-source-shell\"><pre>git clone git@github.com:thelastpickle/docker-cassandra-bootstrap.git\ncd docker-cassandra-bootstrap\ncp .env.template .env\ndocker-compose build</pre></div><p>If you would like to see a hosted log service interact seemlessly with this\nDocker Compose stack, sign up for <a href=\"https://papertrailapp.com/?thank=1ad15b\" rel=\"nofollow\">Papertrail</a>.</p><p>Then find your specific port number by looking at your\n<a href=\"https://papertrailapp.com/account/destinations\" rel=\"nofollow\">Log Destinations</a> and update\nyour <code>.env</code> setting accordingly.</p><div class=\"highlight highlight-source-shell\"><pre># turn off all running Docker containers\ndocker-compose down\n# delete any persistent data\nrm -rf data/\n# rebuild the images\ndocker-compose build</pre></div><p>Start our Docker-integrated logging connector:</p><div class=\"highlight highlight-source-shell\"><pre># start Docker logging connector\ndocker-compose up logspout\n# view logging HTTP endpoint\ncurl http://localhost:8000/logs</pre></div><p>Start Cassandra and setup the required schema:</p><div class=\"highlight highlight-source-shell\"><pre># start Cassandra\ndocker-compose up cassandra\n# view cluster status\ndocker-compose run nodetool status\n# create schema\ndocker-compose run cqlsh -f /schema.cql\n# confirm schema\ndocker-compose run cqlsh -e \"DESCRIBE SCHEMA;\"</pre></div><p>Start Reaper for Apache Cassandra and monitor your new cluster:</p><div class=\"highlight highlight-source-shell\"><pre># start Reaper for Apache Cassandra\ndocker-compose up cassandra-reaper\nopen http://localhost:8080/webui/\n# add one-off repair\n# add scheduled repair</pre></div><p>Start Prometheus and become familiar with the UI:</p><div class=\"highlight highlight-source-shell\"><pre># start Prometheus\ndocker-compose up prometheus\nopen http://localhost:9090</pre></div><p>Start Grafana, connect it to the Prometheus data source, and upload the TLP\nDashboards.</p><div class=\"highlight highlight-source-shell\"><pre># start Grafana\ndocker-compose up grafana\n# create \n./grafana/bin/create-data-sources.sh\n# user/pass: admin/admin\nopen http://localhost:3000\n# upload dashboards\n./grafana/bin/upload-dashboards.sh</pre></div><p>Generate fake workforce and activity:</p><div class=\"highlight highlight-source-shell\"><pre>docker-compose run pickle-factory</pre></div><p>Sample timesheets:</p><div class=\"highlight highlight-source-shell\"><pre>docker-compose run pickle-shop</pre></div>",
        "created_at": "2018-05-25T21:15:40+0000",
        "updated_at": "2018-08-03T00:01:48+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/30403496?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9814"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 907,
            "label": "mesos",
            "slug": "mesos"
          },
          {
            "id": 956,
            "label": "streaming",
            "slug": "streaming"
          }
        ],
        "is_public": false,
        "id": 9790,
        "uid": null,
        "title": "Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos …",
        "url": "https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1",
        "content": "Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos …\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-1-638.jpg?cb=1475599971\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-1-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-1-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-1-1024.jpg?cb=1475599971\" alt=\"Rahul Kumar&#10;Technical Lead&#10;Sigmoid&#10;Real Time data pipeline with Spark Streaming and&#10;Cassandra with Mesos&#10;\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-2-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-2-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-2-1024.jpg?cb=1475599971\" alt=\"About Sigmoid&#10;© DataStax, All Rights Reserved. 2&#10;We build reactive real-time big data systems.&#10;\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-3-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-3-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-3-1024.jpg?cb=1475599971\" alt=\"1 Data Management&#10;2 Cassandra Introduction&#10;3 Apache Spark Streaming&#10;4 Reactive Data Pipelines&#10;5 Use cases&#10;3© DataStax, All...\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-4-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-4-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-4-1024.jpg?cb=1475599971\" alt=\"Data Management&#10;© DataStax, All Rights Reserved. 4&#10;Managing data and analyzing&#10;data have always greatest&#10;benefit and the g...\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-5-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-5-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-5-1024.jpg?cb=1475599971\" alt=\"Three V’s of Big data&#10;© DataStax, All Rights Reserved. 5&#10;\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-6-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-6-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-6-1024.jpg?cb=1475599971\" alt=\"Scale Vertically&#10;© DataStax, All Rights Reserved. 6&#10;\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-7-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-7-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-7-1024.jpg?cb=1475599971\" alt=\"Scale Horizontally&#10;© DataStax, All Rights Reserved. 7&#10;\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-8-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-8-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-8-1024.jpg?cb=1475599971\" alt=\"Understanding Distributed Application&#10;© DataStax, All Rights Reserved. 8&#10;“ A distributed system is a software system in wh...\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-9-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-9-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-9-1024.jpg?cb=1475599971\" alt=\"Principles Of Distributed Application Design&#10;© DataStax, All Rights Reserved. 9&#10; Availability&#10; Performance&#10; Reliability...\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-10-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-10-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-10-1024.jpg?cb=1475599971\" alt=\"Reactive Application&#10;© DataStax, All Rights Reserved. 10&#10;\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-11-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-11-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-11-1024.jpg?cb=1475599971\" alt=\"Reactive libraries, tools and frameworks&#10;© DataStax, All Rights Reserved. 11&#10;\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-12-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-12-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-12-1024.jpg?cb=1475599971\" alt=\"Cassandra Introduction&#10;© DataStax, All Rights Reserved. 13&#10;Cassandra - is an Open Source, distributed store for structured...\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-13-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-13-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-13-1024.jpg?cb=1475599971\" alt=\"Why Cassandra&#10;© DataStax, All Rights Reserved. 14&#10;\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-14-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-14-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-14-1024.jpg?cb=1475599971\" alt=\"Highly scalable NoSQL database&#10;© DataStax, All Rights Reserved. 15&#10; Cassandra supplies linear&#10;scalability&#10; Cassandra is ...\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-15-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-15-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-15-1024.jpg?cb=1475599971\" alt=\"High Availability&#10;© DataStax, All Rights Reserved. 16&#10; In a Cassandra cluster all&#10;nodes are equal.&#10; There are no masters...\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-16-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-16-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-16-1024.jpg?cb=1475599971\" alt=\"Read/Write any where&#10;© DataStax, All Rights Reserved. 17&#10; Cassandra is a R/W&#10;anywhere architecture, so&#10;any user/app can c...\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-17-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-17-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-17-1024.jpg?cb=1475599971\" alt=\"High Performance&#10;© DataStax, All Rights Reserved. 18&#10; All disk writes are&#10;sequential, append-only&#10;operations.&#10; Ensure No...\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-18-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-18-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-18-1024.jpg?cb=1475599971\" alt=\"Cassandra &amp; CAP&#10;© DataStax, All Rights Reserved. 19&#10; Cassandra is classified as&#10;an AP system&#10; System is still available&#10;...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-19-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-19-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-19-1024.jpg?cb=1475599971\" alt=\"CQL&#10;© DataStax, All Rights Reserved. 20&#10;CREATE KEYSPACE MyAppSpace WITH&#10;REPLICATION = { 'class' : 'SimpleStrategy', 'repli...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-20-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-20-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-20-1024.jpg?cb=1475599971\" alt=\"Apache Spark&#10;© DataStax, All Rights Reserved. 21&#10;Introduction&#10; Apache Spark is a fast and&#10;general execution engine&#10;for la...\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-21-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-21-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-21-1024.jpg?cb=1475599971\" alt=\"RDD Introduction&#10;© DataStax, All Rights Reserved. 22&#10;Resilient Distributed Datasets (RDDs), a distributed memory&#10;abstracti...\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-22-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-22-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-22-1024.jpg?cb=1475599971\" alt=\"RDD Operations&#10;© DataStax, All Rights Reserved. 23&#10;Two Kind of Operations&#10;• Transformation&#10;• Action&#10;\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-23-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-23-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-23-1024.jpg?cb=1475599971\" alt=\"What is Spark Streaming?&#10;© DataStax, All Rights Reserved. 26&#10;Framework for large scale stream processing&#10;➔ Created at UC B...\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-24-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-24-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-24-1024.jpg?cb=1475599971\" alt=\"Spark Streaming&#10;© DataStax, All Rights Reserved. 27&#10;Introduction&#10;• Spark Streaming is an&#10;extension of the core spark&#10;API t...\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-25-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-25-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-25-1024.jpg?cb=1475599971\" alt=\"Spark Streaming over a HA Mesos Cluster&#10;© DataStax, All Rights Reserved. 31&#10;To use Mesos from Spark, you need a Spark bina...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-26-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-26-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-26-1024.jpg?cb=1475599971\" alt=\"Spark Cassandra Connector&#10;© DataStax, All Rights Reserved. 32&#10; It allows us to expose Cassandra tables as Spark RDDs&#10; Wr...\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-27-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-27-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-27-1024.jpg?cb=1475599971\" alt=\"© DataStax, All Rights Reserved. 33&#10;resolvers += &quot;Spark Packages Repo&quot; at &quot;https://dl.bintray.com/spark-packages/maven&quot;&#10;li...\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-28-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-28-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-28-1024.jpg?cb=1475599971\" alt=\"© DataStax, All Rights Reserved. 34&#10;val rdd = sc.cassandraTable(“applog”, “accessTable”)&#10;println(rdd.count)&#10;println(rdd.fi...\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-29-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-29-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-29-1024.jpg?cb=1475599971\" alt=\"Many more higher order functions:&#10;© DataStax, All Rights Reserved. 35&#10;repartitionByCassandraReplica : It be used to reloca...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-30-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-30-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-30-1024.jpg?cb=1475599971\" alt=\"Hint to scalable pipeline&#10;© DataStax, All Rights Reserved. 36&#10;Figure out the bottleneck : CPU, Memory, IO, Network&#10;If pars...\" /></i></section><section data-index=\"31\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-31-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-31-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-31-1024.jpg?cb=1475599971\" alt=\"Thank You&#10;@rahul_kumar_aws&#10;\" /></i></section><section data-index=\"32\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-32-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-32-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-32-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"33\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-33-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-33-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-33-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"34\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-34-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-34-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-34-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"35\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-35-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-35-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-35-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"36\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-36-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-36-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-36-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"37\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-37-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-37-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-37-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"38\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-38-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-38-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-38-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><section data-index=\"39\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016?next_slideshow=1\" data-small=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/85/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-39-320.jpg?cb=1475599971\" data-normal=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-39-638.jpg?cb=1475599971\" data-full=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-39-1024.jpg?cb=1475599971\" alt=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  1 Like\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"ThorstenHeimes\" rel=\"nofollow\" href=\"https://www.slideshare.net/ThorstenHeimes?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Thorsten Heimes\n                            \n                              \n                                , \n                                Data Engineer at Fineway GmbH\n                              \n                              \n                                 at \n                                Fineway GmbH\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p>Volume : Terabytes, Records, Transactions, Tables, files <br />Velocity : Batch, Near real time, realtime <br />Variety : Structured, unstructured, semi structuredVertical scaling means that you scale by adding more power (CPU, RAM) to an existing machine. <p>In vertical-scaling the data resides on a single node and scaling is done through multi-core i.e. spreading the load between the CPU and RAM resources of that machine.</p>Horizontal scaling means that you scale by adding more machines into your pool of resources. <br />In a database horizontal-scaling is often based on partitioning of the data <br />i.e. each node contains only part of the data. <br />With horizontal-scaling it is often easier to scale dynamically by adding more machines into the existing pool. <br />If a cluster requires more resources to improve performance and provide high availability (HA), an administrator can scale out by adding more machine to the cluster.Scalability : Hyper scale, load balancing, scale out. <br />Availability : Failure resilient, rolling updates, recovery from failures. <br />Manageability : Granular versioning, micro service Responsive: The system responds in a timely manner if at all possible.  <p>Resilient: The system stays responsive in the face of failure. This applies not only to highly-available, mission critical systems — any system that is not resilient will be unresponsive after a failure.  </p><p>Elastic: The system stays responsive under varying workload. Reactive Systems can react to changes in the input rate by increasing or decreasing the resources allocated to service these inputs. </p><p>Message Driven: Reactive Systems rely on asynchronous message-passing to establish a boundary between components that ensures loose coupling, isolation and location transparency.  <br /></p>Micro service: <p>33TB Monthly  1.1 TB daily </p>The distributed storage system Cassandra, for example, runs on top of hundreds of commodity nodes spread across different data centers. Because the commodity hardware is scaled out horizontally, Cassandra is fault tolerant and does not have a single point of failure (SPoF).Cassandra supports a per-operation tradeoff between consistency and availability through Consistency Levels. <p>The following consistency levels are available: <br />ONE : Only a single replica must respond. <br />TWO :Two replicas must respond. <br />THREE : Three replicas must respond. <br />QUORUMA : majority (n/2 + 1) of the replicas must respond. <br />ALL :All of the replicas must respond. <br />LOCAL_QUORUMA :majority of the replicas in the local datacenter (whichever datacenter the coordinator is in) must respond. <br />EACH_QUORUMA : majority of the replicas in each datacenter must respond. <br />LOCAL_ONE : Only a single replica must respond. In a multi-datacenter cluster, this also gaurantees that read requests are not sent to replicas in a remote datacenter. <br />ANY : A single replica may respond, or the coordinator may store a hint. If a hint is stored, the coordinator will later attempt to replay the hint and deliver the mutation to the replicas. This consistency level is only accepted for write operations.</p>Spark and Spark Streaming with the RDD concept at the core are inherently designed to recover from worker failures.  <br />Stateful exactly-once semantics out of the box. <p>Spark Streaming recovers both lost work and operator state (e.g. sliding windows) out of the box, without any extra code on your part.</p>sc.cassandraTable(\"keyspace name\", \"table name\")</div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    Rahul Kumar\nTechnical Lead\nSigmoid\nReal Time data pipeline with Spark Streaming and\nCassandra with Mesos\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-2-638.jpg?cb=1475599971\" title=\"About Sigmoid&#10;© DataStax, All Rights Reserved. 2&#10;We build r...\" target=\"_blank\">\n        2.\n      </a>\n    About Sigmoid\n© DataStax, All Rights Reserved. 2\nWe build reactive real-time big data systems.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-3-638.jpg?cb=1475599971\" title=\"1 Data Management&#10;2 Cassandra Introduction&#10;3 Apache Spark S...\" target=\"_blank\">\n        3.\n      </a>\n    1 Data Management\n2 Cassandra Introduction\n3 Apache Spark Streaming\n4 Reactive Data Pipelines\n5 Use cases\n3© DataStax, All Rights Reserved.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-4-638.jpg?cb=1475599971\" title=\"Data Management&#10;© DataStax, All Rights Reserved. 4&#10;Managing...\" target=\"_blank\">\n        4.\n      </a>\n    Data Management\n© DataStax, All Rights Reserved. 4\nManaging data and analyzing\ndata have always greatest\nbenefit and the greatest\nchallenges for organization.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-5-638.jpg?cb=1475599971\" title=\"Three V’s of Big data&#10;© DataStax, All Rights Reserved. 5&#10;\" target=\"_blank\">\n        5.\n      </a>\n    Three V’s of Big data\n© DataStax, All Rights Reserved. 5\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-6-638.jpg?cb=1475599971\" title=\"Scale Vertically&#10;© DataStax, All Rights Reserved. 6&#10;\" target=\"_blank\">\n        6.\n      </a>\n    Scale Vertically\n© DataStax, All Rights Reserved. 6\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-7-638.jpg?cb=1475599971\" title=\"Scale Horizontally&#10;© DataStax, All Rights Reserved. 7&#10;\" target=\"_blank\">\n        7.\n      </a>\n    Scale Horizontally\n© DataStax, All Rights Reserved. 7\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-8-638.jpg?cb=1475599971\" title=\"Understanding Distributed Application&#10;© DataStax, All Right...\" target=\"_blank\">\n        8.\n      </a>\n    Understanding Distributed Application\n© DataStax, All Rights Reserved. 8\n“ A distributed system is a software system in which\ncomponents located on networked computers\ncommunicate and coordinate their actions by passing\nmessages.”\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-9-638.jpg?cb=1475599971\" title=\"Principles Of Distributed Application Design&#10;© DataStax, Al...\" target=\"_blank\">\n        9.\n      </a>\n    Principles Of Distributed Application Design\n© DataStax, All Rights Reserved. 9\n Availability\n Performance\n Reliability\n Scalability\n Manageability\n Cost\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-10-638.jpg?cb=1475599971\" title=\"Reactive Application&#10;© DataStax, All Rights Reserved. 10&#10;\" target=\"_blank\">\n        10.\n      </a>\n    Reactive Application\n© DataStax, All Rights Reserved. 10\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-11-638.jpg?cb=1475599971\" title=\"Reactive libraries, tools and frameworks&#10;© DataStax, All Ri...\" target=\"_blank\">\n        11.\n      </a>\n    Reactive libraries, tools and frameworks\n© DataStax, All Rights Reserved. 11\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-12-638.jpg?cb=1475599971\" title=\"Cassandra Introduction&#10;© DataStax, All Rights Reserved. 13&#10;...\" target=\"_blank\">\n        12.\n      </a>\n    Cassandra Introduction\n© DataStax, All Rights Reserved. 13\nCassandra - is an Open Source, distributed store for structured data\nthat scale-out on cheap, commodity hardware.\nBorn at Facebook, built on Amazon’s Dynamo and Google’s BigTable\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-13-638.jpg?cb=1475599971\" title=\"Why Cassandra&#10;© DataStax, All Rights Reserved. 14&#10;\" target=\"_blank\">\n        13.\n      </a>\n    Why Cassandra\n© DataStax, All Rights Reserved. 14\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-14-638.jpg?cb=1475599971\" title=\"Highly scalable NoSQL database&#10;© DataStax, All Rights Reser...\" target=\"_blank\">\n        14.\n      </a>\n    Highly scalable NoSQL database\n© DataStax, All Rights Reserved. 15\n Cassandra supplies linear\nscalability\n Cassandra is a partitioned\nrow store database\n Automatic data distribution\n Built-in and customizable\nreplication\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-15-638.jpg?cb=1475599971\" title=\"High Availability&#10;© DataStax, All Rights Reserved. 16&#10; In ...\" target=\"_blank\">\n        15.\n      </a>\n    High Availability\n© DataStax, All Rights Reserved. 16\n In a Cassandra cluster all\nnodes are equal.\n There are no masters or\ncoordinators at the cluster\nlevel.\n Gossip protocol allows\nnodes to be aware of each\nother.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-16-638.jpg?cb=1475599971\" title=\"Read/Write any where&#10;© DataStax, All Rights Reserved. 17&#10; ...\" target=\"_blank\">\n        16.\n      </a>\n    Read/Write any where\n© DataStax, All Rights Reserved. 17\n Cassandra is a R/W\nanywhere architecture, so\nany user/app can connect\nto any node in any DC and\nread/write the data.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-17-638.jpg?cb=1475599971\" title=\"High Performance&#10;© DataStax, All Rights Reserved. 18&#10; All ...\" target=\"_blank\">\n        17.\n      </a>\n    High Performance\n© DataStax, All Rights Reserved. 18\n All disk writes are\nsequential, append-only\noperations.\n Ensure No reading before\nwrite.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-18-638.jpg?cb=1475599971\" title=\"Cassandra &amp; CAP&#10;© DataStax, All Rights Reserved. 19&#10; Cassa...\" target=\"_blank\">\n        18.\n      </a>\n    Cassandra &amp; CAP\n© DataStax, All Rights Reserved. 19\n Cassandra is classified as\nan AP system\n System is still available\nunder partition\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-19-638.jpg?cb=1475599971\" title=\"CQL&#10;© DataStax, All Rights Reserved. 20&#10;CREATE KEYSPACE MyA...\" target=\"_blank\">\n        19.\n      </a>\n    CQL\n© DataStax, All Rights Reserved. 20\nCREATE KEYSPACE MyAppSpace WITH\nREPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };\nUSE MyAppSpace ;\nCREATE COLUMNFAMILY AccessLog(id text, ts timestamp ,ip text, port text,\nstatus text, PRIMARY KEY(id));\nINSERT INTO AccessLog (id, ts, ip, port, status) VALUES (’id-001-1', 2016-01-01\n00:00:00+0200', ’10.20.30.1’,’200’);\nSELECT * FROM AccessLog ;\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-20-638.jpg?cb=1475599971\" title=\"Apache Spark&#10;© DataStax, All Rights Reserved. 21&#10;Introducti...\" target=\"_blank\">\n        20.\n      </a>\n    Apache Spark\n© DataStax, All Rights Reserved. 21\nIntroduction\n Apache Spark is a fast and\ngeneral execution engine\nfor large-scale data\nprocessing.\n Organize computation as\nconcurrent tasks\n Handle fault-tolerance,\nload balancing\n Developed on Actor Model\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-21-638.jpg?cb=1475599971\" title=\"RDD Introduction&#10;© DataStax, All Rights Reserved. 22&#10;Resili...\" target=\"_blank\">\n        21.\n      </a>\n    RDD Introduction\n© DataStax, All Rights Reserved. 22\nResilient Distributed Datasets (RDDs), a distributed memory\nabstraction that lets programmers perform in-memory computations\non large clusters in a fault-tolerant manner.\nRDD shared the data over a cluster, like a virtualized, distributed\ncollection.\nUsers create RDDs in two ways: by loading an external dataset, or\nby distributing a collection of objects such as List, Map etc.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-22-638.jpg?cb=1475599971\" title=\"RDD Operations&#10;© DataStax, All Rights Reserved. 23&#10;Two Kind...\" target=\"_blank\">\n        22.\n      </a>\n    RDD Operations\n© DataStax, All Rights Reserved. 23\nTwo Kind of Operations\n• Transformation\n• Action\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-23-638.jpg?cb=1475599971\" title=\"What is Spark Streaming?&#10;© DataStax, All Rights Reserved. 2...\" target=\"_blank\">\n        23.\n      </a>\n    What is Spark Streaming?\n© DataStax, All Rights Reserved. 26\nFramework for large scale stream processing\n➔ Created at UC Berkeley\n➔ Scales to 100s of nodes\n➔ Can achieve second scale latencies\n➔ Provides a simple batch-like API for implementing complex algorithm\n➔ Can absorb live data streams from Kafka, Flume, ZeroMQ, Kinesis etc.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-24-638.jpg?cb=1475599971\" title=\"Spark Streaming&#10;© DataStax, All Rights Reserved. 27&#10;Introdu...\" target=\"_blank\">\n        24.\n      </a>\n    Spark Streaming\n© DataStax, All Rights Reserved. 27\nIntroduction\n• Spark Streaming is an\nextension of the core spark\nAPI that enables scalable,\nhigh-throughput, fault-\ntolerant stream processing\nof live data streams.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-25-638.jpg?cb=1475599971\" title=\"Spark Streaming over a HA Mesos Cluster&#10;© DataStax, All Rig...\" target=\"_blank\">\n        25.\n      </a>\n    Spark Streaming over a HA Mesos Cluster\n© DataStax, All Rights Reserved. 31\nTo use Mesos from Spark, you need a Spark binary package available in a place\naccessible (http/s3/hdfs) by Mesos, and a Spark driver program configured to\nconnect to Mesos.\nConfiguring the driver program to connect to Mesos:\nval sconf = new SparkConf()\n.setMaster(\"mesos://zk://10.121.93.241:2181,10.181.2.12:2181,10.107.48.112:2181/mesos\")\n.setAppName(”HAStreamingApp\")\n.set(\"spark.executor.uri\",\"hdfs://Sigmoid/executors/spark-1.6.0-bin-hadoop2.6.tgz\")\n.set(\"spark.mesos.coarse\", \"true\")\n.set(\"spark.cores.max\", \"30\")\n.set(\"spark.executor.memory\", \"10g\")\nval sc = new SparkContext(sconf)\nval ssc = new StreamingContext(sc, Seconds(1))\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-26-638.jpg?cb=1475599971\" title=\"Spark Cassandra Connector&#10;© DataStax, All Rights Reserved. ...\" target=\"_blank\">\n        26.\n      </a>\n    Spark Cassandra Connector\n© DataStax, All Rights Reserved. 32\n It allows us to expose Cassandra tables as Spark RDDs\n Write Spark RDDs to Cassandra tables\n Execute arbitrary CQL queries in your Spark applications.\n Compatible with Apache Spark 1.0 through 2.0\n It Maps table rows to CassandraRow objects or tuples\n Do Join with a subset of Cassandra data\n Partition RDDs according to Cassandra replication\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-27-638.jpg?cb=1475599971\" title=\"© DataStax, All Rights Reserved. 33&#10;resolvers += &quot;Spark Pac...\" target=\"_blank\">\n        27.\n      </a>\n    © DataStax, All Rights Reserved. 33\nresolvers += \"Spark Packages Repo\" at \"https://dl.bintray.com/spark-packages/maven\"\nlibraryDependencies += \"datastax\" % \"spark-cassandra-connector\" % \"1.6.0-s_2.10\"\nbuild.sbt should include:\nimport com.datastax.spark.connector._\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-28-638.jpg?cb=1475599971\" title=\"© DataStax, All Rights Reserved. 34&#10;val rdd = sc.cassandraT...\" target=\"_blank\">\n        28.\n      </a>\n    © DataStax, All Rights Reserved. 34\nval rdd = sc.cassandraTable(“applog”, “accessTable”)\nprintln(rdd.count)\nprintln(rdd.first)\nprintln(rdd.map(_.getInt(\"value\")).sum)\ncollection.saveToCassandra(“applog”, \"accessTable\", SomeColumns(”city\", ”count\"))\nSave Data Back to Cassandra\nGet a Spark RDD that represents a Cassandra table\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-29-638.jpg?cb=1475599971\" title=\"Many more higher order functions:&#10;© DataStax, All Rights Re...\" target=\"_blank\">\n        29.\n      </a>\n    Many more higher order functions:\n© DataStax, All Rights Reserved. 35\nrepartitionByCassandraReplica : It be used to relocate data in an RDD to match\nthe replication strategy of a given table and keyspace\njoinWithCassandraTable : The connector supports using any RDD as a source of\na direct join with a Cassandra Table\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-30-638.jpg?cb=1475599971\" title=\"Hint to scalable pipeline&#10;© DataStax, All Rights Reserved. ...\" target=\"_blank\">\n        30.\n      </a>\n    Hint to scalable pipeline\n© DataStax, All Rights Reserved. 36\nFigure out the bottleneck : CPU, Memory, IO, Network\nIf parsing is involved, use the one which gives high performance.\nProper Data modeling\nCompression, Serialization\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139/95/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016-31-638.jpg?cb=1475599971\" title=\"Thank You&#10;@rahul_kumar_aws&#10;\" target=\"_blank\">\n        31.\n      </a>\n    Thank You\n@rahul_kumar_aws\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"66696272\" title=\"Learning Study Skills\" href=\"https://www.linkedin.com/learning/learning-study-skills?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Learning Study Skills\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Learning Study Skills\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=AkCmIodsxaZhRytp%2F97%2FxxQM0Q8%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iXCej-NWfY3DrcMXfZLSiolwQfy0HkQQxfe6rRTbmFI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Learning Study Skills</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66696272\" title=\"PowerPoint for Teachers: Creating Interactive Lessons\" href=\"https://www.linkedin.com/learning/powerpoint-for-teachers-creating-interactive-lessons?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint for Teachers: Creating Interactive Lessons\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint for Teachers: Creating Interactive Lessons\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=NrZ6QSg%2BBWntE4IYVUxrWwdlQFs%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXCeu-NKfZHDrfcXYZLSiol4QfyoDmQEyfuerQzPmFY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint for Teachers: Creating Interactive Lessons</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"66696272\" title=\"PowerPoint 2016: Shortcuts\" href=\"https://www.linkedin.com/learning/powerpoint-2016-shortcuts?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_PowerPoint 2016: Shortcuts\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"PowerPoint 2016: Shortcuts\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Q8CAKihtghsRRL8hygXfdU0PiFk%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXiyu8t2fZHLgfc_XZLSioVQTcSsBmAQ2d-2rRzbpFY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>PowerPoint 2016: Shortcuts</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"45485034\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Breakout: Hadoop and the Operational Data Store\" href=\"https://www.slideshare.net/cloudera/hadoop-and-the-operational-data-store\">\n    \n    <div class=\"related-content\"><p>Breakout: Hadoop and the Operational Data Store</p><p>Cloudera, Inc.</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"55316371\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Using Event-Driven Architectures with Cassandra\" href=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\">\n    \n    <div class=\"related-content\"><p>Using Event-Driven Architectures with Cassandra</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"95288307\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar: DataStax Enterprise 6: 10 Ways to Multiply the Power of Apache Cassandra™ Without the Complexity\" href=\"https://www.slideshare.net/DataStax/webinar-datastax-enterprise-6-10-ways-to-multiply-the-power-of-apache-cassandra-without-the-complexity-95288307\">\n    \n    <div class=\"related-content\"><p>Webinar: DataStax Enterprise 6: 10 Ways to Multiply the Power of Apache Cassa...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"93695860\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar: DataStax and Microsoft Azure: Empowering the Right-Now Enterprise with Real-Time Apps at Cloud Scale\" href=\"https://www.slideshare.net/DataStax/webinar-datastax-and-microsoft-azure-empowering-the-rightnow-enterprise-with-realtime-apps-at-cloud-scale-93695860\">\n    \n    <div class=\"related-content\"><p>Webinar: DataStax and Microsoft Azure: Empowering the Right-Now Enterprise wi...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"88770359\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Webinar - Real-Time Customer Experience for the Right-Now Enterprise featuring Forrester Research\" href=\"https://www.slideshare.net/DataStax/webinar-realtime-customer-experience-for-the-rightnow-enterprise-featuring-forrester-research\">\n    \n    <div class=\"related-content\"><p>Webinar - Real-Time Customer Experience for the Right-Now Enterprise featurin...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"88713641\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Datastax - The Architect's guide to customer experience (CX)\" href=\"https://www.slideshare.net/DataStax/datastax-the-architects-guide-to-customer-experience-cx\">\n    \n    <div class=\"related-content\"><p>Datastax - The Architect's guide to customer experience (CX)</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"86697242\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"An Operational Data Layer is Critical for Transformative Banking Applications\" href=\"https://www.slideshare.net/DataStax/an-operational-data-layer-is-critical-for-transformative-banking-applications\">\n    \n    <div class=\"related-content\"><p>An Operational Data Layer is Critical for Transformative Banking Applications</p><p>DataStax</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n    \n  \n    \n    \n  \n  \n  <noscript>\n    </noscript>",
        "created_at": "2018-05-24T02:16:24+0000",
        "updated_at": "2018-05-24T02:16:42+0000",
        "published_at": null,
        "published_by": [
          "DataStax"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 8,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/realtimedatapipelinewithsparkstreamingandcassandrawithmesos-final-161004031139-thumbnail-4.jpg?cb=1475599971",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9790"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 9789,
        "uid": null,
        "title": "Using Event-Driven Architectures with Cassandra",
        "url": "https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra",
        "content": "Using Event-Driven Architectures with Cassandra\n      \n      \n      <div id=\"main-nav\" class=\"contain-to-grid fixed\"><p><a class=\"item\" href=\"https://www.slideshare.net/\" aria-labelledby=\"#home\">\n            \n            </a><label id=\"home\">SlideShare</label>\n          \n          <a class=\"item\" href=\"https://www.slideshare.net/explore\" aria-labelledby=\"#explore\">\n            <i class=\"fa fa-compass\">\n            </i></a><label id=\"explore\">Explore</label>\n          \n          \n            <a class=\"item\" href=\"https://www.slideshare.net/login\" aria-labelledby=\"#you\">\n              <i class=\"fa fa-user\">\n              </i></a><label id=\"you\">You</label>\n            </p></div>\n    <div class=\"wrapper\"><p>Successfully reported this slideshow.</p><div id=\"slideview-container\" class=\"\"><div class=\"row\"><div id=\"main-panel\" class=\"small-12 large-8 columns\"><div class=\"sectionElements\"><div class=\"playerWrapper\"><div><div class=\"player lightPlayer fluidImage presentation_player\" id=\"svPlayerId\">Using Event-Driven Architectures with Cassandra<div class=\"stage valign-first-slide\"><div class=\"slide_container\"><section data-index=\"1\" class=\"slide show\" itemprop=\"image\"><img class=\"slide_image\" src=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-1-638.jpg?cb=1447975617\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-1-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-1-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-1-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Event-Driven Architectures with&#10;Cassandra&#10;Aaron Stannard, CTO &amp; Cofounder Petabridge&#10;DataStax M...\" /></section><section data-index=\"2\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-2-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-2-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-2-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Crash Course in EDA&#10;\" /></i></section><section data-index=\"3\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-3-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-3-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-3-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Successful Product&#10;\" /></i></section><section data-index=\"4\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-4-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-4-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-4-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Struggling Business&#10;\" /></i></section><section data-index=\"5\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-5-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-5-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-5-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;A Radical New Product&#10;\" /></i></section><section data-index=\"6\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-6-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-6-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-6-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Allow our customers to start&#10;conversations with specific&#10;types of users.&#10;\" /></i></section><section data-index=\"7\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-7-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-7-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-7-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Had to be done in real-time for&#10;best results.&#10;\" /></i></section><section data-index=\"8\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-8-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-8-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-8-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Prototype: HTTP + Database&#10;\" /></i></section><section data-index=\"9\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-9-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-9-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-9-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;(Read after Write)&#10;\" /></i></section><section data-index=\"10\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-10-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-10-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-10-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Theory&#10;\" /></i></section><section data-index=\"11\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-11-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-11-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-11-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Reality&#10;\" /></i></section><section data-index=\"12\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-12-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-12-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-12-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Breakthrough!&#10;\" /></i></section><section data-index=\"13\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-13-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-13-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-13-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;I had STATEFUL, REACTIVE,&#10;STREAM PROCESSING&#10;problem!&#10;\" /></i></section><section data-index=\"14\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-14-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-14-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-14-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Implementation:&#10;HTTP -&gt; EDA -&gt; C*&#10;\" /></i></section><section data-index=\"15\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-15-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-15-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-15-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Why Do We Care About EDA?&#10;\" /></i></section><section data-index=\"16\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-16-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-16-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-16-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Databases Aren’t Magical&#10;\" /></i></section><section data-index=\"17\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-17-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-17-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-17-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Event-Driven Architecture&#10;Concepts&#10;\" /></i></section><section data-index=\"18\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-18-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-18-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-18-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Key Terms&#10;• Event – a significant change in “state”&#10;• Message – a notification of an event&#10;• Em...\" /></i></section><section data-index=\"19\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-19-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-19-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-19-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Goals &amp; Benefits&#10;• Extreme decoupling&#10;• Easily distributed&#10;• Inherently asynchronous and concur...\" /></i></section><section data-index=\"20\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-20-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-20-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-20-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;EDA Styles&#10;• Simple Event Processing&#10;• Event Stream Processing&#10;• Complex Event Processing (CEP)&#10;\" /></i></section><section data-index=\"21\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-21-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-21-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-21-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Event Stream Processing w/&#10;FSMs&#10;\" /></i></section><section data-index=\"22\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-22-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-22-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-22-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;\" /></i></section><section data-index=\"23\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-23-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-23-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-23-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;\" /></i></section><section data-index=\"24\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-24-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-24-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-24-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;\" /></i></section><section data-index=\"25\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-25-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-25-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-25-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;CQL Schema&#10;CREATE TABLE IF NOT EXISTS FSMState&#10;(&#10;persistence_id text,&#10;state_id int,&#10;state_data ...\" /></i></section><section data-index=\"26\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-26-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-26-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-26-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Event Sourcing with C*&#10;\" /></i></section><section data-index=\"27\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-27-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-27-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-27-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;\" /></i></section><section data-index=\"28\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-28-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-28-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-28-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;\" /></i></section><section data-index=\"29\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-29-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-29-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-29-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;EDA + C* Tips and Tricks&#10;• Large number of small sinks works best&#10;• Define simple, reusable jou...\" /></i></section><section data-index=\"30\" class=\"slide\"><i class=\"fa fa-spinner fa-spin\"><img class=\"slide_image\" src=\"https://www.slideshare.net/planetcassandra/using-eventdriven-architectures-with-cassandra\" data-small=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/85/using-eventdriven-architectures-with-cassandra-30-320.jpg?cb=1447975617\" data-normal=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-30-638.jpg?cb=1447975617\" data-full=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-30-1024.jpg?cb=1447975617\" alt=\"@petabridge Petabridge.com&#10;Have questions? Ask us!&#10;http://petabridge.com/&#10;\" /></i></section><div class=\"j-next-container next-container\"><div class=\"content-container\"><div class=\"next-slideshow-wrapper\"><div class=\"j-next-slideshow next-slideshow\"><p>Upcoming SlideShare</p></div><p>Loading in …5</p><p>×</p></div></div></div></div></div></div></div></div></div><div class=\"slideshow-info-container\" itemscope=\"itemscope\" itemtype=\"https://schema.org/MediaObject\"><div class=\"slideshow-tabs-container show-for-medium-up\"><ul class=\"tabs\" data-tab=\"\" role=\"tablist\"><li class=\"active\" role=\"presentation\">\n                <a href=\"#comments-panel\" role=\"tab\" aria-selected=\"true\" aria-controls=\"comments-panel\">\n                  \n                    0 Comments\n                </a>\n              </li>\n            <li class=\"\" role=\"presentation\">\n              <a href=\"#likes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"likes-panel\">\n                <i class=\"fa fa-heart\">\n                \n                  2 Likes\n                \n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#stats-panel\" class=\"j-stats-tab\" role=\"tab\" aria-selected=\"false\" aria-controls=\"stats-panel\">\n                <i class=\"fa fa-bar-chart\">\n                Statistics\n              </i></a>\n            </li>\n            <li role=\"presentation\">\n              <a href=\"#notes-panel\" role=\"tab\" aria-selected=\"false\" aria-controls=\"notes-panel\">\n                <i class=\"fa fa-file-text\">\n                Notes\n              </i></a>\n            </li>\n          </ul><div class=\"tabs-content\"><div class=\"content\" id=\"likes-panel\" role=\"tabpanel\" aria-hidden=\"false\"><ul id=\"favsList\" class=\"j-favs-list notranslate user-list no-bullet\" itemtype=\"http://schema.org/UserLikes\" itemscope=\"itemscope\"><li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"RadhaKrishnaProddatu\" rel=\"nofollow\" href=\"https://www.slideshare.net/RadhaKrishnaProddatu?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Radha Krishna Proddaturi\n                            \n                              \n                                , \n                                Principal Software Engineer at Coupons.com\n                              \n                              \n                                 at \n                                Coupons.com\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n                    <li itemtype=\"http://schema.org/Person\" itemscope=\"itemscope\">\n                      <div class=\"row\"><div class=\"small-11 columns\"><a class=\"favoriter notranslate\" title=\"galapea\" rel=\"nofollow\" href=\"https://www.slideshare.net/galapea?utm_campaign=profiletracking&amp;utm_medium=sssite&amp;utm_source=ssslideshow\">\n                            Alif Ruliarso\n                            \n                              \n                                , \n                                Tech-focused delivering innovative technology and products\n                              \n                              \n                                 at \n                                Bridestory\n                              \n                            \n                            \n                            </a></div></div>\n                    </li>\n              </ul></div><div class=\"content\" id=\"downloads-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No Downloads</p></div><div class=\"content\" id=\"notes-panel\" role=\"tabpanel\" aria-hidden=\"true\"><p>No notes for slide</p>C* users should care about their application architecture for the simple reason that C* isn’t magic and can’t do everything. Cassandra can’t solve the problem of real-time correlation or reaction at the application layer. Developers have to solve these problems themselves, and Cassandra is just one part of the solution.Event Stream Processing and CEP play nicely with Cassandra, and that’s what we’ll be focusing onIntentionally small and stupid</div></div></div><div class=\"notranslate transcript add-padding-right j-transcript\"><ol class=\"j-transcripts transcripts no-bullet no-style\" itemprop=\"text\"><li>\n      1.\n    @petabridge Petabridge.com\nEvent-Driven Architectures with\nCassandra\nAaron Stannard, CTO &amp; Cofounder Petabridge\nDataStax MVP 2015\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-2-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Crash Course in EDA&#10;\" target=\"_blank\">\n        2.\n      </a>\n    @petabridge Petabridge.com\nCrash Course in EDA\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-3-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Successful Product&#10;\" target=\"_blank\">\n        3.\n      </a>\n    @petabridge Petabridge.com\nSuccessful Product\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-4-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Struggling Business&#10;\" target=\"_blank\">\n        4.\n      </a>\n    @petabridge Petabridge.com\nStruggling Business\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-5-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;A Radical New Product&#10;\" target=\"_blank\">\n        5.\n      </a>\n    @petabridge Petabridge.com\nA Radical New Product\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-6-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Allow our customers to start&#10;con...\" target=\"_blank\">\n        6.\n      </a>\n    @petabridge Petabridge.com\nAllow our customers to start\nconversations with specific\ntypes of users.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-7-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Had to be done in real-time for&#10;...\" target=\"_blank\">\n        7.\n      </a>\n    @petabridge Petabridge.com\nHad to be done in real-time for\nbest results.\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-8-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Prototype: HTTP + Database&#10;\" target=\"_blank\">\n        8.\n      </a>\n    @petabridge Petabridge.com\nPrototype: HTTP + Database\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-9-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;(Read after Write)&#10;\" target=\"_blank\">\n        9.\n      </a>\n    @petabridge Petabridge.com\n(Read after Write)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-10-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Theory&#10;\" target=\"_blank\">\n        10.\n      </a>\n    @petabridge Petabridge.com\nTheory\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-11-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Reality&#10;\" target=\"_blank\">\n        11.\n      </a>\n    @petabridge Petabridge.com\nReality\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-12-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Breakthrough!&#10;\" target=\"_blank\">\n        12.\n      </a>\n    @petabridge Petabridge.com\nBreakthrough!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-13-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;I had STATEFUL, REACTIVE,&#10;STREAM...\" target=\"_blank\">\n        13.\n      </a>\n    @petabridge Petabridge.com\nI had STATEFUL, REACTIVE,\nSTREAM PROCESSING\nproblem!\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-14-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Implementation:&#10;HTTP -&gt; EDA -&gt; C*&#10;\" target=\"_blank\">\n        14.\n      </a>\n    @petabridge Petabridge.com\nImplementation:\nHTTP -&gt; EDA -&gt; C*\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-15-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Why Do We Care About EDA?&#10;\" target=\"_blank\">\n        15.\n      </a>\n    @petabridge Petabridge.com\nWhy Do We Care About EDA?\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-16-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Databases Aren’t Magical&#10;\" target=\"_blank\">\n        16.\n      </a>\n    @petabridge Petabridge.com\nDatabases Aren’t Magical\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-17-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Event-Driven Architecture&#10;Concep...\" target=\"_blank\">\n        17.\n      </a>\n    @petabridge Petabridge.com\nEvent-Driven Architecture\nConcepts\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-18-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Key Terms&#10;• Event – a significan...\" target=\"_blank\">\n        18.\n      </a>\n    @petabridge Petabridge.com\nKey Terms\n• Event – a significant change in “state”\n• Message – a notification of an event\n• Emitter – detect, gather, transfer events\n• Sinks – react to events immediately\n• Channels – conduit between emitters and\nsinks\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-19-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Goals &amp; Benefits&#10;• Extreme decou...\" target=\"_blank\">\n        19.\n      </a>\n    @petabridge Petabridge.com\nGoals &amp; Benefits\n• Extreme decoupling\n• Easily distributed\n• Inherently asynchronous and concurrent\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-20-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;EDA Styles&#10;• Simple Event Proces...\" target=\"_blank\">\n        20.\n      </a>\n    @petabridge Petabridge.com\nEDA Styles\n• Simple Event Processing\n• Event Stream Processing\n• Complex Event Processing (CEP)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-21-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Event Stream Processing w/&#10;FSMs&#10;\" target=\"_blank\">\n        21.\n      </a>\n    @petabridge Petabridge.com\nEvent Stream Processing w/\nFSMs\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-22-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;\" target=\"_blank\">\n        22.\n      </a>\n    @petabridge Petabridge.com\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-23-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;\" target=\"_blank\">\n        23.\n      </a>\n    @petabridge Petabridge.com\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-24-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;\" target=\"_blank\">\n        24.\n      </a>\n    @petabridge Petabridge.com\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-25-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;CQL Schema&#10;CREATE TABLE IF NOT E...\" target=\"_blank\">\n        25.\n      </a>\n    @petabridge Petabridge.com\nCQL Schema\nCREATE TABLE IF NOT EXISTS FSMState\n(\npersistence_id text,\nstate_id int,\nstate_data blob,\nPRIMARY KEY (persistence_id, state_id)\n)\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-26-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Event Sourcing with C*&#10;\" target=\"_blank\">\n        26.\n      </a>\n    @petabridge Petabridge.com\nEvent Sourcing with C*\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-27-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;\" target=\"_blank\">\n        27.\n      </a>\n    @petabridge Petabridge.com\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-28-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;\" target=\"_blank\">\n        28.\n      </a>\n    @petabridge Petabridge.com\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-29-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;EDA + C* Tips and Tricks&#10;• Large...\" target=\"_blank\">\n        29.\n      </a>\n    @petabridge Petabridge.com\nEDA + C* Tips and Tricks\n• Large number of small sinks works best\n• Define simple, reusable journals and\nsnapshots\n• C* == durable backup, App = single\nsource of truth\n• TTL everywhere your business domain\nsupports it\n \n  </li>\n  <li>\n      <a href=\"https://image.slidesharecdn.com/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891/95/using-eventdriven-architectures-with-cassandra-30-638.jpg?cb=1447975617\" title=\"@petabridge Petabridge.com&#10;Have questions? Ask us!&#10;http://p...\" target=\"_blank\">\n        30.\n      </a>\n    @petabridge Petabridge.com\nHave questions? Ask us!\nhttp://petabridge.com/\n \n  </li>\n              </ol></div></div></div><aside id=\"side-panel\" class=\"small-12 large-4 columns j-related-more-tab\"><dl class=\"tabs related-tabs small\" data-tab=\"\"><dd class=\"active\">\n      <a href=\"#related-tab-content\" data-ga-cat=\"bigfoot_slideview\" data-ga-action=\"relatedslideshows_tab\">\n        Recommended\n      </a>\n    </dd>\n</dl><div class=\"tabs-content\"><ul id=\"related-tab-content\" class=\"content active no-bullet notranslate\"><li class=\"lynda-item\">\n  <a data-ssid=\"55316371\" title=\"Visual Aesthetics for Elearning\" href=\"https://www.linkedin.com/learning/visual-aesthetics-for-elearning?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Visual Aesthetics for Elearning\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Visual Aesthetics for Elearning\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=Hf12LV5YZaVLCcNTSkttsN6Dxx8%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-kXCei8tyfZXDrcc_WZLSiol4XcC8HkQwzfO-hSTPkFY69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Visual Aesthetics for Elearning</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"55316371\" title=\"College Prep: Writing a Strong Essay\" href=\"https://www.linkedin.com/learning/college-prep-writing-a-strong-essay?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_College Prep: Writing a Strong Essay\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"College Prep: Writing a Strong Essay\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=UlSFF0nnPn2Agyz5a5T2WQKxkbA%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-iUyav_dSfY3_qfMDeZLSiol4TfSkFlAA0duqsQzfhGo69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>College Prep: Writing a Strong Essay</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n          <li class=\"lynda-item\">\n  <a data-ssid=\"55316371\" title=\"Test Prep: GRE\" href=\"https://www.linkedin.com/learning/test-prep-gre?trk=slideshare_sv_learning\" data-ga-cat=\"sv_loggedout\" data-ga-label=\"lil_rec_Test Prep: GRE\" data-ga-action=\"click\">\n    <div class=\"lynda-thumbnail\"><img class=\"j-thumbnail j-lazy-thumb\" alt=\"Test Prep: GRE\" src=\"https://www.linkedin.com/media-proxy/ext?w=1200&amp;h=675&amp;hash=zRN9cYKLcI5pMFN7G1fZgp5AHQw%3D&amp;ora=1%2CaFBCTXdkRmpGL2lvQUFBPQ%2CxAVta5g-0R6plxVUzgUv5K_PrkC9q0RIUJDPBy-lXSSj-9OfZHHocMbZZLSiol8QcS4DkQA2e-ahSTLnEI69LcLmY4Yx3A\" /></div>\n    <div class=\"lynda-content\"><p>Test Prep: GRE</p><p>Online Course - LinkedIn Learning</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"66696272\" data-algo-id=\"21\" data-source-name=\"BROWSEMAP\" data-source-model=\"21\" data-urn-type=\"Slideshow\" data-score=\"1\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul Kumar, Sigmoid) | C* Summit 2016\" href=\"https://www.slideshare.net/DataStax/realtime-data-pipeline-with-spark-streaming-and-cassandra-with-mesos-rahul-kumar-sigmoid-c-summit-2016\">\n    \n    <div class=\"related-content\"><p>Realtime Data Pipeline with Spark Streaming and Cassandra with Mesos (Rahul K...</p><p>DataStax</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"77306108\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Forrester CXNYC 2017 - Delivering great real-time cx is a true craft\" href=\"https://www.slideshare.net/planetcassandra/forrester-cxnyc-2017-delivering-great-realtime-cx-is-a-true-craft\">\n    \n    <div class=\"related-content\"><p>Forrester CXNYC 2017 - Delivering great real-time cx is a true craft</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64947458\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Introduction to DataStax Enterprise Graph Database\" href=\"https://www.slideshare.net/planetcassandra/introduction-to-datastax-enterprise-graph-database\">\n    \n    <div class=\"related-content\"><p>Introduction to DataStax Enterprise Graph Database</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64947204\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Introduction to DataStax Enterprise Advanced Replication with Apache Cassandra\" href=\"https://www.slideshare.net/planetcassandra/introduction-to-datastax-enterprise-advanced-replication-with-apache-cassandra\">\n    \n    <div class=\"related-content\"><p>Introduction to DataStax Enterprise Advanced Replication with Apache Cassandra</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64223739\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra on Docker @ Walmart Labs\" href=\"https://www.slideshare.net/planetcassandra/cassandra-on-docker-walmart-labs\">\n    \n    <div class=\"related-content\"><p>Cassandra on Docker @ Walmart Labs</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64222983\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra 3.0 Data Modeling\" href=\"https://www.slideshare.net/planetcassandra/cassandra-30-data-modeling\">\n    \n    <div class=\"related-content\"><p>Cassandra 3.0 Data Modeling</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n        <li class=\"j-related-item\">\n  <a data-ssid=\"64222866\" data-algo-id=\"\" data-source-name=\"MORE_FROM_USER\" data-source-model=\"\" data-urn-type=\"Slideshow\" data-score=\"\" class=\"j-related-impression slideview_related_item j-recommendation-tracking\" title=\"Cassandra Adoption on Cisco UCS &amp; Open stack\" href=\"https://www.slideshare.net/planetcassandra/cassandra-adoption-on-cisco-ucs-open-stack\">\n    \n    <div class=\"related-content\"><p>Cassandra Adoption on Cisco UCS &amp; Open stack</p><p>DataStax Academy</p></div>\n  </a>\n</li>\n    </ul></div>\n    </aside></div></div><footer>\n          <div class=\"row\"><div class=\"columns\"><ul class=\"main-links text-center\"><li><a href=\"https://www.slideshare.net/about\">About</a></li>\n                \n                <li><a href=\"http://blog.slideshare.net/\">Blog</a></li>\n                <li><a href=\"https://www.slideshare.net/terms\">Terms</a></li>\n                <li><a href=\"https://www.slideshare.net/privacy\">Privacy</a></li>\n                <li><a href=\"http://www.linkedin.com/legal/copyright-policy\">Copyright</a></li>\n                \n              </ul></div></div>\n          \n          <div class=\"row\"><div class=\"columns\"><p class=\"copyright text-center\">LinkedIn Corporation © 2018</p></div></div>\n        </footer></div>\n    \n    <div class=\"modal_popup_container\"><div id=\"top-clipboards-modal\" class=\"reveal-modal xlarge top-clipboards-modal\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\"><h4 class=\"modal-title\">Public clipboards featuring this slide</h4><hr /><p>No public clipboards found for this slide</p></div><div id=\"select-clipboard-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h4 class=\"modal-title\">Select another clipboard</h4>\n    <hr /><a class=\"close-reveal-modal button-lrg\" href=\"#\" aria-label=\"Close\">×</a><div class=\"modal-content\"><div class=\"default-clipboard-panel radius\"><p>Looks like you’ve clipped this slide to <strong class=\"default-clipboard-title\"> already.</strong></p></div><div class=\"clipboard-list-container\"><div class=\"clipboard-create-new\"><p>Create a clipboard</p></div></div></div></div><div id=\"clipboard-create-modal\" class=\"reveal-modal medium\" data-reveal=\"\" aria-labelledby=\"modal-title\" aria-hidden=\"true\" role=\"dialog\" translate=\"no\"><p></p><h3>You just clipped your first slide!</h3>\n      \n        Clipping is a handy way to collect important slides you want to go back to later. Now customize the name of a clipboard to store your clips.<h4 class=\"modal-title\" id=\"modal-title\">\n    \n    <label>Description\n          \n        </label></h4></div>\n    <div class=\"row\"><label>Visibility\n        <small id=\"privacy-switch-description\">Others can see my Clipboard</small>\n          </label><label for=\"privacy-switch\">\n      </label></div>\n        \n    </div>\n    \n    \n      <noscript>\n    </noscript>",
        "created_at": "2018-05-24T02:16:03+0000",
        "updated_at": "2018-05-24T02:16:19+0000",
        "published_at": null,
        "published_by": [
          "DataStax Academy"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "www.slideshare.net",
        "preview_picture": "https://cdn.slidesharecdn.com/ss_thumbnails/event-drivenarchitectureswithcassandra-151119232426-lva1-app6891-thumbnail-4.jpg?cb=1447975617",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9789"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 27,
            "label": "search",
            "slug": "search"
          },
          {
            "id": 36,
            "label": "solr",
            "slug": "solr"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 956,
            "label": "streaming",
            "slug": "streaming"
          },
          {
            "id": 1071,
            "label": "kafka.streams",
            "slug": "kafka-streams"
          }
        ],
        "is_public": false,
        "id": 9788,
        "uid": null,
        "title": "Search and Analytics on Streaming Data With Kafka, Solr, Cassandra, Spark",
        "url": "http://saumitra.me/blog/tweet-search-and-analysis-with-kafka-solr-cassandra/",
        "content": "<p>In this blog post we will see how to setup a simple search and anlytics pipeline on streaming data in scala.</p><ul><li>For sample timeseries data, we will use twitter stream.</li>\n<li>For data pipelining, we will use kafka</li>\n<li>For search, we will use Solr. We will use Banana for a UI query interface for solr data.</li>\n<li>For analytics, we will store data in cassandra. We will see example of using spark for running analytics query. We will use zeppelin for a UI query interface.</li>\n</ul><p>Full code for this post is avaliable at <a href=\"https://github.com/saumitras/twitter-analysis\">https://github.com/saumitras/twitter-analysis</a></p>\n<h2>Dependencies</h2>\n<p>Create a new project and add following dependecies in build.sbt. Note that there are few conflicting dependecies in kafka so exclude them:</p>\n<figure class=\"code\"><div class=\"highlight\"><table><tr><td class=\"gutter\"><pre class=\"line-numbers\">1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n</pre></td><td class=\"code\"><pre class=\"\">libraryDependencies ++= Seq(\n  \"org.twitter4j\" % \"twitter4j-core\" % \"4.0.4\",\n  \"org.twitter4j\" % \"twitter4j-stream\" % \"4.0.4\",\n  \"com.typesafe.akka\" % \"akka-actor_2.11\" % \"2.4.17\",\n  \"org.apache.kafka\" % \"kafka_2.11\" % \"0.10.0.0\" withSources() exclude(\"org.slf4j\",\"slf4j-log4j12\") exclude(\"javax.jms\", \"jms\") exclude(\"com.sun.jdmk\", \"jmxtools\") exclude(\"com.sun.jmx\", \"jmxri\"),\n  \"org.apache.avro\" % \"avro\" % \"1.7.7\" withSources(),\n  \"org.apache.solr\" % \"solr-solrj\" % \"6.4.1\" withSources(),\n  \"com.typesafe.scala-logging\" %% \"scala-logging\" % \"3.1.0\",\n  \"ch.qos.logback\" % \"logback-classic\" % \"1.1.2\",\n  \"com.datastax.cassandra\" % \"cassandra-driver-core\"  % \"3.0.2\",\n  \"org.apache.cassandra\" % \"cassandra-clientutil\"  % \"3.0.2\",\n  \"org.apache.spark\" %% \"spark-core\" % \"2.1.0\",\n  \"org.apache.spark\" %% \"spark-sql\" % \"2.1.0\",\n  \"org.apache.spark\" %% \"spark-hive\" % \"2.1.0\",\n  \"com.datastax.spark\" %% \"spark-cassandra-connector\" % \"2.0.0\"\n)</pre></td></tr></table></div></figure><h2>Setting up twiiter stream</h2>\n<p>For streaming data from twitter you need access keys and token. You can go to <a href=\"https://apps.twitter.com\">https://apps.twitter.com</a> and creata a new app to get these. After creating an app, click on “Keys and access token” and copy following:</p>\n<ul><li>Consumer Key (API Key)</li>\n<li>Consumer Secret (API Secret)</li>\n<li>Access Token</li>\n<li>Access Token Secret</li>\n</ul><p>We will use twitter4j. Build a configuration using token and key</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\nval cb = new ConfigurationBuilder()\ncb.setDebugEnabled(true)\ncb.setOAuthConsumerKey(\"p5vABCjRWWSXNBkypnb8ZnSzk\")   //replace this with your own keys\ncb.setOAuthConsumerSecret(\"wCVFIpwWxEyOcM9lrHa9TYExbNsLGvEUgJucePPjcTx83bD1Gt\") //replace this with your own keys\ncb.setOAuthAccessToken(\"487652626-kDOFZLu8bDjFyCKUOCDa7FtHsr22WC3PMH4iuNtn\")  //replace this with your own keys\ncb.setOAuthAccessTokenSecret(\"4W3LaQTAgGoW5SsHUAgp6gK9b5AKgl8hRcFnNYgvPTylU\")  //replace this with your own keys\n</figcaption></figure><p>You can now open a stream and listen for tweets with some specific keyswords or hashtags:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\nval stream = new TwitterStreamFactory(cb.build()).getInstance()\nval listener = new StatusListener {\n  override def onTrackLimitationNotice(i: Int): Unit = logger.warn(s\"Track limited $i tweets\")\n  override def onStallWarning(stallWarning: StallWarning): Unit = logger.error(\"Stream stalled\")\n  override def onDeletionNotice(statusDeletionNotice: StatusDeletionNotice): Unit = logger.warn(\"Status ${statusDeletionNotice.getStatusId} deleted\")\n  override def onScrubGeo(l: Long, l1: Long): Unit = logger.warn(s\"Geo info scrubbed. userId:$l, upToStatusId:$l1\")\n  override def onException(e: Exception): Unit = logger.error(\"Exception occurred. \" + e.getMessage)\n  override def onStatus(status: Status): Unit = {\n    logger.info(\"Msg: \" + status.getText)\n  }\n}\nval keywords = List(\"#scala\", \"#kafka\", \"#cassandra\", \"#solr\", \"#bigdata\", \"#apachespark\", \"#streamingdata\")\nstream.addListener(listener)\nval fq = new FilterQuery()\nfq.track(keywords.mkString(\",\"))\nstream.filter(fq)\n</figcaption></figure><p><code>StatusListener</code> provide couple of callback to handle different scenarios. <code>onStatus</code> is the one which will get the <code>tweet</code> and its metadata. <code>stream.filter(fq)</code> will start the stream.</p>\n<p>If you run this, you should start seeing the tweets:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\nMsg: RT @botbigdata: How to build a data science team https://t.co/xJWWgueGAV #bigdata\nMsg: RT @ATEKAssetScan: Why Velocity Of Innovation Is A Data Friction Problem https://t.co/Eo1pTNCEv9 #BigData #IoT #IIoT #InternetOfThings #Art…\nMsg: Making the Most of Big Data https://t.co/X52AZ5n5nT #BigData\nMsg: RT @botbigdata: Create editable Microsoft Office charts from R https://t.co/LnSDU0iSMq #bigdata\nMsg: RT @YarmolukDan: How #Twitter Users Can Generate Better Ideas https://t.co/b0O9iEULHG #DataScience #DataScientist #BigData #IoT… \nMsg: RT @botbigdata: VIDEO: Installing TOR on an Ubuntu Virtual Machine https://t.co/Q3FPhY8CGm #bigdata</figcaption></figure><p>Lets define a type and extract out tweet metadata</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\ncase class Tweet(id:String, username:String, userId:Long, userScreenName:String,\n                   userDesc:String, userProfileImgUrl:String, favCount:Long, retweetCount:Long,\n                   lang:String, place:String, message:String, isSensitive:Boolean,\n                   isTruncated:Boolean, isFavorited:Boolean, isRetweeted:Boolean,\n                   isRetweet:Boolean, createdAt:Long)\n</figcaption></figure><figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\noverride def onStatus(status: Status): Unit = {\n  val retweetCount = if(status.getRetweetedStatus == null) 0 else status.getRetweetedStatus.getRetweetCount\n  val userDesc = if(status.getUser.getDescription == null) \"null\" else status.getUser.getDescription\n  val userProfileImgUrl = if(status.getUser.getProfileImageURL == null) \"null\" else status.getUser.getProfileImageURL\n  val lang = if(status.getLang == null) \"null\" else status.getLang\n  val place = if(status.getPlace == null) \"null\" else status.getPlace.getFullName\n  val tweet = Tweet(\n    id = status.getId.toString,\n    username = status.getUser.getName,\n    userId = status.getUser.getId,\n    userScreenName = status.getUser.getScreenName,\n    userDesc = userDesc,\n    userProfileImgUrl = userProfileImgUrl,\n    createdAt = status.getCreatedAt.getTime,\n    favCount = status.getFavoriteCount,\n    retweetCount = retweetCount,\n    lang = lang,\n    place = place,\n    message = status.getText,\n    isSensitive = status.isPossiblySensitive,\n    isTruncated = status.isTruncated,\n    isFavorited = status.isFavorited,\n    isRetweeted = status.isRetweeted,\n    isRetweet = status.isRetweet\n  )\n  logger.info(\"Msg: \" + tweet.message)\n  }\n</figcaption></figure><p>Next we will send these tweets to kafka.</p>\n<h2>Zookeeper setup</h2>\n<p>ZooKeeper is a centralized service for maintaining configuration information, naming, providing distributed synchronization, and providing group services. All of these kinds of services are used in some form or another by distributed applications. In our example, both Kafka and Solr will need zookeeper for their state and config management, so you need to first start zookeeper.</p>\n<ul><li>Download it from <code>http://apache.org/dist/zookeeper/zookeeper-3.4.6/zookeeper-3.4.6.tar.gz</code></li>\n<li>Extract it and go inside conf directory</li>\n<li>Make a copy of zoo_sample.conf as zoo.cfg</li>\n<li>Run it using <code>bin/zkServer.sh start</code></li>\n<li>Verify its started successfully by running <code>bin/zkServer.sh status</code> command.</li>\n</ul><h2>Putting data in Kafka</h2>\n<p>Here’s steps to send data to kafka.</p>\n<ul><li>Start kafka server and broker(s)</li>\n<li>Create a topic in kafka to which data will be send</li>\n<li>Define a avro schema for the tweets</li>\n<li>Create a kafka producer which will serialize tweets using avro schema and send it to kafka</li>\n</ul><p>Download kafka from here.</p>\n<p>Start server</p>\n<figure class=\"code\"><figcaption>1\nbin/kafka-server-start.sh config/server.properties\n</figcaption></figure><p>Create a topic</p>\n<figure class=\"code\"><figcaption>1\nbin/kafka-topics.sh --create --zookeeper localhost:2181/kafka --replication-factor 1 --partitions 1 --topic tweet1\n</figcaption></figure><p>You can see if topic is created successfully</p>\n<figure class=\"code\"><figcaption>1\nbin/kafka-topics.sh --list --zookeeper localhost:2181/kafka\n</figcaption></figure><h3>Avro schema</h3>\n<p>Avro is a data serialization system. It has a JSON like data model, but can be represented as either JSON or in a compact binary form. It comes with a very sophisticated schema description language that describes data. Lets define avro schema for our <code>Tweet</code> type:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n{\n  \"type\": \"record\",\n  \"namespace\": \"tweet\",\n  \"name\": \"tweet\",\n  \"fields\":[\n    { \"name\": \"id\", \"type\":\"string\" },\n    { \"name\": \"username\", \"type\":\"string\" },\n    { \"name\": \"userId\", \"type\":\"long\" },\n    { \"name\": \"userScreenName\", \"type\":\"string\" },\n    { \"name\": \"userDesc\", \"type\":\"string\" },\n    { \"name\": \"userProfileImgUrl\", \"type\":\"string\" },\n    { \"name\": \"favCount\", \"type\":\"int\" },\n    { \"name\": \"retweetCount\", \"type\":\"int\" },\n    { \"name\": \"lang\", \"type\":\"string\" },\n    { \"name\": \"place\", \"type\":\"string\" },\n    { \"name\": \"message\", \"type\":\"string\" },\n    { \"name\": \"isSensitive\", \"type\":\"boolean\" },\n    { \"name\": \"isTruncated\", \"type\":\"boolean\" },\n    { \"name\": \"isFavorited\", \"type\":\"boolean\" },\n    { \"name\": \"isRetweeted\", \"type\":\"boolean\" },\n    { \"name\": \"isRetweet\", \"type\":\"boolean\" },\n    { \"name\": \"createdAt\", \"type\":\"long\" }\n  ]\n}\n</figcaption></figure><p>Kafka supports lot of other formats too, but avro is the preferred format for streaming data. You can read more about it here <code>https://www.confluent.io/blog/avro-kafka-data/</code></p>\n<p>Next create a producer</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\nval props = new Properties()\nprops.put(\"bootstrap.servers\", brokerList)\nprops.put(\"client.id\", \"KafkaTweetProducer\")\nprops.put(\"key.serializer\", \"org.apache.kafka.common.serialization.StringSerializer\")\nprops.put(\"value.serializer\", \"org.apache.kafka.common.serialization.ByteArraySerializer\")\nval producer = new KafkaProducer[String, Array[Byte]](props)\n</figcaption></figure><p>Creata a <code>Schema</code> using the avro schema definition</p>\n<figure class=\"code\"><figcaption>1\nval schema = new Parser().parse(Source.fromURL(getClass.getResource(\"/tweet.avsc\")).mkString)\n</figcaption></figure><p>Serialize the tweet and send it to producer</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\ndef writeToKafka(tweet: Tweet) = {\n  val row = new GenericData.Record(schema)\n  row.put(\"id\", tweet.id)\n  row.put(\"username\", tweet.username)\n  row.put(\"userId\", tweet.userId)\n  row.put(\"userScreenName\", tweet.userScreenName)\n  row.put(\"userDesc\", tweet.userDesc)\n  row.put(\"userProfileImgUrl\", tweet.userProfileImgUrl)\n  row.put(\"favCount\", tweet.favCount)\n  row.put(\"retweetCount\", tweet.retweetCount)\n  row.put(\"lang\", tweet.lang)\n  row.put(\"place\", tweet.place)\n  row.put(\"message\", tweet.message)\n  row.put(\"isSensitive\", tweet.isSensitive)\n  row.put(\"isTruncated\", tweet.isTruncated)\n  row.put(\"isFavorited\", tweet.isFavorited)\n  row.put(\"isRetweeted\", tweet.isRetweeted)\n  row.put(\"isRetweet\", tweet.isRetweet)\n  row.put(\"createdAt\", tweet.createdAt)\n  val writer = new SpecificDatumWriter[GenericRecord](schema)\n  val out = new ByteArrayOutputStream()\n  val encoder = EncoderFactory.get().binaryEncoder(out, null)\n  writer.write(row, encoder)\n  encoder.flush()\n  logger.info(\"Pushing to kafka. TweetId= \" + tweet.id)\n  val data = new ProducerRecord[String, Array[Byte]](topic, out.toByteArray)\n  producer.send(data)\n}\n</figcaption></figure><p>We will create an ActorSystem and put all this inside a <code>KafkaTweetProducer</code> actor. We will then send a message to <code>KafkaTweetProducer</code> whenever a new tweet is recieved.</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nval zkHostKafka = \"localhost:2181/kafka\"\nval kafkaBrokers = \"localhost:9092\"\nval topic = \"tweet1\"\nval system = ActorSystem(\"TwitterAnalysis\")\nval kafkaProducer = system.actorOf(Props(new KafkaTweetProducer(kafkaBrokers, topic)), name = \"kafka_tweet_producer\")\nval twitterStream = new TwitterWatcher(cb, topics, kafkaProducer)\ntwitterStream.startTracking()\n</figcaption></figure><figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nclass TwitterWatcher(cb:ConfigurationBuilder, keywords:List[String], destination:ActorRef) extends Logging {\n override def onStatus(status: Status): Unit = {\n    ...\n    val tweet = Tweet(\n      ...\n    )\n    destination ! tweet\n  }\n}\n</figcaption></figure><figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\nclass KafkaTweetProducer(brokerList:String, topic:String) extends Actor with Logging {\n  override def receive: Receive = {\n    case t:Tweet =&gt;\n      writeToKafka(t)\n    ...\n  }\n}\n</figcaption></figure><p>To test whether this data is getting written in kafka properly on not, you can use the command line console consumer and watch for the topic <code>tweet1</code>:</p>\n<figure class=\"code\"><figcaption>1\nbin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic tweet1 --from-beginning\n</figcaption></figure><p>Next we will consume this data in solr and cassandra</p>\n<h2>Putting data in solr</h2>\n<p>Here’s steps for writing data to solr:</p>\n<ul><li>Define a solr schema(config-set) corresponding to tweet type</li>\n<li>Upload the schmea to zookeeper</li>\n<li>Creata a collection in solr using this config set</li>\n<li>Create a solr consumer which will read from <code>tweet1</code> topic from kafka</li>\n<li>Deserialize the data read from kafka and create solr documents from it</li>\n<li>Send documents to solr</li>\n</ul><p>Here’s what the shema definition will look like:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n &lt;field name=\"id\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"username\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"userId\" type=\"tlong\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"userScreenName\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"userDesc\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"userProfileImgUrl\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"favCount\" type=\"tlong\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"retweetCount\" type=\"tlong\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"lang\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"place\" type=\"string\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"message\" type=\"text_en\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isSensitive\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isTruncated\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isFavorited\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isRetweeted\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"isRetweet\" type=\"boolean\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n &lt;field name=\"createdAt\" type=\"tdate\" indexed=\"true\" stored=\"true\" required=\"true\" multiValued=\"false\" /&gt;\n</figcaption></figure><p>Upload the configset to solr and create a collection:</p>\n<figure class=\"code\"><figcaption>1\n./server/scripts/cloud-scripts/zkcli.sh -cmd upconfig -zkhost localhost:2181 -confdir tweet-schema -confname tweet-schema\n</figcaption></figure><p>Create the collection</p>\n<figure class=\"code\"><figcaption>1\nhttp://localhost:8983/solr/admin/collections?action=create&amp;name=tweet&amp;collection.configName=tweet-schema&amp;numShards=1\n</figcaption></figure><p>Next create a <code>SolrWriter</code> actor which will recieve a <code>Tweet</code> message from a <code>KafkaSolrComsumer</code> (which we will define next), convert it to <code>SolrInputDocument</code> and send it to solr</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n39\n40\n41\n42\n43\n44\n45\n46\n47\n48\n49\n50\nclass SolrWriter(zkHost: String, collection: String, commitAfterBatch: Boolean) extends Actor with Logging {\n  val client = new CloudSolrClient.Builder().withZkHost(zkHost).build()\n  client.setDefaultCollection(collection)\n  ...\n  var batch = List[SolrInputDocument]()\n  val MAX_BATCH_SIZE = 100\n  override def receive: Receive = {\n    case doc: Tweet =&gt;\n      val solrDoc = new SolrInputDocument()\n      solrDoc.setField(\"id\", doc.id)\n      solrDoc.setField(\"username\", doc.username)\n      ...\n      batch = solrDoc :: batch\n      if (batch.size &gt; MAX_BATCH_SIZE) indexBatch()\n    case FlushBuffer =&gt;\n      indexBatch()\n    case _ =&gt;\n      logger.warn(\"Unknown message\")\n  }\n  def indexBatch(): Boolean = {\n    try {\n      logger.info(\"Flushing batch\")\n      client.add(batch.asJavaCollection)\n      batch = List[SolrInputDocument]()\n      if (commitAfterBatch) client.commit()\n      true\n    } catch {\n      case ex: Exception =&gt;\n        logger.error(s\"Failed to indexing solr batch. Exception is \" + ex.getMessage)\n        ex.printStackTrace()\n        batch = List[SolrInputDocument]()\n        false\n    }\n  }\n  ...\n}\n</figcaption></figure><p>Now we need to define a kafka consumer which will read data from solr and send it to <code>SolrWriter</code></p>\n<h3>Kafka Consumer</h3>\n<p>Consumer will read data from kafka, deserialize it using avro schema, and convert it to <code>Tweet</code> type and forward the message to a destination actor. We will keep the consumer generic so that any destination actor(solr or cassandra) can be passed to it.</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\nclass KafkaTweetConsumer(zkHost:String, groupId:String, topic:String, destination:ActorRef) extends Actor with Logging {\n  ...\n  def read() = try {\n    ...\n    destination ! tweet   //destination will be either solr or cassandra\n    ...\n  }\n}\n</figcaption></figure><p>Create consumer and avro schema object</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\nprivate val props = new Properties()\nprops.put(\"group.id\", groupId)\nprops.put(\"zookeeper.connect\", zkHost)\nprops.put(\"auto.offset.reset\", \"smallest\")\nprops.put(\"consumer.timeout.ms\", \"120000\")\nprops.put(\"auto.commit.interval.ms\", \"10000\")\nprivate val consumerConfig = new ConsumerConfig(props)\nprivate val consumerConnector = Consumer.create(consumerConfig)\nprivate val filterSpec = new Whitelist(topic)\nval schemaString = Source.fromURL(getClass.getResource(\"/tweet.avsc\")).mkString\nval schema = new Schema.Parser().parse(schemaString)\n</figcaption></figure><p>Convert binary data to <code>Tweet</code> type using avro</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\nprivate def getTweet(message: Array[Byte]): Tweet = {\n    val reader = new SpecificDatumReader[GenericRecord](schema)\n    val decoder = DecoderFactory.get().binaryDecoder(message, null)\n    val record = reader.read(null, decoder)\n    val tweet = Tweet(\n      id = record.get(\"id\").toString,\n      username = record.get(\"username\").toString,\n      ...\n    )\n    tweet\n  }\n</figcaption></figure><p>Start consuming from kafka and send messages to destination, Solr in this specific case.</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\nval streams = consumerConnector.createMessageStreamsByFilter(filterSpec, 1,new DefaultDecoder(), new DefaultDecoder())(0)\nlazy val iterator = streams.iterator()\nwhile (iterator.hasNext()) {\n  val tweet = getTweet(iterator.next().message())\n  //logger.info(\"Consuming tweet: \" + tweet.id)\n  destination ! tweet\n}\n</figcaption></figure><p>You shoud now start seeing data in solr:</p>\n<figure class=\"code\"><figcaption>1\nhttp://localhost:8983/solr/tweet/select?q=*:*&amp;wt=json&amp;rows=1\n</figcaption></figure><figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\n34\n35\n36\n37\n38\n{\n   \"responseHeader\":{\n      \"zkConnected\":true,\n      \"status\":0,\n      \"QTime\":1,\n      \"params\":{\n         \"q\":\"*:*\",\n         \"rows\":\"1\",\n         \"wt\":\"json\"\n      }\n   },\n   \"response\":{\n      \"numFound\":42,\n      \"start\":0,\n      \"docs\":[\n         {\n            \"id\":\"923302396612182016\",\n            \"username\":\"Tawanna Kessler\",\n            \"userId\":898322458742337536,\n            \"userScreenName\":\"tawanna_kessler\",\n            \"userDesc\":\"null\",\n            \"userProfileImgUrl\":\"http://pbs.twimg.com/profile_images/898323854417940484/lke3BSjt_normal.jpg\",\n            \"favCount\":0,\n            \"retweetCount\":183,\n            \"lang\":\"en\",\n            \"place\":\"null\",\n            \"message\":\"RT @craigbrownphd: Two upcoming webinars: Two new Microsoft webinars are taking place over the next week that may… https://t.co/SAb9CMmVXY…\",\n            \"isSensitive\":false,\n            \"isTruncated\":false,\n            \"isFavorited\":false,\n            \"isRetweeted\":false,\n            \"isRetweet\":true,\n            \"createdAt\":\"2017-10-26T03:07:00Z\",\n            \"_version_\":1582267022370144256\n         }\n      ]\n   }\n}\n</figcaption></figure><h2>Querying solr data with banana</h2>\n<p>Banana is a data visualization tool that uses solr for data analysis and display. It can be run in same container as solr. Here’s how to set it up:</p>\n<p>Here’s how to set it up for our tweet data. We will run it in same container as solr:</p>\n<p>Download banana and put it in solr’s webapp direcory</p>\n<figure class=\"code\"><figcaption>1\n2\ncd SOLR_HOME/server/solr-webapp/webapp/\ngit clone https://github.com/lucidworks/banana --depth 1\n</figcaption></figure><p>To save dashboards and setting, banana expects a collection named <code>banana-int</code>. Lets go ahead and create it. Configset for that collection can be obtained found in <code>banana/resources/banana-int-solr-5.0/</code>.</p>\n<p>Upload banana config to zookeeper</p>\n<figure class=\"code\"><figcaption>1\n$SOLR_HOME/server/scripts/cloud-scripts/zkcli.sh -cmd upconfig -zkhost localhost:2181 -confdir banana-int-solr-5.0/conf/ -confname banana\n</figcaption></figure><p>Create the collection</p>\n<figure class=\"code\"><figcaption>1\nhttp://localhost:8983/solr/admin/collections?action=create&amp;name=banana-int&amp;collection.configName=banana&amp;numShards=1\n</figcaption></figure><p>Navigate to banana UI at <code>http://localhost:8983/solr/banana/src/index.html</code> and change the collection in settings to point to <code>tweet</code> collection in</p>\n<p>Here’s what it will look like for our tweets data:</p>\n<p><img src=\"http://saumitra.me/images/posts/banana1.png\" alt=\"image\" /></p>\n<p>Next we will create a cassandra consumer.</p>\n<h2>Putting data in cassandra</h2>\n<ul><li>Download cassandra from <a href=\"http://archive.apache.org/dist/cassandra/3.0.12/apache-cassandra-3.0.12-bin.tar.gz\">http://archive.apache.org/dist/cassandra/3.0.12/apache-cassandra-3.0.12-bin.tar.gz</a> and uncompress it</li>\n<li>Run <code>bin/cassandra</code> to start it</li>\n</ul><p>We need to first create a keyspace and table for storing tweets</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\nCREATE KEYSPACE twitter WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\nCREATE TABLE twitter.tweet (\n  topic text,\n  id text,\n  username text,\n  userId text,\n  userScreenName text,\n  userDesc text,\n  userProfileImgUrl text,\n  favCount bigint,\n  retweetCount bigint,\n  lang text,\n  place text,\n  message text,\n  isSensitive boolean,\n  isTruncated boolean,\n  isFavorited boolean,\n  isRetweeted boolean,\n  isRetweet boolean,\n  createdAt timestamp,\n  creationDate timestamp,\n  PRIMARY KEY ((topic, creationDate), username, id)\n)\n</figcaption></figure><p>Then we will create a <code>CassWriter</code> actor similar to solr one which will accept a tweet message and write it to cassandra.</p>\n<p>Connect to cluster.</p>\n<figure class=\"code\"><figcaption>1\n2\nlazy val cluster = Cluster.builder().addContactPoint(seeds).build()\nlazy val session = cluster.connect(keyspace)\n</figcaption></figure><p>Since we will be using same query repeatedly to insert data with different parameters, hence we will use prepared statement to improve performance:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\nlazy val prepStmt = session.prepare(s\"INSERT INTO $cf (\" +\n      \"topic, id, username, userId, userScreenName, userDesc, userProfileImgUrl, favCount,\" +\n      \"retweetCount, lang, place, message, isSensitive, isTruncated, isFavorited, isRetweeted,\" +\n      \"isRetweet, createdAt, creationDate\" +\n      \") values (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\")\n</figcaption></figure><p>Take <code>Tweet</code>, create a <code>BoundStatement</code> by setting values for all fields and write it to cassandra</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\n24\n25\n26\n27\n28\n29\n30\n31\n32\n33\ndef writeToCass(t:Tweet) = {\n  try {\n    val boundStmt = prepStmt.bind()\n      .setString(\"topic\", topic)\n      .setString(\"id\",t.id)\n      .setString(\"username\", t.username)\n      .setString(\"userId\", t.userId.toString)\n      .setString(\"userScreenName\",t.userScreenName)\n      .setString(\"userDesc\",t.userDesc)\n      .setString(\"userProfileImgUrl\",t.userProfileImgUrl)\n      .setLong(\"favCount\",t.favCount)\n      .setLong(\"retweetCount\",t.retweetCount)\n      .setString(\"lang\",t.lang)\n      .setString(\"place\",t.place)\n      .setString(\"message\",t.message)\n      .setBool(\"isSensitive\",t.isSensitive)\n      .setBool(\"isTruncated\",t.isTruncated)\n      .setBool(\"isFavorited\",t.isFavorited)\n      .setBool(\"isRetweeted\",t.isRetweeted)\n      .setBool(\"isRetweet\",t.isRetweet)\n      .setTimestamp(\"createdAt\", new Date(t.createdAt))\n      .setTimestamp(\"creationDate\", new Date(t.createdAt))\n    session.execute(boundStmt)\n  } catch {\n    case ex: Exception =&gt;\n      logger.error(\"C* insert exception. Message: \" + ex.getMessage)\n  }\n}\n</figcaption></figure><p>We will create a new instance of this actor</p>\n<figure class=\"code\"><figcaption>1\nval cassWriter = system.actorOf(Props(new CassWriter(cassSeeds, cassKeyspace, cassCf, topic)), name = \"cass_writer\")\n</figcaption></figure><p>And then create a new <code>KafkaTweetConsumer</code> whose destination will be this <code>cassWriter</code> actor</p>\n<figure class=\"code\"><figcaption>1\n2\nval cassConsumer = system.actorOf(Props(\n    new KafkaTweetConsumer(zkHostKafka, \"tweet-cass-consumer\", topic, cassWriter)), name = \"cass_consumer\")\n</figcaption></figure><p>You should start seeing data in cassandra</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\ncqlsh&gt; select creationdate, userscreenname, lang, message from twitter.tweet limit 1;\n creationdate             | userscreenname | lang | message\n--------------------------+----------------+------+--------------------------------------------------------------------------------------------------------------------------------------------\n 2017-10-25 21:56:30+0000 |   alevergara78 |   en | RT @HomesAtMetacoda: data in motion &gt;&gt; Online learning: #MachineLearning’s secret for #bigdata via\\n@SASsoftware https://t.co/eGbAumJzEt…\n</figcaption></figure><p>Next we will setup spark and use it to query cassandra data.</p>\n<h2>Query cassandra data with spark</h2>\n<p>We will use datastax spark cassandra connector <a href=\"https://github.com/datastax/spark-cassandra-connector.\">https://github.com/datastax/spark-cassandra-connector.</a> Download the correct connection version jar and place it in lib directory of your project:</p>\n<p>First thing which we need is a spark context</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\n9\n10\nval CASS_SEEDS = \"127.0.0.1\"\nval SPARK_MASTER = \"spark://sam-ub:7077\"\nval conf = new SparkConf(true)\n  .set(\"spark.cassandra.connection.host\", CASS_SEEDS)\n  .setJars(Seq(\"lib/spark-cassandra-connector-assembly-2.0.0.jar\"))\n  .setMaster(SPARK_MASTER)\n  .setAppName(\"cass_query\")\nlazy val sc = new SparkContext(conf)\n</figcaption></figure><p>Then you can query and apply different aggregrations. This query will be picked up as a spark job and exectuted on you spark cluster:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n6\n7\n8\nval data = sc.cassandraTable(\"twitter\", \"tweets\")\n             .select(\"topic\", \"creationdate\", \"retweetcount\", \"id\", \"isretweet\")\n             .where(\"topic = 'tweets' and creationdate = '2017-10-25 20:15:05+0000'\")\n             .groupBy(_.getLong(\"retweetcount\"))\n             .map(r =&gt; (r._1, r._2.size))\n             .collect()\nlogger.info(\"Count of rows = \" + data)\n</figcaption></figure><p>If job is successfull, you will see the result:</p>\n<figure class=\"code\"><figcaption>1\nCount of rows = 38\n</figcaption></figure><h2>Visulizing cassandra data with zeppelin</h2>\n<p>Zeppelin is a web-based notebook that can be used for interactive data analytics on cassandra data using spark.</p>\n<p>Download the binary from <a href=\"https://zeppelin.apache.org/download.html\">https://zeppelin.apache.org/download.html</a> and uncompress it.\nDefault port used by it is <code>8080</code> which conflicts with spark master web ui port, so change the port in <code>conf/zeppelin-site.xml</code>.</p>\n<p>Create a new notebook and select <code>spark interpreter</code></p>\n<p>Create a view of our <code>tweet</code> table from cassandra</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n4\n5\n%spark.sql\ncreate temporary view mytweets\nusing org.apache.spark.sql.cassandra\noptions (keyspace \"twitter\", table \"tweet\")\n</figcaption></figure><p>We can now run aggregations or other analytics queries on this view:</p>\n<figure class=\"code\"><figcaption>1\n2\n3\n%spark.sql\nselect lang, count(*) as occur from mytweets where lang != 'und' group by lang order by occur desc limit 10\n</figcaption></figure><p>Here’s what output of above query will look like:</p>\n<p><img src=\"http://saumitra.me/images/posts/zepp1.png\" alt=\"image\" /></p>\n<h2>Conclusion</h2>\n<p>I hope you got the idea of how to get started with creating a search and analytics pipeline.</p>",
        "created_at": "2018-05-24T02:01:47+0000",
        "updated_at": "2018-05-24T02:02:04+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 19,
        "domain_name": "saumitra.me",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9788"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 995,
            "label": "testing",
            "slug": "testing"
          }
        ],
        "is_public": false,
        "id": 9764,
        "uid": null,
        "title": "smartcat-labs/berserker",
        "url": "https://github.com/smartcat-labs/berserker",
        "content": "<p>Load generator with modular architecture.</p><p><a href=\"https://travis-ci.org/smartcat-labs/berserker\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/8ac158e9462f9535c5012441dd3a9bf79a8b706a/68747470733a2f2f7472617669732d63692e6f72672f736d6172746361742d6c6162732f6265727365726b65722e7376673f6272616e63683d6d6173746572\" alt=\"Build Status\" data-canonical-src=\"https://travis-ci.org/smartcat-labs/berserker.svg?branch=master\" /></a>\n<a href=\"https://bintray.com/smartcat-labs/maven/berserker/_latestVersion\" rel=\"nofollow\"> <img src=\"https://camo.githubusercontent.com/5ab44ddb3c261cc5f0e932342329abb69ff7e3ca/68747470733a2f2f6170692e62696e747261792e636f6d2f7061636b616765732f736d6172746361742d6c6162732f6d6176656e2f6265727365726b65722f696d616765732f646f776e6c6f61642e737667\" alt=\"Download\" data-canonical-src=\"https://api.bintray.com/packages/smartcat-labs/maven/berserker/images/download.svg\" /></a></p><p>Berserker is designed to be modular from beginning as illustrated on the following diagram.</p><p><a target=\"_blank\" href=\"https://github.com/smartcat-labs/berserker/blob/dev/images/core-design.png\"><img src=\"https://github.com/smartcat-labs/berserker/raw/dev/images/core-design.png\" alt=\"Core Design\" /></a></p><p>Rate generator controls the rate at which load generator operates, rate is expressed on per second basis, or better say, number of impulses which will be generated within one second. Each time impulse is generated load generator fetches data from data source and passes it to worker. Since those are simple interfaces, it is easy to add additional module implementing either data source, worker and even rate generator.\nFollowing diagram represents possible modules for Load Generator of which some are already implemented.</p><p><a target=\"_blank\" href=\"https://github.com/smartcat-labs/berserker/blob/dev/images/architecture.png\"><img src=\"https://github.com/smartcat-labs/berserker/raw/dev/images/architecture.png\" alt=\"Architecture\" /></a></p><p>Berserker is designed as command line tool, but having modular architecture makes it easy to use it as Java library as well.</p><h3>Berserker Commons</h3><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-commons\">Berserker Commons</a> holds interface for core and configuration and it provides signature all the modules need to confront to be able to work together.</p><h3>Berserker Core</h3><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-core\">Berserker Core</a> contains load generator implementation, and common implementations of data source, rate generator and worker.</p><h3>Berserker Runner</h3><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-runner\">Berserker Runner</a> represents runnable jar where desired data source, rate generator and worker can be specified within YAML configuration.\nFollowing section illustrates YAML configuration example.</p><div class=\"highlight highlight-source-yaml\"><pre>load-generator-configuration:\n  data-source-configuration-name: Ranger\n  rate-generator-configuration-name: default\n  worker-configuration-name: Cassandra\n  metrics-reporter-configuration-name: JMX\n  thread-count: 10\n  queue-capacity: 100000\ndata-source-configuration:\n  values:\n    id: uuid()\n    firstName: random(['Peter', 'Mike', 'Steven', 'Joshua', 'John', 'Brandon'])\n    lastName: random(['Smith', 'Johnson', 'Williams', 'Davis', 'Jackson', 'White', 'Lewis', 'Clark'])\n    age: random(20..45)\n    email: string('{}@domain.com', randomLengthString(5))\n    statement:\n      consistencyLevel: ONE\n      query: string(\"INSERT INTO person (id, first_name, last_name, age, email) VALUES ({}, '{}', '{}', {}, '{}');\", $id, $firstName, $lastName, $age, $email)\n  output: $statement\nrate-generator-configuration:\n  rates:\n    r: 1000\n  output: $r\nworker-configuration:\n  connection-points: 0.0.0.0:32770\n  keyspace: my_keyspace\n  async: false\n  bootstrap-commands:\n    - \"CREATE KEYSPACE IF NOT EXISTS my_keyspace WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};\"\n    - USE my_keyspace;\n    - CREATE TABLE IF NOT EXISTS person (id uuid, first_name text, last_name text, age int, email text, primary key (id));\nmetrics-reporter-configuration:\n  domain: berserker\n  filter:</pre></div><p>Main part of configuration is <code>load-generator-configuration</code> where concrete modules which will be used for data source, rate generator and worker need to be specified. After <code>load-generator-configuration</code> section, there should be exactly one section for data source, rate generator and worker.\nEach section is allowed to contain module specific configuration as configuration interpretation will be done by module itself.\nIn order for berserker-runner to be able to find particular module, each module jar must be in classpath.</p><h4>Rate generator configuration</h4><p>Documentation on rate generator configuration can be found <a href=\"https://github.com/smartcat-labs/berserker/blob/dev/rate-generator-configuration.md\">here</a>.</p><h3>Modules</h3><p>List of existing modules:</p><h4>Berserker Ranger</h4><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-ranger\">Berserker Ranger</a> is Ranger data source implementation.</p><h4>Berserker Kafka</h4><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-kafka\">Berserker Kafka</a> is worker implementation which sends messages to Kafka cluster.</p><h4>Berserker Cassandra</h4><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-cassandra\">Berserker Cassandra</a> is worker implementation which executes CQL statements on Cassandra cluster.</p><h4>Berserker HTTP</h4><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-http\">Berserker HTTP</a> is worker implementation which sends HTTP request on configured endpoint.</p><h4>Berserker RabbitMQ</h4><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-rabbitmq\">Berserker RabbitMQ</a> is worker implementation which sends AMQP messages to RabbitMQ.</p><h4>Berserker MQTT</h4><p><a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-mqtt\">Berserker MQTT</a> is worker implementation which publishes messages to MQTT broker.</p><h3>Usage</h3><p>Berserker can be used either as a library or as a stand-alone command line tool.</p><h4>Library usage</h4><p>Artifact can be fetched from bintray.</p><p>Add following <code>repository</code> element to your <code>&lt;repositories&gt;</code> section in <code>pom.xml</code>:</p><div class=\"highlight highlight-text-xml\"><pre>&lt;repository&gt;\n  &lt;id&gt;bintray-smartcat-labs-maven&lt;/id&gt;\n  &lt;name&gt;bintray&lt;/name&gt;\n  &lt;url&gt;https://dl.bintray.com/smartcat-labs/maven&lt;/url&gt;\n&lt;/repository&gt;</pre></div><p>Add the <code>dependency</code> element to your <code>&lt;dependencies&gt;</code> section in <code>pom.xml</code> depending which <code>artifact</code> and <code>version</code> you need:</p><div class=\"highlight highlight-text-xml\"><pre>&lt;dependency&gt;\n  &lt;groupId&gt;io.smartcat&lt;/groupId&gt;\n  &lt;artifactId&gt;artifact&lt;/artifactId&gt;\n  &lt;version&gt;version&lt;/version&gt;\n&lt;/dependency&gt;</pre></div><h4>Command line tool usage</h4><ul><li>Download latest <a href=\"https://bintray.com/smartcat-labs/maven/berserker\" rel=\"nofollow\">Berserker Runner</a> version.</li>\n<li>Create config file (example can be found <a href=\"https://github.com/smartcat-labs/berserker/blob/dev/berserker-runner/src/example/resources/ranger-cassandra.yml\">here</a>).</li>\n<li>Run following command: <code>java -jar berserker-runner-&lt;version&gt;.jar -c &lt;path_to_config_file&gt;</code></li>\n<li>If you need to specify logging options, you can run berserker this way: <code>java -jar -Dlogback.configurationFile=&lt;path to logback.xml&gt; berserker-runner-&lt;version&gt;.jar -c &lt;path_to_config_file&gt;</code></li>\n</ul>",
        "created_at": "2018-05-17T18:23:25+0000",
        "updated_at": "2018-07-07T19:50:48+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/12434092?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9764"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 11,
            "label": "database",
            "slug": "database"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          }
        ],
        "is_public": false,
        "id": 9743,
        "uid": null,
        "title": "KairosDB",
        "url": "https://kairosdb.github.io/",
        "content": "<div class=\"jumbotron\"><div class=\"container\"><img src=\"https://kairosdb.github.io/img/kairosdb_dark.png\" alt=\"KairosDB logo\" /><p class=\"subtitle\">Fast Time Series Database on Cassandra.</p></div></div>\n\t<div class=\"container\"><div class=\"features\"><p></p><h2>Rich features to answer your needs</h2>\n\t\t\t\t<hr class=\"blue-divider\" /><div class=\"row  text-center\"><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-cogs fa-stack-1x fa-inverse\">CollectorsData can be pushed in KairosDB via multiple\n\t\t\t\t\t\tprotocols : Telnet, Rest, Graphite. Other mechanisms such as\n\t\t\t\t\t\tplugins can also be used\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/PushingData.html\" role=\"button\">Learn more »</a>\n\t\t\t\t\t</i></i></div><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-database fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tStorage\n\t\t\t\t\t\n\t\t\t\t\t\tKairosDB stores time series in <a href=\"http://cassandra.apache.org/\">Cassandra</a>, the popular\n\t\t\t\t\t\tand performant NoSQL datastore. The schema consists of 3 column\n\t\t\t\t\t\tfamilies...\n\t\t\t\t\t\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/CassandraSchema.html\" role=\"button\">Learn\n\t\t\t\t\t\t\tmore »</a>\n\t\t\t\t\t</i></i></div><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-code fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tRest API\n\t\t\t\t\tThis API provides operations to list existing\n\t\t\t\t\t\tmetric names, list tag names and values, store metric data points,\n\t\t\t\t\t\tand query for metric data points.\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/restapi/Overview.html\" role=\"button\">Learn\n\t\t\t\t\t\t\tmore »</a>\n\t\t\t\t\t</i></i></div><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-line-chart fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tWeb UI\n\t\t\t\t\tWith a default install, KairosDB serve up a\n\t\t\t\t\t\tquery page whereby you can query data within the data store. It's\n\t\t\t\t\t\tdesigned primarily for development purposes.\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/WebUI.html\" role=\"button\">Learn more »</a>\n\t\t\t\t\t</i></i></div></div><div class=\"row text-center\"><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-cog fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tAggregators\n\t\t\t\t\tAggregators perform an operation on data points\n\t\t\t\t\t\tand down samples. Standard functions like min, max, sum, count,\n\t\t\t\t\t\tmean and more are available\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/restapi/QueryMetrics.html\" role=\"button\">Learn\n\t\t\t\t\t\t\tmore »</a>\n\t\t\t\t\t</i></i></div><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-exchange fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tTools\n\t\t\t\t\tImport and export is available on the KairosDB\n\t\t\t\t\t\tserver from the command line. Internal metrics to the data store\n\t\t\t\t\t\tcan monitor the server’s performance\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/ImportExport.html\" role=\"button\">Learn\n\t\t\t\t\t\t\tmore »</a>\n\t\t\t\t\t</i></i></div><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-coffee fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tClient library\n\t\t\t\t\tThe KairosDB client is a Java library, using\n\t\t\t\t\t\tthe HttpClient class, that makes sending metrics and querying the\n\t\t\t\t\t\tKairosDB server simple.\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://github.com/kairosdb/kairosdb-client/\" role=\"button\">Learn\n\t\t\t\t\t\t\tmore »</a>\n\t\t\t\t\t</i></i></div><div class=\"col-md-3\"><i class=\"fa fa-circle fa-stack-2x\"><i class=\"fa fa-plug fa-stack-1x fa-inverse\">\n\t\t\t\t\t\tPlugins\n\t\t\t\t\tKairosDB can be extended in various ways (data\n\t\t\t\t\t\tpoint listeners, data stores, protocol handlers,...) using plugins\n\t\t\t\t\t\tbased on Guice.\n\t\t\t\t\t\t<a class=\"btn btn-default\" href=\"https://kairosdb.github.io/docs/build/html/kairosdevelopment/Plugins.html\" role=\"button\">Learn more »</a>\n\t\t\t\t\t</i></i></div></div></div></div>",
        "created_at": "2018-05-15T01:40:30+0000",
        "updated_at": "2018-05-15T01:40:46+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "kairosdb.github.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9743"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 93,
            "label": "data",
            "slug": "data"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 931,
            "label": "data.engineering",
            "slug": "data-engineering"
          }
        ],
        "is_public": false,
        "id": 9723,
        "uid": null,
        "title": "Getting started with the Kafka Connect Cassandra Source",
        "url": "https://medium.com/walmartlabs/getting-started-with-the-kafka-connect-cassandra-source-e6e06ec72e97",
        "content": "<p id=\"ebf2\" class=\"graf graf--p graf-after--h3\">This post will look at how to setup and tune the <a href=\"http://lenses.stream/connectors/source/cassandra.html\" data-href=\"http://lenses.stream/connectors/source/cassandra.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Cassandra Source connector</a> that is available from <a href=\"http://www.landoop.com/\" data-href=\"http://www.landoop.com/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Landoop</a>. The Cassandra Source connector is used to read data from a Cassandra table, writing the contents into a Kafka topic using only a configuration file. This enables data that has been saved to be easily turned into an event stream.</p><figure id=\"f902\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*Fi0Zp7l0lbj-y9QT.png\" data-width=\"720\" data-height=\"240\" data-action=\"zoom\" data-action-value=\"0*Fi0Zp7l0lbj-y9QT.png\" src=\"https://cdn-images-1.medium.com/max/1600/0*Fi0Zp7l0lbj-y9QT.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">all logos are trademark of Apache Foundation</figcaption></div></figure><p id=\"85be\" class=\"graf graf--p graf-after--figure\">In our example we will be capturing data representing a pack (i.e. a large box) of items being shipped. Each pack is pushed to consumers in a JSON format on a Kafka topic.</p><h3 id=\"d7da\" class=\"graf graf--h3 graf-after--p\">The Cassandra data model and Cassandra Source connector</h3><p id=\"10c8\" class=\"graf graf--p graf-after--h3\">Modeling data in Cassandra must be done around the queries that are needed to access the data (see <a href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" data-href=\"https://www.datastax.com/dev/blog/basic-rules-of-cassandra-data-modeling\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">this article</a> for details). Typically this means that there will be one table for each query and data (in our case about the pack) will be duplicated across numerous tables.</p><p id=\"afac\" class=\"graf graf--p graf-after--p\">Regardless of the other tables used for the product, the Cassandra Source connector needs a table that will allow us to query for data using a time range. The connector is designed around its ability to generate a CQL query based on configuration. It uses this query to retrieve data from the table that is available within a configurable time range. Once all of this data has been published, Kafka Connect will mark the upper end of the time range as an offset. The connector will then query the table for more data using the next time range starting with the date/time stored in the offset. We will look at how to configure this later. For now we want to focus on the constraints for the table. Since Cassandra doesn’t support joins, the table we are pulling data from must have all of the data that we want to put onto the Kafka topic. Data in other tables will not be available to Kafka Connect.</p><p id=\"361f\" class=\"graf graf--p graf-after--p\">In it’s simplest form a table used by the Cassandra Source connector might look like this:</p><pre id=\"e45f\" class=\"graf graf--pre graf-after--p\">CREATE TABLE IF NOT EXISTS “pack_events” (<br />event_id TEXT, <br />event_ts TIMESTAMP, <br />event_data TEXT, <br />PRIMARY KEY ((event_id),event_ts));</pre><p id=\"8ca8\" class=\"graf graf--p graf-after--pre\">The <code class=\"markup--code markup--p-code\">event_id</code> is the partition key. This is used by Cassandra to determine which nodes in the cluster will store the data. The <code class=\"markup--code markup--p-code\">event_ts</code> is part of the cluster key. It determines the order of the data within the partition (see <a href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" data-href=\"https://www.datastax.com/dev/blog/the-most-important-thing-to-know-in-cassandra-data-modeling-the-primary-key\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">this article for details</a>). It is also the column that is used by the Cassandra source connector to manage time ranges. In this example, the <code class=\"markup--code markup--p-code\">event_data</code> column stores the JSON representation of the pack.</p><p id=\"92bc\" class=\"graf graf--p graf-after--p\">This is not the only structure for a table that will work. The table that is queried by the Cassandra Source connector can use numerous columns to represent the partition key and the data. However, <strong class=\"markup--strong markup--p-strong\">the connector requires a single time based column</strong> (either <code class=\"markup--code markup--p-code\">TIMESTAMP</code> or <code class=\"markup--code markup--p-code\">TIMEUUID</code>) in order to work correctly.</p><p id=\"10c3\" class=\"graf graf--p graf-after--p\">This would be an equally valid table for use with the Cassandra Source connector.</p><pre id=\"3608\" class=\"graf graf--pre graf-after--p\">CREATE TABLE IF NOT EXISTS “kc_events” (<br />event_id1 TEXT, <br />event_id2 TEXT, <br />event_ts TIMEUUID, <br />event_data1 TEXT, <br />event_data2 TEXT, <br />PRIMARY KEY ((event_id1, event_id2)));</pre><p id=\"6eb6\" class=\"graf graf--p graf-after--pre\">The most efficient way to access data in this table is to query for data with the partition key. This would allow Cassandra to quickly identify the node containing the data we are interested in.</p><pre id=\"1c5f\" class=\"graf graf--pre graf-after--p\">SELECT * FROM pack_events WHERE event_id = “1234”;</pre><p id=\"9ca8\" class=\"graf graf--p graf-after--pre\">However, the Cassandra Source connector has no way of knowing the ids of the data that it will need to publish to a Kafka topic. That is why it uses a time range.</p><p id=\"89b1\" class=\"graf graf--p graf-after--p\">The reason we can’t use the <code class=\"markup--code markup--p-code\">event_ts</code> as the partition key is because Cassandra does not support these operators (&gt;, &gt;=, &lt;=, &lt;) on the partition key when querying. And without these we would not be able to query across date/time ranges (see <a href=\"https://www.datastax.com/dev/blog/a-deep-look-to-the-cql-where-clause\" data-href=\"https://www.datastax.com/dev/blog/a-deep-look-to-the-cql-where-clause\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">this article for details</a>).</p><p id=\"88ff\" class=\"graf graf--p graf-after--p\">There’s just one more thing. If we tried to run the following query it would fail.</p><pre id=\"a243\" class=\"graf graf--pre graf-after--p\">SELECT * FROM pack_events <br />WHERE event_ts &gt; ‘2018–01–22T20:28:20.869Z’ <br />AND event_ts &lt;= '2018-01-22T20:28:50.869Z';</pre><p id=\"25fc\" class=\"graf graf--p graf-after--pre\">The connector must supply the <code class=\"markup--code markup--p-code\">ALLOW FILTERING</code> option to the end of this query for it to work. This addition allows Cassandra to search all of the nodes in the cluster for the data in the specified time range (see<a href=\"https://www.datastax.com/dev/blog/allow-filtering-explained-2\" data-href=\"https://www.datastax.com/dev/blog/allow-filtering-explained-2\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"> this article for details</a>).</p><h3 id=\"3efd\" class=\"graf graf--h3 graf-after--p\">Configuring the connector: KCQL basics</h3><p id=\"0b92\" class=\"graf graf--p graf-after--h3\">The Landoop connectors are configured using Kafka Connect Query Language (KCQL). This provides a concise and consistent way to configure the connectors (at least the ones from Landoop). The KCQL and other basic properties are provided via a JSON formatted property file.</p><p id=\"f64d\" class=\"graf graf--p graf-after--p\">For the sake of this post, let’s create a file named <code class=\"markup--code markup--p-code\">connect-cassandra-source.json</code>.</p><pre id=\"a5e0\" class=\"graf graf--pre graf-after--p\">{ <br />“name”: “packs”, <br />“config”: { <br />“tasks.max”: “1”, <br />“connector.class”: … </pre><p id=\"46c8\" class=\"graf graf--p graf-after--pre\">The <code class=\"markup--code markup--p-code\">name</code> of the connector needs to be unique across all the connectors installed into Kafka Connect.</p><p id=\"bad2\" class=\"graf graf--p graf-after--p\">The <code class=\"markup--code markup--p-code\">connector.class</code> is used to specify which connector is being used.​​</p><ul class=\"postList\"><li id=\"036c\" class=\"graf graf--li graf-after--p\"><code class=\"markup--code markup--li-code\">com.datamountaineer.streamreactor.connect.cassandra.source.CassandraSourceConnector</code></li></ul><p id=\"ebc5\" class=\"graf graf--p graf-after--li\">The next set of configuration (shown below) is used to specify the information needed to connect to the Cassandra cluster and which keyspace to use.</p><ul class=\"postList\"><li id=\"f802\" class=\"graf graf--li graf-after--p\"><code class=\"markup--code markup--li-code\">​connect.cassandra.contact.points</code></li><li id=\"a421\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.port</code></li><li id=\"1a09\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.username</code></li><li id=\"978e\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.password</code></li><li id=\"a2b0\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.consistency.level</code></li><li id=\"f3e5\" class=\"graf graf--li graf-after--li\"><code class=\"markup--code markup--li-code\">connect.cassandra.key.space</code></li></ul><pre id=\"86de\" class=\"graf graf--pre graf-after--li\">{ <br />“name”: “packs”, <br />“config”: { <br />“tasks.max”: “1”, <br />“connector.class”: “com.datamountaineer.streamreactor.connect.cassandra.source.CassandraSourceConnector”, <br />“connect.cassandra.contact.points”: “localhost”,    <br />“connect.cassandra.port”: 9042, <br />“connect.cassandra.username”: “cassandra”,   <br />“connect.cassandra.password”: “cassandra”,<br />“connect.cassandra.consistency.level”: “LOCAL_ONE”,<br />“connect.cassandra.key.space”: “blog”, “connect.cassandra.import.mode”: “incremental”, <br />“connect.cassandra.kcql”: “INSERT INTO test_topic SELECT event_data, event_ts FROM pack_events IGNORE event_ts PK event_ts WITHUNWRAP INCREMENTALMODE=TIMESTAMP”, … <br />} <br />}</pre><p id=\"682e\" class=\"graf graf--p graf-after--pre\">There are two values for the <code class=\"markup--code markup--p-code\">connect.cassandra.import.mode</code>. Those are <code class=\"markup--code markup--p-code\">bulk</code> and <code class=\"markup--code markup--p-code\">incremental</code>. The <code class=\"markup--code markup--p-code\">bulk</code> option will query everything in the table <em class=\"markup--em markup--p-em\">every time</em> that the Kafka Connect polling occurs. We will set this to <code class=\"markup--code markup--p-code\">incremental</code>.</p><p id=\"7bdb\" class=\"graf graf--p graf-after--p\">The interesting part of the configuration is the <code class=\"markup--code markup--p-code\">connect.cassandra.kcql</code> property (shown above). The KCQL statement tells the connector which table in the Cassandra cluster to use, how to use the columns on the table, and where to publish the data.</p><p id=\"0cb8\" class=\"graf graf--p graf-after--p\">The first part of the KCQL statement tells the connector the name of the Kafka topic where the data will be published. In our case that is the topic named <code class=\"markup--code markup--p-code\">test_topic</code>.</p><pre id=\"7d9c\" class=\"graf graf--pre graf-after--p\">INSERT INTO test_topic</pre><p id=\"4e45\" class=\"graf graf--p graf-after--pre\">The next part of the KCQL statement tells the connector how to deal with the table. The <code class=\"markup--code markup--p-code\">SELECT/FROM</code> specifies the table to poll with the queries. It also specifies the columns whose values should be retrieved. The column that keeps track of the date/time must be part of the <code class=\"markup--code markup--p-code\">SELECT</code>statement. However, if we don't want that data as part of what we publish to the Kafka topic we can use the <code class=\"markup--code markup--p-code\">IGNORE.</code></p><pre id=\"a537\" class=\"graf graf--pre graf-after--p\">SELECT event_data, event_ts FROM pack_events IGNORE event_ts</pre><p id=\"af45\" class=\"graf graf--p graf-after--pre\">The next part of the statement, the <code class=\"markup--code markup--p-code\">PK</code>, tells the connector which of the columns is used to manage the date/time. This is considered the primary key for the connector.</p><pre id=\"ba12\" class=\"graf graf--pre graf-after--p\">PK event_ts WITHUNWRAP INCREMENTALMODE=”TIMESTAMP”</pre><p id=\"3009\" class=\"graf graf--p graf-after--pre\">The <code class=\"markup--code markup--p-code\">INCREMENTALMODE</code> tells the connector what the data type of the <code class=\"markup--code markup--p-code\">PK</code> column is. That is going to be either <code class=\"markup--code markup--p-code\">TIMESTAMP</code> or <code class=\"markup--code markup--p-code\">TIMEUUID</code>.</p><p id=\"43cc\" class=\"graf graf--p graf-after--p\">Finally, the <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> option tells the connector to publish the data to the topic as a String rather than as a JSON object.</p><p id=\"cf66\" class=\"graf graf--p graf-after--p\">For example, if we had the following value in the <code class=\"markup--code markup--p-code\">event_data</code> column:</p><pre id=\"69e1\" class=\"graf graf--pre graf-after--p\">{ “foo”:”bar” }</pre><p id=\"e638\" class=\"graf graf--p graf-after--pre\">We would want to publish this as seen above.</p><p id=\"462c\" class=\"graf graf--p graf-after--p\">Leaving the <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> option off will result in the following value being published to the topic.</p><pre id=\"4821\" class=\"graf graf--pre graf-after--p\">{ <br />“schema”: {<br />“type”: “struct”, <br />“fields”: [{ <br />“type”: “string”,<br />“optional”: true,<br />“field”: “event_data” <br />}],<br />“optional”: false, <br />“name”: “blog.pack_events” <br />}, <br />“payload”: { <br />“event_data”: “{\\”foo\\”:\\”bar\\”}” <br />} <br />}</pre><p id=\"b0d5\" class=\"graf graf--p graf-after--pre\">If we leave <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> off, when using the <code class=\"markup--code markup--p-code\">StringConverter</code> (more on that later) we would get the following:</p><pre id=\"b7c7\" class=\"graf graf--pre graf-after--p\">Struct:{event_data={“foo”:”bar\"}}</pre><p id=\"8d85\" class=\"graf graf--p graf-after--pre\">We will need to use the combination of <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> and the<code class=\"markup--code markup--p-code\">StringConverter</code> to get the result we want.</p><h3 id=\"38f1\" class=\"graf graf--h3 graf-after--p\">Configuring the connector: Tuning Parameters</h3><p id=\"c7f8\" class=\"graf graf--p graf-after--h3\">We’ll explore these in another post. But for now let’s start looking for data in our table with a starting date/time of today. We’ll also poll every second.</p><pre id=\"a759\" class=\"graf graf--pre graf-after--p\">{ <br />“name”: “packs”, <br />“config”: { <br />“tasks.max”: “1”,<br />… <br />“connect.cassandra.initial.offset”: “2018–01–22 00:00:00.0000000Z”, <br />“connect.cassandra.import.poll.interval”: 1000 <br />} <br />}</pre><h3 id=\"070d\" class=\"graf graf--h3 graf-after--pre\">Setting up the infrastructure</h3><p id=\"3e92\" class=\"graf graf--p graf-after--h3\">We will be using the following products:</p><ul class=\"postList\"><li id=\"20b6\" class=\"graf graf--li graf-after--p\">Apache Cassandra 3.11.1</li><li id=\"a854\" class=\"graf graf--li graf-after--li\">Apache Kafka and Kafka Connect 1.0</li><li id=\"0b78\" class=\"graf graf--li graf-after--li\">Landoop Cassandra Source 1.0</li></ul><h3 id=\"cd3f\" class=\"graf graf--h3 graf-after--li\">Installing Cassandra</h3><p id=\"15df\" class=\"graf graf--p graf-after--h3\">Installation instructions for Apache Cassandra can be found on the web (<a href=\"https://cassandra.apache.org/doc/latest/getting_started/installing.html\" data-href=\"https://cassandra.apache.org/doc/latest/getting_started/installing.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>). Once installed and started the cluster can be verified using the following command:</p><pre id=\"3564\" class=\"graf graf--pre graf-after--p\">nodetool -h [IP] status</pre><p id=\"f19b\" class=\"graf graf--p graf-after--pre\">this will generate a response as follows:</p><pre id=\"6a27\" class=\"graf graf--pre graf-after--p\">Datacenter: dc1<br />===============<br />Status=Up/Down<br />|/ State=Normal/Leaving/Joining/Moving<br />--  Address   Load       Tokens       Owns (effective)  Host ID   Rack<br />UN  10.x.x.x  96.13 GiB   64           39.6%            [UUID]    r6<br />UN  10.x.x.x  148.98 GiB  64           33.6%            [UUID]    r5<br />UN  10.x.x.x  88.08 GiB   64           36.4%            [UUID]    r5<br />UN  10.x.x.x  97.96 GiB   64           30.4%            [UUID]    r6<br />UN  10.x.x.x  146.89 GiB  64           33.2%            [UUID]    r7<br />UN  10.x.x.x  205.24 GiB  64           36.8%            [UUID]    r7</pre><h3 id=\"b219\" class=\"graf graf--h3 graf-after--pre\">Installing Kafka and Kafka Connect</h3><p id=\"c5fe\" class=\"graf graf--p graf-after--h3\">Kafka Connect is shipped and installed as part of Apache Kafka. Instructions for these are also available on the web (<a href=\"https://kafka.apache.org/quickstart\" data-href=\"https://kafka.apache.org/quickstart\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>).</p><ol class=\"postList\"><li id=\"1fbd\" class=\"graf graf--li graf-after--p\">Download the tar file (<a href=\"https://kafka.apache.org/downloads\" data-href=\"https://kafka.apache.org/downloads\" class=\"markup--anchor markup--li-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>).</li><li id=\"85ed\" class=\"graf graf--li graf-after--li\">Install the tar file</li></ol><pre id=\"f250\" class=\"graf graf--pre graf-after--li\">tar -xzf kafka_2.11–1.0.0.tgz <br />cd kafka_2.11–1.0.0</pre><h3 id=\"0bdd\" class=\"graf graf--h3 graf-after--pre\">Starting Kafka</h3><p id=\"ff0d\" class=\"graf graf--p graf-after--h3\">This post will not attempt to explain the architecture behind a Kafka cluster. However, a typical installation will have several Kafka brokers and Apache Zookeeper.</p><figure id=\"8095\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*HoKUvzx_Q25-P5Rq.png\" data-width=\"439\" data-height=\"230\" src=\"https://cdn-images-1.medium.com/max/1600/0*HoKUvzx_Q25-P5Rq.png\" alt=\"image\" /></div><figcaption class=\"imageCaption\">all logos are trademark of Apache Foundation</figcaption></div></figure><p id=\"846b\" class=\"graf graf--p graf-after--figure\">To run Kafka, first start Zookeeper, then start the Kafka brokers. The commands below assume a local installation with only one node.</p><pre id=\"eed8\" class=\"graf graf--pre graf-after--p\">bin/zookeeper-server-start.sh config/zookeeper.properties</pre><p id=\"77cd\" class=\"graf graf--p graf-after--pre\">and</p><pre id=\"ded3\" class=\"graf graf--pre graf-after--p\">bin/kafka-server-start.sh config/server.properties</pre><p id=\"7528\" class=\"graf graf--p graf-after--pre\">Once we have Kafka installed and running, we need to create four topics. One is used by our application to publish our pack JSON. The other three are required by Kafka Connect. We will continue to assume that most are running this initially on a laptop so we will set the replication factor to 1.</p><pre id=\"541c\" class=\"graf graf--pre graf-after--p\">bin/kafka-topics.sh — create — topic test_topic -zookeeper localhost:2181 — replication-factor 1 — partitions 3</pre><p id=\"1251\" class=\"graf graf--p graf-after--pre\">and</p><pre id=\"054b\" class=\"graf graf--pre graf-after--p\">bin/kafka-topics.sh — create — zookeeper localhost:2181 — topic connect-configs — replication-factor 1 — partitions 1 — config cleanup.policy=compact </pre><pre id=\"43f4\" class=\"graf graf--pre graf-after--pre\">bin/kafka-topics.sh — create — zookeeper localhost:2181 — topic connect-offsets — replication-factor 1 — partitions 50 — config cleanup.policy=compact </pre><pre id=\"ad4c\" class=\"graf graf--pre graf-after--pre\">bin/kafka-topics.sh — create — zookeeper localhost:2181 — topic connect-status — replication-factor 1 — partitions 10 — config cleanup.policy=compact</pre><p id=\"1461\" class=\"graf graf--p graf-after--pre\">In order to verify that the four topics have been created, run the following command:</p><pre id=\"f0fe\" class=\"graf graf--pre graf-after--p\">bin/kafka-topics.sh — list — zookeeper localhost:2181</pre><h3 id=\"e7e9\" class=\"graf graf--h3 graf-after--pre\">Installing the Cassandra Source connector</h3><p id=\"9f7c\" class=\"graf graf--p graf-after--h3\">Landoop offers numerous connectors for Kafka Connect. These are all available as open source. The first thing we need to do is download the Cassandra Source connector jar file (<a href=\"https://github.com/Landoop/stream-reactor/releases\" data-href=\"https://github.com/Landoop/stream-reactor/releases\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>).</p><ul class=\"postList\"><li id=\"9df6\" class=\"graf graf--li graf-after--p\">kafka-connect-cassandra-1.0.0–1.0.0-all.tar.gz</li></ul><p id=\"c2c7\" class=\"graf graf--p graf-after--li\">Unzip the tar file and copy the jar file to the <code class=\"markup--code markup--p-code\">libs</code> folder under the Kafka install directory.</p><h3 id=\"0a03\" class=\"graf graf--h3 graf-after--p\">Configuring Kafka Connect</h3><p id=\"f0d8\" class=\"graf graf--p graf-after--h3\">We need to tell Kafka Connect where the Kafka cluster is. In the <code class=\"markup--code markup--p-code\">config</code> folder where Kafka was installed we will find the file: <code class=\"markup--code markup--p-code\">connect-distributed.properties.</code>Look for the <code class=\"markup--code markup--p-code\">bootstrap.servers</code> key. Update that to point to the cluster.</p><pre id=\"01dd\" class=\"graf graf--pre graf-after--p\">bootstrap.servers=localhost:9092</pre><h3 id=\"66ff\" class=\"graf graf--h3 graf-after--pre\">Starting Kafka Connect</h3><p id=\"18b5\" class=\"graf graf--p graf-after--h3\">We can now start up our distributed Kafka Connect service. For more information on stand-alone vs distributed mode, see the documentation (<a href=\"https://docs.confluent.io/current/connect/userguide.html\" data-href=\"https://docs.confluent.io/current/connect/userguide.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">link</a>).</p><pre id=\"62ce\" class=\"graf graf--pre graf-after--p\">bin/connect-distributed.sh config/connect-distributed.properties</pre><p id=\"24bf\" class=\"graf graf--p graf-after--pre\">If all has gone well you should see the following on your console:</p><figure id=\"867a\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"0*M6D9vzHCNtCScBNN.png\" data-width=\"817\" data-height=\"260\" data-action=\"zoom\" data-action-value=\"0*M6D9vzHCNtCScBNN.png\" src=\"https://cdn-images-1.medium.com/max/1600/0*M6D9vzHCNtCScBNN.png\" alt=\"image\" /></div></div></figure><p id=\"627d\" class=\"graf graf--p graf-after--figure\">In case you are wondering , “Data Mountaineer”, was the name of the company before being renamed to Landoop.</p><h3 id=\"fb57\" class=\"graf graf--h3 graf-after--p\">Adding the Cassandra Source connector</h3><p id=\"cfca\" class=\"graf graf--p graf-after--h3\">Kafka Connect has a REST API to interact with connectors (<a href=\"https://docs.confluent.io/current/connect/restapi.html\" data-href=\"https://docs.confluent.io/current/connect/restapi.html\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">check this out for details</a> on the API). We need to add the Cassandra Source connector to the Kafka Connect. This is done by sending the property file (<code class=\"markup--code markup--p-code\">connect-cassandra-source.json</code>) to Kafka Connect through the REST API.</p><pre id=\"bb81\" class=\"graf graf--pre graf-after--p\">curl -X POST -H “Content-Type: application/json” -d @connect-cassandra-source.json localhost:8083/connectors</pre><p id=\"47f0\" class=\"graf graf--p graf-after--pre\">Once we have successfully loaded the connector, we can check to see the installed connectors using this API:</p><pre id=\"6cc1\" class=\"graf graf--pre graf-after--p\">curl localhost:8083/connectors</pre><p id=\"dacd\" class=\"graf graf--p graf-after--pre\">That should return a list of the connectors by their configured names.</p><pre id=\"b394\" class=\"graf graf--pre graf-after--p\">[“packs”]</pre><h3 id=\"4ce8\" class=\"graf graf--h3 graf-after--pre\">Testing the Cassandra Source connector</h3><p id=\"2758\" class=\"graf graf--p graf-after--h3\">In order to test everything out we will need to insert some data into our table.</p><pre id=\"3d44\" class=\"graf graf--pre graf-after--p\">INSERT INTO pack_events (event_id, event_ts, event_data) <br />VALUES (‘500’, ‘2018–01–22T20:28:50.869Z’, ‘{“foo”:”bar”}’);</pre><p id=\"5f22\" class=\"graf graf--p graf-after--pre\">We can check what is being written to the Kafka topic by running the following command:</p><pre id=\"7fa3\" class=\"graf graf--pre graf-after--p\">bin/kafka-console-consumer.sh — bootstrap-server localhost:9092 — topic test_topic</pre><p id=\"b4a1\" class=\"graf graf--p graf-after--pre\">At this point, we might be surprised to see something like this:</p><pre id=\"eaa5\" class=\"graf graf--pre graf-after--p\">{ <br />“schema”:{ <br />“type”:”string”, <br />“optional”:false <br />}, <br />“payload”:”{\\”foo\\”:\\”bar\\”}” <br />}</pre><p id=\"361b\" class=\"graf graf--p graf-after--pre\">That is better than what we were getting without <code class=\"markup--code markup--p-code\">WITHUNWRAP</code> but isn't exactly what we were hoping for. To get the JSON value that was written to the table column we need to update the <code class=\"markup--code markup--p-code\">connect-distributed.properties</code>file. Open this up and look for <code class=\"markup--code markup--p-code\">JsonConverter</code>. Replace those lines with the following:</p><pre id=\"e984\" class=\"graf graf--pre graf-after--p\">key.converter=org.apache.kafka.connect.storage.StringConverter value.converter=org.apache.kafka.connect.storage.StringConverter</pre><p id=\"b8d7\" class=\"graf graf--p graf-after--pre\">Restart Kafka Connect.<br /> Insert another row into the table.<br /> Now we should get what we want.</p><pre id=\"012c\" class=\"graf graf--pre graf-after--p\">{ “foo”:”bar” }</pre><p id=\"08e6\" class=\"graf graf--p graf-after--pre\">Happy coding!</p><p id=\"3cbb\" class=\"graf graf--p graf-after--p graf--trailing\">This originally appeared on TheAgileJedi blog (<a href=\"https://theagilejedi.wordpress.com/2018/01/23/using-the-kafka-connect-cassandra-source-part-1/\" data-href=\"https://theagilejedi.wordpress.com/2018/01/23/using-the-kafka-connect-cassandra-source-part-1/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">here</a>)</p>",
        "created_at": "2018-05-12T02:38:32+0000",
        "updated_at": "2018-10-22T23:15:49+0000",
        "published_at": "2018-03-13T22:05:21+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 11,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/0*Fi0Zp7l0lbj-y9QT.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9723"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 253,
            "label": "analytics",
            "slug": "analytics"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 955,
            "label": "blockchain",
            "slug": "blockchain"
          }
        ],
        "is_public": false,
        "id": 9660,
        "uid": null,
        "title": "How NerdWallet and BlockCypher are Building Data Platforms",
        "url": "https://medium.com/sfdata/data-platforms-in-fintech-spark-kafka-and-amazon-redshift-6c393a0c29ae",
        "content": "<div class=\"section-divider\"><hr class=\"section-divider\" /></div><div class=\"section-content\"><div class=\"section-inner sectionLayout--insetColumn\"><h1 id=\"a67d\" class=\"graf graf--h3 graf--leading graf--title\">How NerdWallet and BlockCypher are Building Data Platforms</h1></div><div class=\"section-inner sectionLayout--fullWidth\"><figure id=\"acd1\" class=\"graf graf--figure graf--layoutFillWidth graf-after--h3\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><img class=\"graf-image\" data-image-id=\"1*rCWlWxJ9-REAtidxv5XG3w.jpeg\" data-width=\"1024\" data-height=\"683\" src=\"https://cdn-images-1.medium.com/max/2000/1*rCWlWxJ9-REAtidxv5XG3w.jpeg\" alt=\"image\" /></div></div></figure></div><div class=\"section-inner sectionLayout--insetColumn\"><p id=\"f1bf\" class=\"graf graf--p graf-after--figure\">On February 23 in San Francisco, SF DATA hosted a tech talk on <a href=\"https://www.eventbrite.com/e/data-platforms-in-fintech-spark-kafka-and-amazon-redshift-tickets-31978038173\" data-href=\"https://www.eventbrite.com/e/data-platforms-in-fintech-spark-kafka-and-amazon-redshift-tickets-31978038173\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Data Platforms in FinTech</a>. In their talks, speakers revealed how they’re building out data platforms to support blockchain applications, cryptocurrencies, and detect fraudulent activity.</p><p id=\"5a1f\" class=\"graf graf--p graf-after--p\"><a href=\"https://www.linkedin.com/in/mriou/\" data-href=\"https://www.linkedin.com/in/mriou/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\">Matthieu Riou</strong></a>, CTO and co-founder of <a href=\"https://www.blockcypher.com/\" data-href=\"https://www.blockcypher.com/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\">BlockCypher</strong></a>, explained how BlockCypher used data to hunt down $70 million in stolen Bitcoins for the Department of Homeland Security.</p><p id=\"34fa\" class=\"graf graf--p graf-after--p\"><a href=\"https://www.linkedin.com/in/vaibhavjajoo/\" data-href=\"https://www.linkedin.com/in/vaibhavjajoo/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\">Vaibhav Jajoo</strong></a>, head of Data Infrastructure at <a href=\"https://www.nerdwallet.com/\" data-href=\"https://www.nerdwallet.com/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\"><strong class=\"markup--strong markup--p-strong\">NerdWallet</strong></a>, described how the data team at NerdWallet is using a new brand of data analytics solutions with Kafka, Python, EMR, and Redshift.</p><h3 id=\"c822\" class=\"graf graf--h3 graf-after--p\">Analytics for Bitcoin and Other Cryptocurrencies at BlockCypher</h3><p id=\"8a80\" class=\"graf graf--p graf-after--h3\">Developers, companies, and government agencies use the BlockCypher API to build cryptocurrency applications and analyze patterns in blockchain transactions. Bitcoin alone has 8.2 million new transactions per month, with 250 million IP addresses to monitor. The Department of Homeland Security recently used the BlockCypher Analytics’ API to track down $70 million in stolen Bitcoins from the <a href=\"https://blog.blockcypher.com/finding-bitfinexs-bits-de7044185ed3\" data-href=\"https://blog.blockcypher.com/finding-bitfinexs-bits-de7044185ed3\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">Bitfinex heist</a>.</p><figure id=\"7be7\" class=\"graf graf--figure graf--iframe graf--layoutOutsetLeft graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe data-width=\"960\" data-height=\"540\" width=\"525\" height=\"295\" src=\"https://medium.com/media/4377ca6aac11b6c6349ca838d083a628?postId=6c393a0c29ae\" data-media-id=\"4377ca6aac11b6c6349ca838d083a628\" data-thumbnail=\"https://i.embed.ly/1/image?url=https%3A%2F%2Fembed-ssl.wistia.com%2Fdeliveries%2F865d222608c8979e32fcef060b57f00552298fd7.jpg%3Fimage_crop_resized%3D960x540&amp;key=a19fcc184b9711e1b4764040d3dc5c07\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div><figcaption class=\"imageCaption\">BlockCypher — Analytics for Bitcoin and other Cryptocurrencies</figcaption></div></figure><p id=\"3f1f\" class=\"graf graf--p graf-after--figure\">In August 2016, BlockCypher noticed that 0.75% of Bitcoins suddenly started moving in unusual patterns.</p><p id=\"670a\" class=\"graf graf--p graf-after--p\">While the culprits are still unknown, BlockCypher was able to filter data to pinpoint where the transactions were coming from — in this instance bitcoin wallet provider <a href=\"https://www.bitgo.com/\" data-href=\"https://www.bitgo.com/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">BitGo</a>. The BlockCypher architecture uses a combination of <a href=\"http://cassandra.apache.org/\" data-href=\"http://cassandra.apache.org/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Cassandra</a>, Redshift and <a href=\"https://spark.apache.org/\" data-href=\"https://spark.apache.org/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Spark</a>.</p><p id=\"2f46\" class=\"graf graf--p graf-after--p\">According to Matthieu, the Holy Grail in cryptocurrency is to deanonymize every transaction, have the ability to tie it with off-chain transactions, classify transactions using machine learning, and provide APIs for law enforcement and industry.</p><h3 id=\"6685\" class=\"graf graf--h3 graf-after--p\">Building Data Solutions and Innovations at NerdWallet</h3><p id=\"6afd\" class=\"graf graf--p graf-after--h3\">NerdWallet gives consumers and small businesses clarity around all of life’s financial decisions by building accessible online tools and providing research and expert advice.</p><figure id=\"d9bd\" class=\"graf graf--figure graf--iframe graf--layoutOutsetLeft graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe data-width=\"960\" data-height=\"540\" width=\"525\" height=\"295\" src=\"https://medium.com/media/02e86dc8e60b86dfca59d3f5fbc050cc?postId=6c393a0c29ae\" data-media-id=\"02e86dc8e60b86dfca59d3f5fbc050cc\" data-thumbnail=\"https://i.embed.ly/1/image?url=https%3A%2F%2Fembed-ssl.wistia.com%2Fdeliveries%2F651547aa35906912cc5f477d1f55114fc2a4f2bb.jpg%3Fimage_crop_resized%3D960x540&amp;key=a19fcc184b9711e1b4764040d3dc5c07\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div></div></figure><p id=\"0c81\" class=\"graf graf--p graf-after--figure\"><a href=\"https://www.linkedin.com/in/vaibhavjajoo/\" data-href=\"https://www.linkedin.com/in/vaibhavjajoo/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Vaibhav </a>joined NerdWallet in 2014 and started the Data Analytics Team to help everybody in NerdWallet create meaning from the large volume and variety of data that NerdWallet customers generate every day (popular products, click-through rates, platform attributes etc.). NerdWallet has ~450 employees, and data consumers (i.e. “everybody”) stretch from analysts to the CEO. Product and marketing analysts care about granular views for a specific product or campaign. The CEO cares about high level views about the business. The data platform at NerdWallet needs to be flexible enough to serve data in a form each audience can understand, while allowing to slice and dice data across many dimensions.</p><figure id=\"9130\" class=\"graf graf--figure graf--iframe graf--layoutOutsetLeft graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><div class=\"aspectRatioPlaceholder-fill\"><div class=\"iframeContainer\"><iframe data-width=\"600\" data-height=\"500\" width=\"525\" height=\"438\" src=\"https://medium.com/media/b1848d1a087d859733bc230071a298fc?postId=6c393a0c29ae\" data-media-id=\"b1848d1a087d859733bc230071a298fc\" data-thumbnail=\"https://i.embed.ly/1/image?url=https%3A%2F%2Fcdn.slidesharecdn.com%2Fss_thumbnails%2Fdataanalyticsnerdwallet1-170906233719-thumbnail-4.jpg%3Fcb%3D1504741267&amp;key=a19fcc184b9711e1b4764040d3dc5c07\" allowfullscreen=\"allowfullscreen\" frameborder=\"0\">[embedded content]</iframe></div></div><figcaption class=\"imageCaption\">FinTech Data Challenges <a href=\"https://medium.com/@NerdWallet\" data-href=\"https://medium.com/@NerdWallet\" data-anchor-type=\"2\" data-user-id=\"e2e4cdb6512a\" data-action-value=\"e2e4cdb6512a\" data-action=\"show-user-card\" data-action-type=\"hover\" class=\"markup--user markup--figure-user\" target=\"_blank\">NerdWallet</a></figcaption></div></figure><p id=\"e418\" class=\"graf graf--p graf-after--figure\">Using a combination of <a href=\"https://kafka.apache.org/\" data-href=\"https://kafka.apache.org/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Kafka</a>, <a href=\"https://aws.amazon.com/redshift/\" data-href=\"https://aws.amazon.com/redshift/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Amazon Redshift</a> and <a href=\"https://aws.amazon.com/emr/\" data-href=\"https://aws.amazon.com/emr/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">EMR</a>, NerdWallet has been able to create “ETL as Scale” and manage dynamic workloads. There are 250+ named SQL users, with different levels of SQL skills. That can pose a lot of challenges for managing the Redshift environment, especially for situations where some users write large ad-hoc queries. A key to balancing resources with workloads is Redshift’s WLM (<a href=\"http://docs.aws.amazon.com/redshift/latest/mgmt/workload-mgmt-config.html\" data-href=\"http://docs.aws.amazon.com/redshift/latest/mgmt/workload-mgmt-config.html\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow\" target=\"_blank\">Workload Management</a>).</p><p id=\"8968\" class=\"graf graf--p graf-after--p\">— —</p><p id=\"edd8\" class=\"graf graf--p graf-after--p\">Interested in building data platforms? Subscribe to <a href=\"https://www.getrevue.co/profile/sfdata\" data-href=\"https://www.getrevue.co/profile/sfdata\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener nofollow nofollow noopener\" target=\"_blank\">SF Data Weekly</a>, for more stories on data engineering you don’t want to miss.</p><p id=\"ff4f\" class=\"graf graf--p graf-after--p graf--trailing\">Attend our next event? Follow the <a href=\"https://www.facebook.com/sfdataevents/\" data-href=\"https://www.facebook.com/sfdataevents/\" class=\"markup--anchor markup--p-anchor\" rel=\"nofollow noopener\" target=\"_blank\">SF Data Facebook page</a>.</p></div></div>",
        "created_at": "2018-04-24T14:35:48+0000",
        "updated_at": "2018-04-24T14:36:02+0000",
        "published_at": "2017-08-28T23:38:50+0000",
        "published_by": [
          "Lars Kamp"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 2,
        "domain_name": "medium.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*rCWlWxJ9-REAtidxv5XG3w.jpeg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9660"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 996,
            "label": "monitoring",
            "slug": "monitoring"
          }
        ],
        "is_public": false,
        "id": 9652,
        "uid": null,
        "title": "thelastpickle/cassandra-zipkin-tracing",
        "url": "https://github.com/thelastpickle/cassandra-zipkin-tracing",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Cassandra-3.4 provides <a href=\"http://www.planetcassandra.org/blog/cassandra-3-4-release-overview/\" rel=\"nofollow\">pluggable tracing</a>. By adding 3 jar files to the Cassandra classpath and one jvm option, Cassandra's tracing is replaced with Zipkin. It can even identify incoming Zipkin traces and add Cassandra's own internal tracing on to it.</p>\n<pre>mvn install\ncp target/*.jar $CASSANDRA_HOME/lib/\n</pre>\n<p>Then start Cassandra with</p>\n<pre>JVM_OPTS \\\n  =\"-Dcassandra.custom_tracing_class=com.thelastpickle.cassandra.tracing.ZipkinTracing\" \\\n    cassandra\n</pre>\n<p>Or edit the <code>jvm.options</code>.</p>\n<p>The default SpanCollector sends the tracing messages via HTTP to <code>http://127.0.0.1:9411/</code>. This is the default port for the <a href=\"https://github.com/openzipkin/zipkin-java\">zipkin-java</a> server. The same url is used for the UI.</p>\n<h2><a id=\"user-content-continuing-existing-zipkin-traces\" class=\"anchor\" aria-hidden=\"true\" href=\"#continuing-existing-zipkin-traces\"></a>Continuing existing Zipkin traces</h2>\n<p>To continue existing Zipkin traces from application code through the DataStax CQL driver and into the Cassandra cluster.</p>\n<p>The Cassandra nodes need to be started also with the <code>cassandra.custom_query_handler_class</code> jvm option to a query handler that accepts incoming payloads over the CQL protocol:</p>\n<pre>JVM_OPTS \\\n  =\"-Dcassandra.custom_tracing_class=com.thelastpickle.cassandra.tracing.ZipkinTracing\" \\\n    -Dcassandra.custom_query_handler_class=org.apache.cassandra.cql3.CustomPayloadMirroringQueryHandler\"\n  cassandra\n</pre>\n<p>(Or edit the <code>jvm.options</code>)</p>\n<p>Then in the application code where the DataStax CQL driver is used put the Zipkin traceId and spanId into the <em>outgoing payload</em> like</p>\n<pre>SpanId spanId = clientTracer.startNewSpan(statement.toString());\nByteBuffer traceHeaders = ByteBuffer.wrap(spanId.bytes());\nstatement.setOutgoingPayload(singletonMap(\"zipkin\", traceHeaders.array()));\nclientTracer.setCurrentClientServiceName(serviceName);\nclientTracer.setClientSent();\nResultSet result = session.execute(statement);\nclientTracer.setClientReceived();\nreturn result;\n</pre>\n<h2><a id=\"user-content-more-information\" class=\"anchor\" aria-hidden=\"true\" href=\"#more-information\"></a>More information</h2>\n<p>See this <a href=\"http://thelastpickle.com/files/2015-09-24-using-zipkin-for-full-stack-tracing-including-cassandra/presentation/tlp-reveal.js/tlp-cassandra-zipkin.html\" rel=\"nofollow\">presentation</a>.</p>\n<h2><a id=\"user-content-background\" class=\"anchor\" aria-hidden=\"true\" href=\"#background\"></a>Background</h2>\n<p>See <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-10392\" rel=\"nofollow\">CASSANDRA-10392</a> for the patch to extend Cassandra's tracing that this project plugs into.</p>\n<h2><a id=\"user-content-troubleshooting\" class=\"anchor\" aria-hidden=\"true\" href=\"#troubleshooting\"></a>Troubleshooting</h2>\n<p>When this tracing is used instead of Cassandra's default tracing, any cqlsh statements run after enabling tracing with\n<code>TRACING ON;</code> are going to time out eventually giving</p>\n<pre>Unable to fetch query trace: Trace information was not available within …\n</pre>\n<p>This is because cqlsh is polling for tracing information in system_traces which isn't any longer being created. Zipkin tracing doesn't support this interaction with cqlsh (it's more of a thing to use with a tracing sampling rate). Improvements in this area are possible though, for example we could use zipkin tracing when the custom payload contains a zipkin traceId and spanId and fall back to normal tracing otherwise (which would work for cqlsh interaction). For the meantime an easy fix around this behaviour in cqlsh is to reduce Session.max_trace_wait down to 1 second.</p>\n</article>",
        "created_at": "2018-04-21T12:35:54+0000",
        "updated_at": "2018-04-21T12:36:05+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars0.githubusercontent.com/u/30403496?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9652"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 12,
            "label": "video",
            "slug": "video"
          },
          {
            "id": 33,
            "label": "internet.architecture",
            "slug": "internet-architecture"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          }
        ],
        "is_public": false,
        "id": 9634,
        "uid": null,
        "title": "How Uber scaled its Real Time Infrastructure to Trillion events per day",
        "url": "http://www.youtube.com/oembed?format=xml&url=https://www.youtube.com/watch?v=K-fI2BeTLkk",
        "content": "<iframe id=\"video\" width=\"480\" height=\"270\" src=\"https://www.youtube.com/embed/K-fI2BeTLkk?feature=oembed\" frameborder=\"0\" allowfullscreen=\"allowfullscreen\">[embedded content]</iframe>",
        "created_at": "2018-04-20T02:07:26+0000",
        "updated_at": "2018-04-20T02:07:47+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/xml",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.youtube.com",
        "preview_picture": "https://i.ytimg.com/vi/K-fI2BeTLkk/maxresdefault.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9634"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 963,
            "label": "akka",
            "slug": "akka"
          },
          {
            "id": 964,
            "label": "scala",
            "slug": "scala"
          }
        ],
        "is_public": false,
        "id": 9627,
        "uid": null,
        "title": "jamesward/koober",
        "url": "https://github.com/jamesward/koober",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>An uber data pipeline sample app.  Play Framework, Akka Streams, Kafka, Flink, Spark Streaming, and Cassandra.</p>\n<p>Start Kafka:</p>\n<pre>./sbt kafkaServer/run\n</pre>\n<p>Web App:</p>\n<ol><li>Obtain an API key from <a href=\"https://www.mapbox.com/\" rel=\"nofollow\">mapbox.com</a></li>\n<li>Start the Play web app: <code>MAPBOX_ACCESS_TOKEN=YOUR-MAPBOX-API-KEY ./sbt webapp/run</code></li>\n</ol><p>Try it out:</p>\n<ol><li>Open the driver UI: <a href=\"http://localhost:9000/driver\" rel=\"nofollow\">http://localhost:9000/driver</a></li>\n<li>Open the rider UI: <a href=\"http://localhost:9000/rider\" rel=\"nofollow\">http://localhost:9000/rider</a></li>\n<li>In the Rider UI, click on the map to position the rider</li>\n<li>In the Driver UI, click on the rider to initiate a pickup</li>\n</ol><p>Start Flink:</p>\n<ol><li><code>./sbt flinkClient/run</code></li>\n<li>Initiate a few pickups and see the average pickup wait time change (in the stdout console for the Flink process)</li>\n</ol><p>Start Cassandra:</p>\n<pre>./sbt cassandraServer/run\n</pre>\n<p>Start the Spark Streaming process:</p>\n<ol><li><code>./sbt kafkaToCassandra/run</code></li>\n<li>Watch all of the ride data be micro-batched from Kafka to Cassandra</li>\n</ol><p>Setup PredictionIO Pipeline:</p>\n<ol><li>\n<p>Setup PIO</p>\n</li>\n<li>\n<p>Set the PIO Access Key:</p>\n<pre> export PIO_ACCESS_KEY=&lt;YOUR PIO ACCESS KEY&gt;\n</pre>\n</li>\n<li>\n<p>Start the PIO Pipeline:</p>\n<pre> ./sbt pioClient/run\n</pre>\n</li>\n</ol><p>Copy demo data into Kafka or PIO:</p>\n<p>For fake data, run:</p>\n<pre>./sbt \"demoData/run &lt;kafka|pio&gt; fake &lt;number of records&gt; &lt;number of months&gt; &lt;number of clusters&gt;\"\n</pre>\n<p>For New York data, run:</p>\n<pre>./sbt \"demoData/run &lt;kafka|pio&gt; ny &lt;number of months&gt; &lt;sample rate&gt;\"\n</pre>\n<p>Start the Demand Dashboard</p>\n<pre>PREDICTIONIO_URL=http://asdf.com MAPBOX_ACCESS_TOKEN=YOUR_MAPBOX_TOKEN ./sbt demandDashboard/run\n</pre>\n</article>",
        "created_at": "2018-04-20T01:24:53+0000",
        "updated_at": "2018-04-20T01:25:14+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/65043?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9627"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 23,
            "label": "elasticsearch",
            "slug": "elasticsearch"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 921,
            "label": "kafka",
            "slug": "kafka"
          },
          {
            "id": 955,
            "label": "blockchain",
            "slug": "blockchain"
          },
          {
            "id": 1039,
            "label": "redis",
            "slug": "redis"
          }
        ],
        "is_public": false,
        "id": 9599,
        "uid": null,
        "title": "Landoop/stream-reactor",
        "url": "https://github.com/Landoop/stream-reactor",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\"><p><a href=\"https://datamountaineer.ci.landoop.com/job/stream-reactor/\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/66bb79f11d5cecbbc57c12f7b8bf4cfb61be30e7/68747470733a2f2f646174616d6f756e7461696e6565722e63692e6c616e646f6f702e636f6d2f6275696c645374617475732f69636f6e3f6a6f623d73747265616d2d72656163746f72267374796c653d666c6174262e706e67\" alt=\"Build Status\" data-canonical-src=\"https://datamountaineer.ci.landoop.com/buildStatus/icon?job=stream-reactor&amp;style=flat&amp;.png\" /></a>\n<a href=\"http://lenses.stream/connectors/index.html\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/0b233e4a210326868c5be7197bd9ccb416aacff0/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f646f63732d2d6f72616e67652e7376673f\" data-canonical-src=\"https://img.shields.io/badge/docs--orange.svg?\" alt=\"image\" /></a>\n<a href=\"http://search.maven.org/#search%7Cga%7C1%7Cg%3A%22com.datamountaineer%22\" rel=\"nofollow\"><img src=\"https://camo.githubusercontent.com/c0fb8d333898ab15428ba7e8b2420b7427908d7e/68747470733a2f2f696d672e736869656c64732e696f2f62616467652f6c617465737425323072656c656173652d312e302e302d626c75652e7376673f6c6162656c3d6c617465737425323072656c65617365\" data-canonical-src=\"https://img.shields.io/badge/latest%20release-1.0.0-blue.svg?label=latest%20release\" alt=\"image\" /></a></p>\n<p>Join us on slack <a href=\"https://launchpass.com/landoop-community\" rel=\"nofollow\"><img src=\"https://github.com/Landoop/stream-reactor/raw/master/images/slack.jpeg\" alt=\"Alt text\" /></a></p>\n<p>Lenses offers SQL (for data browsing and Kafka Streams), Kafka Connect connector management, cluster monitoring and more.</p>\n<p>You can find more on <a href=\"http://www.landoop.com/kafka-lenses/\" rel=\"nofollow\">landoop.com!</a></p>\n<p><a target=\"_blank\" href=\"https://github.com/Landoop/stream-reactor/blob/master/images/streamreactor-logo.png\"><img src=\"https://github.com/Landoop/stream-reactor/raw/master/images/streamreactor-logo.png\" alt=\"Alt text\" /></a></p>\n<p>A collection of components to build a real time ingestion pipeline.</p>\n<h3><a id=\"user-content-connectors\" class=\"anchor\" aria-hidden=\"true\" href=\"#connectors\"></a>Connectors</h3>\n<p><strong>Please take a moment and read the documentation and make sure the software prerequisites are met!!</strong></p>\n<table><thead><tr><th>Connector</th>\n<th>Type</th>\n<th>Description</th>\n<th>Docs</th>\n</tr></thead><tbody><tr><td>AzureDocumentDb</td>\n<td>Sink</td>\n<td>Kafka connect Azure DocumentDb sink to subscribe to write to the cloud Azure Document Db.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/azuredocdb.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>BlockChain</td>\n<td>Source</td>\n<td>Kafka connect Blockchain source to subscribe to Blockchain streams and write to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/blockchain.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Bloomberg</td>\n<td>Source</td>\n<td>Kafka connect source to subscribe to Bloomberg streams and write to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/bloomberg.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Cassandra</td>\n<td>Source</td>\n<td>Kafka connect Cassandra source to read Cassandra and write to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/cassandra.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>*Cassandra</td>\n<td>Sink</td>\n<td>Certified DSE Kafka connect Cassandra sink task to write Kafka topic payloads to Cassandra.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/cassandra.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Coap</td>\n<td>Source</td>\n<td>Kafka connect Coap source to read from IoT Coap endpoints using Californium.</td>\n<td><a href=\"https://lenses.stream/connectors/source/coap.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Coap</td>\n<td>Sink</td>\n<td>Kafka connect Coap sink to write kafka topic payload to IoT Coap endpoints using Californium.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/coap.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Druid</td>\n<td>Sink</td>\n<td>Kafka connect Druid sink to write Kafka topic payloads to Druid.</td>\n<td>\n</td></tr><tr><td>Elastic</td>\n<td>Sink</td>\n<td>Kafka connect Elastic Search sink to write Kafka topic payloads to Elastic Search 2.x</td>\n<td><a href=\"https://lenses.stream/connectors/sink/elastic.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Elastic 5</td>\n<td>Sink</td>\n<td>Kafka connect Elastic Search sink to write payloads to Elastic Search 5.x w. tcp or http</td>\n<td><a href=\"https://lenses.stream/connectors/sink/elastic5.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Elastic 6</td>\n<td>Sink</td>\n<td>Kafka connect Elastic Search sink to write payloads to Elastic Search 6.x w. tcp or http</td>\n<td><a href=\"https://lenses.stream/connectors/sink/elastic6.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>FTP/HTTP</td>\n<td>Source</td>\n<td>Kafka connect FTP and HTTP source to write file data into Kafka topics.</td>\n<td><a href=\"https://lenses.stream/connectors/source/ftp.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>HBase</td>\n<td>Sink</td>\n<td>Kafka connect HBase sink to write Kafka topic payloads to HBase.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/hbase.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Hazelcast</td>\n<td>Sink</td>\n<td>Kafka connect Hazelcast sink to write Kafka topic payloads to Hazelcast.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/hazelcast.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Kudu</td>\n<td>Sink</td>\n<td>Kafka connect Kudu sink to write Kafka topic payloads to Kudu.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/kudu.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>InfluxDb</td>\n<td>Sink</td>\n<td>Kafka connect InfluxDb sink to write Kafka topic payloads to InfluxDb.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/influx.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>JMS</td>\n<td>Source</td>\n<td>Kafka connect JMS source to write from JMS to Kafka topics.</td>\n<td><a href=\"https://lenses.stream/connectors/source/jms.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>JMS</td>\n<td>Sink</td>\n<td>Kafka connect JMS sink to write Kafka topic payloads to JMS.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/jms.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>MongoDB</td>\n<td>Sink</td>\n<td>Kafka connect MongoDB sink to write Kafka topic payloads to MongoDB.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/mongo.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>MQTT</td>\n<td>Source</td>\n<td>Kafka connect MQTT source to write data from MQTT to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/mqtt.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>MQTT</td>\n<td>Sink</td>\n<td>Kafka connect MQTT sink to write data from Kafka to MQTT.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/mqtt.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Pulsar</td>\n<td>Source</td>\n<td>Kafka connect Pulsar source to write data from Pulsar to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/pulsar.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Pulsar</td>\n<td>Sink</td>\n<td>Kafka connect Pulsar sink to write data from Kafka to Pulsar.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/pulsar.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>Redis</td>\n<td>Sink</td>\n<td>Kafka connect Redis sink to write Kafka topic payloads to Redis.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/redis.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>ReThinkDB</td>\n<td>Source</td>\n<td>Kafka connect RethinkDb source subscribe to ReThinkDB changefeeds and write to Kafka.</td>\n<td><a href=\"https://lenses.stream/connectors/source/rethink.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>ReThinkDB</td>\n<td>Sink</td>\n<td>Kafka connect RethinkDb sink to write Kafka topic payloads to RethinkDb.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/rethink.html\" rel=\"nofollow\">Docs</a></td>\n</tr><tr><td>VoltDB</td>\n<td>Sink</td>\n<td>Kafka connect Voltdb sink to write Kafka topic payloads to Voltdb.</td>\n<td><a href=\"https://lenses.stream/connectors/sink/voltdb.html\" rel=\"nofollow\">Docs</a></td>\n</tr></tbody></table><h2><a id=\"user-content-release-notes\" class=\"anchor\" aria-hidden=\"true\" href=\"#release-notes\"></a>Release Notes</h2>\n<p><strong>1.1.0</strong> Pending</p>\n<ul><li>Added SSL, subscription, partitioning, batching and key selection to Pulsar source and sink</li>\n<li>Elastic6 connector @caiooliveiraeti !</li>\n<li>HTTP Basic Auth for Elasticsearch http client thanks @justinsoong !</li>\n<li>Add polling timeout on the JMS source connector to avoid high CPU in the source connector poll thanks #373 @matthedude</li>\n<li>Fixes on the elastic primary key separator thanks @caiooliveiraeti!</li>\n<li>Fix on the MQTT class loader</li>\n<li>Fix on the JMS class loader</li>\n<li>Fix on JMS to close down connections cleanly #363 thanks @matthedude!</li>\n<li>Fix on MQTT to correctly handle authentication</li>\n<li>Moved MongoDB batch size to KCQL. <code>connect.mongodb.batch.size</code> is deprecated</li>\n<li>Added <code>connect.mapping.collection.to.json</code> to treat maps, list, sets as json when inserting into Cassandra</li>\n<li>Added support for Elastic Pipelines thanks @caiooliveiraeti!</li>\n<li>Moved ReThinkDB batch size to KCQL <code>connect.rethink.batch.size</code> is deprecated</li>\n<li>MQTT source allows full control of matching the topic <code>INSERT INTO targetTopic SELECT * FROM mqttTopic ... WITHREGEX=`$THE_REGEX`</code></li>\n</ul><p><strong>1.0.0</strong></p>\n<ul><li>Kafka 1.0.o Support</li>\n</ul><p><strong>0.4.0</strong></p>\n<ul><li>Add FTPS support to FTP connector, new configuration option <code>ftp.protocol</code> introduced, either ftp (default) or ftps.</li>\n<li>Fix for MQTT source High CPU Thanks @masahirom!</li>\n<li>Improve logging on Kudu</li>\n<li>DELETE functionality add to the Cassandra sink, deletion now possible for null payloads, thanks @sandonjacobs !</li>\n<li>Fix in kafka-connect-common to handle primary keys with doc strings thanks, @medvekoma !</li>\n<li>Fix writing multiple topics to the same table in Cassandra #284</li>\n<li>Upgrade to Cassandra driver 3.3.0 and refactor Cassandra tests</li>\n<li>Fix on JMS source transacted queues #285 thanks @matthedude !</li>\n<li>Fix on Cassandra source, configurable timespan queries. You can now control the timespan the Connector will query for</li>\n<li>Allow setting initial query timestamp on Cassandra source</li>\n<li>Allow multiple primary keys on the redis sink</li>\n</ul><p><strong>0.3.0</strong></p>\n<ul><li>Upgrade CoAP to 2.0.0-M4</li>\n<li>Upgrade to Confluent 3.3 and Kafka 0.11.0.0.</li>\n<li>Added MQTT Sink.</li>\n<li>Add MQTT wildcard support.</li>\n<li>Upgrade CoAP to 2.0.0-M4.</li>\n<li>Added WITHCONVERTERS and WITHTYPE to JMS and MQTT connectors in KCQL to simplify configuration.</li>\n<li>Added FLUSH MODE to Kudu. Thanks! @patsak</li>\n</ul><p><strong>0.2.6</strong></p>\n<h3><a id=\"user-content-features\" class=\"anchor\" aria-hidden=\"true\" href=\"#features\"></a>Features</h3>\n<ul><li>Added MQTT Sink</li>\n<li>Upgrade to Confluent 3.2.2</li>\n<li>Upgrade to KCQL 2x</li>\n<li>Add CQL generator to Cassandra source</li>\n<li>Add KCQL INCREMENTALMODE support to the Cassandra source, bulk mode and the timestamp column type is now take from KCQL</li>\n<li>Support for setting key and truststore type on Cassandra connectors</li>\n<li>Added token based paging support for Cassandra source</li>\n<li>Added default bytes converter to JMS Source</li>\n<li>Added default connection factory to JMS Source</li>\n<li>Added support for SharedDurableConsumers to JMS Connectors</li>\n<li>Upgraded JMS Connector to JMS 2.0</li>\n<li>Moved to Elastic4s 2.4</li>\n<li>Added Elastic5s with TCP, TCP+XPACK and HTTP client support</li>\n<li>Upgrade Azure Documentdb to 1.11.0</li>\n<li>Added optional progress counter to all connectors, it can be enabled with <code>connect.progress.enabled</code> which will periodically report log messages processed</li>\n<li>Added authentication and TLS to ReThink Connectors</li>\n<li>Added TLS support for ReThinkDB, add batch size option to source for draining the internal queues.</li>\n<li>Upgrade Kudu Client to 1.4.0</li>\n<li>Support for dates in Elastic Indexes and custom document types</li>\n<li>Upgrade Connect CLI to 1.0.2 (Renamed to connect-cli)</li>\n</ul><h3><a id=\"user-content-bug-fixes\" class=\"anchor\" aria-hidden=\"true\" href=\"#bug-fixes\"></a>Bug Fixes</h3>\n<ul><li>Fixes for high CPU on CoAP source</li>\n<li>Fixes for high CPU on Cassandra source</li>\n<li>Fixed Avro double fields mapping to Kudu columns</li>\n<li>Fixes on JMS properties converter, Invalid schema when extracting properties</li>\n</ul><h3><a id=\"user-content-misc\" class=\"anchor\" aria-hidden=\"true\" href=\"#misc\"></a>Misc</h3>\n<ul><li>Refactored Cassandra Tests to use only one embedded instance</li>\n<li>Removed unused batch size and bucket size options from Kudu, they are taken from KCQL</li>\n<li>Removed unused batch size option from DocumentDb</li>\n<li>Rename Azure DocumentDb <code>connect.documentdb.db</code> to <code>connect.documentdb.db</code></li>\n<li>Rename Azure DocumentDb <code>connect.documentdb.database.create</code> to <code>connect.documentdb.db.create</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.kcql</code> to <code>connect.cassandra.kcql</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.timestamp.type</code> to <code>connect.cassandra.timestamp.type</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.import.poll.interval</code> to <code>connect.cassandra.import.poll.interval</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.error.policy</code> to <code>connect.cassandra.error.policy</code></li>\n<li>Rename Cassandra Source <code>connect.cassandra.source.max.retries</code> to <code>connect.cassandra.max.retries</code></li>\n<li>Rename Cassandra Sink <code>connect.cassandra.source.retry.interval</code> to <code>connect.cassandra.retry.interval</code></li>\n<li>Rename Cassandra Sink <code>connect.cassandra.sink.kcql</code> to <code>connect.cassandra.kcql</code></li>\n<li>Rename Cassandra Sink <code>connect.cassandra.sink.error.policy</code> to <code>connect.cassandra.error.policy</code></li>\n<li>Rename Cassandra Sink <code>connect.cassandra.sink.max.retries</code> to <code>connect.cassandra.max.retries</code></li>\n<li>Rename Cassandra Sink Sink <code>connect.cassandra.sink.retry.interval</code> to <code>connect.cassandra.retry.interval</code></li>\n<li>Rename Coap Source <code>connect.coap.bind.port</code> to <code>connect.coap.port</code></li>\n<li>Rename Coap Sink <code>connect.coap.bind.port</code> to <code>connect.coap.port</code></li>\n<li>Rename Coap Source <code>connect.coap.bind.host</code> to <code>connect.coap.host</code></li>\n<li>Rename Coap Sink <code>connect.coap.bind.host</code> to <code>connect.coap.host</code></li>\n<li>Rename MongoDb <code>connect.mongo.database</code> to <code>connect.mongo.db</code></li>\n<li>Rename MongoDb <code>connect.mongo.sink.batch.size</code> to <code>connect.mongo.batch.size</code></li>\n<li>Rename Druid <code>connect.druid.sink.kcql</code> to <code>connect.druid.kcql</code></li>\n<li>Rename Druid <code>connect.druid.sink.conf.file</code> to <code>connect.druid.kcql</code></li>\n<li>Rename Druid <code>connect.druid.sink.write.timeout</code> to <code>connect.druid.write.timeout</code></li>\n<li>Rename Elastic <code>connect.elastic.sink.kcql</code> to <code>connect.elastic.kcql</code></li>\n<li>Rename HBase <code>connect.hbase.sink.column.family</code> to <code>connect.hbase.column.family</code></li>\n<li>Rename HBase <code>connect.hbase.sink.kcql</code> to <code>connect.hbase.kcql</code></li>\n<li>Rename HBase <code>connect.hbase.sink.error.policy</code> to <code>connect.hbase.error.policy</code></li>\n<li>Rename HBase <code>connect.hbase.sink.max.retries</code> to <code>connect.hbase.max.retries</code></li>\n<li>Rename HBase <code>connect.hbase.sink.retry.interval</code> to <code>connect.hbase.retry.interval</code></li>\n<li>Rename Influx <code>connect.influx.sink.kcql</code> to <code>connect.influx.kcql</code></li>\n<li>Rename Influx <code>connect.influx.connection.user</code> to <code>connect.influx.username</code></li>\n<li>Rename Influx <code>connect.influx.connection.password</code> to <code>connect.influx.password</code></li>\n<li>Rename Influx <code>connect.influx.connection.database</code> to <code>connect.influx.db</code></li>\n<li>Rename Influx <code>connect.influx.connection.url</code> to <code>connect.influx.url</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.kcql</code> to <code>connect.kudu.kcql</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.error.policy</code> to <code>connect.kudu.error.policy</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.retry.interval</code> to <code>connect.kudu.retry.interval</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.max.retries</code> to <code>connect.kudu.max.reties</code></li>\n<li>Rename Kudu <code>connect.kudu.sink.schema.registry.url</code> to <code>connect.kudu.schema.registry.url</code></li>\n<li>Rename Redis <code>connect.redis.connection.password</code> to <code>connect.redis.password</code></li>\n<li>Rename Redis <code>connect.redis.sink.kcql</code> to <code>connect.redis.kcql</code></li>\n<li>Rename Redis <code>connect.redis.connection.host</code> to <code>connect.redis.host</code></li>\n<li>Rename Redis <code>connect.redis.connection.port</code> to <code>connect.redis.port</code></li>\n<li>Rename ReThink <code>connect.rethink.source.host</code> to <code>connect.rethink.host</code></li>\n<li>Rename ReThink <code>connect.rethink.source.port</code> to <code>connect.rethink.port</code></li>\n<li>Rename ReThink <code>connect.rethink.source.db</code> to <code>connect.rethink.db</code></li>\n<li>Rename ReThink <code>connect.rethink.source.kcql</code> to <code>connect.rethink.kcql</code></li>\n<li>Rename ReThink Sink <code>connect.rethink.sink.host</code> to <code>connect.rethink.host</code></li>\n<li>Rename ReThink Sink <code>connect.rethink.sink.port</code> to <code>connect.rethink.port</code></li>\n<li>Rename ReThink Sink <code>connect.rethink.sink.db</code> to <code>connect.rethink.db</code></li>\n<li>Rename ReThink Sink <code>connect.rethink.sink.kcql</code> to <code>connect.rethink.kcql</code></li>\n<li>Rename JMS <code>connect.jms.user</code> to <code>connect.jms.username</code></li>\n<li>Rename JMS <code>connect.jms.source.converters</code> to <code>connect.jms.converters</code></li>\n<li>Remove JMS <code>connect.jms.converters</code> and replace my kcql <code>withConverters</code></li>\n<li>Remove JMS <code>connect.jms.queues</code> and replace my kcql <code>withType QUEUE</code></li>\n<li>Remove JMS <code>connect.jms.topics</code> and replace my kcql <code>withType TOPIC</code></li>\n<li>Rename Mqtt <code>connect.mqtt.source.kcql</code> to <code>connect.mqtt.kcql</code></li>\n<li>Rename Mqtt <code>connect.mqtt.user</code> to <code>connect.mqtt.username</code></li>\n<li>Rename Mqtt <code>connect.mqtt.hosts</code> to <code>connect.mqtt.connection.hosts</code></li>\n<li>Remove Mqtt <code>connect.mqtt.converters</code> and replace my kcql <code>withConverters</code></li>\n<li>Remove Mqtt <code>connect.mqtt.queues</code> and replace my kcql <code>withType=QUEUE</code></li>\n<li>Remove Mqtt <code>connect.mqtt.topics</code> and replace my kcql <code>withType=TOPIC</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.kcql</code> to <code>connect.hazelcast.kcql</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.group.name</code> to <code>connect.hazelcast.group.name</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.group.password</code> to <code>connect.hazelcast.group.password</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.cluster.members</code> tp <code>connect.hazelcast.cluster.members</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.batch.size</code> to <code>connect.hazelcast.batch.size</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.error.policy</code> to <code>connect.hazelcast.error.policy</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.max.retries</code> to <code>connect.hazelcast.max.retries</code></li>\n<li>Rename Hazelcast <code>connect.hazelcast.sink.retry.interval</code> to <code>connect.hazelcast.retry.interval</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.kcql</code> to <code>connect.volt.kcql</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.connection.servers</code> to <code>connect.volt.servers</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.connection.user</code> to <code>connect.volt.username</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.connection.password</code> to <code>connect.volt.password</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.error.policy</code> to <code>connect.volt.error.policy</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.max.retries</code> to <code>connect.volt.max.retries</code></li>\n<li>Rename VoltDB <code>connect.volt.sink.retry.interval</code> to <code>connect.volt.retry.interval</code></li>\n</ul><p><strong>0.2.5 (8 Apr 2017)</strong></p>\n<ul><li>Added Azure DocumentDB Sink Connector</li>\n<li>Added JMS Source Connector.</li>\n<li>Added UPSERT to Elastic Search</li>\n<li>Support Confluent 3.2 and Kafka 0.10.2.</li>\n<li>Cassandra improvements <code>withunwrap</code></li>\n<li>Upgrade to Kudu 1.0 and CLI 1.0</li>\n<li>Add ingest_time to CoAP Source</li>\n<li>InfluxDB bug fixes for tags and field selection.</li>\n<li>Added Schemaless Json and Json with schema support to JMS Sink.</li>\n<li>Support for Cassandra data type of <code>timestamp</code> in the Cassandra Source for timestamp tracking.</li>\n</ul><p><strong>0.2.4</strong> (26 Jan 2017)</p>\n<ul><li>Added FTP and HTTP Source.</li>\n<li>Added InfluxDB tag support. KCQL: INSERT INTO targetdimension <code>SELECT * FROM influx-topic WITHTIMESTAMP sys_time() WITHTAG(field1, CONSTANT_KEY1=CONSTANT_VALUE1, field2,CONSTANT_KEY2=CONSTANT_VALUE1)</code></li>\n<li>Added InfluxDb consistency level. Default is <code>ALL</code>. Use <code>connect.influx.consistency.level</code> to set it to ONE/QUORUM/ALL/ANY</li>\n<li>InfluxDb <code>connect.influx.sink.route.query</code> was renamed to <code>connect.influx.sink.kcql</code></li>\n<li>Added support for multiple contact points in Cassandra</li>\n</ul><p><strong>0.2.3</strong> (5 Jan 2017)</p>\n<ul><li>Added CoAP Source and Sink.</li>\n<li>Added MongoDB Sink.</li>\n<li>Added MQTT Source.</li>\n<li>Hazelcast support for ring buffers.</li>\n<li>Redis support for Sorted Sets.</li>\n<li>Added start scripts.</li>\n<li>Added Kafka Connect and Schema Registry CLI.</li>\n<li>Kafka Connect CLI now supports pause/restart/resume; checking connectors on the classpath and validating configuration of connectors.</li>\n<li>Support for <code>Struct</code>, <code>Schema.STRING</code> and <code>Json</code> with schema in the Cassandra, ReThinkDB, InfluxDB and MongoDB sinks.</li>\n<li>Rename <code>export.query.route</code> to <code>sink.kcql</code>.</li>\n<li>Rename <code>import.query.route</code> to <code>source.kcql</code>.</li>\n<li>Upgrade to KCQL 0.9.5 - Add support for <code>STOREAS</code> so specify target sink types, e.g. Redis Sorted Sets, Hazelcast map, queues, ringbuffers.</li>\n</ul><h3><a id=\"user-content-building\" class=\"anchor\" aria-hidden=\"true\" href=\"#building\"></a>Building</h3>\n<p><em><strong>Requires gradle 3.0 to build.</strong></em></p>\n<p>To build</p>\n<div class=\"highlight highlight-source-shell\"><pre>gradle compile</pre></div>\n<p>To test</p>\n<div class=\"highlight highlight-source-shell\"><pre>gradle test</pre></div>\n<p>To create a fat jar</p>\n<div class=\"highlight highlight-source-shell\"><pre>gradle shadowJar</pre></div>\n<p>You can also use the gradle wrapper</p>\n<pre>./gradlew shadowJar\n</pre>\n<p>To view dependency trees</p>\n<pre>gradle dependencies # or\ngradle :kafka-connect-cassandra:dependencies\n</pre>\n<p>To build a particular project</p>\n<pre>gradle :kafka-connect-elastic5:build\n</pre>\n<p>To create a jar of a particular project:</p>\n<pre>gradle :kafka-connect-elastic5:shadowJar\n</pre>\n<h2><a id=\"user-content-contributing\" class=\"anchor\" aria-hidden=\"true\" href=\"#contributing\"></a>Contributing</h2>\n<p>We'd love to accept your contributions! Please use GitHub pull requests: fork the repo, develop and test your code,\n<a href=\"http://karma-runner.github.io/1.0/dev/git-commit-msg.html\" rel=\"nofollow\">semantically commit</a> and submit a pull request. Thanks!</p>\n</article>",
        "created_at": "2018-04-18T20:32:51+0000",
        "updated_at": "2018-04-18T20:33:12+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 11,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/11728472?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9599"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9586,
        "uid": null,
        "title": "Making the Change from Thrift to CQL (Cassandra Query Language)",
        "url": "https://academy.datastax.com/planet-cassandra/making-the-change-from-thrift-to-cql",
        "content": "<h2>CQL Under the Hood</h2><p>At this point, most users should be aware that CQL has replaced Thrift as the standard (and therefore recommended) interface for working with Cassandra. Yet it remains largely misunderstood, as its resemblance to common SQL has left both Thrift veterans and Cassandra newcomers confused about how it translates to the underlying storage layer. This fog must be lifted if you hope to create data models that scale, perform, and insure availability.</p><p>First, let’s define some terms:</p><ul><li>Thrift: a legacy RPC protocol combined with a code generation tool. Deprecated in favor of the native protocol.</li>\n<li>Native protocol: replacement for Thrift that only supports CQL.</li>\n<li>Storage rows: keys and columns as stored on disk.</li>\n<li>CQL rows: an abstraction layer on top of the storage rows.</li>\n</ul><p>It is important to understand that the CQL data representation does not always match the underlying storage structure. This can be challenging for those accustomed to Thrift-based operations, as those were performed directly against the storage layer. But CQL introduces an abstraction on top of the storage rows, and only maps directly in the simplest of schemas.</p><p>If you want to be successful at modeling and querying data in Cassandra, keep in mind that while CQL improves the learning curve, it is not SQL. You must understand what’s happening under the covers, or you will end up with data models that are poorly suited to Cassandra.</p><p>So let’s pull back the curtain and look at what our CQL statements translate to at the storage layer, starting with a simple table.</p><h2> </h2><h3>Single Primary Key</h3><p>The first model we will examine is a straightforward table, which we’ll call books with a single primary key, title:</p><p>We can then insert some data, as follows:</p><p>And finally we can read our newly inserted rows:</p><p>What we’ve done so far looks a lot like ANSI SQL, and in fact these statements would have been valid when run against most modern relational systems. But we know that something very different is happening under the hood.</p><p>To see what this looks like at the storage layer, we can use the old command line interface, <em>cassandra-cli</em>, which allows us to interact directly with storage rows. This CLI is deprecated and will likely disappear in the 3.0 release, but for now we can use it to inspect the books table we created using CQL. Listing the contents of our table produces the following output:</p><p>RowKey: Without Remorse</p><p>As you can see, this is nearly a direct mapping to the CQL rows, except that we have an empty column at the beginning of each row (which is not a mistake; it is used internally by Cassandra).</p><p>Let’s point out a couple of important features of this data. First, remember that the row key is distributed randomly using a hash algorithm, so the results are returned in no particular order. By contrast, columns are stored in sorted order by name, using the natural ordering of the type. In this case, “author” comes before “year” lexicographically, so it appears first in the list. These are critical points, as they are central to effective data modeling.</p><h2> </h2><h3>Compound Keys</h3><p>Now let’s look at a slightly more complex example, one which uses a compound key. In this case, we’ll create a new table, authors, with a compound key using name, year, and title:</p><p>And this is what our data looks like after inserting two CQL rows:</p><p>This is where CQL can begin to cause confusion for those who are unfamiliar with what’s happening at the storage layer. To make sense of this, it’s important to understand the difference between partition keys and clustering columns.</p><h3> </h3><h4>Partition Keys</h4><p>When declaring a primary key, the first field in the list is always the partition key. This translates directly to the storage row key, which is randomly distributed in the cluster via the hash algorithm. In general, you must provide the partition key when issuing queries, so that Cassandra will know which nodes contain the requested data.</p><h3> </h3><h4>Clustering Columns</h4><p>The remaining fields in the primary key declaration are called clustering columns, and these determine the ordering of the data on disk. They are not, however, part of the partition key, so they do not help determine the distribution of data in the cluster. But they play a key role in determining the kinds of queries you can run against your data, as we will see in the remainder of this section.</p><p>Now that you know the difference, it’s time to see what our authors table looks like in its storage layer representation (with the timestamp and marker columns omitted for clarity):</p><p>You will note that our two CQL rows translated to a single storage row, because both of our inserts used the same partition key. But perhaps more interesting is the location of our year and title column values. They are stored as parts of the column name, rather than column values!</p><p>Those who are experienced with Thrift-based data models will recognize this structure, which is referred to as composite columns. You can also observe that the rows are sorted first by year, then by title, which is the way we specified them in our primary key declaration. It is also possible to reverse the stored sort order by adding the WITH CLUSTERING ORDER BY clause, as follows:</p><p>Then, when selecting our rows, we can see that the ordering starts with the latest year and ends with the earliest:</p><p>While this may seem to be a trivial point, it can matter a great deal depending on the types of queries you intend to run on your data. We will examine these implications later in this post when we discuss queries.</p><h3> </h3><h4>Composite Partition Keys</h4><p>In the previous examples we demonstrated the use of a single partition key with multiple clustering columns. But it’s also possible to create a multi-part (or “composite”) partition key. The most common reason for doing this is to improve data distribution characteristics. A prime example of this is the use of time buckets as keys when modeling time-series data. We will cover this in detail later on.</p><p>For now, let’s see what it looks like to create a composite partition key:</p><p>The difference, in case it’s not obvious, is the addition of parentheses around the name and year columns, which specifies that these two columns should form the composite partition key. This leaves title as the only remaining clustering column.</p><p>At the storage layer, this has the effect of moving the year from a component of the column name to a component of the row key, as follows:</p><h3>Why This Matters</h3><p>You may be wondering why it matters how the data is stored internally. In fact it matters a great deal, for several important reasons:</p><ul><li>Your queries must respect the underlying storage. Cassandra doesn’t allow ad hoc queries of the sort that you can perform using SQL on a relational system. If you don’t understand how the data is stored, at best you will be constantly frustrated by the error messages you receive when you try to query your data, and at worst you will suffer poor performance.</li>\n<li>You must choose your partition key carefully, because it must be known at query time, and must also distribute well across the cluster.</li>\n<li>Because of its log-structured storage, Cassandra handles range queries very well. A range query simply means that you select a range of columns for a given key, in the order they are stored.</li>\n<li>You have to carefully order your clustering columns, because the order affects the sort order of your data on disk and therefore determines the kinds of queries you can perform.</li>\n</ul><p>Proper data modeling in Cassandra requires you to structure your data in terms of your queries. This is backward compared to the approach taken in most relational models, where normalization is typically the objective. With Cassandra you must consider your queries first.</p><p>With these principles in mind, let’s examine what happens when you run different kinds of queries, so you can better understand how to structure your data.</p><h2>Understanding Queries</h2><p>In order to make sense of the various types of queries, we will start with a common data model to be used across the following examples. For this data model, we will return to the authors table, with name as the partition key, followed by year and title as clustering columns. We’ll also sort the year in descending order. This table can be created as follows:</p><p>Also, for purposes of these examples, we will assume a replication factor of three and consistency level of QUORUM.</p><h2> </h2><h3>Query by Key</h3><p>We’ll start with a basic query by key:</p><p>For this simple select, the query makes the request to the coordinator node, which owns a replica for our key. The coordinator then retrieves the row from another replica node to satisfy the quorum. Thus, we need a total of two nodes to satisfy the query:</p><p>At the storage layer, this query first locates the partition key, then scans all the columns in order, as follows:</p><p>So even though this appears to be a simple query by key, at the storage layer it actually translates to a range query!</p><h2> </h2><h3>Range Queries</h3><p>If this basic query results in a range query, let’s see what happens when we specifically request a range, like this:</p><p>In this case we’re still selecting a single partition, so the query must only check with two nodes as in the previous example. The difference is that in this case, Cassandra simply scans the columns until it finds one that fails the query predicate:</p><p>Once it finds the year 1991, Cassandra knows there are no more records to scan. Therefore, this query is efficient because it must only read the required number of columns, plus one.</p><p>To recap, there are three key points you should take from this discussion:</p><ol><li>Sequential queries are fast, because they take advantage of Cassandra’s natural sort order at the storage layer.</li>\n<li>Queries by key and combination of key plus clustering column are sequential at the storage layer, which of course means they are optimal.</li>\n<li>Write your data the way you intend to read it. Or, put another way, model your data in terms of your queries, not the other way around. Following this rule will help you avoid the most common data modeling pitfalls that plague those who are transitioning from a relational database.</li>\n</ol><p>Now that we’ve covered the basics of how to build data models that make optimal use of the storage layer, let’s look at one of Cassandra’s newer features: collections.</p><h2>Collections</h2><p>The introduction of collections to CQL addresses some of the concerns that frequently arose regarding Cassandra’s primitive data model. They add richer capabilities that give developers more flexibility when modeling certain types of data.</p><p>Cassandra supports three collection types: sets, lists, and maps. In this section we will examine each of these and take a look at how they’re stored under the hood. But first, it’s important to understand some basic rules regarding collections:</p><ul><li>Each item in a collection must not be more than 64 KB</li>\n<li>A maximum of 64,000 items may be stored in a single collection</li>\n<li>Querying a collection always returns the entire collection</li>\n<li>Collections are best used for relatively small, bounded data sets</li>\n</ul><p>With those rules in mind, we can examine each type of collection in detail, starting with sets.</p><h3>Sets</h3><p>A set in CQL is very similar to a set in your favorite programming language. It is a unique collection of items, meaning it does not allow for duplicates. In most languages sets have no specific ordering; Cassandra, however, stores them in their natural sort order, as you might expect.</p><p>Here is an example of a table of <em>authors</em> which contains a set of <em>books</em>:</p><p>We can then insert some values as follows:</p><p>Cassandra also supports removing items from a set using the UPDATE statement:</p><p>At the storage layer, set values are stored as column names, with the values left blank. This guarantees uniqueness, as any attempt to rewrite the same item would simply result in overwriting the old column name. The storage representation of the books set would look like this:</p><p><img alt=\"Screen Shot 2014-12-03 at 4.33.07 PM\" height=\"40\" src=\"http://planetcassandra.org/wp-content/uploads/2014/12/Screen-Shot-2014-12-03-at-4.33.07-PM.png\" width=\"16\" />                  <img alt=\"Screen Shot 2014-12-03 at 4.33.07 PM\" height=\"40\" src=\"http://planetcassandra.org/wp-content/uploads/2014/12/Screen-Shot-2014-12-03-at-4.33.07-PM.png\" width=\"16\" /><br /><strong>                     Set Name</strong>       <strong>Item</strong></p><p>You can see that the name of the set is stored as the first component of the composite column name, with the item as the second component. Unfortunately Cassandra does not support a contains operation, so you must retrieve the entire set and perform this on the client. But sets can be quite useful as a container for unique items in a variety of data models.</p><h3>Lists</h3><p>At the CQL level, lists look very similar to sets. In the following table, we substitute the set of books from the previous example for a list:</p><p>Insertion is also similar to the set syntax, except that the curly braces are traded for brackets:</p><p>And since lists are ordered, CQL supports prepend and append operations, which involve simply placing the item as either the first (prepend) or second (append) operands, as follows:</p><p>To delete an item, you can refer to it by name:</p><p>Unlike the set, the list structure at the storage layer places the list item in the column value, and the column name instead contains a UUID for ordering purposes. Here’s what it looks like:</p><h3>Maps</h3><p>Lastly, maps are a highly useful structure, as they can offer similar flexibility to the old dynamic column names many grew accustomed to in the Thrift days, as long as the total number of columns is kept to a reasonable number.</p><p>For example, we can use a map to store not only the book title, but the year as well. Here is what that would look like:</p><p>To insert or update an entire map, use the following syntax:</p><p>You can also insert or update a single key using array-like syntax, as follows:</p><p>Specific values can be also be removed by using a DELETE statement:</p><p>At the storage layer, maps look very similar to lists, except the ordering ID is replaced by the map key:</p><p>As you can see, all these collection types make use of composite columns, in the same manner as clustering columns.</p><h2>Multi-Key Queries</h2><p>And now for one of the most common query errors, the IN clause, where we ask for multiple partition keys in a single query. Let’s recall the authors schema we introduced earlier:</p><p>Using this schema, let’s say we want to retrieve a number of books from a list of known authors. Obviously we could write a separate query for each author, but Cassandra also provides a familiar SQL-style syntax for specifying multiple partition keys using the <em>IN</em> clause:</p><p>The question is how will Cassandra fulfill this request? The system will hash the partition key—name in this case—and assign replicas to nodes based on the hash. Using the three authors in our query as examples, we will end up with a distribution resembling the following:</p><p>The important characteristic to note in this distribution is that the keys are dispersed throughout the cluster. If we also remember that a QUORUM read requires consulting with at least two out of three replicas, it is easy to see how this query will result in consulting many nodes. In the following diagram, our client makes a request to one of the nodes, which will act as coordinator. The coordinator must then make requests to at least two replicas for each key in the query:</p><p>The end result is that we required five out of six nodes to fulfill this query! If any one of these calls fails, the entire query will fail. It is easy to see how a query with many keys could require participation from every node in the cluster.</p><p>When using the IN clause, it’s best to keep the number of keys small. There are valid use cases for this clause, such as querying across time buckets for time-series models, but in such cases you should try to size your buckets such that you only need at most two in order to fulfill the request.</p><p>In fact, it is often advisable to issue multiple queries in parallel as opposed to utilizing the IN clause. While the IN clause may save you from multiple network requests to Cassandra, the coordinator must do more work. You can often reduce overall latency and workload with several token-aware queries, as you’ll be talking directly to the nodes that contain the data.</p><h2>Secondary Indices</h2><p>If range queries can be considered optimal for Cassandra’s storage engine, queries based on a <strong>secondary index</strong> fall at the other end of the spectrum. Secondary indices have been part of Cassandra since the 0.7 release, and they are certainly an alluring feature. In fact, for those who are accustomed to modeling data in relational databases, creating an index is often a go-to strategy to achieve better query performance. However, as with most aspects of the transition to Cassandra, this strategy translates poorly.</p><p>To start, let’s get familiar with what secondary indices are and how they work. First off, secondary indices are the only type of index that Cassandra will manage for you, so the terms “index” and “secondary index” actually refer to the same mechanism. The purpose of an index is to allow <strong>query-by-value</strong> functionality, which is not supported naturally. This should be a clue as to the potential danger involved in relying on the index functionality.</p><p>As an example, suppose we want to be able to query authors for a given publisher. Using our earlier <em>authors </em>table, remember that the <em>publisher</em> column has no special properties. It is a simple text column, meaning that by default we cannot filter based on its value. We can take a look at what happens when attempting to do so, as in the following query:</p><p>Running this query results in the following error message, indicating that we’re trying to query by the value of a non-indexed column:</p><p>The obvious remedy is to simply create an index on publisher, as follows:</p><p>Now we can filter on publisher, so our problems are solved, right? Not exactly! Let’s look closely at what Cassandra does to make this work.</p><h3>Secondary Indices Under the Hood</h3><p>At the storage layer, a secondary index is simply another column family, where the key is the value of the indexed column, and the columns contain the row keys of the indexed table. This can be a bit confusing to describe, so let’s visualize it.</p><p>Imagine our authors table contains the following CQL rows:</p><p>An index on publisher would then look like this at the storage layer:</p><p>So a query filtering on publisher will use the index to each author name, then query all the authors by key. This is similar to using the IN clause, since we must query replicas for every key with an entry in the index.</p><p>But it’s actually even worse than the IN clause, because of a very important difference between indices and standard tables. Cassandra co-locates index entries with their associated original table keys. In other words, you will end up with a key for “Random House” in author_publishers on every node that has keys for “Anne Rice” or “Charles Dickens” in authors.</p><p>To make this a bit clearer, the following diagram shows how our co-located <em>authors</em> table and <em>author_publisher </em>index might be distributed across a four-node cluster:</p><p><img alt=\"\" src=\"https://academy.datastax.com/sites/default/files/example-cluster.png\" /></p><p>The objective in using this approach is to be able to determine which nodes own indexed keys, as well as to obtain the keys themselves in a single request. But the problem is we have no idea which token ranges contain indexed keys until we ask each range. So now we end up with a query pattern like this:</p><p><img alt=\"\" src=\"https://academy.datastax.com/sites/default/files/pattern.png\" /></p><p>Obviously the use of secondary indices has an enormous impact on both performance and availability, since so many nodes must participate in fulfilling the query. For this reason it’s best to avoid using them in favor of writing your own indices or choosing another data model entirely.</p><p>If you decide to use a secondary index for a use case where performance and availability are not critical, make sure to only index on low cardinality values, as high cardinality indices do not scale well. But don’t go so low that your index is rendered useless. For example, booleans are bad, as are UUIDs, but birth year could be a reasonable column to index.</p><h2>Deleting Immutable Data</h2><p>We have established that Cassandra employs a log-structured storage engine, where all writes are immutable appends to the log. The implication is that data cannot actually be deleted at the time a DELETE statement is issued. Cassandra solves this by writing a marker, called a <strong>tombstone</strong>, with a timestamp greater than the previous value. This has the effect of overwriting the previous value with an empty one, which will then be compiled in subsequent queries for that column in the same manner as any other update. The actual value of the tombstone is set to the time of deletion, which is used to determine when the tombstone can be removed.</p><h3>Unexpected deletes</h3><p>Of course you can explicitly delete a column using the DELETE statement, but you may be surprised that a tombstone will be generated for every affected storage layer column. To make this clear, let’s remind ourselves about the structure of a single CQL row as represented at the storage layer:</p><p>To this point we have been using a simplified version of the storage row representation. In fact there is a third column used as an internal marker, which has been omitted for clarity. So then, let’s remove the “Patriot Games” entry, as follows:</p><p>Using cqlsh with tracing turned on, we then attempt to read the newly deleted record:</p><p>If you carefully examine the resulting trace, you will notice a line resembling the following:</p><p>So what happened? Our query that returned zero records actually had to read three tombstones to produce the results! The important point to remember is that tombstones cover single storage layer columns, so deleting a CQL row with many columns results in many tombstones.</p><h2> </h2><h3>The Problem with Tombstones</h3><p>You may be wondering why we’re so concerned about tombstones. The last example should provide a hint as to the reason. When a query requires reading tombstones, Cassandra must perform many additional reads to return your results.</p><p>In addition, a query for a key in an sstable that has only tombstones associated with it will still pass through the bloom filter, because the system must reconcile tombstones with other replicas. Since the bloom filter is designed to prevent unnecessary reads for missing data, this means Cassandra will perform extra reads after data has been deleted.</p><p>Now that you understand the basics of deletes and the problems associated with them, it’s important to point out the other ways that deletes can be generated—sometimes in ways you would not expect.</p><h3>Expiring Columns</h3><p>Cassandra offers us a handy feature for purging old data through setting an expiration time, called a <strong>TTL</strong>, at the column level. There are many valid reasons to set TTL values, and they can help to avoid unbounded data accumulation over time. Setting a TTL on a column is straightforward, and can be accomplished using either an <em>INSERT</em> or <em>UPDATE</em>statement as follows (note that TTL values are in seconds):</p><p>This can be useful when dealing with ephemeral data, but you must take care when employing this strategy, because an expired column results in a tombstone as in any other form of delete.</p><h4>How NOT to use TTLs</h4><p>A common reason to expire columns is in the case of time-series data. Imagine we want to display a feed of comments associated with a news article, where the newest post appears on top. To avoid holding onto them indefinitely, we set them to expire after a few hours.</p><p>So we end up with a model that resembles the following:</p><p>It’s important to note that this model is perfectly acceptable so far. Where we can run into problems is when we naively attempt to query for the latest values. It can be tempting to assume that we can simply query everything for a given articleID, with the expectation that old columns will simply disappear. In other words, we perform a query like this:</p><p>In some ways this expectation is correct. Old values will disappear from the result set, and for a period of time this query will perform perfectly well. But gradually we will accumulate tombstones as columns reach their expiration time, and this query requires that we read all columns in the storage row. Eventually, we will reach a point where Cassandra will be reading more tombstones than real values!</p><p>The solution is simple. We must add a range filter on timestamp, which will tell Cassandra to stop scanning columns at approximately as far back in time as the tombstones will start. In this case, we don’t want to read any columns older than three hours, so our new query looks like this:</p><p>Note that you will have to calculate the timestamp in your application, as CQL does not currently support arithmetic operations.</p><p>To sum up, expiring columns can be highly useful as long as you do so wisely. Make sure your usage pattern avoids reading excessive numbers of tombstones. Often you can use range filters to accomplish this goal.</p><h3>When Null Does Not Mean Empty</h3><p>There is an even subtler (and more insidious) way to inadvertently create tombstones: by inserting <em>null</em> values. Let’s take a look at how we might cause this situation unwittingly.</p><p>We know that Cassandra stores columns sparsely, meaning that unspecified values simply aren’t written. So it would seem logical that setting a column to <em>null</em> would result in a missing column. In fact, writing a <em>null</em> is the same thing as explicitly deleting a column, and therefore a tombstone is written for that column!</p><p>There is a simple reason why this is the case. While Cassandra supports separate INSERT and UPDATE statements, all writes are fundamentally the same under the covers. And because all writes are simply append operations, there is no way for the system to know whether a previous value exists for the column. Therefore Cassandra must actually write a tombstone in order to guarantee any old values are deleted.</p><p>While it may seem as though this would be easy to avoid—by just not writing <em>null</em> values—it is fairly easy to mistakenly allow this to happen when using prepared statements. Imagine a data model that includes many sparsely populated columns. It is tempting to create a single prepared statement with all potential columns, then set the unused columns to<em>null</em>. It is also possible that callers of an insert method might pass in <em>null</em> values. If this condition is not checked, it is easy to see how tombstones could be accumulated without realizing this is happening.</p><p>So to wrap things up, remember two things:</p><ol><li>Thrift is dead. Switch to CQL as soon as possible.</li>\n<li>But make sure you understand your data models and queries. It’s not SQL.</li>\n</ol><p><strong><a href=\"https://twitter.com/rs_atl\" target=\"_blank\">Robbie Strickland</a>, Director of Software Development at The Weather Channel</strong><br />Robbie works for The Weather Channel’s digital division, as part of the team that builds backend services for weather.com and the TWC mobile apps. He has been involved in the Cassandra project since 2010 and  contributed in a variety of ways over the years; this includes work on drivers for Scala and C#,  Hadoop integration, leading the <a href=\"http://www.meetup.com/atlcassandra/\" target=\"_blank\">Atlanta Cassandra Users Group</a>, and answering lots of questions on StackOverflow. This post is adapted from Robbie Strickland’s  book, Cassandra High Availability, available for purchase <a href=\"http://www.amazon.com/Cassandra-High-Availability-Robbie-Strickland/dp/1783989122/\" target=\"_blank\">here</a>.</p>",
        "created_at": "2018-04-15T19:43:56+0000",
        "updated_at": "2018-04-15T19:43:59+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 23,
        "domain_name": "academy.datastax.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9586"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          }
        ],
        "is_public": false,
        "id": 9483,
        "uid": null,
        "title": "Snap Cassandra to S3 with tablesnap",
        "url": "https://www.linkedin.com/pulse/snap-cassandra-s3-tablesnap-vijaya-kumar-hosamani/",
        "content": "<p>We were trying to find solution to backup and restore Cassandra hosted on AWS without loosing too much data . We tried using the native <strong>nodetool snapshot </strong>but we will loose data between the snapshots. </p><p>We found a tool which seems to do the job well <a href=\"https://github.com/JeremyGrosser/tablesnap\" target=\"_blank\" rel=\"nofollow noopener\">tablesnap</a> . This tool provides scripts which can help backup , restore SSTables. We can restore data from specific time in past. </p> \n<p>The following are the scripts which are part of this project.</p> \n<ul><li><strong>tablesnap : </strong> This script can be used for backup Cassandra data folder to S3</li> \n <li><strong>tableslurp :</strong> Used to restore the data files from S3</li> \n <li><strong>tablechop :</strong> Used to delete older data files from S3.</li> \n</ul><p><strong>tablesnap</strong> script monitors the Cassandra data folder continuously and copies any new files which are created to configured S3 bucket. This tool also creates a JSON file which lists all the files in the data folder at the time of snapshot providing option to restore at a specific time in past.</p> \n<p>The usage of the tool is as below</p> \n<pre spellcheck=\"false\">tablesnap [-h] -k AWS_KEY -s AWS_SECRET [-r] [-a] [-B] [-p PREFIX]\n                 [--without-index] [--keyname-separator KEYNAME_SEPARATOR]\n                 [-t THREADS] [-n NAME] [-e EXCLUDE | -i INCLUDE]\n                 [--listen-events {IN_MOVED_TO,IN_CLOSE_WRITE}]\n                 [--max-upload-size MAX_UPLOAD_SIZE]\n                 [--multipart-chunk-size MULTIPART_CHUNK_SIZE]\n                 bucket paths [paths ...]\n</pre> \n<p>This tool depends few basic principles of the Cassandra data </p> \n<ul><li>The data is stored as series of files.</li> \n <li>The SSTables are immutable structures and once created they are never modified.</li> \n <li>During compaction the old SSTable files are deleted &amp; a new file is created instead of updating the existing file.</li> \n <li>New SSTables are created in temporary folder and then moved into the data folder. The tool listens to IN_MOVED or CLOSE_WRITE events hence file will be consistent when uploaded to S3</li> \n</ul><p>The usage of the script is as below, in a cluster this needs to be configured on each node with respective node names.</p> \n<pre spellcheck=\"false\">tablesnap -B -a –r --aws-region ap-southeast-1 mybucket -n node1 /var/lib/cassandra/data/mykeyspace\n</pre> \n<p>The content of the JSON file is as shown below which lists all the files which are part of the snapshot. </p> \n<pre spellcheck=\"false\">{  \n   \"/var/lib/cassandra/data/mykeyspace/users-cf815f100f9711e78211dbd467a9ea7d\":[  \n      \"backups\",\n      \"lb-1- big-Index.db\",\n      \"lb-1- big-Digest.adler32\",\n      \"lb-1- big-Statistics.db\",\n      \"lb-1- big-CompressionInfo.db\",\n      \"lb-1- big-Data.db\",\n      \"lb-1- big-Summary.db\",\n      \"lb-1- big-Filter.db\",\n      \"lb-1- big-TOC.txt\"\n   ]\n</pre> \n<pre spellcheck=\"false\">}\n</pre> \n<p>In order to restore the data from the S3 , we can use <strong>tableslurp </strong>tool . We need to identify the snapshot which we need to recover and use the tool to download the data from S3 as below.</p> \n<pre spellcheck=\"false\">udo tableslurp -- aws-region ap-southeast- 1 mybucket -n node1\n/var/lib/cassandra/data/mykeyspace/users-cf815f100f9711e78211dbd467a9ea7d /var/lib/cassandra/data/mykeyspace/users-f234fgsdfsfdsfsdd\n --file lb-2-big-Data.db -o cassandra -g cassandra\n</pre> \n \n<p>In the above commands we are restoring the data on <strong>node1 </strong>which was backed up while SSTable <strong>lb-2-big-Data.db</strong> is created. </p> \n<p>To recover the entire cluster we need to shutdown new nodes &amp; recover respective data with <strong>tableslurp </strong>and then restart the nodes (seed nodes followed by others) . Once the cluster is up run <strong>nodetool repair </strong>to restore all the data. </p> \n<p>One additional step which we have to take care to avoid loosing data which is in <strong>memtable</strong> is by periodic <strong>nodetool flush</strong> which will help to avoid loosing data .</p>",
        "created_at": "2018-04-03T23:50:13+0000",
        "updated_at": "2018-04-04T12:13:21+0000",
        "published_at": "2017-03-29T00:00:00+0000",
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "www.linkedin.com",
        "preview_picture": "https://media.licdn.com/mpr/mpr/shrinknp_400_400/gcrc/dms/image/C4E12AQFxCdkqPl_1Yw/article-cover_image-shrink_600_2000/0?e=2122002000&v=alpha&t=0DhdTB45E9HPx90dNMs6SIb8l5YJ2ZkbtkfeTOfupAA",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9483"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          }
        ],
        "is_public": false,
        "id": 9482,
        "uid": null,
        "title": "JeremyGrosser/tablesnap",
        "url": "https://github.com/JeremyGrosser/tablesnap",
        "content": "<h2>Theory of Operation</h2><p>Tablesnap is a script that uses inotify to monitor a directory for <code>IN_MOVED_TO</code>\nevents and reacts to them by spawning a new thread to upload that file to\nAmazon S3, along with a JSON-formatted list of what other files were in the\ndirectory at the time of the copy.</p><p>When running a Cassandra cluster, this behavior can be quite useful as it\nallows for automated point-in-time backups of SSTables. Theoretically,\ntablesnap should work for any application where files are written to some\ntemporary location, then moved into their final location once the data is\nwritten to disk. Tablesnap also makes the assumption that files are immutable\nonce written.</p><h2>Installation</h2><p>The simplest way to install tablesnap is from the Python Package Index, PyPI.\n<a href=\"https://pypi.python.org/pypi/tablesnap\" rel=\"nofollow\">https://pypi.python.org/pypi/tablesnap</a></p><pre>pip install tablesnap\n</pre><p>This distribution provides a debian/ source directory, allowing it to be built\nas a standard Debian/Ubuntu package and stored in a repository. The Debian\npackage includes an init script that can run and daemonize tablesnap for you.\nTablesnap does not daemonize itself. This is best left to tools like\ninit, supervisord, daemontools, etc.</p><p>We do not currently maintain binary packages of tablesnap. To build the debian\npackage from source, assuming you have a working pbuilder environment:</p><pre>git checkout debian\ngit-buildpackage --git-upstream-branch=master --git-debian-branch=debian --git-builder='pdebuild'\n</pre><p>The daemonized version of the Debian/Ubuntu package uses syslog for logging.\nThe messages are sent to the <code>DAEMON</code> logging facility and tagged with\n<code>tablesnap</code>. If you want to redirect the log output to a log file other than\n<code>/var/log/daemon.log</code> you can filter by this tag. E.g. if you are using\nsyslog-ng you could add</p><pre># tablesnap\nfilter f_tablesnap { filter(f_daemon) and match(\"tablesnap\" value(\"PROGRAM\")); };\ndestination d_tablesnap { file(\"/var/log/tablesnap.log\"); };\nlog { source(s_src); filter(f_tablesnap); destination(d_tablesnap); flags(final); };\n</pre><p>to <code>/etc/syslog-ng/syslog-ng.conf</code>.</p><p>If you are not a Debian/Ubuntu user or do not wish to install the tablesnap\npackage, you may copy the tablesnap script anywhere you'd like and run it from\nthere. Tablesnap depends on the pyinotify and boto Python packages. These are\navailable via \"pip install pyinotify; pip install boto;\", or as packages from\nmost common Linux distributions.</p><h2>Configuration</h2><p>All configuration for tablesnap happens on the command line. If you are using\nthe Debian package, you'll set these options in the <code>DAEMON_OPTS</code> variable in\n<code>/etc/default/tablesnap</code>.</p><pre>usage: tablesnap [-h] -k AWS_KEY -s AWS_SECRET [-r] [-a] [-B] [-p PREFIX]\n                 [--without-index] [--keyname-separator KEYNAME_SEPARATOR]\n                 [-t THREADS] [-n NAME] [-e EXCLUDE | -i INCLUDE]\n                 [--listen-events {IN_MOVED_TO,IN_CLOSE_WRITE}]\n                 [--max-upload-size MAX_UPLOAD_SIZE]\n                 [--multipart-chunk-size MULTIPART_CHUNK_SIZE]\n                 bucket paths [paths ...]\nTablesnap is a script that uses inotify to monitor a directory for events and\nreacts to them by spawning a new thread to upload that file to Amazon S3,\nalong with a JSON-formatted list of what other files were in the directory at\nthe time of the copy.\npositional arguments:\n  bucket                S3 bucket\n  paths                 Paths to be watched\noptional arguments:\n  -h, --help            show this help message and exit\n  -k AWS_KEY, --aws-key AWS_KEY\n  -s AWS_SECRET, --aws-secret AWS_SECRET\n  -r, --recursive       Recursively watch the given path(s)s for new SSTables\n  -a, --auto-add        Automatically start watching new subdirectories within\n                        path(s)\n  -B, --backup          Backup existing files to S3 if they are not already\n                        there\n  -p PREFIX, --prefix PREFIX\n                        Set a string prefix for uploaded files in S3\n  --without-index       Do not store a JSON representation of the current\n                        directory listing in S3 when uploading a file to S3.\n  --keyname-separator KEYNAME_SEPARATOR\n                        Separator for the keyname between name and path.\n  -t THREADS, --threads THREADS\n                        Number of writer threads\n  -n NAME, --name NAME  Use this name instead of the FQDN to identify the\n                        files from this host\n  -e EXCLUDE, --exclude EXCLUDE\n                        Exclude files matching this regular expression from\n                        upload.WARNING: If neither exclude nor include are\n                        defined, then all files matching \"-tmp\" are excluded.\n  -i INCLUDE, --include INCLUDE\n                        Include only files matching this regular expression\n                        into upload.WARNING: If neither exclude nor include\n                        are defined, then all files matching \"-tmp\" are\n                        excluded.\n  --listen-events {IN_MOVED_TO,IN_CLOSE_WRITE,IN_CREATE}\n                        Which events to listen on, can be specified multiple\n                        times. Values: IN_MOVED_TO, IN_CLOSE_WRITE, IN_CREATE\n                        (default: IN_MOVED_TO, IN_CLOSE_WRITE)\n  --max-upload-size MAX_UPLOAD_SIZE\n                        Max size for files to be uploaded before doing\n                        multipart (default 5120M)\n  --multipart-chunk-size MULTIPART_CHUNK_SIZE\n                        Chunk size for multipart uploads (default: 256M or 10%\n                        of free memory if default is not available)\n</pre><p>For example:</p><pre>$ tablesnap -k AAAAAAAAAAAAAAAA -s BBBBBBBBBBBBBBBB me.synack.sstables /var/lib/cassandra/data/GiantKeyspace\n</pre><p>This would cause tablesnap to use the given Amazon Web Services credentials to\nbackup the SSTables for my <code>GiantKeyspace</code> to the S3 bucket named\n<code>me.synack.sstables</code>.</p><h2>Questions, Comments, and Help</h2><p>The fine folks in <code>#cassandra-ops</code> on <code>irc.freenode.net</code> are an excellent\nresource for getting tablesnap up and running, and also for solving more\ngeneral Cassandra issues.</p>",
        "created_at": "2018-04-03T23:50:04+0000",
        "updated_at": "2018-05-25T21:12:40+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 4,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/2151?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9482"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9480,
        "uid": null,
        "title": "Cassandra Backup and Restore - Backup in AWS using EBS Volumes",
        "url": "http://thelastpickle.com/blog/2018/04/03/cassandra-backup-and-restore-aws-ebs.html",
        "content": "<p>Data is critical to modern business and operational teams need to have a Disaster Recovery Plan (DRP) to deal with the risks of potential data loss. At TLP, we are regularly involved in the data recovery and restoration process and in this post we will share information we believe will be useful for those interested in initiating or improving their backup and restore strategy for Apache Cassandra. We will consider some common solutions, and detail the solution we consider the most efficient in AWS + EBS environments as it allows the best Recovery Time Objective (RTO) and is relatively easy to implement.</p><ul><li><a href=\"#why-do-we-need-to-backup-cassandra-data\">Why Do We Need to Backup Cassandra Data?</a></li>\n  <li><a href=\"#backup-and-restore-solutions\">Backup and Restore Solutions</a></li>\n  <li><a href=\"#using-snapshots\">Using Snapshots</a></li>\n  <li><a href=\"#using-native-incremental-backups\">Using Native Incremental Backups</a></li>\n  <li><a href=\"#open-source-tools-tablesnap\">Open-source Tools: TableSnap</a></li>\n  <li><a href=\"#commercial-solutions-datastax-enterprise-datosio\">Commercial Solutions (Datastax Enterprise, datos.io)</a></li>\n  <li><a href=\"#the-manual-copypaste-option\">The Manual Copy/Paste Option</a>\n    <ul><li><a href=\"#limitations\">Limitations</a></li>\n      <li><a href=\"#demonstration-of-the-manual-copypaste-approach\">Demonstration of the Manual Copy/Paste Approach</a></li>\n      <li><a href=\"#create-a-backup-using-copypaste\">Create a Backup using Copy/Paste</a></li>\n      <li><a href=\"#simulation-of-a-major-outage\">Simulation of a Major Outage</a></li>\n      <li><a href=\"#restore-the-service-and-data-with-copypaste\">Restore the Service and Data with Copy/Paste</a></li>\n    </ul></li>\n  <li><a href=\"#the-copypaste-approach-on-aws-ec2-with-ebs-volumes\">The ‘Copy/Paste’ Approach on AWS EC2 with EBS Volumes</a>\n    <ul><li><a href=\"#backup-policy-and-automated-snapshots\">Backup policy and automated snapshots</a></li>\n      <li><a href=\"#restore-procedure-aws\">Restore procedure (AWS)</a></li>\n      <li><a href=\"#summing-up\">Summing up</a></li>\n    </ul></li>\n  <li><a href=\"#lets-compare\">Let’s compare!</a></li>\n</ul><h2 id=\"why-do-we-need-to-backup-cassandra-data\">Why Do We Need to Backup Cassandra Data?</h2>\n<p>Apache Cassandra data is replicated, it makes sense to take a moment to understand why backups still matters for the distributed databases.</p>\n<p>In certain cases when a cluster is poorly configured it can be prone to total data loss. Examples of such cases are:</p>\n<ul><li>Using only a datacenter or non-physically distributed hardware</li>\n  <li>Placing all nodes in one rack</li>\n  <li>Using SAN for storage (other than within a rack and properly configured)</li>\n</ul><p>If the hardware being rely on crashes in any of the above cases, the data might be definitely lost. Thus, it is vital to have a backup even prior to fixing any design mistakes. This holds even if the RPO (Recovery Point Objective), the date of your last backup, was a week ago. Losing a week of data is better than losing years of data, in most cases.</p>\n<p>Note that a backup strategy cannot replace a well distributed system, mostly because restore is a slow and heavy process and will always generate a small gap - a period of time during which data will be lost. However, it can prevent a bigger, or total, data loss.</p>\n<p>In fact, even when Apache Cassandra is well configured, it makes sense to have some backups. Imagine an operator needs to wipe the data on a staging or testing cluster and runs the command <code class=\"highlighter-rouge\">rm -rf /var/lib/cassandra/*</code> in parallel via Chef or Capsitrano, only to find out the command was accidentally run in the production cluster instead (wrong terminal, bad alias, bad script configuration, etc…). If you are responsible for this cluster, or are just the person unlucky enough to have pressed the wrong button, you will be very glad to have a backup somewhere.</p>\n<p>Apache Cassandra will gently replicate any operation on a node throughout the cluster, including user mistakes that could potentially lose data, such as <code class=\"highlighter-rouge\">DROP TABLE X</code> or <code class=\"highlighter-rouge\">TRUNCATE TABLE Y</code>. Luckily for people facing this, there is a safeguard as automatic snapshots are taken by default (see the snapshots section below).</p>\n<p>But if the problem comes from heavy batch of standard operations (<code class=\"highlighter-rouge\">INSERT</code> / <code class=\"highlighter-rouge\">UPDATE</code> / <code class=\"highlighter-rouge\">DELETE</code>), it is sometimes better to go back to a known safe point and accept losing some data than trying to deal with this new situation.</p>\n<p>A backup strategy is not foolproof, rather it just reduces the odds that something goes very wrong to a very low level. Having a full set of cluster data somewhere else, on a cold storage can be very useful. Even though it costs money and effort to put a backup and restore strategy in place, it is insignificant compared to a possible total loss of data.</p>\n<h2 id=\"backup-and-restore-solutions\">Backup and Restore Solutions</h2>\n<p>Any backup strategy will be limited by technical possibilities, as there is often a lot of data to move around when making a backup of a Cassandra cluster. Budget considerations are the second biggest constraints in many cases, as is task prioritization. Building the backup strategy is about finding the best tradeoff between these constraints and the desired RPO and RTO.</p>\n<p>Other considerations are how valuable the data is, how the risk data events is evaluated, and on the maximum data loss that is acceptable. The only recommendation we would make in this regard is to plan for your worst case scenario.</p>\n<p>Depending on the cluster, distinct solutions can be extremely efficient or perform very poorly and not reach RPO and RTO goals. What matters in order to make the right call is to understand your needs and what performances each solution provides in your own environment. There are a number of articles already covering the most common backup and restore solutions, such as https://devops.com/things-know-planning-cassandra-backup/, thus we are going to focus here on presenting one of these options, that is probably less well known, but is an excellent way to backup on AWS when using EBS Volumes.</p>\n<p>Specifically, in the following sections we will review the RPO, RTO, set up and running costs, and ease of setup for the following backup methods:\nSnapshots\nIncremental backups\nTableSnap\nSome commercial solutions\nThe copy/paste method\nAWS Snapshots/EBS Attach.</p>\n<h2 id=\"using-snapshots\">Using Snapshots</h2>\n<p>After flushing all in-memory writes to disk, snapshots creates hard links of each SSTable that is part of the snapshot scope. Doing a snapshot is a simple command and comes with no immediate impacts on performance, capacity storage, or money.</p>\n<p>Running a snapshot on all the nodes and for all the keyspaces solves the potential inconsistencies issues related to copying the data out of the production disk, as the snapshot can be taken relatively simultaneously on all the nodes and will take an instantaneous ‘picture’ of the data at a specific moment.</p>\n<p>As compaction merges SSTables and depending on the compaction pace, the snapshots start consuming significant space on disk. Thus, they will need to be removed as soon as they are extracted from the disk and put into a safe place. Critically for the utility of this approach, removal has to be handled manually, as Apache Cassandra does not automatically remove snapshots.</p>\n<p>Extracting the data from the machines and then copying it to the new machines leads to a relatively bad RPO and RTO as the dataset per node grows. The transfer of such a large amount of data is also likely to raise costs making this operation prohibitively expensive to be performed often enough to be useful, thus not allowing for a good RPO in most cases.</p>\n<h2 id=\"using-native-incremental-backups\">Using Native Incremental Backups</h2>\n<p>Incremental backups allow the operator to take snapshots of the missing SSTables since the latest snapshot, removing the need to snapshot all the data every time. This significantly reduces the snapshot size after the first full snapshot, reducing both the cost of extraction from the local machine and cold storage.</p>\n<p>Thus native incremental backups provide a much better RPO than the full snapshot method alone, considering that data extraction time to the external storage is part of the backup.</p>\n<p>On the downside, incremental backups are made of a lot of rather small SSTables that will need to be compacted together at the recovery time, possibly inducing a slow start from new machines after an outage and creating the need to catch up with compactions. If this is pushed to an extreme and data is spread across a lot of SSTables, the read latency could make the node completely unresponsive due to the resources used for compactions and the need for reads to open a lot of SSTables on disk, thus lowering RTO.</p>\n<p>When picking this option, it is vital to still make a full snapshot from time to time in order to prevent the situation mentioned above. For more information on incremental backups, this article is a bit old, but very well detailed: http://techblog.constantcontact.com/devops/cassandra-and-backups/</p>\n<h2 id=\"open-source-tools-tablesnap\">Open-source Tools: TableSnap</h2>\n<p>Some open-source tools are based on the snapshots or incremental backups methods, described above. These tools aim to make operators’ lives easier by providing some automation to manage snapshots and extract them to the cold storage.</p>\n<p>TableSnap copies any new SSTable in the Apache Cassandra data folder as soon as it is created, thus providing a very good RPO and the ability to go back to a specific point in time. However, it comes at the price of streaming more data than if it were just sending incremental data less frequently. This is because each compaction generates an entirely new SSTables from existing SSTables. The newly generated SSTables are then streamed to the backup destination. Regardless of whether the data exists in old SSTables at the backup destination, the new SSTables will be streamed to the backup destination. If considering this option you will want to have a look at TableSnap, which lives here: https://github.com/JeremyGrosser/tablesnap. You should also consider reading: https://www.linkedin.com/pulse/snap-cassandra-s3-tablesnap-vijaya-kumar-hosamani/.</p>\n<h2 id=\"commercial-solutions-datastax-enterprise-datosio\">Commercial Solutions (Datastax Enterprise, datos.io)</h2>\n<p>In the same way, there are a handful of commercial solutions that handle backups for you. The best known is the backup / restore feature included in DSE - https://www.datastax.com/products/datastax-enterprise. However, this does not work with Apache Cassandra (or open source / community version) and in order to leverage this feature, the entire DSE product will need to be purchased and used. Another alternative that has been around for a while is http://datos.io/. In fact, they do their own comparison of existing backup solutions for Cassandra here: http://datos.io/2017/02/02/choose-right-backup-solution-cassandra/.</p>\n<h2 id=\"the-manual-copypaste-option\">The Manual Copy/Paste Option</h2>\n<p>While this is normally an inefficient backup solution, we spent some time working with it and had some interesting results.  This section will explore the copy/paste option in detail and evaluate the utility. Specifically the scenario involving extracting all the data from the node and putting it back on a new node or cluster.</p>\n<p>Because each node is responsible for a specific range, building a new node or a new cluster for the restore process will change the token distribution, and it is something that can be hard to control when restoring data, especially when using vnodes. Copying the data from all the keyspaces on all the nodes will induce a very messy restore procedure, or a fairly slow one using the <code class=\"highlighter-rouge\">sstableloader</code> for example. This is not really reliable or efficient.</p>\n<p>A way to workaround this problem is to store the entire <code class=\"highlighter-rouge\">data</code> folder for each Cassandra node. This way when restoring a node or a cluster, it is possible to have the information about the schema and token range distribution saved alongside the data, in the <code class=\"highlighter-rouge\">system</code> keyspace.</p>\n<p>With this strategy when the node bootstraps, it detects the IP change, but this is handled and the replacement nodes come back online with the latest copy of the data, including the schema description and the token ranges distribution. This kind of backup stores the data and all the metadata used by Apache Cassandra next to it, which is really convenient, but has its limitations.</p>\n<h3 id=\"limitations\">Limitations</h3>\n<p>Using this strategy the <em>cluster</em> and <em>datacenter</em> names must be identical to the original cluster. Using the exact same configuration to restore is the best approach, just changing the <code class=\"highlighter-rouge\">seeds</code> in <code class=\"highlighter-rouge\">cassandra.yaml</code> and the ‘dynamic’ part of the configuration, <code class=\"highlighter-rouge\">listen_address</code>, <code class=\"highlighter-rouge\">rpc_address</code>, mostly IP related informations.</p>\n<p>In addition, the topology configuration must be identical to the original cluster; that is, each rack must contain the same number of nodes as the original cluster. When data is placed back on the new nodes, data copied from nodes in rack 1 must be placed on the new nodes in rack 1 and so on for each of the other racks in the cluster.</p>\n<h3 id=\"demonstration-of-the-manual-copypaste-approach\">Demonstration of the Manual Copy/Paste Approach</h3>\n<p>To explain how to do this process and show it working, here is a short and simple example using <a href=\"https://github.com/riptano/ccm\">CCM</a>.</p>\n<p>In this example we use a predefined dataset and CCM to reduce the time taken to create the example. In practice this will work the same way with bigger datasets, but will likely take more time if copying the data from an external source of course. It will just be linearly slower to backup and restore as the dataset per node grows.</p>\n<p>Here are the files we are going to use:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ cat schema.cql\nDROP KEYSPACE IF EXISTS tlp_lab;\nCREATE KEYSPACE tlp_lab WITH replication = {'class': 'NetworkTopologyStrategy', 'datacenter1' : 2};\nCREATE TABLE tlp_lab.test_backup_restore (id text,  column1 text, PRIMARY KEY (id, column1));\n$ cat insert.cql\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('1', '100');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('1', '200');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('1', '300');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('2', '100');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('2', '200');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('2', '300');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('3', '100');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('3', '200');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('3', '300');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('4', '100');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('4', '200');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('4', '300');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('5', '100');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('5', '200');\nINSERT INTO tlp_lab.test_backup_restore (id, column1) VALUES ('5', '300');\n</pre></div></div>\n<p>Let’s start by creating the data and make sure the memtables are flushed to an on-disk SSTable, as follows:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node1 cqlsh -- -f schema.cql\n$ ccm node1 cqlsh -- -f insert.cql\n</pre></div></div>\n<p>Then let’s query the data and make sure we got all the expected data;</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node1 cqlsh -- -e \"SELECT * FROM tlp_lab.test_backup_restore;\"\n id | column1\n----+---------\n  4 |     100\n  4 |     200\n  4 |     300\n  3 |     100\n  3 |     200\n  3 |     300\n  5 |     100\n  5 |     200\n  5 |     300\n  2 |     100\n  2 |     200\n  2 |     300\n  1 |     100\n  1 |     200\n  1 |     300\n(15 rows)\n</pre></div></div>\n<p>Looks good, all the data is there, and we can make sure the memtable was flushed and the SSTables have been written to the disk:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ./flush.sh\n$ ll /Users/alain/.ccm/6xCassandra-3-11-1/node*/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88\n/Users/alain/.ccm/6xCassandra-3-11-1/node1/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88:\ntotal 72\ndrwxr-xr-x  11 alain  staff   352 Jan 31 18:07 .\ndrwxr-xr-x   3 alain  staff    96 Feb  2 10:13 ..\n-rw-r--r--   1 alain  staff    43 Jan 31 18:07 mc-1-big-CompressionInfo.db\n-rw-r--r--   1 alain  staff    78 Jan 31 18:07 mc-1-big-Data.db\n-rw-r--r--   1 alain  staff    10 Jan 31 18:07 mc-1-big-Digest.crc32\n-rw-r--r--   1 alain  staff    16 Jan 31 18:07 mc-1-big-Filter.db\n-rw-r--r--   1 alain  staff    10 Jan 31 18:07 mc-1-big-Index.db\n-rw-r--r--   1 alain  staff  4613 Jan 31 18:07 mc-1-big-Statistics.db\n-rw-r--r--   1 alain  staff    47 Jan 31 18:07 mc-1-big-Summary.db\n-rw-r--r--   1 alain  staff    92 Jan 31 18:07 mc-1-big-TOC.txt\n/Users/alain/.ccm/6xCassandra-3-11-1/node2/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88:\ntotal 72\ndrwxr-xr-x  11 alain  staff   352 Jan 31 18:07 .\ndrwxr-xr-x   5 alain  staff   160 Jan 31 18:07 ..\ndrwxr-xr-x   2 alain  staff    64 Jan 31 18:07 backups\n-rw-r--r--   1 alain  staff    43 Jan 31 18:07 mc-1-big-CompressionInfo.db\n-rw-r--r--   1 alain  staff    52 Jan 31 18:07 mc-1-big-Data.db\n-rw-r--r--   1 alain  staff     9 Jan 31 18:07 mc-1-big-Digest.crc32\n-rw-r--r--   1 alain  staff    16 Jan 31 18:07 mc-1-big-Filter.db\n-rw-r--r--   1 alain  staff     5 Jan 31 18:07 mc-1-big-Index.db\n-rw-r--r--   1 alain  staff  4610 Jan 31 18:07 mc-1-big-Statistics.db\n-rw-r--r--   1 alain  staff    47 Jan 31 18:07 mc-1-big-Summary.db\n-rw-r--r--   1 alain  staff    92 Jan 31 18:07 mc-1-big-TOC.txt\n/Users/alain/.ccm/6xCassandra-3-11-1/node3/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88:\ntotal 0\ndrwxr-xr-x  3 alain  staff   96 Jan 31 18:07 .\ndrwxr-xr-x  5 alain  staff  160 Jan 31 18:07 ..\ndrwxr-xr-x  2 alain  staff   64 Jan 31 18:07 backups\n/Users/alain/.ccm/6xCassandra-3-11-1/node4/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88:\ntotal 72\ndrwxr-xr-x  11 alain  staff   352 Jan 31 18:07 .\ndrwxr-xr-x   5 alain  staff   160 Jan 31 18:07 ..\ndrwxr-xr-x   2 alain  staff    64 Jan 31 18:07 backups\n-rw-r--r--   1 alain  staff    43 Jan 31 18:07 mc-1-big-CompressionInfo.db\n-rw-r--r--   1 alain  staff    78 Jan 31 18:07 mc-1-big-Data.db\n-rw-r--r--   1 alain  staff     9 Jan 31 18:07 mc-1-big-Digest.crc32\n-rw-r--r--   1 alain  staff    16 Jan 31 18:07 mc-1-big-Filter.db\n-rw-r--r--   1 alain  staff    10 Jan 31 18:07 mc-1-big-Index.db\n-rw-r--r--   1 alain  staff  4614 Jan 31 18:07 mc-1-big-Statistics.db\n-rw-r--r--   1 alain  staff    47 Jan 31 18:07 mc-1-big-Summary.db\n-rw-r--r--   1 alain  staff    92 Jan 31 18:07 mc-1-big-TOC.txt\n/Users/alain/.ccm/6xCassandra-3-11-1/node5/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88:\ntotal 72\ndrwxr-xr-x  11 alain  staff   352 Jan 31 18:07 .\ndrwxr-xr-x   5 alain  staff   160 Jan 31 18:07 ..\ndrwxr-xr-x   2 alain  staff    64 Jan 31 18:07 backups\n-rw-r--r--   1 alain  staff    43 Jan 31 18:07 mc-1-big-CompressionInfo.db\n-rw-r--r--   1 alain  staff   104 Jan 31 18:07 mc-1-big-Data.db\n-rw-r--r--   1 alain  staff    10 Jan 31 18:07 mc-1-big-Digest.crc32\n-rw-r--r--   1 alain  staff    16 Jan 31 18:07 mc-1-big-Filter.db\n-rw-r--r--   1 alain  staff    15 Jan 31 18:07 mc-1-big-Index.db\n-rw-r--r--   1 alain  staff  4618 Jan 31 18:07 mc-1-big-Statistics.db\n-rw-r--r--   1 alain  staff    47 Jan 31 18:07 mc-1-big-Summary.db\n-rw-r--r--   1 alain  staff    92 Jan 31 18:07 mc-1-big-TOC.txt\n/Users/alain/.ccm/6xCassandra-3-11-1/node6/data0/tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88:\ntotal 72\ndrwxr-xr-x  11 alain  staff   352 Jan 31 18:07 .\ndrwxr-xr-x   5 alain  staff   160 Jan 31 18:07 ..\ndrwxr-xr-x   2 alain  staff    64 Jan 31 18:07 backups\n-rw-r--r--   1 alain  staff    43 Jan 31 18:07 mc-1-big-CompressionInfo.db\n-rw-r--r--   1 alain  staff    82 Jan 31 18:07 mc-1-big-Data.db\n-rw-r--r--   1 alain  staff    10 Jan 31 18:07 mc-1-big-Digest.crc32\n-rw-r--r--   1 alain  staff    16 Jan 31 18:07 mc-1-big-Filter.db\n-rw-r--r--   1 alain  staff    10 Jan 31 18:07 mc-1-big-Index.db\n-rw-r--r--   1 alain  staff  4614 Jan 31 18:07 mc-1-big-Statistics.db\n-rw-r--r--   1 alain  staff    47 Jan 31 18:07 mc-1-big-Summary.db\n-rw-r--r--   1 alain  staff    92 Jan 31 18:07 mc-1-big-TOC.txt\n</pre></div></div>\n<h3 id=\"create-a-backup-using-copypaste\">Create a Backup using Copy/Paste</h3>\n<p>Now let’s make a backup (a simple copy) of the entire <code class=\"highlighter-rouge\">data0</code> folder for all nodes.</p>\n<p>In our test case, this would be enough:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ for i in {1..6}; do cp -rp /Users/alain/.ccm/6xCassandra-3-11-1/node$i/data0 backup/node$i/; done\n$ cd backups/\n$ ll -d */*/\ndrwxr-xr-x  26 alain  staff  832 Jan 31 17:34 node1/system/\ndrwxr-xr-x   6 alain  staff  192 Jan 31 17:34 node1/system_auth/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 17:34 node1/system_distributed/\ndrwxr-xr-x  12 alain  staff  384 Jan 31 17:34 node1/system_schema/\ndrwxr-xr-x   4 alain  staff  128 Jan 31 17:34 node1/system_traces/\ndrwxr-xr-x   3 alain  staff   96 Feb  2 10:13 node1/tlp_lab/\ndrwxr-xr-x  26 alain  staff  832 Jan 31 17:34 node2/system/\ndrwxr-xr-x   6 alain  staff  192 Jan 31 17:34 node2/system_auth/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 17:34 node2/system_distributed/\ndrwxr-xr-x  12 alain  staff  384 Jan 31 17:34 node2/system_schema/\ndrwxr-xr-x   4 alain  staff  128 Jan 31 17:34 node2/system_traces/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 18:07 node2/tlp_lab/\ndrwxr-xr-x  26 alain  staff  832 Jan 31 17:34 node3/system/\ndrwxr-xr-x   6 alain  staff  192 Jan 31 17:34 node3/system_auth/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 17:34 node3/system_distributed/\ndrwxr-xr-x  12 alain  staff  384 Jan 31 17:34 node3/system_schema/\ndrwxr-xr-x   4 alain  staff  128 Jan 31 17:34 node3/system_traces/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 18:07 node3/tlp_lab/\ndrwxr-xr-x  26 alain  staff  832 Jan 31 17:34 node4/system/\ndrwxr-xr-x   6 alain  staff  192 Jan 31 17:34 node4/system_auth/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 17:34 node4/system_distributed/\ndrwxr-xr-x  12 alain  staff  384 Jan 31 17:34 node4/system_schema/\ndrwxr-xr-x   4 alain  staff  128 Jan 31 17:34 node4/system_traces/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 18:07 node4/tlp_lab/\ndrwxr-xr-x  26 alain  staff  832 Jan 31 17:34 node5/system/\ndrwxr-xr-x   6 alain  staff  192 Jan 31 17:34 node5/system_auth/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 17:34 node5/system_distributed/\ndrwxr-xr-x  12 alain  staff  384 Jan 31 17:34 node5/system_schema/\ndrwxr-xr-x   4 alain  staff  128 Jan 31 17:34 node5/system_traces/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 18:07 node5/tlp_lab/\ndrwxr-xr-x  26 alain  staff  832 Jan 31 17:34 node6/system/\ndrwxr-xr-x   6 alain  staff  192 Jan 31 17:34 node6/system_auth/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 17:34 node6/system_distributed/\ndrwxr-xr-x  12 alain  staff  384 Jan 31 17:34 node6/system_schema/\ndrwxr-xr-x   4 alain  staff  128 Jan 31 17:34 node6/system_traces/\ndrwxr-xr-x   5 alain  staff  160 Jan 31 18:07 node6/tlp_lab/\n</pre></div></div>\n<h3 id=\"simulation-of-a-major-outage\">Simulation of a Major Outage</h3>\n<p>Then we simulate the loss of two nodes by shutting them down, as one would not affect the cluster, working with a Replication Factor of 2 (RF = 2).</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node1 stop\n$ ccm node2 stop\n$ ccm node3 nodetool status\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack\nDN  127.0.0.1  176.15 KiB  1            33.3%             b6497c83-0e85-425e-a739-506dd882b013  rack1\nDN  127.0.0.2  149.68 KiB  1            33.3%             e4919f5a-3b89-4fd1-a163-aaae99aa5cbd  rack1\nUN  127.0.0.3  149.02 KiB  1            33.3%             c6e8e8c4-25b1-444a-ac50-ff00505fbaf7  rack1\nUN  127.0.0.4  129.8 KiB  1            33.3%             23bdc0df-1ba5-4bc7-be41-01436fa23925  rack1\nUN  127.0.0.5  139.19 KiB  1            33.3%             fb438687-8c3e-4ad7-b83d-275948f4241f  rack1\nUN  127.0.0.6  144.11 KiB  1            33.3%             63777a4b-af44-4fc2-baff-74c585d4a217  rack1\n</pre></div></div>\n<p>And now reading the data again, we notice the query is failing because some token ranges that were owned only by these 2 nodes are no longer available and we are requesting all the data.</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node3 cqlsh -- -e \"SELECT * FROM tlp_lab.test_backup_restore;\"\n&lt;stdin&gt;:1:NoHostAvailable:\n</pre></div></div>\n<p>In this example, these two nodes are now considered completely lost and there is no way to get the data back. What we have are the backups we made, just in time.</p>\n<h3 id=\"restore-the-service-and-data-with-copypaste\">Restore the Service and Data with Copy/Paste</h3>\n<p>To restore the service and the data, we first have to create two replacement nodes, without having them joining the cluster yet. With CCM it can be done like this:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>ccm add node7 -i 127.0.0.7 -j 7700\nccm add node8 -i 127.0.0.8 -j 7800\n</pre></div></div>\n<p>Then we want to copy the data we saved from <code class=\"highlighter-rouge\">node1</code> to the new <code class=\"highlighter-rouge\">node7</code> and from the backup of <code class=\"highlighter-rouge\">node2</code> to <code class=\"highlighter-rouge\">node8</code> after cleaning any data possibly present:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ rm -rf /Users/alain/.ccm/6xCassandra-3-11-1/node7/data0/*\n$ rm -rf /Users/alain/.ccm/6xCassandra-3-11-1/node8/data0/*\n$ cp -rp backup/node1/* /Users/alain/.ccm/6xCassandra-3-11-1/node7/data0/\n$ cp -rp backup/node2/* /Users/alain/.ccm/6xCassandra-3-11-1/node8/data0/\n</pre></div></div>\n<p>Note: If some data or commit logs are already present, it could conflict with data we want to restore and even in worst case mess up the ownership as the commit logs files would be replayed, possibly on the system table as well. Always start in clean environment then restore old files.</p>\n<p>All the data from <code class=\"highlighter-rouge\">node1</code> is now in <code class=\"highlighter-rouge\">node7</code>, including the schema and information about the cluster:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ cd /Users/alain/.ccm/6xCassandra-3-11-1/node7/data0\n$ ll -d */*/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/IndexInfo-9f5c6374d48532299a0a5094af9ad1e3/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/available_ranges-c539fcabd65a31d18133d25605643ee3/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/batches-919a4bc57a333573b03e13fc3f68b465/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/batchlog-0290003c977e397cac3efdfdc01d626b/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/built_views-4b3c50a9ea873d7691016dbc9c38494a/\ndrwxr-xr-x  35 alain  staff  1120 Feb  2 10:16 system/compaction_history-b4dbb7b4dc493fb5b3bfce6e434832ca/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/hints-2666e20573ef38b390fefecf96e8f0c7/\ndrwxr-xr-x  19 alain  staff   608 Feb  2 10:12 system/local-7ad54392bcdd35a684174e047860b377/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/paxos-b7b7f0c2fd0a34108c053ef614bb7c2d/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/peer_events-59dfeaea8db2334191ef109974d81484/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system/peers-37f71aca7dc2383ba70672528af04d4f/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/prepared_statements-18a9c2576a0c3841ba718cd529849fef/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/range_xfers-55d764384e553f8b9f6e676d4af3976d/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_aggregates-a5fc57fc9d6c3bfda3fc01ad54686fea/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_columnfamilies-45f5b36024bc3f83a3631034ea4fa697/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_columns-296e9c049bec3085827dc17d3df2122a/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_functions-d1b675fe2b503ca48e49c0f81989dcad/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_keyspaces-b0f2235744583cdb9631c43e59ce3676/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_triggers-0359bc7171233ee19a4ab9dfb11fc125/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/schema_usertypes-3aa752254f82350b8d5c430fa221fa0a/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system/size_estimates-618f817b005f3678b8a453f3930b8e86/\ndrwxr-xr-x  35 alain  staff  1120 Feb  2 10:16 system/sstable_activity-5a1ff267ace03f128563cfae6103c65e/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/transferred_ranges-6cad20f7d4f53af2b6e20da33c6c1f83/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system/views_builds_in_progress-b7f2c10878cd3c809cd5d609b2bd149c/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_auth/resource_role_permissons_index-5f2fbdad91f13946bd25d5da3a5c35ec/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_auth/role_members-0ecdaa87f8fb3e6088d174fb36fe5c0d/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_auth/role_permissions-3afbe79f219431a7add7f5ab90d8ec9c/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_auth/roles-5bc52802de2535edaeab188eecebb090/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_distributed/parent_repair_history-deabd734b99d3b9c92e5fd92eb5abf14/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_distributed/repair_history-759fffad624b318180eefa9a52d1f627/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_distributed/view_build_status-5582b59f8e4e35e1b9133acada51eb04/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/aggregates-924c55872e3a345bb10c12f37c1ba895/\ndrwxr-xr-x  19 alain  staff   608 Feb  2 10:16 system_schema/columns-24101c25a2ae3af787c1b40ee1aca33f/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/dropped_columns-5e7583b5f3f43af19a39b7e1d6f5f11f/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/functions-96489b7980be3e14a70166a0b9159450/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/indexes-0feb57ac311f382fba6d9024d305702f/\ndrwxr-xr-x  35 alain  staff  1120 Feb  2 10:16 system_schema/keyspaces-abac5682dea631c5b535b3d6cffd0fb6/\ndrwxr-xr-x  19 alain  staff   608 Feb  2 10:16 system_schema/tables-afddfb9dbc1e30688056eed6c302ba09/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/triggers-4df70b666b05325195a132b54005fd48/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/types-5a8b1ca866023f77a0459273d308917a/\ndrwxr-xr-x  27 alain  staff   864 Feb  2 10:16 system_schema/views-9786ac1cdd583201a7cdad556410c985/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_traces/events-8826e8e9e16a372887533bc1fc713c25/\ndrwxr-xr-x   3 alain  staff    96 Jan 31 17:34 system_traces/sessions-c5e99f1686773914b17e960613512345/\ndrwxr-xr-x  11 alain  staff   352 Jan 31 18:07 tlp_lab/test_backup_restore-9e8e3ce006b111e89daea3c19988ea88/\n</pre></div></div>\n<p>At this point we can turn <code class=\"highlighter-rouge\">node7</code> up by simply starting Cassandra service, normally. As I am using CCM:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node7 start\n</pre></div></div>\n<p>To reach this state, where <code class=\"highlighter-rouge\">node1</code> with ip <code class=\"highlighter-rouge\">127.0.0.1</code> have been replaced by <code class=\"highlighter-rouge\">node7</code> with ip <code class=\"highlighter-rouge\">127.0.0.7</code>. Note that <code class=\"highlighter-rouge\">node7</code> is now using the old <code class=\"highlighter-rouge\">node1</code> <code class=\"highlighter-rouge\">Host ID</code>: b6497c83-0e85-425e-a739-506dd882b013</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node3 nodetool status\nDatacenter: datacenter1\n=======================\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address    Load       Tokens       Owns (effective)  Host ID                               Rack\nDN  127.0.0.2  149.68 KiB  1            33.3%             e4919f5a-3b89-4fd1-a163-aaae99aa5cbd  rack1\nUN  127.0.0.3  160.84 KiB  1            33.3%             c6e8e8c4-25b1-444a-ac50-ff00505fbaf7  rack1\nUN  127.0.0.4  151.68 KiB  1            33.3%             23bdc0df-1ba5-4bc7-be41-01436fa23925  rack1\nUN  127.0.0.5  161.16 KiB  1            33.3%             fb438687-8c3e-4ad7-b83d-275948f4241f  rack1\nUN  127.0.0.6  166.05 KiB  1            33.3%             63777a4b-af44-4fc2-baff-74c585d4a217  rack1\nUN  127.0.0.7  202.51 KiB  1            33.3%             b6497c83-0e85-425e-a739-506dd882b013  rack1\n</pre></div></div>\n<p>At this stage we can already access the data fully due to our configuration allowing a tolerance of one node being down:</p>\n<div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>$ ccm node3 cqlsh -- -e \"SELECT * FROM tlp_lab.test_backup_restore;\"\n id | column1\n----+---------\n  4 |     100\n  4 |     200\n  4 |     300\n  3 |     100\n  3 |     200\n  3 |     300\n  5 |     100\n  5 |     200\n  5 |     300\n  2 |     100\n  2 |     200\n  2 |     300\n  1 |     100\n  1 |     200\n  1 |     300\n(15 rows)\n</pre></div></div>\n<p>Finally, after bringing <code class=\"highlighter-rouge\">node8</code> online, we have a fully operational cluster again. We can achieve that with the exact same steps using data from <code class=\"highlighter-rouge\">node2</code> this time.</p>\n<p><strong>Note:</strong> It is important to bring these new nodes up with the same configuration as the node that went down, except when the node IP is used of course, like in <code class=\"highlighter-rouge\">listen_address</code> and possibly in <code class=\"highlighter-rouge\">rpc_address</code>. Usually, having a management system such as <a href=\"https://www.chef.io/chef/\">Chef</a>, <a href=\"https://www.ansible.com/\">Ansible</a>, <a href=\"http://saltstack.com/\">Salt</a>, <a href=\"http://puppet.com\">Puppet</a> or using containers will make adding nodes very straightforward. In the worst case, Apache Cassandra will probably just not start.</p>\n<p>This process can be repeated with all the nodes of the cluster if the entire cluster goes down and a new replacement cluster is to be built.</p>\n<p>This solution is slow, expensive, hard to set up and error-prone if done manually. Yet it works and is quite robust if performed carefully (or even better, automatically).</p>\n<h2 id=\"the-copypaste-approach-on-aws-ec2-with-ebs-volumes\">The ‘Copy/Paste’ Approach on AWS EC2 with EBS Volumes</h2>\n<p>As demonstrated above, the basic ‘copy/paste’ option can be made to work. However, in an emergency situation when an entire cluster is down, the process could be difficult to manage and terribly slow for big datasets.</p>\n<p>Some of the AWS features can take this basic backup and restore option to the next level. I have no interest in advertising for Amazon services, and even personally believe that not locking oneself with a provider or vendor is a great idea. However, in case you are already using AWS and EBS volumes, this solution might be very convenient and not really commit you much more than you already have with AWS. The set of tools offered by AWS for backups on EC2 with EBS volume storage are worth a look for those of you using this environment.</p>\n<p>Instead of copying the the data folder out of the node as we saw earlier, AWS offers to snapshot the EBS volumes. The option is available through the console or API and makes asynchronous and incremental snapshots, transferred to S3 under the hood.</p>\n<p>Because the snapshots are incremental, we can make frequent backups without technical issues or any substantial extra cost.</p>\n<p>The procedure itself to make a full backup is to <code class=\"highlighter-rouge\">snapshot</code> the EBS volume(s) used for Apache Cassandra data storage. To make it simple, a few clicks or lines of code will allow a full backup. It is way more convenient and performant to use the API and make a small script that request all the snapshots at once, even more so when the cluster contains a large number of nodes. Using this functionality results in a more consistent dataset and allows for the automation of backups according to a backup schedule and policy.</p>\n<p><img src=\"http://thelastpickle.com/images/backup-restore-aws/create_snapshot.png\" alt=\"Create Snapshot\" /></p>\n<p>That’s it. At this point we have a backup, in a distant and redundant system. Truth is it takes ‘some time’ to make the backup, but Amazon handles it asynchronously and incrementally. We will observe impacts on performance carefully, specially for the first snapshot. Subsequent snapshot will be incremental, thus probably less impacting.</p>\n<p>While a snapshot of all the EBS volumes attached to nodes in the cluster can be taken simultaneously, be sure that only a single snapshot is run against each EBS volume at a time to prevent harming the EBS volume performances.</p>\n<p>It is a good idea to tag the cluster name, data center, Availability Zone (AZ), and IP address the disk belongs to in the snapshot metadata (name / description / …). This is to help identify the snapshot needed when a partial failure occurs, involving just part of the cluster(s).</p>\n<p>Here is the description of the snapshot feature from AWS: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-creating-snapshot.html</p>\n<h3 id=\"backup-policy-and-automated-snapshots\">Backup policy and automated snapshots</h3>\n<p>Once we have the script to take a snapshot, it is really straightforward to build a script responsible for maintaining a snapshot policy such as:</p>\n<ul><li>Take / keep a snapshot every 30 min for the latest 3 hours, and</li>\n  <li>Keep a snapshot every 6 hours for the last day, delete other snapshots, and</li>\n  <li>Keep a snapshot every day for the last month, delete other snapshots.</li>\n</ul><p>A script in combination with a scheduler to call the script should be enough to have backups in place. Most probably the script will be called at a frequency determined by the lower interval between 2 backup. Here we would call the script every 30 min or less with the example above.</p>\n<p>This script will contain the rules to use to make backups and calls to AWS API to take and delete snapshots depending on the date of the snapshot and the current date, or anything else you would like to use to trigger backups.</p>\n<p>Using AWS Lambda service to execute the backups is possible and should be efficient. The backups being asynchronous, would mean the script should run completely in a matter of a seconds and the Lambda service cost is based on the execution time, thus it could be a good fit.</p>\n<p>To run the script on a regular basis, AWS CloudWatch Events provide events based on time. It is possible to schedule the call to the script in charge of the backup policy this way.</p>\n<p>About Lambda in AWS: https://docs.aws.amazon.com/lambda/latest/dg/welcome.html\nAbout AWS CloudWatch Events: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html</p>\n<h3 id=\"restore-procedure-aws\">Restore procedure (AWS)</h3>\n<p>The restore procedure can be manual, which can be enough to handle a small outage involving a few nodes, or if there is no real time constraints for the RTO. For big clusters and in general, using the API and a script to restore the latest (or a specific) backup will make this process more reliable and scalable than using the AWS console. Often, using the console is nice to test things once, but unsuitable for large scale operations. The AWS API is far more powerful and lends itself well to automated tasks. For the example we will be using the console. Independently of the tool used, the process will always be the same to restore a node.</p>\n<ul><li>\n    <p>Reusing existing instances…</p>\n    <p>If the instance is still accessible but the data is corrupted or unaccessible, we can reuse the same nodes. The node is already configured as required and reusing the instance is practical. In this case:</p>\n    <ol><li>Locate the snapshot to use.</li>\n    </ol><div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>It has to be a snapshot taken from the node to be replaced. If we are replacing or re-creating an entire rack, be sure to use the snapshots for the AZ the rack is in.\n</pre></div></div>\n    <ol><li>\n        <p>Stop Cassandra on the node to restore (if the node is not already down)</p>\n        <div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> nodetool drain &amp;&amp; sudo service cassandra stop\n</pre></div></div>\n      </li>\n      <li>\n        <p>Unmount the currently attached EBS volume at the operating system level, supposing the EBS Volume is mounted as <code class=\"highlighter-rouge\">/var/lib/cassandra/data/</code>, from a bash console, run:</p>\n        <div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> umount /var/lib/cassandra/data/\n</pre></div></div>\n      </li>\n      <li>\n        <p>Detach and terminate the old EBS volume from the AWS console or API.</p>\n        <p><img src=\"http://thelastpickle.com/images/backup-restore-aws/detach_volume.png\" alt=\"Detach Volume\" /></p>\n      </li>\n    </ol></li>\n  <li>\n    <p>…<strong>OR</strong> using new instances…:</p>\n    <ol><li>Create a new instance in the right AZ to replace all the nodes that need to be. The instances can be created without EBS attached to them, but all the other options must be exactly identical to a new node that you would add normally to the existing cluster.</li>\n    </ol></li>\n  <li>\n    <p><strong>Then</strong> restore:</p>\n    <ol><li>\n        <p>For each node that is down, create a new volume from the most recent associated snapshot taken. It is here that the snapshot tags are critical for ensuring the correct snapshot is restored for the node..</p>\n        <p><img src=\"http://thelastpickle.com/images/backup-restore-aws/restore_volume.png\" alt=\"Restore Volume\" /><img src=\"http://thelastpickle.com/images/backup-restore-aws/pick-az-restore.png\" alt=\"Pick the right AZ and disk size\" /></p>\n      </li>\n      <li>\n        <p>Attach the newly created volume to the instance; whether it was a newly created or the original instance..</p>\n        <p><img src=\"http://thelastpickle.com/images/backup-restore-aws/attach_volume.png\" alt=\"Attach the volume to the instance\" /></p>\n        <p><strong>Note:</strong> If the instance started while the EBS volume was not yet attached, be sure to remove any newly created <em>data</em>, <em>commitlog</em>, and <em>saved_caches</em> directories that would have been created on the node’s local storage. Failure to do so could potentially mess up the token ownership as commit logs would be replayed when the node start, thus updating the system data with new token ownership from the previous ‘failed’ start. This breaks consistency guarantees and could eventually lead to a data loss.</p>\n      </li>\n      <li>\n        <p>Mount the new volume from the instance operating system perspective, for example:</p>\n        <div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> mount /dev/xvdq1 /var/lib/cassandra/data/\n</pre></div></div>\n      </li>\n      <li>\n        <p>Start Apache Cassandra:</p>\n        <div class=\"highlighter-rouge\"><div class=\"highlight\"><pre> service cassandra start &amp;&amp; tail -100f /var/log/cassandra/system.log\n</pre></div></div>\n      </li>\n      <li>\n        <p>Finally run a repair on all the nodes and for all the tables where consistency is a concern.</p>\n      </li>\n    </ol></li>\n</ul><p>The node should join the cluster, and other nodes should detect the new IP replacing the old one. Repeat this procedure for all the nodes that need to be brought back (up to 100% of the nodes). Even with big datasets, the cluster should go back to normal <em>fairly</em> quickly and the service should go back online as soon as enough servers have been restored, depending on the configuration.</p>\n<p>It is hard to be precise here as the speed will depend on the use of the API versus the use of the console and then the volume creation will depend on the data size.</p>\n<p>Yet it is optimized by AWS and is definitely way faster than a standard backup transfer. Again, given the number of manual steps and considering how fast a cluster restore should be done in a critical situation, scripting this procedure using the API instead of the console is probably better in most (all?) cases.</p>\n<h3 id=\"summing-up\">Summing up</h3>\n<p>The AWS EBS backup solution comes with some drawbacks:</p>\n<ul><li>The topology used for the restore cluster has to be identical to that of the original cluster. Same number of node, same cluster name, same data center name, same number of nodes per rack, vnodes configuration. The safest approach is not to change anything that does need to be changed.</li>\n  <li>It is somewhat expensive. Enabling snapshots feature for EBS comes at a price.</li>\n  <li>You have to code the snapshot retention policy yourself. Yet it is basic scripting in the language you prefer using AWS API; nothing new for most of AWS users.</li>\n  <li>It is an advanced operation, that bypasses some Apache Cassandra’s safe guards around consistency. An incorrect operation can lead to a data loss. It is important to test the process then probably automate it.</li>\n</ul><p>On the bright side, with this operation we make important improvements on our backup / restore objectives:</p>\n<ul><li>The AWS Snapshots feature provides an immediate snapshot and incremental transfer of the data asynchronously, and because of this it is possible to achieve a very good Recovery Point Objective. It can be <strong>seconds</strong> if needed and as long as machines can handle it.</li>\n  <li>The Recovery Time Objective for this process is quick and consistent. That is, the time required to restore an EBS volume from a snapshot  is <em>fixed</em>. As soon as the EBS volume is mounted and the instance joins the cluster, data is is available. You can expect degraded performances as data will be loading in the background. However this is probably bearable, given the cluster was recovered from complete data loss in a short amount of time. From <a href=\"https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-restoring-volume.html\">AWS documentation</a>:</li>\n</ul><blockquote>\n  <p>New volumes created from existing EBS snapshots load lazily in the background. This means that after a volume is created from a snapshot, there is no need to wait for all of the data to transfer from Amazon S3 to your EBS volume before your attached instance can start accessing the volume and all its data. If your instance accesses data that hasn’t yet been loaded, the volume immediately downloads the requested data from Amazon S3, and continues loading the rest of the data in the background.</p>\n</blockquote>\n<ul><li>AWS service comes at a cost, but:\n    <ul><li>The incremental transfer of the data during the backup phase can save a lot of money compared to a full backup.</li>\n      <li>The cost to setup the backup service is reduced if AWS Lambda, Events scheduler, Snapshots and API are configured to do all of the work.</li>\n      <li>Restore comes at a negligible cost and is very efficient</li>\n    </ul></li>\n  <li>System tables are saved alongside the data, meaning the backup store almost everything we need to restore, including  the schema, the token range owned, topology information.</li>\n</ul><h2 id=\"lets-compare\">Let’s compare!</h2>\n<p>We have been through the overview of some available backup and restore solutions for Apache Cassandra. Hereafter is a table that aims at being a quick evaluation of them, a visual sum up of what is said herein.</p>\n<p>To be fair,  a backup was considered completed when the data was moved off the node to another location. For example <code class=\"highlighter-rouge\">snapshot</code> or <code class=\"highlighter-rouge\">incremental backups</code> solutions can easily have a RPO of 1 second, but the data still remains on the volume as the original data. Hence, if the machine is unreachable, the backup is useless. This reduces the backup efficiency to a smaller scope including for example when recovering from human errors. That is why <code class=\"highlighter-rouge\">snapshots</code> have a bad RPO in the table below, we are considering all the data has to be extracted to some external storage. <code class=\"highlighter-rouge\">Incremental backups</code> perform a bit better as only increments (ie. new data) are extracted.</p>\n<p>We did not compare the commercial solutions. Those evolve quickly and all have a support that will answer any question better than I would. Thus I invite you to contact companies providing this service directly.</p>\n<p><img src=\"http://thelastpickle.com/images/backup-restore-aws/Compare_backup_strategies.png\" alt=\"Compare Backup Strategies\" /></p>\n<p><strong>Note:</strong> We saw how an overall poorly performing solution such as ‘copy/paste’ can turn out to be one of the best option in a specific environment. It is reasonable for a process that performs poorly in the above table to be a reasonable solution that is suitable for your requirements. Feel free to share your experience with us in the comments here or share with the community in the <a href=\"http://cassandra.apache.org/community/\">Apache Cassandra User mailing list</a>.</p>",
        "created_at": "2018-04-03T23:44:39+0000",
        "updated_at": "2018-09-24T19:28:38+0000",
        "published_at": null,
        "published_by": [
          ""
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 37,
        "domain_name": "thelastpickle.com",
        "preview_picture": "http://thelastpickle.com/android-chrome-192x192.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9480"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 22,
            "label": "open.source",
            "slug": "open-source"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 253,
            "label": "analytics",
            "slug": "analytics"
          },
          {
            "id": 955,
            "label": "blockchain",
            "slug": "blockchain"
          }
        ],
        "is_public": false,
        "id": 9437,
        "uid": null,
        "title": "GraphSense - Blockchain Analytics",
        "url": "http://graphsense.info/",
        "content": "<p>GraphSense is an open source platform for analyzing cryptocurrencies\nsuch as <a href=\"https://bitcoin.org/en/\">Bitcoin</a>.</p><ul><li>\n    <p><strong>Address Clustering</strong>: partition the set of addresses observed in a cryptocurrency\necosystem into maximal subsets (clusters) that are likely to be controlled by the same\nreal-world entity.</p>\n  </li>\n  <li>\n    <p><strong>Micro- and Macroscopic Analysis</strong>: inspect main cryptocurrency entities\n(block, transaction, address) and compute summary statistics over the entire\nblockchain.</p>\n  </li>\n  <li>\n    <p><strong>Network Perspective</strong>: apply a network-centric perspective and traverse currency\nflows between addresses and clusters.</p>\n  </li>\n  <li>\n    <p><strong>Horizontal Scalability</strong>: cryptocurrency blockchains <a href=\"https://blockchain.info/charts/blocks-size?timespan=all\">are growing</a>\nand new currencies <a href=\"https://coinmarketcap.com/\">appear on the horizon</a>. To make GraphSense\nfuture-proof, it is built on <a href=\"https://spark.apache.org/\">Apache Spark</a> and <a href=\"http://cassandra.apache.org/\">Cassandra</a> for\nhorizontal scalability.</p>\n  </li>\n</ul><h2 id=\"technical-architecture\">Technical Architecture</h2><p>GraphSense is built on scalable and distributed cluster technology and therefore requires a number of software components. They must be setup and/or executed in the following order:</p><ul><li>\n    <p><a href=\"https://github.com/graphsense/bitcoin-client\">bitcoin-client</a>: a Docker container encapsuling the most-recent Bitcoin client version</p>\n  </li>\n  <li>\n    <p><a href=\"https://github.com/graphsense/graphsense-datafeed\">datafeed</a>: a component for ingesting raw blockchain data and exchange rates into Cassandra</p>\n  </li>\n  <li>\n    <p><a href=\"https://github.com/graphsense/graphsense-transformation\">transformation</a>: a Spark pipeline for computing statistics and network representations from raw blockchain data stored in Cassandra.</p>\n  </li>\n  <li>\n    <p><a href=\"https://github.com/graphsense/graphsense-REST\">rest-api</a>: an API for retrieving data from the underlying Cassandra store</p>\n  </li>\n  <li>\n    <p><a href=\"https://github.com/graphsense/graphsense-dashboard\">dashboard</a>: a user-interface allowing search, inspection, and traversal of cryptocurrency entities</p>\n  </li>\n</ul><h2 id=\"example\">Example</h2><p>The following example shows details about an example Bitcoin address.</p><p><img src=\"http://graphsense.info/assets/screenshot_dashboard.jpeg\" alt=\"screenshot\" /></p><h2 id=\"publications\">Publications</h2><p>Some more technical details about GraphSense are described <a href=\"http://ceur-ws.org/Vol-1695/paper20.pdf\">here</a>; please cite as:</p><div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>@inproceedings{Haslhofer:2016a,\n    title={O Bitcoin Where Art Thou? Insight into Large-Scale Transaction Graphs.},\n    author={Haslhofer, Bernhard and Karl, Roman and Filtz, Erwin},\n    booktitle={SEMANTiCS (Posters, Demos)},\n    year={2016}\n}\n</pre></div></div><p>So far, GraphSense has been used for computing statistics in the following\nscientific papers:</p><p>Filtz, E., Polleres, A., Karl, R., Haslhofer, B.:\n<strong>Evolution of the Bitcoin Address Graph - An Exploratory Longitudinal Study.</strong>\nInternational Data Science Conference (DSC 2017), Salzburg, Austria, 2017.\n<a href=\"https://aic.ai.wu.ac.at/~polleres/publications/filtz-etal-2017IDSC.pdf\">(pdf)</a></p><h2 id=\"contributors\">Contributors</h2><ul><li><a href=\"http://bernhardhaslhofer.info/\">Bernhard Haslhofer</a></li>\n  <li>Roman Karl</li>\n  <li>Mihai Bartha</li>\n  <li>Rainer Stütz</li>\n</ul>",
        "created_at": "2018-03-22T11:26:48+0000",
        "updated_at": "2018-04-16T17:42:41+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 1,
        "domain_name": "graphsense.info",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9437"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9436,
        "uid": null,
        "title": "Static columns in Cassandra and their benefits - Info Support Blog",
        "url": "https://blogs.infosupport.com/static-columns-in-cassandra-and-their-benefits/",
        "content": "<p>In my previous blogpost “<a href=\"http://blogs.infosupport.com/analysis-of-the-impact-of-new-use-cases-on-a-cassandra-data-model-and-operations-in-code/\" target=\"_blank\">Analysis of the impact of new use cases on a Cassandra data model and operations in code</a>” I looked at the risks of deploying Cassandra. In Cassandra we face the challenges of data modelling and designing the CRUD operations (create, read, update, delete), because in Cassandra it is best practice to denormalize the data models. With static columns (available from Cassandra 2.0.6) we are able to restrain some of these challenges for one-to-many relationships.</p><p>To explain the approach of using static columns we first need to understand how the models for one-to-many relationships look like in Cassandra. With that explanation I also show the operations on the data models. Next, I will explain what the static columns are exactly. I will also elaborate on the operations on these models and mention some technical limitation of static columns. Next we will take a deeper dive into the read operations for the one side of the one-to-many relation. To conclude I will summarize my findings.</p><p>In Cassandra it is best practice to denormalize the data models. We denormalize because the goal in Cassandra is to execute the read operations of the use case with one query or, more preferably, one disk I/O. To illustrate the use case, the data model and the operations I will use an example about teams and team members of Formula 1 racing teams.</p><p>In the example we have two entities; teams and team members. A single team can have multiple members and a given team member is member of only a single team. The first use case is that we should be able to create, read, update and delete teams. The second use case is that we should be able to create, read, update, and delete team members of a given team. Obviously we should also be able to read all members for a given team and read the team that a team member is member of.</p><p>Assume that for a team we wish to register: the name, the manager and the location. For a team member we wish to register: the name, the nationality and the position within the team. In Cassandra we would store all these fields in one table. In that table the team name will be the partition key and the member name will be the clustering key. The table is created with the following CQL statement.</p><pre>CREATE TABLE teammember_by_team (\n  teamname text,\n  manager text,\n  location text,\n  membername text,\n  nationality text,\n  position text,\n  PRIMARY KEY ((teamname), membername)\n);</pre><h2>Insert operation</h2><p>We can insert a row with the following CQL statement:</p><pre>INSERT INTO teammember_by_team (teamname, manager, location, membername, nationality, position)\nVALUES (‘Toro Rosso’, ‘Franz Tost’, ‘Faenza’, ‘Verstappen’, ‘Dutch’, ‘test driver’);</pre><p>This statement results in a new row in the table. When we wish to add another team member to the team we need to add values for the team’s fields, again.</p><h2>Update operation</h2><p>Updating a team member is relatively easy. For example with the following CQL statement:</p><pre>UPDATE teammember_by_team SET position = ‘driver’\nWHERE teamname = ‘Toro Rosso’ AND membername = ‘Verstappen’;</pre><p>But when we wish to update a team we need to execute the update for all team members. This is one of the risks and challenges of this model. This update operation may cost many queries, for example the following queries which result in the following table with data.</p><pre>UPDATE teammember_by_team SET manager = ‘Christian Horner’\nWHERE teamname = ‘Red Bull’ AND membername = ‘Ricciardo’;\nUPDATE teammember_by_team SET manager = ‘Christian Horner’\nWHERE teamname = ‘Red Bull’ AND membername = ‘Kvyat’;\nteamname   | membername | location      | manager          | nationality | position\n-----------+------------+---------------+------------------+-------------+----------\n  Red Bull |  Ricciardo | Milton Keynes | Christian Horner |  Australian |   driver\n  Red Bull |      Kvyat | Milton Keynes | Christian Horner |     Russian |   driver\nToro Rosso | Verstappen |        Faenza |       Franz Tost |       Dutch |   driver</pre><p>A static column is defined upon creation of the table in CQL using the keyword “static”. A normal column is stored within the clustering key, within the partition. This column has a value for every combination of partitioning key and clustering key (in our example team name and team member name). A static column resides one level higher, directly within the partition key (in our example team name).</p><p>All fields belonging to the one side of the one-to-many relation may be static columns. In our example this means that the team manager and team location can be static columns. Review the following CQL statement:</p><pre>CREATE TABLE teammember_by_team (\n  teamname text,\n  manager text static,\n  location text static,\n  membername text,\n  nationality text,\n  position text,\n  PRIMARY KEY ((teamname), membername)\n);</pre><p>Because the static columns are not stored within the clustering key, but are still associated to the partitioning key, we are able to insert team data without inserting team member data. When we read this table, we get back the team with all other field having “null”.</p><pre>INSERT INTO teammember_by_team (teamname, manager, location)\nVALUES (‘Red Bull’, ‘Christian Horner’, ‘&lt;unknown&gt;’);\nteamname  | membername | location | manager          | nationality | position\n----------+------------+----------+------------------+-------------+----------\n Red Bull |       null | &lt;unkown&gt; | Christian Horner |        null |     null</pre><p>Inserting team members is much easier using static columns, because team data no longer has to be duplicated.</p><pre>INSERT INTO teammember_by_team (teamname, membername, nationality, position)\nVALUES (‘Red Bull’, ‘Ricciardo’, ‘Australian’, ‘driver’);\nINSERT INTO teammember_by_team (teamname, membername, nationality, position)\nVALUES (‘Red Bull’, ‘Kvyat’, ‘Russian’, ‘driver’);</pre><p>When we read the table after the preceding statements, the result is as expected.</p><pre>teamname  | membername | location  | manager          | nationality | position\n----------+------------+-----------+------------------+-------------+----------\n Red Bull |  Ricciardo | &lt;unknown&gt; | Christian Horner |  Australian |   driver\n Red Bull |      Kvyat | &lt;unknown&gt; | Christian Horner |     Russian |   driver</pre><p>When we now wish to update a team we are able to do so with just one query.</p><pre>UPDATE teammember_by_team SET location = ‘Milton Keynes’\nWHERE teamname = ‘Red Bull’;\nteamname  | membername | location      | manager          | nationality | position\n----------+------------+---------------+------------------+-------------+----------\n Red Bull |  Ricciardo | Milton Keynes | Christian Horner |  Australian |   driver\n Red Bull |      Kvyat | Milton Keynes | Christian Horner |     Russian |   driver</pre><h2>Technical limitations</h2><p>There are however a few limitations to using static columns. A table can only contain static columns when the table has at least one clustering key. This actually makes sense because if the table has no clustering key then all fields are stored within the partition. Next to this limitation, static columns cannot be used on tables using the COMPACT STORAGE option.</p><p>When we wish to read fields from only the one side of the one-to-many relation this requires a different approach compared to reading fields on a table without static columns. Review the following query:</p><pre>SELECT teamname, manager, location\nFROM teammember_by_team\nWHERE teamname = ‘Red Bull’;</pre><p>Without static columns these fields may be inconsistent and we will get back two rows. With static columns these values are always consistent but still we get back two rows. We get back two rows because there are two team members. If we wish to get back one row per team then all we have to do is add the DISTINCT keyword:</p><pre>SELECT DISTINCT teamname, manager, location\nFROM teammember_by_team\nWHERE teamname = ‘Red Bull’;</pre><p>Cassandra understands that the distinct keyword requires a special treatment because all fields queried are static (besides the partition key). The data can be returned faster compared to not using static columns because in the latter case the values actually have to be compared.</p><p>Static columns provide benefits for one-to-many relations. These benefits constrain a couple of great risks as described in the previous blogpost. The insert operations become easier and more flexible at the one side on the one-to-many relation. The update performed on the entity side of the one-to-many relation is reduced from many queries to just one query. Also select queries at the single entity side of the one-to-many relation are improved.</p><h2>References</h2><p>[1] <a href=\"http://blogs.infosupport.com/analysis-of-the-impact-of-new-use-cases-on-a-cassandra-data-model-and-operations-in-code/\" target=\"_blank\">Analysis of the impact of new use cases on a Cassandra data model and operations in code</a></p><p>[2] <a href=\"http://www.datastax.com/dev/blog/cql-in-2-0-6\" target=\"_blank\">http://www.datastax.com/dev/blog/cql-in-2-0-6</a></p><p>[3] <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-6561\" target=\"_blank\">https://issues.apache.org/jira/browse/CASSANDRA-6561</a></p><p>[4] <a href=\"http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/refStaticCol.html\" target=\"_blank\">http://www.datastax.com/documentation/cql/3.1/cql/cql_reference/refStaticCol.html</a></p><p>[5] <a href=\"http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use-batch-static.html\" target=\"_blank\">http://www.datastax.com/documentation/cql/3.1/cql/cql_using/use-batch-static.html</a></p><p>[6] <a href=\"http://cassandra.apache.org/doc/cql3/CQL.html#createTablepartitionClustering\" target=\"_blank\">http://cassandra.apache.org/doc/cql3/CQL.html#createTablepartitionClustering</a></p><p>[7] <a href=\"https://books.google.nl/books?id=n1nTBgAAQBAJ\" target=\"_blank\">https://books.google.nl/books?id=n1nTBgAAQBAJ</a></p>",
        "created_at": "2018-03-21T17:13:06+0000",
        "updated_at": "2018-03-21T20:53:53+0000",
        "published_at": "2015-03-17T00:00:00+0000",
        "published_by": [
          "Maarten van Duren"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "nl",
        "reading_time": 6,
        "domain_name": "blogs.infosupport.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9436"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9432,
        "uid": null,
        "title": "Sysdig | Troubleshooting Cassandra column selection to boost database performance",
        "url": "https://sysdig.com/blog/column-selection-effects-query-performance/",
        "content": "<img width=\"1024\" height=\"768\" src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/monitoring-cassandra-1024x768.jpg\" class=\"alignleft img-responsive wp-post-image\" alt=\"\" srcset=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/monitoring-cassandra-1024x768.jpg 1024w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/monitoring-cassandra-300x225.jpg 300w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/monitoring-cassandra-768x576.jpg 768w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/monitoring-cassandra-575x431.jpg 575w, https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/monitoring-cassandra-175x131.jpg 175w\" /><h3>Introduction</h3><i>This post was written by one of Sysdig’s engineers Gianluca Borello and was originally featured on <a href=\"http://www.planetcassandra.org/blog/a-tale-of-troubleshooting-database-performance-with-cassandra-and-sysdig/\">Planet Cassandra</a>.As far as databases go, I’m a huge fan of Cassandra: it’s an incredibly powerful and flexible database that can ingest a massive amount of data while scaling across an arbitrary number of nodes. For these reasons, my team uses it very often for our internal applications.But as with any piece of software, Cassandra has its own quirks and habits that you need to understand in order to effectively develop with it. In this article, I’ll show you a Cassandra performance issue that we recently dealt with, and I want to cover how we spotted the problem, what kind of troubleshooting we did to better understand it, and how we eventually solved it.The problemOne of our internal applications requires storing, and later processing, several thousands streams, where each stream consists of about a dozen binary blobs arriving periodically. Since we have many streams, and each blob can be fairly big (in the range of 10KB), we decided to use Cassandra 2.1 (a very stable version at the time of writing) and this very simple table:CREATE TABLE streams ( <br />&#13;\n    stream_id int,&#13;\n    timestamp int, &#13;\n    column0 blob, &#13;\n    column1 blob,&#13;\n    column2 blob, &#13;\n    ... &#13;\n    column10 blob,&#13;\n    primary key (stream_id, timestamp));In this data model, all the blobs (10 in the above example) for a specific timestamp are stored in the same row, as separate columns. This allowed us to write a very simple application code, consisting essentially of a single write per stream during every period. Our typical read use case requires only one or two blobs at any given time. With this data model we have the flexibility of querying an arbitrary portion of the data with a single query, like this:SELECT columnX from streams where stream_id=STREAMFor a while this schema worked well for us, and the response time we experienced for the database has been very good.Recently we noticed deteriorated performance when querying streams containing particularly large blobs. Intuitively this would seem very reasonable since the data to be processed is larger, but there was something else that felt strange: despite the blobs being bigger in average, the specific ones we were retrieving in our queries were always roughly the same size as before.In other words, it seemed as if Cassandra was always processing all 10 columns (including the large ones) despite us just asking for a particular, small column, thus causing degraded response times. This hypothesis seemed hard to believe at first, because Cassandra stores every single column separately, and there’s heavy indexing that allows you to efficiently lookup specific columns.To validate our hypothesis, we wrote a separate test: in a table of N columns like ours, asking one single column should always take almost the same time regardless of the number and size of the other columns. With a little <a href=\"http://(https://github.com/gianlucaborello/cassandra-benchmarks/blob/master/cassandra_benchmark_1.py\">script</a>, we got these results:$ python ./cassandra_benchmark_1.py&#13;\nResponse time for querying a single column on a large table (column size 100 KB):&#13;\n10 columns: 185 ms&#13;\n20 columns: 400 ms&#13;\n30 columns: 613 ms&#13;\n40 columns: 668 ms&#13;\n50 columns: 800 ms&#13;\n60 columns: 1013 ms&#13;\n70 columns: 1205 ms&#13;\n80 columns: 1376 ms&#13;\n90 columns: 1604 ms&#13;\n100 columns: 1681 msWe couldn’t have been more wrong! The tests proved the exact opposite of our assumption, and in fact the response time seemed to take a time directly proportional to the number of columns in the table, even if the query asked for just one of them!&#13;\n&#13;\nDigging into CassandraIn order to understand the problem better, I used <a href=\"http://www.sysdig.org/\">sysdig</a>, an open source troubleshooting tool. If you’re not familiar with sysdig, it has the ability to capture system state and activity from a running Linux instance, and then save, filter and analyze it. Think of sysdig as strace + tcpdump + htop + iftop + lsof in one.Back to our story: I took a sysdig trace file while executing the same query of the previous test on a table with 100 columns:SELECT column7 from streams where stream_id=1This query returns a very small amount of data compared to the whole data set (around 10 MB), as sysdig can easily tell me by looking at the network activity generated by the database:$ sysdig -r trace.scap -c topprocs_net &#13;\nBytes           Process            PID&#13;\n-------------------------------------------------------------------------&#13;\n9.82M            java             34323Despite this, the query takes almost 4 seconds to run, which is way more than what we want to wait in a similar scenario. Let’s take a look at the Cassandra file I/O activity while serving this single query:&#13;\n&#13;\n$ sysdig -r trace.scap -c topfiles_bytes&#13;\nBytes              Filename &#13;\n-------------------------------------------------------------------------------- &#13;\n971.04M &#13;\n/var/lib/cassandra/data/benchmarks/test-23182450d5c011e5acecb7882d261790/benchmarks-test-ka-130-Data.db &#13;\n538.80KB &#13;\n/var/lib/cassandra/data/benchmarks/test-23182450d5c011e5acecb7882d261790/benchmarks-test-ka-130-Index.db&#13;\n…Wow, Cassandra seems to have read almost 1 GB from the file storing the data for my table, pretty much the whole size of the table:$ du -hs /var/lib/cassandra/data/benchmarks/test-23182450d5c011e5acecb7882d261790/*&#13;\n...&#13;\n972M    /var/lib/cassandra/data/benchmarks/test-23182450d5c011e5acecb7882d261790/benchmarks-test-ka-130-Data.db&#13;\n…Which means that Cassandra essentially read the entire file, and this probably explains why the response time depends on the total number of columns of the table.To take a deeper look, I used the spectrogram within csysdig on the I/O events over the duration of the trace file, so that we can visualize Cassandra’s latency and frequency of I/O operations:$ csysdig -d 100 -r trace.scap -v spectro_file<a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog1.png\"><img alt=\"csysdig spectrogram\" src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog1.png\" /></a>&#13;\nIf you’re not familiar with a spectrogram, here’s the high level:&#13;\nIt captures every system call (e.g. open, close, read, write, socket…) and it measures the call’s latency&#13;\n\tEvery 100 ms it organizes the calls into buckets&#13;\n\tThe spectrogram uses color to indicate how many calls are in each bucket&#13;\nBlack means no calls during the last sample&#13;\n\tShades of green mean 0 to 100 calls&#13;\n\tShades of yellow mean hundreds of calls&#13;\n\tShades of red mean thousands of calls or more&#13;\nAs a consequence, the left side of the diagram tends to show stuff that is fast, while the right side shows stuff that is slow.What this image clearly tells us is that there seems to be a constant amount of I/O activity across the whole duration of the query, so, as predicted, accessing that 1 GB is likely what is negatively impacting the response time. Also, the latency of the I/O activity seems to be concentrated across two ranges, one very short (around 100 ns – 1 us) and one much more significant (around 10 us – 1 ms).Let’s zoom into the band on the right, by selecting the area that I’m interested in exploring:&#13;\n&#13;\n<a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog2.png\"><img alt=\"csysdig spectrogram\" src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog2.png\" /></a>&#13;\nThis selection will show the list of system events that match the time range and&#13;\nlatency spectrum:<a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog3.png\"><img alt=\"csysdig spectrogram\" src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog3.png\" /></a>&#13;\nRead operations on the table file, tons of them. Indeed, this is essentially a full scan of the table done by reading it in chunks of 64 KB, and we can see how all those hundreds of microseconds blocked waiting for I/O add up in the final response time.Solution: Cassandra Schema RefactorWe were pretty puzzled after discovering this behavior. As I mentioned before, rows and columns in Cassandra are heavily indexed, so we knew that there was no technical limitation for the query to not be served efficiently by reading exactly those 10 MB from disk instead of the full 1 GB.Doing some research, we were able to find an issue (https://issues.apache.org/jira/browse/CASSANDRA-6586) describing this problem, and we understood that the behavior was meant to be that way in order to respect some semantics of the CQL query language adopted by Cassandra (omitted here for brevity, but clearly described in the issue), and the developers agreed this behavior could cause a significant performance penalty in cases like ours. While the issue is planned to be addressed in future versions of Cassandra, that left us with a bug in a production application that we had to somehow solve.We opted for a workaround: since Cassandra will always read all the columns of a CQL row regardless of which ones are actually asked in a query, we decided to refactor our schema by shrinking the size of each row, and instead of putting all the blobs in the same row, we split them across multiple ones, like this:CREATE TABLE streams (<br />&#13;\n    stream_id int,&#13;\n    column_no int,&#13;\n    timestamp int,&#13;\n    column blob,&#13;\n    primary key (stream_id, column_no, timestamp));With this new schema all our rows are much smaller while still allowing us to efficiently query a single portion of the blob, with a query such as:SELECT * from streams where stream_id=1 and column_no=7This approach turned out to be quite effective: the query above that took 4 seconds with the old data model in the extreme test case, now took just around 100 ms on the same data set! It’s also interesting to analyze the new scenario with sysdig, and check Cassandra’s I/O while serving the request:$ sysdig -r trace.scap -c topfiles_bytes&#13;\nBytes             Filename&#13;\n--------------------------------------------------------------------------------&#13;\n9.85M             /var/lib/cassandra/data/benchmarks/test-55d49260d5cb11e5acecb7882d261790/benchmarks-test-ka-16-Data.db&#13;\n…Just 10 MB, exactly the same size of the expected response. We can also use sysdig to answer the question: how did Cassandra know how to efficiently read the exact amount of data in a jungle of more than 1 GB? We can of course look at the system events done by the database process on the file:$ sysdig -r trace.scap fd.filename=benchmarks-test-ka-16-Data.db&#13;\n11285 15:13:40.896875243 1 java (34482) &lt; open fd=59(/var/lib/cassandra/data/benchmarks/test-55d49260d5cb11e5acecb7882d261790/benchmarks-test-ka-16-Data.db) name=/var/lib/cassandra/data/benchmarks/test-55d49260d5cb11e5acecb7882d261790/benchmarks-test-ka-16-Data.db flags=1(O_RDONLY) mode=0&#13;\n11295 15:13:40.896926004 1 java (34482) &gt; lseek fd=59(/var/lib/cassandra/data/benchmarks/test-55d49260d5cb11e5acecb7882d261790/benchmarks-test-ka-16-Data.db) offset=71986679 whence=0(SEEK_SET)&#13;\n11296 15:13:40.896926200 1 java (34482) &lt; lseek res=71986679Here we see Cassandra opening the table file, but notice how immediately there’s a lseek operation that essentially skips in one single operation 70 MB of data, by setting the offset to the file descriptor with SEEK_SET to 71986679. This is essentially how typical “file indexing” works when observed from the system call point of view: Cassandra heavily relies on data structures to index the various contents of the table, so that it can move fast and efficiently to arbitrary and meaningful locations. In this case, the index contained the information that the columns with “stream_id=1” and “column_no=7” started at offset 71986679.11400 15:13:40.898199496 1 java (34482) &gt; read fd=59(/var/lib/cassandra/data/benchmarks/test-55d49260d5cb11e5acecb7882d261790/benchmarks-test-ka-16-Data.db) size=65798&#13;\n11477 15:13:40.899058641 1 java (34482) &lt; read res=65798 data=................................................................................Right after jumping to the correct position of the file, we see normal sequential reads of 64 KB, in order to bring all the data into memory and process them. This loop of jumping to the right position and reading from it continues until all data (10 MB) is fully read.It’s also quite interesting to see how this second case looks like in a spectrogram:<a href=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog4.png\"><img alt=\"csysdig spectrogram\" src=\"https://478h5m1yrfsa3bbe262u7muv-wpengine.netdna-ssl.com/wp-content/uploads/2016/04/cassandrablog4.png\" /></a>&#13;\nThe spectrum of the I/O operations is pretty much the same as before and latencies are concentrated around the previous values, but notice how the height of the chart is completely different. This is because each horizontal line represents 100 ms of time, and since the whole query took about 100ms, the whole activity can be rendered in a much shorter chart. In other words, the height of the chart is directly proportional to the response time of Cassandra, so short means faster.ConclusionsThis story has two important messages into it, focused on monitoring and troubleshooting:Monitoring: Monitoring all the tiers of your infrastructure is very important. In this case, simply correlating the amount of data requested from Cassandra with the amount of I/O activity actually generated by the database helped us finding the cause of the problem, which we noticed in the first place by once again monitoring the response time of the queries.&#13;\n\tTroubleshooting: with <a href=\"http://sysdig.org\">sysdig</a>, I’ve been able to dig into the specific behavior of Cassandra (e.g. confirming that the indexing was actually used by observing the lseek() activity) without even having to read its code and understand its internals, which for sure would have taken me more time. This concept is very powerful: if you already have some expectations of how a specific application is supposed to behave, observing its activity from the system call point of view can in many cases be an extremely effective way to do troubleshooting without spending a lot of time understanding less relevant details.&#13;\n</i>",
        "created_at": "2018-03-20T14:27:04+0000",
        "updated_at": "2018-04-16T17:43:12+0000",
        "published_at": "2016-04-19T23:33:45+0000",
        "published_by": [
          "Gianluca Borello"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en_US",
        "reading_time": 11,
        "domain_name": "sysdig.com",
        "preview_picture": "https://sysdig.com/wp-content/uploads/2016/04/monitoring-cassandra.jpg",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9432"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 16,
            "label": "starter.template",
            "slug": "starter.template"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9422,
        "uid": null,
        "title": "A Reference Application for Apache Cassandra and DataStax Enterprise",
        "url": "https://killrvideo.github.io/",
        "content": "<p>\n            Apache Cassandra's peer-to-peer architecture makes it a great choice when you need scalability and high \n            availability. It's also a lot different from the RDBMS systems you might be used to. You can get free\n            self-paced training for Cassandra at DataStax Academy.\n          </p>",
        "created_at": "2018-03-14T22:28:08+0000",
        "updated_at": "2018-03-14T22:29:14+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "killrvideo.github.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9422"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 24,
            "label": "node",
            "slug": "node-js"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9421,
        "uid": null,
        "title": "KillrVideo/killrvideo-nodejs",
        "url": "https://github.com/KillrVideo/killrvideo-nodejs",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>A reference application for Node.js developers looking to learn more about using\n<a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra</a> and <a href=\"http://www.datastax.com/products/datastax-enterprise\" rel=\"nofollow\">DataStax Enterprise</a> in their applications and\nservices. Learn more at <a href=\"https://killrvideo.github.io/\" rel=\"nofollow\">killrvideo.github.io</a>.</p>\n<h2><a id=\"user-content-running-locally\" class=\"anchor\" aria-hidden=\"true\" href=\"#running-locally\"></a>Running Locally</h2>\n<p>Use these guides to get started running KillrVideo locally on your development machine:</p>\n<ul><li><a href=\"https://killrvideo.github.io/getting-started/\" rel=\"nofollow\">Getting Started with KillrVideo</a>: Follow this to setup common dependencies\nlike Docker.</li>\n<li><a href=\"https://killrvideo.github.io/docs/languages/nodejs/\" rel=\"nofollow\">Getting Started with Node.js</a>: Follow this to get this Node.js code\nrunning.</li>\n</ul><h2><a id=\"user-content-contributing-requests-for-more-examples\" class=\"anchor\" aria-hidden=\"true\" href=\"#contributing-requests-for-more-examples\"></a>Contributing, Requests for More Examples</h2>\n<p>This project will continue to evolve along with Cassandra and you can expect that as\nCassandra, DSE, and the drivers add new features, this application will try and provide\nexamples of those.  We gladly accept any pull requests for bug fixes, new features, etc. and\nif you have a request for an example that you don't see in the code currently, feel free to\nopen an issue here on GitHub or send a message to <a href=\"https://twitter.com/LukeTillman\" rel=\"nofollow\">@LukeTillman</a> on Twitter.</p>\n</article>",
        "created_at": "2018-03-14T22:27:46+0000",
        "updated_at": "2018-03-14T22:29:26+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 0,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/19652081?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9421"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 236,
            "label": "csharp",
            "slug": "csharp"
          }
        ],
        "is_public": false,
        "id": 9420,
        "uid": null,
        "title": "LukeTillman/killrvideo-csharp",
        "url": "https://github.com/LukeTillman/killrvideo-csharp",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>A reference application for .NET developers looking to learn more about using <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">Apache Cassandra</a> and\n<a href=\"http://www.datastax.com/products/datastax-enterprise\" rel=\"nofollow\">DataStax Enterprise</a> in their applications and services. Learn more at <a href=\"https://killrvideo.github.io/\" rel=\"nofollow\">killrvideo.github.io</a>.</p>\n<h2><a href=\"#running-locally\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-running-locally\"></a>Running Locally</h2>\n<p>Use these guides to get started running KillrVideo locally on your development machine:</p>\n<ul><li><a href=\"https://killrvideo.github.io/getting-started/\" rel=\"nofollow\">Getting Started with KillrVideo</a>: Follow this to setup common dependencies like Docker.</li>\n<li><a href=\"https://killrvideo.github.io/docs/languages/c-sharp/\" rel=\"nofollow\">Getting Started with C#</a>: Follow this to get this C# code running.</li>\n</ul><h2><a href=\"#pull-requests-requests-for-more-examples\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-pull-requests-requests-for-more-examples\"></a>Pull Requests, Requests for More Examples</h2>\n<p>This project will continue to evolve along with Cassandra and you can expect that as Cassandra and the DataStax driver add new features, this sample application will try and provide examples of those.  I'll gladly accept any pull requests for bug fixes, new features, etc.  and if you have a request for an example that you don't see in the code currently, send me a message <a href=\"https://twitter.com/LukeTillman\" rel=\"nofollow\">@LukeTillman</a> on Twitter or open an issue here on GitHub.</p>\n<h2><a href=\"#license\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-license\"></a>License</h2>\n<p>Copyright 2016 Luke Tillman</p>\n<p>Licensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at</p>\n<p><a href=\"http://www.apache.org/licenses/LICENSE-2.0\" rel=\"nofollow\">http://www.apache.org/licenses/LICENSE-2.0</a></p>\n<p>Unless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.</p>\n</article>",
        "created_at": "2018-03-14T22:27:36+0000",
        "updated_at": "2018-03-23T21:56:31+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars2.githubusercontent.com/u/428023?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9420"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          }
        ],
        "is_public": false,
        "id": 9419,
        "uid": null,
        "title": "PatrickCallaghan/datastax-eventsourcing",
        "url": "https://github.com/PatrickCallaghan/datastax-eventsourcing",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>This demo shows how Cassandra and DSE can be using to store and replay events.</p>\n<p>To use Spark you will need to provide your own Cassandra and Spark deployments. In this demo we will use DSE as they are already integrated.</p>\n<p>First we start DSE in SearchAnalyics mode to allow us to use both Spark and DSE Search -\n<a href=\"http://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/startStop/startDseStandalone.html?hl=starting\" rel=\"nofollow\">http://docs.datastax.com/en/dse/5.1/dse-admin/datastax_enterprise/operations/startStop/startDseStandalone.html?hl=starting</a></p>\n<p>The implementation uses bucketing to group all data into particular time buckets for replay. The time bucket used in this example is 1 minute but any time bucket can be used. Also depending how many days, months, years of events that need to be kept, it may be beneficial to spread the events over different tiers of tables.</p>\n<p>To create the schema, run the following</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.demo.SchemaSetup\" -DcontactPoints=localhost\n</pre>\n<p>To create the solr core to make our table searchable, run the following</p>\n<pre>dsetool create_core datastax.eventsource generateResources=true\n</pre>\n<p>To create events, run the following (Default of 10 million events)</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.events.Main\"  -DcontactPoints=localhost -DnoOfEvents=10000000\n</pre>\n<p>To replay a sample event set, run</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.events.ReadEvents\"  -DcontactPoints=localhost -Dfrom=yyyyMMdd-hhmmss -Dto=yyyyMMdd-hhmmss\n</pre>\n<p>eg</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.events.ReadEvents\"  -DcontactPoints=localhost -Dfrom=20160805-000000 -Dto=20160805-010000\n</pre>\n<p>This replays 2 scenarios</p>\n<pre>1. Replay all events for a specified time range\n2. Replay all events for a specified time range and a specific event type.\t\t\n</pre>\n<p>To run the webservice</p>\n<pre>mvn jetty:run -Djetty.port=8081 \n</pre>\n<p>To run a rest query, go the brower and enter a url in the format <a href=\"http://localhost:8080/datastax-eventsourcing/rest/getevents/from/to\" rel=\"nofollow\">http://localhost:8080/datastax-eventsourcing/rest/getevents/from/to</a>,\nwhere the date format is 'yyyyMMdd-hhmmss' e.g. For all events from midnight to 1:00 am on the 1st of August 2016 run -</p>\n<pre>http://localhost:8081/datastax-eventsourcing/rest/getevents/20160801-000000/20160801-010000/\n</pre>\n<p>We can also use cql to query using the Solr query from DSE Search</p>\n<p>Get all LOGIN Events from 9th Aug 2016 at 12:30 to 11th Aug 2016 at 12:30</p>\n<pre>select * from datastax.eventsource where solr_query = '{\"q\":\"eventtype:LOGIN\", \"fq\": \"time:[2016-08-09T12:30:00.000Z TO 2016-08-11T12:30:00.000Z]\", \"sort\":\"time desc\"}' limit 10000;\n</pre>\n<p>To use Spark, using DSE we can just 'dse spark' to use the repl.</p>\n<p>First we will create an Event object which will hold our events objects</p>\n<pre>case class Event (date: String, bucket: Int, id: java.util.UUID, data: String, eventtype: String, \naggregatetype: String, time: java.util.Date, loglevel: String, host: String); \nval events =  sc.cassandraTable[Event](\"datastax\", \"eventsource\").cache; \nevents.count\nval max = events.map(_.time).max\nval min = events.map(_.time).min\n</pre>\n<p>We can query our data and return events before or after a certain time.</p>\n<pre>val yesterday = new java.util.Date(java.util.Calendar.getInstance().getTime().getTime()-200000000);\nyesterday\nval before = events.filter(_.time.before(yesterday)); \nbefore.take(10).foreach(print) \nbefore.count\n \nval after = events.filter(_.time.after(yesterday)); \nafter.take(10).foreach(print) \nafter.count\n</pre>\n<p>Or we can use filtering to just get the events between two dates.</p>\n<pre>val start = new java.util.Date(java.util.Calendar.getInstance().getTime().getTime()-200000000);\nval end = new java.util.Date(java.util.Calendar.getInstance().getTime().getTime()-190000000);\nval filtered = events.filter(_.time.after(start)).filter(_.time.before(end)).cache;\nfiltered.count\n</pre>\n<p>Lets get all number of events per host and a list of all distinct hosts.</p>\n<pre>var hostCounts =  events.map(f =&gt; (f.host, 1)).reduceByKey(_ + _)\nhostCounts.collect().foreach(println)\nvar hosts =  hostCounts.map(f =&gt; (f._1))\nhosts.collect().foreach(println)\n</pre>\n<p>To use spark sql - try the following with a valid date</p>\n<pre>val results = sqlContext.sql(\"SELECT * from datastax.eventsource where date = '20161019'\")\n \nresults.take(5).foreach(println)\nval results = sqlContext.sql(\"SELECT * from datastax.eventsource where time &gt; '2016-10-22 16:18:07' \");\nresults.take(5).foreach(println)\nval results = sqlContext.sql(\"SELECT * from datastax.eventsource where time &gt; '2016-10-22 16:18:07' and time &lt; '2016-10-23 16:18:07'\");\nresults.count\n</pre>\n<p>To remove the tables and the schema, run the following.</p>\n<pre>mvn clean compile exec:java -Dexec.mainClass=\"com.datastax.demo.SchemaTeardown\"\n</pre>\n</article>",
        "created_at": "2018-03-14T22:19:56+0000",
        "updated_at": "2018-03-14T22:30:23+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 3,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/1006924?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9419"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 36,
            "label": "solr",
            "slug": "solr"
          },
          {
            "id": 50,
            "label": "article",
            "slug": "article"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 90,
            "label": "spark",
            "slug": "spark"
          }
        ],
        "is_public": false,
        "id": 9418,
        "uid": null,
        "title": "Report Number Three (Cassandra, Spark and Solr)",
        "url": "http://pkiraly.github.io/2016/04/09/third-report/",
        "content": "<p>Changing from Hadoop to Spark, refining mandatory calculation, adding field statistics, storing records \nin Cassandra, indexing with Solr and calculating uniqueness.</p><h2 id=\"changing-to-spark\">Changing to Spark</h2><p>Last year I did some Coursera courses on Big Data and Data Science (I recommend you <a href=\"https://www.coursera.org/learn/data-manipulation\" target=\"_blank\">Bill Howe’s course</a> \nfrom the University of Washington if you like to understand theoretical background behind relational databases and\ndata science, and I don’t recommend <a href=\"https://www.coursera.org/specializations/big-data\" target=\"_blank\">these courses</a>\nprovided by the University of California San Diego) where I have learnt\nabout <a href=\"http://spark.apache.org/\" target=\"_blank\">Apache Spark</a>. Spark’s big promise is that it is quicker \nthan <a href=\"http://hadoop.apache.org/\" target=\"_blank\">Hadoop</a>’s MapReduce and more memory effective. For me it\nwas even more important, that I really don’t use the “reduce” part of MapReduce, and Spark is fine with that.\nThe change was not hard at all, since the business logic is separated in different classes to which Hadoop was \njust a client (the only existing client actually, but I planned to add other interfaces). For the same functionality Spark\nran 4 times faster than Hadoop (1.5 hours versus 6 hours). It was a real gain, it means that I can change the codes and\nrun it again several times a day if I want to implement something new.</p><h2 id=\"refining-mandatory-calculation\">Refining mandatory calculation</h2><p>Thanks to the feedbacks from the Europeana Data Quality Comittee we improved the mandatory dimension of the \ncompleteness measure. This tell us how a record fit to the basic Europeanaa Data Model (EDM) schema requirements. Previously\nI calculated each field individually, but that was bad: some fields are alternative to each other, so a record is valid if\neither dc:title, dcterms:alternative or dc:description is present. Now the mandatory score gives what is expected, and it \nis a real discriminator of bad records.</p><p>Now the program assigns 1 if a field is existing and 0 if not for 30+ fields. The R script creates record sets summary\nof it, and on the interface I introduces the d3.js data visualization library to display those values. I also introduces \nsome filters in the UI: the user can hide those collections which doesn’t have a particular field, those in which\nevery records have it, and those in which some records have it. The user can investigate each fields.</p><p>The usage of the fields accross all the records:</p><p><img src=\"http://pkiraly.github.io/assets/field-frequency.png\" class=\"real\" title=\"Field frequency\" alt=\"Field frequency\" /></p><p>The usage of dcterms:alternative in those data providers, which uses it in some of the records, but not in all:</p><p><img src=\"http://pkiraly.github.io/assets/field-alternative-per-data-providers.png\" class=\"real\" title=\"dcterms:alternative frequency\" alt=\"dcterms:alternative frequency\" /></p><p>An <a href=\"http://144.76.218.178/europeana-qa/field.php?field=proxy_dcterms_alternative&amp;type=data-providers&amp;exclusions%5B%5D=0&amp;exclusions%5B%5D=1\" target=\"_blank\">example</a>\nabout the dcterms:alternative across data providers.</p><h2 id=\"storing-records-in-apache-cassandra\">Storing records in Apache Cassandra</h2><p>So far it was a problem, that in the record view of the UI I was nat able to extract an individual record from the \nhuge JSON files I stored the data. After sime investigation I choosed \n<a href=\"http://cassandra.apache.org/\" target=\"_blank\">Apache Cassandra</a>, which has an interface for \nboth Java and PHP. I imported every records to it, but now I still use thhe JSON files stored in HDFS (Hadoop Distributed \nFile System) in the Spark analysis. It is on my TODO list to compare the performance if using files and interating over \nevery Cassandra records – my hipothesis is that file based iteration is quicker. Now only the ID and the JSON content is\nstored in Cassandra, I am thinking about to store the measurements as well: it would be good if I would like to search for \neach record having a score between say 0.2 and 0.4.</p><h2 id=\"uniqueness-calculation\">Uniqueness calculation</h2><p>I have started investigating the uniqueness or entropy of some selected fields (dc:title, dcterms:alternative \nand dc:description). The basic idea is that if the terms in these fields are frequent accross the records, then \nit is less unique, so less important. If a term is frequent in the same field it is more important than terms \nappear only once. This is called information entropy or in the search engive world TF-IDF formula (term frequency, \ninverse document frequency) – see <a href=\"https://en.wikipedia.org/wiki/Tf%E2%80%93idf\" target=\"_blank\">tf-idf</a> Wikipedia article.</p><p>The <a href=\"http://lucene.apache.org/solr/\" target=\"_blank\">Apache Solr</a> search engine’s relevancy ranking \nmostly based on this formula (however there are lots of tuning \npossibilities, such as give fields weights etc.), but Solr doesn’t provide an interface by default for \nextracting the terms TF-IDF score. There is a Term Vector Component however which provides this interface \ngiven that you apply some additional indexing rules. It is not available in the ordinary Europeana Solr \nsetup so I have created a new Solr instance with this special settings, and created a brand new index with \nlimited fieldset. (If you want to check how to setup Solr and what interface you can use, check this \n<a href=\"https://cwiki.apache.org/confluence/display/solr/The+Term+Vector+Component\" target=\"_blank\">wiki page</a>.</p><p>When the index were created (it took five days, but it is improvable) the scores of a field (in this case the dc:title “Fleming/Mair wedding, Slaithwaite, Huddersfield” – from <a href=\"http://www.europeana.eu/portal/record/2022320/3F61C612ED9C42CCB85E533B4736795E8BDC7E77.html\" target=\"_blank\">this record</a>) can be read from Solr API in the following form:</p><div class=\"highlighter-rouge\"><div class=\"highlight\"><pre>\"dc_title_txt\":{\n  \"fleming\":{\n    \"tf\":1,\n    \"df\":1073,\n    \"tf-idf\":9.319664492078285E-4\n  },\n  \"huddersfield\":{\n    \"tf\":1,\n    \"df\":12073,\n    \"tf-idf\":8.282945415389712E-5\n  },\n  \"mair\":{\n    \"tf\":1,\n    \"df\":178,\n    \"tf-idf\":0.0056179775280898875\n  },\n  \"slaithwaite\":{\n    \"tf\":1,\n    \"df\":477,\n    \"tf-idf\":0.0020964360587002098\n  },\n  \"wedding\":{\n    \"tf\":1,\n    \"df\":10226,\n    \"tf-idf\":9.778994719342852E-5\n  }\n}\n</pre></div></div><p>Note: I did not applied truncation and other fancy Solr analyzes on the fields, it is only the lower case transformation\napplied. The API returns the stored terms in alphabetical order. I removed some properties Solr reports but \nirrelevant from our current perspective.</p><p>I have extracted these info for the above mentiond three fields, and created two scores: a cumulative \nscore which summarizes the all terms in the field, and an average, which is the average of \nthe terms’ tf-idf score. Those records which don’t have the field get 0 for both.</p><p><img src=\"http://pkiraly.github.io/assets/uniquness.png\" class=\"real\" title=\"Uniqueness\" alt=\"Uniquenes\" /></p><p>The graphs visualize the average uniqueness of terms in the field. You can see that - as expected - there are lots of records where this value is quite low - it means that the words of the title are usually common words. There are however some titles which have unique words. If the value is higher than 1, it means that a unique word appears multiple times in this field (unique means that it appears in only one record). In this particularly record set there is no such an example, but there are others, such as “Doog, Doog, Doog, Doog” (an indian one) or “Csalo! Csalo!” (a Hungarian one). In this particular dataset the most unique title is “Iganteçtaco pronoua, eta hilen pronoua.” in which “eta” is a common term, “hilen” and “Iganteçtaco” is unique, and “pronoua” is repeated unique.</p><p>In order to make comparision of the scores and the record, a two new features were added to the record view.</p><p>The first one is the term frequency viewer. Here you can see the terms stored, the term frequency (how many times the term appears in the current field instance), the document frequency (how many document has this term in this field) and the tf-idf scores Solr calculated.</p><p><img src=\"http://pkiraly.github.io/assets/term-frequencies.png\" class=\"real\" title=\"Term frequencies\" alt=\"Term frequencies\" /></p><p>The second one is a “naked” record view: it displays the non technical fields of the <code class=\"highlighter-rouge\">ore:Aggregation</code> and <code class=\"highlighter-rouge\">ore:Proxy</code> of the record. Those fields which are not analyzed in the current session (such as tableOfContents) are displayed in grey.</p><p><img src=\"http://pkiraly.github.io/assets/record-view.png\" class=\"real\" title=\"Record view\" alt=\"Record view\" /></p><p>You can access thhe UI in the usual web interface:</p><p><a href=\"http://144.76.218.178/europeana-qa/\" target=\"_blank\">http://144.76.218.178/europeana-qa/</a></p><p>Select one of the last six dimension to get the results.</p><h2 id=\"events-presentation-article\">Events, presentation, article</h2><p>The big news is that the Europeana <a href=\"http://pro.europeana.eu/page/data-quality-committee\" target=\"_blank\">Data Quality Committee</a> as a Europeana Network and EuropeanaTech Working Group is formed in March. It is a great honor, that I was involved. We have a quite active message board, a bi-weekly teleconference and a bi-yearly face-to-face meeting.</p><p><img src=\"http://pkiraly.github.io/assets/gwdg-nachrichten.png\" class=\"real\" title=\"GWDG Nachrichten\" alt=\"GWDG Nachrichten\" /></p><p>I wrote an <a href=\"https://www.gwdg.de/documents/20182/27257/GN_3-2016_www.pdf\" target=\"_blank\">article for GWDG Nachrichten</a> about the metadata quality issues in Europeana covering the roles of the Data Quality Committee, and Mr Yahyapour, the director of GWDG wrote a recommendation in the editorial column. The GWDG Nachrichten is circulated in the Göttingen Campus and in Max Planck Institutes.</p><p><img src=\"http://pkiraly.github.io/assets/networkshop.png\" class=\"real\" title=\"networkshop\" alt=\"networkshop\" /></p><p>I presented the research in Networkshop 2016 conference at the end of March in my home town. It was exceptional for me that I talked at the Auditorium Maximum of the University of Debrecen where I saw soo many unforgottable concerts, movies and speachhes as a teenager. Unfortunatelly I was the very last speaker on that day, and there were no time left for discussions. Here you can see <a href=\"http://www.slideshare.net/pkiraly/a-jk-s-a-rosszak-metaadatok-minsgellenrzse\" target=\"_blank\">the slides</a> (note: they are in Hungarian).</p>",
        "created_at": "2018-03-14T22:19:56+0000",
        "updated_at": "2018-03-14T22:29:56+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 7,
        "domain_name": "pkiraly.github.io",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9418"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 51,
            "label": "blog",
            "slug": "blog"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9393,
        "uid": null,
        "title": "Distributed Musings",
        "url": "http://www.sestevez.com/",
        "content": "<p>Cassandra collections create tombstones? Many new cassandra users learn this the hard way, they choose cassandra collections for the wrong reasons, for the wrong use cases, and then experience what is known as</p>",
        "created_at": "2018-03-07T17:06:25+0000",
        "updated_at": "2018-03-09T20:49:39+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.sestevez.com",
        "preview_picture": null,
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9393"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 217,
            "label": "tool",
            "slug": "tool"
          }
        ],
        "is_public": false,
        "id": 9388,
        "uid": null,
        "title": "thejaspm/cfstatsParseVisualize",
        "url": "https://github.com/thejaspm/cfstatsParseVisualize/blob/master/cfsStats2Csv.py",
        "content": "import os\n      \n        import re\n      \n        import sys\n      \n         \n      \n        from optparse import OptionParser\n      \n        \n\n      \n        KEYSPACE=\"Keyspace\"\n      \n        COLUMNFAMILY = \"Column Family\";\n      \n        READCOUNT = \"Read Count\";\n      \n        READLATENCY = \"Read Latency\";\n      \n        WRITECOUNT = \"Write Count\";\n      \n        WRITELATENCY = \"Write Latency\";\n      \n        PENDINGTASKS = \"Pending Tasks\";\n      \n        SSTABLECOUNT = \"SSTable count\";\n      \n        SSTABLEIEL = \"SSTables in each level\";\n      \n        SPACEUSEDL = \"Space used (live)\";\n      \n        SPACEUSEDT = \"Space used (total)\";\n      \n        SSTABLECR = \"SSTable Compression Ratio\";\n      \n        NUMKEYS = \"Number of Keys (estimate)\";\n      \n        MEMTABLECC = \"Memtable Columns Count\";\n      \n        MEMTABLEDS = \"Memtable Data Size\";\n      \n        MEMTABLESC = \"Memtable Switch Count\";\n      \n        BLOOMFFP = \"Bloom Filter False Positives\";\n      \n        BLOOMFFR = \"Bloom Filter False Ratio\";\n      \n        BLOOMFSU = \"Bloom Filter Space Used\";\n      \n        COMPACTEDRMINS = \"Compacted row minimum size\";\n      \n        COMPACTEDRMAXS = \"Compacted row maximum size\";\n      \n        COMPACTEDRMEANS = \"Compacted row mean size\";\n      \n        \n\n      \n        \n\n      \n        COLUMN_COUNT = 20\n      \n        \n\n      \n        INDEX_MAP = {\n      \n        \"SSTable count\":1,\n      \n        \"Space used (live)\": 2,\n      \n        \"Space used (total)\": 3,\n      \n        \"Number of Keys (estimate)\": 4,\n      \n        \"Memtable Columns Count\": 5,\n      \n        \"Memtable Data Size\": 6,\n      \n        \"Memtable Switch Count\": 7,\n      \n        \"Read Count\": 8,\n      \n        \"Read Latency\": 9,\n      \n        \"Write Count\": 10,\n      \n        \"Write Latency\": 11,\n      \n        \"Pending Tasks\": 12,\n      \n        \"Bloom Filter False Postives\": 13,\n      \n        \"Bloom Filter False Ratio\": 14,\n      \n        \"Bloom Filter Space Used\": 15,\n      \n        \"Compacted row minimum size\": 16,\n      \n        \"Compacted row maximum size\": 17,\n      \n        \"Compacted row mean size\": 18\n      \n        }\n      \n        \n\n      \n        \n\n      \n        def createHeaderRow():\n      \n            HeaderList  = [\"Row Type\",\"Entity\",\"SSTable count\",\"Space used (live)\",\"Space used (total)\",\"Number of Keys (estimate)\",\"Memtable Columns Count\",\"Memtable Data Size\",\"Memtable Switch Count\",\"Read Count\",\"Read Latency\",\"Write Count\",\"Write Latency\",\"Pending Tasks\",\"Bloom Filter False Postives\",\"Bloom Filter False Ratio\",\"Bloom Filter Space Used\",\"Compacted row minimum size\",\"Compacted row maximum size\",\"Compacted row mean size\"]\n      \n            return HeaderList\n      \n        \n\n      \n        \n\n      \n        def parseAndFormatData(data,fp):\n      \n            lines = data.split('\\n')\n      \n            \n      \n            csvList = [];i=0\n      \n            while i &lt; COLUMN_COUNT:\n      \n                csvList.append('')\n      \n                i += 1\n      \n            fp.write('\"'+'\",\"'.join(createHeaderRow())+'\"'+\"\\n\")\n      \n            for each in lines:\n      \n                if COLUMNFAMILY in each or KEYSPACE in each:\n      \n                    #new row in csv\n      \n                    if csvList[0] != '':\n      \n        \t\tfp.write('\"'+'\",\"'.join(csvList)+'\"'+\"\\n\")\n      \n                        while i &lt; COLUMN_COUNT:\n      \n                            csvList.append('')\n      \n                            i += 1\n      \n        \n\n      \n                    csvList[0],csvList[1]=each.split(':')\n      \n               \t    csvList[0] =  csvList[0].lstrip()\n      \n                else:\n      \n                    if \":\" in each:\n      \n        \t\teach = each.lstrip()\n      \n                        elemntsList = each.split(\":\")\n      \n                        elemntsList[0].lstrip()\n      \n                        if elemntsList[0] in INDEX_MAP.keys():\n      \n        \t\t\tif \"NaN \" in elemntsList[1]:\n      \n        \t\t\t\telemntsList[1]=\"\"\n      \n        \t\tcsvList[INDEX_MAP[elemntsList[0]]+1]=elemntsList[1].strip()\n      \n                        else:\n      \n                            continue\n      \n                    else:\n      \n                        continue\n      \n        \n\n      \n            sys.exit(1) \n      \n         \n      \n        def main():\n      \n            usage = 'usage: %prog --input=&lt;path to a file with cfhistograms output&gt;' + \\\n      \n                     ' --output=&lt;path to the output directory&gt;'\n      \n            parser = OptionParser(usage=usage)\n      \n            parser.add_option('--input', dest='input',\n      \n                          help='Path to a file with cfhistograms output')\n      \n            parser.add_option('--output', dest='output',\n      \n                          help='Path to a file where the graphs are saved')\n      \n         \n      \n            (options, args) = parser.parse_args()\n      \n         \n      \n            if not options.input:\n      \n                print('Error: Missing \"--input\" option')\n      \n                print parser.print_usage()\n      \n                sys.exit(1)\n      \n         \n      \n            if not options.output:\n      \n                print('Error: Missing \"--output\" option')\n      \n                print parser.print_usage()\n      \n                sys.exit(1)\n      \n         \n      \n            if not os.path.exists(options.input) or not \\\n      \n        \tos.path.isfile(options.input):\n      \n                print('--input argument is not a valid file path')\n      \n                sys.exit(2)\n      \n         \n      \n            fp1 = open(options.output, 'wb')\n      \n            with open(options.input, 'r') as fp:\n      \n                print('Processing file...') \n      \n                content = fp.read()\n      \n                parseAndFormatData(content,fp1)\n      \n            fp1.close()\n      \n        main()",
        "created_at": "2018-03-07T12:32:41+0000",
        "updated_at": "2018-03-11T17:39:57+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars3.githubusercontent.com/u/3212343?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9388"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 951,
            "label": "datastax",
            "slug": "datastax"
          },
          {
            "id": 994,
            "label": "dynamo",
            "slug": "dynamo"
          }
        ],
        "is_public": false,
        "id": 9385,
        "uid": null,
        "title": "DynamoDB vs DataStax | TrustRadius",
        "url": "https://www.trustradius.com/compare-products/amazon-dynamodb-vs-datastax",
        "content": "<p><a href=\"https://www.trustradius.com/products/amazon-dynamodb/reviews\" class=\"product-link\" target=\"_blank\" data-id=\"5595a8e4104b09180054082c\">Amazon DynamoDB</a> and DataStax <a href=\"https://www.trustradius.com/products/cassandra/reviews\" class=\"product-link\" target=\"_blank\" data-id=\"5595a51a11a8fd0e00e8a466\">Cassandra</a> are similar on masterless architecture and principles, <a href=\"https://www.trustradius.com/products/amazon-dynamodb/reviews\" class=\"product-link\" target=\"_blank\" data-id=\"5595a8e4104b09180054082c\">DynamoDB</a> is managed and needs cost analysis. If you need to have better control, DataStax is better.I also did a prototype with Google Spanner in one of the recent innovation days, it provides the best of both worlds but being a service on Google Cloud Platform(GCP) works if your services are primarily on GCP. <a href=\"https://www.trustradius.com/products/amazon-aurora/reviews\" class=\"product-link\" target=\"_blank\" data-id=\"5463c14b2969e30800e8f2c0\">Amazon Aurora</a> is a relational database with higher performance and is a good candidate if search and default relational behavior is preferred.For now, DataStax worked well for us as it provides best in class performance across different kinds of read/write/mixed workloads. It provides linear scalability which works for the best performance, lowest latency and highest throughput. If configured correctly, there is no downtime and no data loss.</p>",
        "created_at": "2018-03-06T21:50:06+0000",
        "updated_at": "2018-03-10T19:21:37+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 0,
        "domain_name": "www.trustradius.com",
        "preview_picture": "https://www.trustradius.com/3.4.2/images/trustradius_square3.jpg?v=20171005",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9385"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 4,
            "label": "github",
            "slug": "github"
          },
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9383,
        "uid": null,
        "title": "Instagram/cassandra",
        "url": "https://github.com/Instagram/cassandra/tree/rocks_3.0",
        "content": "<h3>\n      \n      README.asc\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\"><div>\n<h2 id=\"user-content-executive-summary\"><a href=\"#executive-summary\" aria-hidden=\"true\" class=\"anchor\"></a>Executive summary</h2>\n<div>\n<div>\n<p>Cassandra is a partitioned row store.  Rows are organized into tables with a required primary key.</p>\n</div>\n<div>\n<p><a href=\"http://wiki.apache.org/cassandra/Partitioners\" rel=\"nofollow\">Partitioning</a> means that Cassandra can distribute your data across multiple machines in an application-transparent matter.  Cassandra will automatically repartition as machines are added and removed from the cluster.</p>\n</div>\n<div>\n<p><a href=\"http://wiki.apache.org/cassandra/DataModel\" rel=\"nofollow\">Row store</a> means that like relational databases, Cassandra organizes data by rows and columns.  The Cassandra Query Language (CQL) is a close relative of SQL.</p>\n</div>\n<div>\n<p>For more information, see <a href=\"http://cassandra.apache.org/\" rel=\"nofollow\">the Apache Cassandra web site</a>.</p>\n</div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-requirements\"><a href=\"#requirements\" aria-hidden=\"true\" class=\"anchor\"></a>Requirements</h2>\n<div>\n<div>\n<ol><li>\n<p>Java &gt;= 1.8 (OpenJDK and Oracle JVMS have been tested)</p>\n</li>\n<li>\n<p>Python 2.7 (for cqlsh)</p>\n</li>\n</ol></div>\n</div>\n</div>\n<div>\n<h2 id=\"user-content-getting-started\"><a href=\"#getting-started\" aria-hidden=\"true\" class=\"anchor\"></a>Getting started</h2>\n<div>\n<div>\n<p>This short guide will walk you through getting a basic one node cluster up\nand running, and demonstrate some simple reads and writes.</p>\n</div>\n<div>\n<p>First, we’ll unpack our archive:</p>\n</div>\n<div>\n<div>\n<pre>$ tar -zxvf apache-cassandra-$VERSION.tar.gz\n$ cd apache-cassandra-$VERSION</pre>\n</div>\n</div>\n<div>\n<p>After that we start the server.  Running the startup script with the -f argument will cause\nCassandra to remain in the foreground and log to standard out; it can be stopped with ctrl-C.</p>\n</div>\n<div>\n<div>\n<pre>$ bin/cassandra -f</pre>\n</div>\n</div>\n<div>\n<div>\n<div>\n<p>Note for Windows users: to install Cassandra as a service, download\n<a href=\"http://commons.apache.org/daemon/procrun.html\" rel=\"nofollow\">Procrun</a>, set the\nPRUNSRV environment variable to the full path of prunsrv (e.g.,\nC:\\procrun\\prunsrv.exe), and run \"bin\\cassandra.bat install\".\nSimilarly, \"uninstall\" will remove the service.</p>\n</div>\n</div>\n</div>\n<div>\n<p>Now let’s try to read and write some data using the Cassandra Query Language:</p>\n</div>\n<div>\n<div>\n<pre>$ bin/cqlsh</pre>\n</div>\n</div>\n<div>\n<p>The command line client is interactive so if everything worked you should\nbe sitting in front of a prompt:</p>\n</div>\n<div>\n<div>\n<pre>Connected to Test Cluster at localhost:9160.\n[cqlsh 2.2.0 | Cassandra 1.2.0 | CQL spec 3.0.0 | Thrift protocol 19.35.0]\nUse HELP for help.\ncqlsh&gt;</pre>\n</div>\n</div>\n<div>\n<p>As the banner says, you can use 'help;' or '?' to see what CQL has to\noffer, and 'quit;' or 'exit;' when you’ve had enough fun. But lets try\nsomething slightly more interesting:</p>\n</div>\n<div>\n<div>\n<pre>cqlsh&gt; CREATE SCHEMA schema1\n       WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\ncqlsh&gt; USE schema1;\ncqlsh:Schema1&gt; CREATE TABLE users (\n                 user_id varchar PRIMARY KEY,\n                 first varchar,\n                 last varchar,\n                 age int\n               );\ncqlsh:Schema1&gt; INSERT INTO users (user_id, first, last, age)\n               VALUES ('jsmith', 'John', 'Smith', 42);\ncqlsh:Schema1&gt; SELECT * FROM users;\n user_id | age | first | last\n---------+-----+-------+-------\n  jsmith |  42 |  john | smith\n cqlsh:Schema1&gt;</pre>\n</div>\n</div>\n<div>\n<p>If your session looks similar to what’s above, congrats, your single node\ncluster is operational!</p>\n</div>\n<div>\n<p>For more on what commands are supported by CQL, see\n<a href=\"https://github.com/apache/cassandra/blob/trunk/doc/cql3/CQL.textile\">the CQL reference</a>.  A\nreasonable way to think of it is as, \"SQL minus joins and subqueries, plus collections.\"</p>\n</div>\n<div>\n<p>Wondering where to go from here?</p>\n</div>\n<div>\n<ul><li>\n<p>Getting started: <a href=\"http://wiki.apache.org/cassandra/GettingStarted\" rel=\"nofollow\">http://wiki.apache.org/cassandra/GettingStarted</a></p>\n</li>\n<li>\n<p>Join us in #cassandra on irc.freenode.net and ask questions</p>\n</li>\n<li>\n<p>Subscribe to the Users mailing list by sending a mail to\n<a href=\"mailto:user-subscribe@cassandra.apache.org\">user-subscribe@cassandra.apache.org</a></p>\n</li>\n<li>\n<p>Planet Cassandra aggregates Cassandra articles and news:\n<a href=\"http://planetcassandra.org/\" rel=\"nofollow\">http://planetcassandra.org/</a></p>\n</li>\n</ul></div>\n</div>\n</div></article>",
        "created_at": "2018-03-06T14:57:59+0000",
        "updated_at": "2018-03-10T19:34:49+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 2,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/549085?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9383"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          },
          {
            "id": 623,
            "label": "aws",
            "slug": "aws"
          }
        ],
        "is_public": false,
        "id": 9382,
        "uid": null,
        "title": "Instagram/cassandra-aws-benchmark",
        "url": "https://github.com/Instagram/cassandra-aws-benchmark",
        "content": "<h3>\n      \n      README.md\n    </h3><article class=\"markdown-body entry-content\" itemprop=\"text\">\n<p>Scripts and templates for cassandra benchmark environment provision</p>\n<h2><a href=\"#requirements\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-requirements\"></a>Requirements</h2>\n<ul><li>aws cli: <code>pip install awscli --upgrade --user</code> and <code>aws configure</code></li>\n</ul><h2><a href=\"#create-benchmark-environment\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-create-benchmark-environment\"></a>Create benchmark environment</h2>\n<p>create cloudformation stack:</p>\n<div class=\"highlight highlight-source-shell\"><pre>aws cloudformation create-stack --stack-name MY_STACK_NAME \\\n    --disable-rollback \\\n    --template-body file://cloudformation.yaml \\\n    --parameters \\\n    ParameterKey=CassandraAMI,ParameterValue=MY_CASSANDRA_AMI_ID \\\n    ParameterKey=KeyName,ParameterValue=MY_SSH_KEY_NAME \\\n    ParameterKey=VPCSubnetId,ParameterValue=MY_VPC_SUBNET_ID \\\n    ParameterKey=InstanceProfile,ParameterValue=NAME_OF_INSTANCE_PROFILE_WITH_EC2_AND_AUOSCALEGROUP_READONLY_ACCESS \\\n    --capabilities CAPABILITY_IAM</pre></div>\n<p>update cloudformation stack:</p>\n<div class=\"highlight highlight-source-shell\"><pre>aws cloudformation udpate-stack --stack-name MY_STACK_NAME \\\n    --template-body file://cloudformation.yaml \\\n    --parameters \\\n    ParameterKey=CassandraAMI,ParameterValue=MY_CASSANDRA_AMI_ID \\\n    ParameterKey=KeyName,ParameterValue=MY_SSH_KEY_NAME \\\n    ParameterKey=VPCSubnetId,ParameterValue=MY_VPC_SUBNET_ID \\\n    ParameterKey=InstanceProfile,ParameterValue=NAME_OF_INSTANCE_PROFILE_WITH_EC2_AND_AUOSCALEGROUP_READONLY_ACCESS \\\n    --capabilities CAPABILITY_IAM</pre></div>\n<h2><a href=\"#lastest-prebuilt-cassandra-ami\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-lastest-prebuilt-cassandra-ami\"></a>Lastest Prebuilt Cassandra AMI</h2>\n<ul><li>cassandra 3.0.15: ami-09cc7371 (us-west-2)</li>\n<li>rocksandra: ami-b770cdcf (us-west-2)</li>\n</ul><h2><a href=\"#build-cassandra-ami-myself\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-build-cassandra-ami-myself\"></a>Build Cassandra AMI Myself</h2>\n<p>prerequists:  packer: <code>brew install packer</code></p>\n<p>cassandra3x</p>\n<pre>$&gt; cd ami/cassandra3x\n$&gt; wget -O resources/cassandra.rpm https://www.apache.org/dist/cassandra/redhat/30x/cassandra-3.0.15-1.noarch.rpm \n$&gt; packer build -var \"image_version=$(date +%s)\" packer.json\n</pre>\n<p>ndbench</p>\n<pre>$&gt; cd ami/bencher\n$&gt; packer build -var \"image_version=$(date +%s)\" packer.json\n</pre>\n<h2><a href=\"#license\" aria-hidden=\"true\" class=\"anchor\" id=\"user-content-license\"></a>License</h2>\n<p>Cassandra AWS Benchmark is Apache 2.0 licensed, as found in the LICENSE file.</p>\n</article>",
        "created_at": "2018-03-06T14:57:49+0000",
        "updated_at": "2018-03-10T19:35:01+0000",
        "published_at": null,
        "published_by": null,
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": "en",
        "reading_time": 1,
        "domain_name": "github.com",
        "preview_picture": "https://avatars1.githubusercontent.com/u/549085?s=400&v=4",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9382"
          }
        }
      },
      {
        "is_archived": 1,
        "is_starred": 0,
        "user_name": "admin",
        "user_email": "rahul.singh@anant.us",
        "user_id": 1,
        "tags": [
          {
            "id": 89,
            "label": "cassandra",
            "slug": "cassandra"
          }
        ],
        "is_public": false,
        "id": 9381,
        "uid": null,
        "title": "Open-sourcing a 10x reduction in Apache Cassandra tail latency",
        "url": "https://engineering.instagram.com/open-sourcing-a-10x-reduction-in-apache-cassandra-tail-latency-d64f86b43589?gi=714b1145b6c3",
        "content": "<p id=\"1d80\" class=\"graf graf--p graf-after--h3\">At Instagram, we have one of the world’s largest deployments of the Apache Cassandra database. We began using Cassandra in 2012 to replace Redis and support product use cases like fraud detection, Feed, and the Direct inbox. At first we ran Cassandra clusters in an AWS environment, but migrated them over to Facebook’s infrastructure when the rest of Instagram moved. We’ve had a really good experience with the reliability and availability of Cassandra, but saw room for improvement in read latency.<br /> <br />Last year Instagram’s Cassandra team started working on a project to reduce Cassandra’s read latency significantly, which we call Rocksandra. In this post, I will describe the motivation for this project, the challenges we overcame, and performance metrics in both internal and public cloud environments.</p><h3 id=\"d01d\" class=\"graf graf--h3 graf-after--p\">Motivation</h3><p id=\"c5aa\" class=\"graf graf--p graf-after--h3\">At Instagram, we use Apache Cassandra heavily as a general key value storage service. The majority of Instagram’s Cassandra requests are online, so in order to provide a reliable and responsive user experience for hundreds of millions of Instagram users, we have very tight SLA on the metrics. <br /> <br />Instagram maintains a 5–9s reliability SLA, which means at any given time, the request failure rate should be less than 0.001%. For performance, we actively monitor the throughput and latency of different Cassandra clusters, especially the P99 read latency. <br /> <br /> Here’s a graph that shows the client-side latency of one production Cassandra cluster. The blue line is the average read latency (5ms) and the orange line is the P99 read latency (in the range of 25ms to 60ms and changing a lot based on client traffic).</p><figure id=\"73e1\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*Scn1Nm33oukOJpUd4Ukszw.png\" data-width=\"1052\" data-height=\"668\" data-action=\"zoom\" data-action-value=\"1*Scn1Nm33oukOJpUd4Ukszw.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*Scn1Nm33oukOJpUd4Ukszw.png\" alt=\"image\" /></div></figure><figure id=\"0b14\" class=\"graf graf--figure graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*ItBORNwCXce82ZNX6qf6Vg.png\" data-width=\"1052\" data-height=\"668\" data-action=\"zoom\" data-action-value=\"1*ItBORNwCXce82ZNX6qf6Vg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*ItBORNwCXce82ZNX6qf6Vg.png\" alt=\"image\" /></div></figure><p id=\"0e33\" class=\"graf graf--p graf-after--figure\">After investigation, we found the JVM garbage collector (GC) contributed a lot to the latency spikes. We defined a metric called GC stall percentage to measure the percentage of time a Cassandra server was doing stop-the-world GC (Young Gen GC) and could not serve client requests. Here’s another graph that shows the GC stall percentage on our production Cassandra servers. It was 1.25% during the lowest traffic time windows, and could be as high as 2.5% during peak hours.</p><p id=\"14d6\" class=\"graf graf--p graf-after--p\">The graph shows that a Cassandra server instance could spend 2.5% of runtime on garbage collections instead of serving client requests. The GC overhead obviously had a big impact on our P99 latency, so if we could lower the GC stall percentage, we would be able to reduce our P99 latency significantly.</p><h3 id=\"87af\" class=\"graf graf--h3 graf-after--p\">Solution</h3><p id=\"5830\" class=\"graf graf--p graf-after--h3\">Apache Cassandra is a distributed database with it’s own LSM tree-based storage engine written in Java. We found that the components in the storage engine, like memtable, compaction, read/write path, etc., created a lot of objects in the Java heap and generated a lot of overhead to JVM. To reduce the GC impact from the storage engine, we considered different approaches and ultimately decided to develop a C++ storage engine to replace existing ones. <br /> <br />We did not want to build a new storage engine from scratch, so we decided to build the new storage engine on top of RocksDB. <br /> <br />RocksDB is an open source, high-performance embedded database for key-value data. It’s written in C++, and provides official API language bindings for C++, C, and Java. RocksDB is optimized for performance, especially on fast storage like SSD. It’s widely used in the industry as the storage engine for MySQL, mongoDB, and other popular databases.</p><h3 id=\"cb1c\" class=\"graf graf--h3 graf-after--p\">Challenges</h3><p id=\"4c10\" class=\"graf graf--p graf-after--h3\">We overcame three main challenges when implementing the new storage engine on RocksDB.<br /> <br />The first challenge was that Cassandra does not have a pluggable storage engine architecture yet, which means the existing storage engine is coupled together with other components in the database. To find a balance between massive refactoring and quick iterations, we defined a new storage engine API, including the most common read/write and streaming interfaces. This way we could implement the new storage engine behind the API and inject it into the related code paths inside Cassandra.<br /> <br />Secondly, Cassandra supports rich data types and table schema, while RocksDB provides purely key-value interfaces. We carefully defined the encoding/decoding algorithms to support Cassandra’s data model within RocksDB’s data structure and supported same-query semantics as original Cassandra. <br /> <br />The third challenge was about streaming. Streaming is an important component for a distributed database like Cassandra. Whenever we join or remove a node from a Cassandra cluster, Cassandra needs to stream data among different nodes to balance the load across the cluster. The existing streaming implementation was based on the details in the current storage engine. Accordingly, we had to decouple them from each other, make an abstraction layer, and re-implement the streaming using RocksDB APIs. For high streaming throughput, we now stream data into temp sst files first, and then use the RocksDB ingest file API to bulk load them into the RocksDB instance at once.</p><h3 id=\"a1f4\" class=\"graf graf--h3 graf-after--p\">Performance metrics</h3><p id=\"7a2f\" class=\"graf graf--p graf-after--h3\">After about a year of development and testing, we have finished a first version of the implementation and successfully rolled it into several production Cassandra clusters in Instagram. In one of our production clusters, the P99 read latency dropped from 60ms to 20ms. We also observed that the GC stalls on that cluster dropped from 2.5% to 0.3%, which was a 10X reduction!<br /> <br />We also wanted to verify whether Rocksandra would perform well in a public cloud environment. We setup a Cassandra cluster in an AWS environment using three i3.8 xlarge EC2 instances, each with 32 cores CPU, 244GB memory, and raid0 with 4 nvme flash disks. <br /> <br />We used <a href=\"https://github.com/Netflix/ndbench\" data-href=\"https://github.com/Netflix/ndbench\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">NDBench</a> for the benchmark, and the default table schema in the framework:</p><blockquote id=\"7425\" class=\"graf graf--blockquote graf-after--p\"><div><code class=\"markup--code markup--blockquote-code\">TABLE emp (</code><br /> <code class=\"markup--code markup--blockquote-code\">emp_uname text PRIMARY KEY,<br />emp_dept text,<br />emp_first text,<br />emp_last text</code><br /> <code class=\"markup--code markup--blockquote-code\">)</code></div></blockquote><p id=\"e302\" class=\"graf graf--p graf-after--blockquote\">We pre-loaded 250M 6KB rows into the database (each server stores about 500GB data on disk). We configured 128 readers and 128 writers in NDBench.<br /> <br />We tested different workloads and measured the avg/P99/P999 read/write latencies. As you can see, Rocksandra provided much lower and consistent tail read/write latency.</p><figure id=\"f950\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*Mpvc-jd61xmcrE4aEth4NA.png\" data-width=\"1132\" data-height=\"725\" data-action=\"zoom\" data-action-value=\"1*Mpvc-jd61xmcrE4aEth4NA.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*Mpvc-jd61xmcrE4aEth4NA.png\" alt=\"image\" /></div></figure><figure id=\"8947\" class=\"graf graf--figure graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*zZO7xeU8fsWosWbkev873g.png\" data-width=\"1131\" data-height=\"724\" data-action=\"zoom\" data-action-value=\"1*zZO7xeU8fsWosWbkev873g.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*zZO7xeU8fsWosWbkev873g.png\" alt=\"image\" /></div></figure><p id=\"7f48\" class=\"graf graf--p graf-after--figure\">We also tested a read-only workload and observed that, at similar P99 read latency (2ms), Rocksandra could provide 10X higher read throughput (300K/s for Rocksandra vs. 30K/s for C* 3.0).</p><figure id=\"7652\" class=\"graf graf--figure graf-after--p\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*E-2efj-mMo0dQWEvZyxn1g.png\" data-width=\"1483\" data-height=\"746\" data-action=\"zoom\" data-action-value=\"1*E-2efj-mMo0dQWEvZyxn1g.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*E-2efj-mMo0dQWEvZyxn1g.png\" alt=\"image\" /></div></figure><figure id=\"b56d\" class=\"graf graf--figure graf-after--figure\"><div class=\"aspectRatioPlaceholder is-locked\"><img class=\"graf-image\" data-image-id=\"1*d5gs5SJzq6laocevBqA1Bg.png\" data-width=\"1359\" data-height=\"731\" data-action=\"zoom\" data-action-value=\"1*d5gs5SJzq6laocevBqA1Bg.png\" src=\"https://cdn-images-1.medium.com/max/1600/1*d5gs5SJzq6laocevBqA1Bg.png\" alt=\"image\" /></div></figure><h3 id=\"d343\" class=\"graf graf--h3 graf-after--figure\">Future work</h3><p id=\"5bac\" class=\"graf graf--p graf-after--h3\">We have open sourced our <a href=\"https://github.com/Instagram/cassandra/tree/rocks_3.0\" data-href=\"https://github.com/Instagram/cassandra/tree/rocks_3.0\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">Rocksandra code base</a> and <a href=\"https://github.com/Instagram/cassandra-aws-benchmark\" data-href=\"https://github.com/Instagram/cassandra-aws-benchmark\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">benchmark framework</a>, which you can download from Github to try out in your own environment! Please let us know how it performs.<br /> <br />As our next step, we are actively working on the development of more C* features support, like secondary indexes, repair, etc. We are also working on a <a href=\"https://issues.apache.org/jira/browse/CASSANDRA-13474\" data-href=\"https://issues.apache.org/jira/browse/CASSANDRA-13474\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">C* pluggable storage engine architecture</a> to contribute our work back to the Apache Cassandra community. <br /> <br />If you are in the Bay Area and are interested in learning more about our Cassandra developments, join us at our next meetup event <a href=\"https://www.meetup.com/Apache-Cassandra-Bay-Area/events/248376266/\" data-href=\"https://www.meetup.com/Apache-Cassandra-Bay-Area/events/248376266/\" class=\"markup--anchor markup--p-anchor\" rel=\"noopener\" target=\"_blank\">here</a>.</p><p id=\"3171\" class=\"graf graf--p graf-after--p graf--trailing\"><em class=\"markup--em markup--p-em\">Dikang Gu is an infrastructure engineer at Instagram.</em></p>",
        "created_at": "2018-03-06T14:56:34+0000",
        "updated_at": "2018-03-10T19:35:18+0000",
        "published_at": "2018-03-05T18:03:23+0000",
        "published_by": [
          "Instagram Engineering"
        ],
        "starred_at": null,
        "annotations": [],
        "mimetype": "text/html",
        "language": null,
        "reading_time": 5,
        "domain_name": "engineering.instagram.com",
        "preview_picture": "https://cdn-images-1.medium.com/max/1200/1*Scn1Nm33oukOJpUd4Ukszw.png",
        "http_status": "200",
        "headers": null,
        "origin_url": null,
        "_links": {
          "self": {
            "href": "/api/entries/9381"
          }
        }
      }
    ]
  }
}